{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aedf2f3-cb43-4bbb-8675-ad747e470131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "import krippendorff\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b8d33f-19ce-494e-9201-2bbf97d4ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/merged_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8837ad-0f09-40cd-b0cd-f7af682d361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9bd0a6b-94fd-4bb6-b2c3-d593544e1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['inclusion', 'phenomenon_short', 'phenomenon_defined', 'definition_scope', 'definition_integrity',\n",
    "           'definition_integrity_detail', 'task_face_validity', \n",
    "           'task_train_val', 'response_format', 'metric_access', 'metric_face_validity', \n",
    "           'result_interpretation', 'results_comparison', 'results_comparison_explanation', \n",
    "           'results_human_baseline', 'results_author_validity', 'results_realism', 'authorship', \n",
    "           'benchmark_availability']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b7043-9427-4ef9-952f-74cd4d2b23c9",
   "metadata": {},
   "source": [
    "## Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60f8d7fe-6626-4296-a537-5817076916aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kappa_per_feature(df, sample_id_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Computes Cohen's Kappa for each feature where two annotations are available per sample.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing all annotations.\n",
    "        sample_id_col (str): Column name for the sample ID.\n",
    "        feature_cols (list): List of column names (features) to compute agreement on.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Cohen's Kappa scores per feature.\n",
    "    \"\"\"\n",
    "    kappas = {}\n",
    "\n",
    "    # Group by sample_id to get annotation pairs\n",
    "    grouped = df.groupby(sample_id_col)\n",
    "\n",
    "    # Only keep samples with exactly 2 annotations\n",
    "    valid_groups = {k: g for k, g in grouped if len(g) == 2}\n",
    "\n",
    "    for feature in feature_cols:\n",
    "        ann1 = []\n",
    "        ann2 = []\n",
    "\n",
    "        for g in valid_groups.values():\n",
    "            # Get the 2 values for this feature\n",
    "            values = g[feature].values\n",
    "            if len(values) == 2:\n",
    "                ann1.append(values[0])\n",
    "                ann2.append(values[1])\n",
    "\n",
    "        if ann1 and ann2:\n",
    "            kappa = cohen_kappa_score(ann1, ann2)\n",
    "            kappas[feature] = kappa\n",
    "        else:\n",
    "            kappas[feature] = None  # or np.nan\n",
    "\n",
    "    return pd.Series(kappas, name='cohen_kappa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4fb3be3-823b-4b5d-9a03-e223789e880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion                         0.000000\n",
      "phenomenon_short                  0.163265\n",
      "phenomenon_defined               -0.134163\n",
      "phenomenon_contested              0.000000\n",
      "definition_scope                  0.001282\n",
      "definition_integrity              0.126627\n",
      "definition_integrity_detail       0.210646\n",
      "task_face_validity                0.484277\n",
      "task_train_val                    0.493306\n",
      "response_format                   0.447439\n",
      "metric_access                     0.372449\n",
      "metric_face_validity             -0.051282\n",
      "result_interpretation            -0.037975\n",
      "results_comparison                0.196078\n",
      "results_comparison_explanation    0.189386\n",
      "results_human_baseline            0.583991\n",
      "results_author_validity           0.154639\n",
      "results_realism                   0.095588\n",
      "authorship                        0.515748\n",
      "benchmark_availability            0.000000\n",
      "Name: cohen_kappa, dtype: float64\n",
      "Mean Cohen's Kappa: 0.191\n"
     ]
    }
   ],
   "source": [
    "# Define your ID column and feature columns\n",
    "sample_id_col = 'bibkey'\n",
    "feature_cols = columns\n",
    "\n",
    "# Compute Kappa per feature\n",
    "kappa_scores = compute_kappa_per_feature(data, sample_id_col, feature_cols)\n",
    "\n",
    "# Display the results\n",
    "print(kappa_scores)\n",
    "\n",
    "# Optional: compute mean kappa across all features\n",
    "mean_kappa = kappa_scores.mean()\n",
    "print(f\"Mean Cohen's Kappa: {mean_kappa:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c33ac-eb0c-436f-8051-7665684b2e56",
   "metadata": {},
   "source": [
    "## Krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12e6318a-a7c4-45c3-bdd8-1ee2cde7c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_krippendorffs_alpha_encoded(df, sample_id_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Computes Krippendorff's Alpha (nominal) per feature after encoding string categories.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Annotations dataframe with sample IDs and categorical features.\n",
    "        sample_id_col (str): Column name indicating sample ID.\n",
    "        feature_cols (list): Feature names (categorical).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Krippendorff's alpha values per feature.\n",
    "    \"\"\"\n",
    "    alpha_scores = {}\n",
    "\n",
    "    grouped = df.groupby(sample_id_col)\n",
    "\n",
    "    for feature in feature_cols:\n",
    "        # Encode string categories to integers\n",
    "        le = LabelEncoder()\n",
    "        try:\n",
    "            encoded_feature = le.fit_transform(df[feature].astype(str))\n",
    "        except Exception:\n",
    "            alpha_scores[feature] = np.nan\n",
    "            continue\n",
    "\n",
    "        df_encoded = df.copy()\n",
    "        df_encoded[feature] = encoded_feature\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for _, group in df_encoded.groupby(sample_id_col):\n",
    "            values = group[feature].values\n",
    "            if len(values) == 2:\n",
    "                data.append(values)\n",
    "            elif len(values) == 1:\n",
    "                data.append([values[0], np.nan])\n",
    "            else:\n",
    "                continue  # skip samples with >2 annotations\n",
    "\n",
    "        if not data:\n",
    "            alpha_scores[feature] = np.nan\n",
    "            continue\n",
    "\n",
    "        matrix = np.array(data).T  # rows = annotators, columns = items\n",
    "        try:\n",
    "            alpha = krippendorff.alpha(reliability_data=matrix, level_of_measurement='nominal')\n",
    "            alpha_scores[feature] = alpha\n",
    "        except Exception:\n",
    "            alpha_scores[feature] = np.nan\n",
    "\n",
    "    return pd.Series(alpha_scores, name='krippendorffs_alpha')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "617e0a84-51ba-491b-8c6b-34047513a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion                         0.000000\n",
      "phenomenon_short                  0.162409\n",
      "phenomenon_defined               -0.144876\n",
      "phenomenon_contested             -0.295242\n",
      "definition_scope                 -0.002606\n",
      "definition_integrity              0.132143\n",
      "definition_integrity_detail       0.217169\n",
      "task_face_validity                0.488959\n",
      "task_train_val                    0.496112\n",
      "response_format                   0.449541\n",
      "metric_access                     0.375321\n",
      "metric_face_validity             -0.040685\n",
      "result_interpretation            -0.028571\n",
      "results_comparison                0.185689\n",
      "results_comparison_explanation    0.183554\n",
      "results_human_baseline            0.587436\n",
      "results_author_validity           0.118607\n",
      "results_realism                   0.063584\n",
      "authorship                        0.521182\n",
      "benchmark_availability           -0.016736\n",
      "Name: krippendorffs_alpha, dtype: float64\n",
      "Mean Krippendorff’s Alpha: 0.173\n"
     ]
    }
   ],
   "source": [
    "sample_id_col = 'bibkey'\n",
    "feature_cols = columns\n",
    "\n",
    "alpha_scores = compute_krippendorffs_alpha_encoded(data, sample_id_col, feature_cols)\n",
    "\n",
    "print(alpha_scores)\n",
    "print(f\"Mean Krippendorff’s Alpha: {alpha_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1659e-bf09-4ae3-bc96-eb9432fe0787",
   "metadata": {},
   "source": [
    "## Gwet’s AC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b58902a-63b7-447d-b0df-791bbbb0e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gwets_ac1(df, sample_id_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Computes Gwet's AC1 per feature assuming 2 annotations per sample (nominal data).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with categorical annotations and sample IDs.\n",
    "        sample_id_col (str): Column with sample ID.\n",
    "        feature_cols (list): Categorical feature columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Gwet's AC1 per feature.\n",
    "    \"\"\"\n",
    "    ac1_scores = {}\n",
    "\n",
    "    grouped = df.groupby(sample_id_col)\n",
    "\n",
    "    for feature in feature_cols:\n",
    "        # Extract annotation pairs\n",
    "        pairs = []\n",
    "        for _, group in grouped:\n",
    "            values = group[feature].dropna().astype(str).values\n",
    "            if len(values) == 2:\n",
    "                pairs.append(values)\n",
    "\n",
    "        if not pairs:\n",
    "            ac1_scores[feature] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Flatten and encode to integers\n",
    "        all_vals = np.array(pairs).flatten()\n",
    "        le = LabelEncoder()\n",
    "        all_vals_encoded = le.fit_transform(all_vals)\n",
    "\n",
    "        # Re-encode pairs with same mapping\n",
    "        encoded_pairs = all_vals_encoded.reshape(len(pairs), 2)\n",
    "\n",
    "        # Agreement proportion\n",
    "        agree = np.sum(encoded_pairs[:, 0] == encoded_pairs[:, 1])\n",
    "        n = len(encoded_pairs)\n",
    "        p_o = agree / n\n",
    "\n",
    "        # Category proportions\n",
    "        num_categories = len(le.classes_)\n",
    "        cat_counts = np.zeros(num_categories)\n",
    "        for i in range(num_categories):\n",
    "            cat_counts[i] = np.sum(encoded_pairs == i)\n",
    "\n",
    "        cat_probs = cat_counts / (2 * n)\n",
    "\n",
    "        # Expected agreement (Pe) under Gwet’s formulation\n",
    "        p_e = np.sum(cat_probs * cat_probs)\n",
    "\n",
    "        # AC1 formula\n",
    "        if p_e != 1:\n",
    "            ac1 = (p_o - p_e) / (1 - p_e)\n",
    "        else:\n",
    "            ac1 = 1.0\n",
    "\n",
    "        ac1_scores[feature] = ac1\n",
    "\n",
    "    return pd.Series(ac1_scores, name='gwets_ac1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "870b95b0-ea58-462d-b963-b1d53ee1051b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion                        -0.012346\n",
      "phenomenon_short                  0.151756\n",
      "phenomenon_defined               -0.159420\n",
      "phenomenon_contested             -0.319588\n",
      "definition_scope                 -0.000755\n",
      "definition_integrity              0.132100\n",
      "definition_integrity_detail       0.240753\n",
      "task_face_validity                0.656652\n",
      "task_train_val                    0.534606\n",
      "response_format                   0.456949\n",
      "metric_access                     0.473684\n",
      "metric_face_validity             -0.043478\n",
      "result_interpretation            -0.038961\n",
      "results_comparison                0.181586\n",
      "results_comparison_explanation    0.191956\n",
      "results_human_baseline            0.680672\n",
      "results_author_validity           0.120301\n",
      "results_realism                   0.063361\n",
      "authorship                        0.539026\n",
      "benchmark_availability           -0.025641\n",
      "Name: gwets_ac1, dtype: float64\n",
      "Mean Gwet’s AC1: 0.191\n"
     ]
    }
   ],
   "source": [
    "ac1_scores = compute_gwets_ac1(data, sample_id_col, feature_cols)\n",
    "\n",
    "print(ac1_scores)\n",
    "print(f\"Mean Gwet’s AC1: {ac1_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d379a52-f4aa-42a1-abc0-cbb8453d9801",
   "metadata": {},
   "source": [
    "## Percent Agreement per Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a32e97f-6915-4710-b1c0-a3fca631758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percent_agreement(df, sample_id_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Computes raw agreement (percent of exact matches) per feature,\n",
    "    assuming 2 annotations per sample and no explicit annotator IDs.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Annotations DataFrame with sample IDs and features.\n",
    "        sample_id_col (str): Column name identifying the sample.\n",
    "        feature_cols (list): List of categorical feature column names.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Percent agreement (0–1) per feature.\n",
    "    \"\"\"\n",
    "    agreement_scores = {}\n",
    "\n",
    "    grouped = df.groupby(sample_id_col)\n",
    "\n",
    "    for feature in feature_cols:\n",
    "        matches = []\n",
    "        for _, group in grouped:\n",
    "            values = group[feature].dropna().values\n",
    "            if len(values) == 2:\n",
    "                matches.append(values[0] == values[1])\n",
    "        if matches:\n",
    "            agreement_scores[feature] = np.mean(matches)\n",
    "        else:\n",
    "            agreement_scores[feature] = np.nan  # no valid pairs\n",
    "\n",
    "    return pd.Series(agreement_scores, name='percent_agreement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f71be6c-6a2b-48a8-99e3-02804e4784cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion                         0.975610\n",
      "phenomenon_short                  0.600000\n",
      "phenomenon_defined                0.525000\n",
      "phenomenon_contested              0.000000\n",
      "definition_scope                  0.564103\n",
      "definition_integrity              0.575000\n",
      "definition_integrity_detail       0.615385\n",
      "task_face_validity                0.975000\n",
      "task_train_val                    0.743590\n",
      "response_format                   0.525000\n",
      "metric_access                     0.950000\n",
      "metric_face_validity              0.897436\n",
      "result_interpretation             0.925000\n",
      "results_comparison                0.600000\n",
      "results_comparison_explanation    0.564103\n",
      "results_human_baseline            0.842105\n",
      "results_author_validity           0.538462\n",
      "results_realism                   0.575000\n",
      "authorship                        0.725000\n",
      "benchmark_availability            0.950000\n",
      "Name: percent_agreement, dtype: float64\n",
      "Mean Percent Agreement: 0.683\n"
     ]
    }
   ],
   "source": [
    "percent_agreement = compute_percent_agreement(data, sample_id_col, feature_cols)\n",
    "\n",
    "print(percent_agreement)\n",
    "print(f\"Mean Percent Agreement: {percent_agreement.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ade24-e294-4f05-8203-42915905d39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
