,main_coder,bibkey,title,inclusion,short_summary,phenomenon_taxonomy_root,phenomenon_taxonomy_leaf,phenomenon_taxonomy_alternate,task_source,task_source_clean,dataset_sampling_method,dataset_sampling_method_clean,response_format,response_format_clean,metric_definition,metric_definition_clean,metric_statistics,metric_statistics_clean
0,Harry Mayne,mundlerSWTBenchTestingValidating2024,SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents,Include,A benchmark for generating code tests (unit tests) from natural language user GitHub issues.,Agents,Coding,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],Whether the faulty code fails on the test and the gold-standard code passes it.,['Reward'],simple mean,['Mean']
1,Jonathan Rystrøm,davidsonEvaluatingLanguageModel2024,"EVALUATING LANGUAGE MODEL AGENCY THROUGH
NEGOTIATIONS",Include,The paper introduces a dynamic framework for evaluating LLMs using negotiation games in self-play and cross-play settings. They find that only closed-source models are able to successfully complete the task and that stronger LLMs don't always win over weaker opponents.,Alignment,Alignment,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Extended interaction (e.g. conversation, calling an API and processing the response)",['Interaction'],"Exact Match (accuracy, F1, precision, recall), Number of rounds completted","['Exact match', 'Reward']",mean with variance,"['Mean', 'Std']"
2,Lennart Luettgau,helweMAFALDABenchmarkComprehensive2024,MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification,Include,"The paper introduces MAFALD, a benchmark that provides a unified classification of fallacies and provides a taxonomy. It features manually annotated data with explanations, a tailored annotation scheme, and an evaluation method for subjective NLP tasks. Various language models and human performance are evaluated on fallacy detection and classification in a zero-shot learning setting.",Reasoning,Logical,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Crowd-sourced', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean/sum,['Mean']
3,Kaili Liu,niuRAGTruthHallucinationCorpus2024,RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,Include,"This paper targets word-level hallucinations in various tasks and domains in the RAG setting. It presents approximately 18,000 responses generated using RAG from diverse LLMs which are annotated at the word level for hallucination intensity. Hallucination frequencies are benchmarked across various LLMs, and hallucination detection methods are assessed versus a small LLM fine-tuned using the proposed dataset, RAGTruth.",Retrieval,,Factuality,"Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Crowd-sourced', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']","Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
4,Anna Gausen,wangIELMOpenInformation2022,"IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models
",Include,"They introduce a new open information extraction (OIE) benchmark designed to evaluate the relational knowledge stored in pre-trained language models (LMs) such as BERT and GPT (published in 2022). Their method involves transforming these pre-trained LMs into zero-shot OIE systems to assess their performance on both existing and novel factual OIE datasets. Their results show that pre-trained LMs achieve competitive performance, even surpassing state-of-the-art supervised OIE methods on certain datasets without any additional training data.",NLP,Extraction,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Based on knowledge graphs (KG) e.g. Wikidata","['Crowd-sourced', 'Procedurally-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"The authors carry out some error analysis: ""We argue that we are measuring a lower bound for what LMs know. To further understand the shortcomings of the current method, we conduct an error analysis of the errors in precision on all datasets. We choose BERTLARGE for the study. We sample 100 documents from the Wikidata-OIE dataset, and manually check the reasons for the errors""",['Other']
5,Jan Batzner,heTGEAErrorAnnotatedDataset2021,TGEA: An Error-Annotated Dataset and Benchmark Tasks for Text Generation from Pretrained Language Models,Include,"TGEA (Text Generation Error Annotation) is an error-annotated dataset with multiple benchmark tasks for text generation. Following the authors hierachical error taxonomy, crowdsourced workers manually labeled 12k erroneous sentences with semantic information, including error types, associated text spans, error corrections and rationals behind errors.",Factuality,,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Distribution (perplexity, calibration, correlation)","['Exact match', 'Soft match', 'Distribution']",Simple means for performance metrics; agreement percentages and Cohen's Kappa for annotation reliability.,"['Mean', 'Other']"
6,Lujain Ibrahim,huangCEvalMultiLevelMultiDiscipline2023,C-EVAL: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models,Include,"The paper introduces C-EVAL evaluation suite for assessing advanced knowledge and reasoning abilities of foundation models in Chinese, It spans four difficulty levels and 52 disciplines. It also introduces C-EVAL HARD a subset of challenging subjects that require advanced reasoning.",Reasoning,"Reasoning, Knowledge",Cultural Knowledge,Human exam questions (e.g. GRE questions),['Human exams'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
7,Anna Sotnikova,myungBLEnDBenchmarkLLMs2024,BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages,Include,"The paper introduces BLEND, a novel benchmark comprising hand-crafted question-answer pairs designed to evaluate LLMs on everyday cultural knowledge across 16 countries/regions and 13 languages, including low-resource ones. It demonstrates significant performance disparities among models, showing cultural and linguistic biases, especially in underrepresented regions.",Knowledge,Cultural,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Crowd-sourced', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']","simple mean, Anova for p-values, Tukey-HSD","['Mean', 'Tests']"
8,Karolina Korgul,yaoWebShopScalableRealWorld2022,"WebShop: Towards Scalable Real-World Web
Interaction with Grounded Language Agents",Include,"The paper introduces WebShop, a simulated online shopping environment where agents try to follow natural language instructions to find and buy the right products. WebShop benchmark is designed to test how well agents can search, navigate, and make decisions on the web. The authors train models using imitation and reinforcement learning, and show that the best ones can even handle similar tasks on real sites like Amazon and eBay.",Agents,Web,,"Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Real task', 'Crowd-sourced']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Free response (e.g. summary paragarph), Extended interaction (e.g. conversation, calling an API and processing the response)","['Multiple choice', 'Free response', 'Interaction']","reward is computed based on the final product chosen by the agent, compared against known attributes, options, and price of the target product.",['Reward'],"The authors report average task score and success rate across trials. They also include standard deviation/error bars in some result plots (e.g. Figure 4), mainly to show the variation across multiple runs.","['Mean', 'Std']"
9,Cornelius Emde,sanyalRobustLRDiagnosticBenchmark2022,"ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of
Deductive Reasoners",Include,"Deductive reasoning is an important skill that modern language models should possess. However, small logical perturbations of deductive reasoning problems can lead to inconsistent model responses. To test this consistency, the paper introduces RobustLR a benchmark consisting of logical problems (""theories"") and variations thereof that should be consistenly answered correctly by models.",Reasoning,Logical,Robustness,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],"Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)","['Random', 'Convenience']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],mean of weighted-F1 scores,['Mean']
10,Hannah Kirk,albalakFETABenchmarkFewSample2022,FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue,Include,"Examines few-sample task transfer across 17 subtasks (e.g., utterance-level classification, dialogue-level classification, span extraction, multiple-choice) in open-domain dialogue with diverse properties (dyadic vs. multi-party, anonymized vs. recurring speaker, varying dialogue lengths).",Language Modelling,In-context Learning,,"Modified from another benchmark (e.g. translation into another language), Human TV show; Human chitchat dialogues","['Another benchmark', 'Author-crafted']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Depends on the subtask category (Utterance Classification, Dialogue Classification, Multiple Choice, Span Extraction)",['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Mean, and they they show a delta (for change in aggregate sources across all tasks). It is unclear if this is a range or a standard deviation. I think it's a range.",['Mean']
11,Hazel Kim,beanLINGOLYBenchmarkOlympiadLevel2024,LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages,Include,"The paper introduces LINGOLY, a new benchmark built on Linguistics Olympiad puzzles in low-resource and extinct languages to test genuine reasoning capabilities in LLMs. The benchmark is crafted covering diverse reasoning complexity, linguistic subject areas, instruction types, and high/low resources. The paper uncovers error pattenrs between high and low resource settings and show the ongoing challenges in multi-step, out-of-domain reasoning.",Reasoning,Logical,,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Human exams', 'Author-crafted']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],The authors use a weighted mean in calculating an approximate human performance threshold but not for model performance. They take a weighted average of the annual medal thresholds for ‘Advanced’ problems. ,['Mean']
12,Negar Foroutan,nasirGameTraversalBenchmarkEvaluatingPlanning2024,GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps,Include,"The paper investigates the planning capabilities of LLMs by proposing GameTraversalBenchmark (GTB), a benchmark consisting of diverse 2D grid-based game maps. The paper also provide metrics to give insights towards planning abilities in LLMs.",Reasoning,Planning,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],Unknown,['Unknown'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), The paper defines a reward score","['Exact match', 'Reward']",simple mean and STD,"['Mean', 'Std']"
13,Chris Schmitz,feiLawBenchBenchmarkingLegal2024,LawBench: Benchmarking Legal Knowledge of Large Language Models,Include,"LawBench tests 21 models on 20 Chinese legal tasks (500 instances each), which are classified along Bloom's taxonomy into knowledge memorization, understanding, and application. It is the first benchmark for the Chinese legal domain, and the first for civil law (vs. common law) jurisdictions.",Law,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'Soft match', 'LLM post-processing']","Simple means and macro-averaging (mean across tasks, which is identical here because each task has same # of instances)",['Mean']
14,Angelika Romanou,yuksekgonulWhenWhyVisionlanguage2023,"When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?",Include,"This paper creates the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. They demonstrate that VLMs can perform well on image-text retrieval over existing datasets without using the composition and order information.",Reasoning,Compositional,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],macro-accuracy,['Mean']
15,Jonathan Rystrøm,xieWhodunitBenchEvaluatingLarge2024,"WhodunitBench: Evaluating Large Multimodal
Agents via Murder Mystery Games",Include,"The paper evaluates LLMs ability to participate in (and answers questions about) murder mystery games. In the arena component (agents play as either detective or murderer in a multi-agent setting), the agents are tested on win rate against the other models. The QA component is split based on capability categories (Perception, Role-Play, Decision-making and Cognition)",Agents,General,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Author-crafted', 'Crowd-sourced']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Extended interaction (e.g. conversation, calling an API and processing the response)","['Multiple choice', 'Interaction']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Win rate","['Exact match', 'LLM-as-a-Judge', 'Reward']",Simple mean (no variance or standard reported),['Mean']
16,Lennart Luettgau,saparinaAMBROSIABenchmarkParsing2024,AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries,Include,"Paper introduces a new benchmark dataset designed to evaluate text-to-SQL parsers' ability to handle ambiguous user requests. The dataset includes questions demonstrating scope ambiguity, attachment ambiguity, and vagueness, along with their interpretations and corresponding SQL queries. The authors highlight that existing large language models (LLMs) struggle with these ambiguities, suggesting a need for improved parser development.",Code Generation,Natural Language,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Human ratings']",mean and variance,"['Mean', 'Std']"
17,Anna Sotnikova,augustyniakThisWayDesigning2022,"This is the way: designing and compiling
LEPISZCZE, a comprehensive NLP benchmark for
Polish",Include,"Authors introduce LEPISZCZE, a new, comprehensive benchmark for
Polish NLP with a large variety of tasks and high-quality operationalization of the
benchmark.  LEPISZCZE was designed with flexibility in mind. Including new models,
datasets, and tasks is as simple as possible while still offering data versioning and
model tracking. In the first run of the benchmark, 13 experiments (task
and dataset pairs) were tested based on the five most recent LMs for Polish. Five
datasets from the Polish benchmark are reused and eight novel datasets are added. ",Multilinguality,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Author-crafted', 'Crowd-sourced', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],mean and standard deviation,"['Mean', 'Std']"
18,Ryan Kearns,huiUDABenchmarkSuite2024,UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis,Include,"The paper introduces the UDA (Unstructured Document Analysis) benchmark. UDA questions are expert-annotated Q&A pairs on PDF and HTML documents, constructed from datasets of academic papers, financial reports, and Wikipedia pages.",Retrieval,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",Simple mean/sum; % improvement between contexts,"['Mean', 'Other']"
19,Jonathan Rystrøm,xiaFOFOBenchmarkEvaluate2024,FOFO: A Benchmark to Evaluate LLMs’ Format-Following Capability,Include,FOFO Is a benchmark for domain-specific format following capabilities. It evaluates a wide array of domains and subdomains across a diverse set of formats from specific medical forms to Maple. The specific examples are generated using GPT-4 and human validation.,Instruction Following,,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],,
20,Jonathan Rystrøm,wangMINTEvaluatingLLMs2024,MINT: EVALUATING LLMS IN MULTI-TURN INTERACTION WITH TOOLS AND LANGUAGE FEEDBACK,Include,"MINT extends existing benchmark to evaluate the effects of code interpreter usage and multi-turn feedback on LLM performance. It filters benchmark task to ones that benefit from feedback and multi-turn interactions and evaluates different feedback types from ""lazy user"" to ""informative user"" and with(out) tools. ",Agents,Coding,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Random sample (creators defined a task space and sampled from it),['Random'],"Short free response (e.g. single word or number), Extended interaction (e.g. conversation, calling an API and processing the response)","['Short free response', 'Interaction']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
21,Jonathan Rystrøm,valmeekamPlanBenchExtensibleBenchmark2023,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,Include,PlanBench introduces a suite of tasks relevant to planning using similar formats to the International Planning Competition. The tasks are taken from either Blocksworld or logistics and also obfuscated to avoid reliance on common-sense knowledge.,Reasoning,Planning,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Random sample (creators defined a task space and sampled from it),['Random'],"Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
22,Negar Foroutan,zhangMELAMultilingualEvaluation2024,MELA: Multilingual Evaluation of Linguistic Acceptability,Include,"The paper intorduces a multilingual acceptability judgement benchmark covering a diverse set of 10 languages, all annotated by expert linguists.  The acceptability judgment task tests a language model’s ability to distinguish syntactically acceptable sentences from unacceptable ones in a human language. The paper establishes LLM baselines on this benchmark, and investigates cross-lingual transfer in acceptability judgements with XLM-R.",Multilinguality,,,"hand-written by linguists in respective languages, taken from textbooks, handbooks and journal articles in theoretical syntax + some examples taken from previous benchmarks ",['Expert-crafted'],Random sample (creators defined a task space and sampled from it),['Random'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), Matthews Correlation Coefficient (MCC, Matthews), which is a measure of similarity between binary distributions taking values from -1 to 1 and always yielding 0 for any two uncorrelated distributions, regardless of class imbalance.","['Exact match', 'Correlation']",simple mean and standard deviation ,"['Mean', 'Std']"
23,Negar Foroutan,etxanizLatxaOpenLanguage2024,Latxa: An Open Language Model and Evaluation Suite for Basque,Include,"The paper introduces 4 multiple-choice evaluation datasets for Basque: EusProfi-ciency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. ",Multilinguality,,,Human exam questions (e.g. GRE questions),['Human exams'],Unknown,['Unknown'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"accuracy, F1, standard deviation","['Mean', 'Std', 'Other']"
24,Negar Foroutan,tangStrucbenchAreLarge2024,Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?,Include,"The paper introduces a new benchmark to assess LLMs’ proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance.",Code Generation,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Random sample (creators defined a task space and sampled from it),['Random'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],P-Score (Prompting Score) and H-Score (Heuristical Score),['LLM-as-a-Judge'],simple mean,['Mean']
25,Cornelius Emde,schiappaRobustnessAnalysisVideolanguage2022,"Robustness Analysis of Video-Language Models
Against Visual and Language Perturbations",Include,The aim of the paper is to study the robustness of text-to-video retrieval models to real-world to perturbations in text or video. The benchmark adapts two popular datasets to introduce such perturbations and compares various retrieval models on their robustness.,,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],retrieval rate R@K metric,['Other']
26,Negar Foroutan,riemenschneiderExploringLargeLanguage2023,Exploring Large Language Models for Classical Philology,Include,They define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. The experiments provide the first benchmarking analysis of existing models of Ancient Greek. ,Multilinguality,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Unknown,['Unknown'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
27,Cornelius Emde,qiPreservingKnowledgeInvariance2023,Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction,Include,"The paper introduces ROBUST, a benchmark designed to evaluate open information extraction models by measuring their ability to generalize knowledge extraction across syntactically diverse sentences that share the same semantic content.",NLP,Extraction,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Another benchmark', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)","['Random', 'Convenience']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"For each tuple, the F1 is computed, then across a clique the minimum is computed and aggregated across the dataset as mean.",['Mean']
28,Anna Sotnikova,shahWhenFLUEMeets2022,"WHEN FLUE MEETS FLANG: Benchmarks and Large Pre-trained
Language Model for Financial Domain",Include," the Financial Language Understanding
Evaluation (FLUE), an open-source comprehensive
suite of benchmarks for the financial
domain. These include new benchmarks across
5 NLP tasks in financial domain as well as common
benchmarks used in the previous research.",Finance,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean: F1 scores and accuracy. MSE. nDCG and MRR. Perplexity,"['Mean', 'Other']"
29,Angelika Romanou,kalyanWikiDONewBenchmark2024,WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models,Include,"The authors argue that current VLM benchmarks are insufficient to assess the OOD generalization capability of models due to high visual and linguistic similarity between the evaluation and finetuning datasets. The propose WIKIDO which consists of image-text data derived from Wikipedia Diversity Observatory, a diverse source of Wikipedia articles spanning several diversity axes including geography, gender, ethnicity and domains/topics.",Retrieval,,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Retrieval ,['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
30,Negar Foroutan,marchisioUnderstandingMitigatingLanguage2024,Understanding and Mitigating Language Confusion in LLMs,Include,"The paper introduces a benchmark to measure language confusion in LLMs. They investigate language confusion on the line and word level in two practical settings: a) Monolingual generation, where a user queries the LLM in a given language, implicitly requesting an answer in the same language; and b) cross-lingual generation, where a user explicitly instructs a model to generate text in a different language.",Multilinguality,,,"Modified from another benchmark (e.g. translation into another language), For some part of the data they include human generated prompts ","['Another benchmark', 'Author-crafted']",Random sample (creators defined a task space and sampled from it),['Random'],Free response (e.g. summary paragarph),['Free response'],The paper introduces 2 new metrics for language confusion. Line-level pass rate (LPR) and Word-level pass rate (WPR).,['Exact match'],simple mean,['Mean']
31,Ryan Kearns,itoGeneralizationCapacityNeural2024,On the generalization capacity of neural networks during generic multimodal reasoning,Include,"The paper introduces gCOG, a multimodal reasoning dataset designed to measure various types of OOD generalisation (distractor generalisation, systematic compositional, and productive compositional). The authors train various encoder architectures from scratch and compare their performances. Transformers can systematically generalise at scale, but no architectures can productively generalise.",Language Modelling,Adaptability,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean/sum,['Mean']
32,Ryan Kearns,liMultimodalArXivDataset2024,Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models,Include,"Multimodal ArXiv consists of ArXivCap, a figure-caption dataset sourced from scientific papers, and ArXivQA, a QA dataset generated by prompting GPT-4V for QA pairs on ArXivCap entries. Results show that fine-tuning on these datasets boosts performance on the MathVista benchmark, and that evaluation results for various scientific plot comprehension subtasks are poor.",NLP,Understanding,,"Real task examples (e.g. GitHub issues), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)","['Soft match', 'LLM post-processing']",simple mean/sum,['Mean']
33,Ryan Kearns,zouVGBenchEvaluatingLarge2024,VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation,Include,"The paper introduces VGBench, a comprehensive benchmark for vector graphics images that tests both visual understanding and generation. Formats like SVG, TikZ, and Graphviz are included, and performance is generally strong, though LLMs do worse with the lower-level SVG format.",Instruction Following,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Another benchmark', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'Soft match', 'LLM post-processing']",simple mean/sum,['Mean']
34,Negar Foroutan,zhangXSemPLRCrosslingualSemantic2023,XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations,Include,"The paper introduces XSEMPLR, a unified benchmark for cross-lingual semantic parsing featuring 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. They use XSEMPLR to conduct a benchmark study on a wide range of multilingual language models, including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and
decoder-based models (Codex, BLOOM). The findings show that large multilingual
language models are still inadequate for performing CLSP tasks. They also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training.",Multilinguality,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Random sample (creators defined a task space and sampled from it),['Random'],Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",Simple mean,['Mean']
35,Negar Foroutan,sunInformalLanguageProcessing2024,Toward Informal Language Processing: Knowledge of Slang in Large Language Models,Include,"Using movie subtitles, the authors construct a dataset that supports evaluation on a diverse
set of tasks pertaining to the automatic processing of slang. For both evaluation and finetuning, they show the effectiveness of their dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences.",Multilinguality,,,Crowd-sourced task examples (e.g. Prolific-created tasks),['Crowd-sourced'],Random sample (creators defined a task space and sampled from it),['Random'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), They also report two metrics to compare an LLM’s predictive confidence in slang usages relative to their literal counterparts.","['Exact match', 'Correlation']",simple mean,['Mean']
36,Anna Sotnikova,wangPretrainingLanguageModel2023,ON PRE-TRAINED LANGUAGE MODELS FOR ANTIBODY,Include,This paper introduces the AnTibody Understanding Evaluation (ATUE) benchmark to systematically assess the representation capabilities of general and antibody-specific pre-trained language models across a range of antibody-related tasks. It also explores how incorporating biological mechanisms into pre-training can enhance model performance and evaluates the transferability of learned representations to real-world applications such as drug discovery and immune system analysis.,Biology,,,Real task examples (e.g. GitHub issues),['Real task'],"Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), Matthews Correlation Coefficient (MCC), and AUC (Area Under the ROC Curve)","['Exact match', 'Correlation']",,
37,Negar Foroutan,bajpaiCanLLMsReplace2024,Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators,Include,"This paper focuses on evaluating the reliability of current LLMs as science communicators. They introduce a dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex
scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria. They also benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families.",General Science,,,Not explained ,['Unknown'],Unknown,['Unknown'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean and standard deviation,"['Mean', 'Std']"
38,Negar Foroutan,hauserLargeLanguageModelsExpertlevel2024,Large Language Models' Expert-level Global History Knowledge Benchmark (HiST-LLM),Include,"The paper introduces the History Seshat Test for LLMs (HiST-LLM), based on a subset of the Seshat Global History Databank, which provides a structured representation of human historical knowledge, containing 36,000 data points across 600 historical societies and over
2,700 scholarly references. Using this dataset, they benchmark a total of seven models from the Gemini, OpenAI, and Llama families.",History,,,Human expert created the examples,['Expert-crafted'],Random sample (creators defined a task space and sampled from it),['Random'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Mean and standard deviation,"['Mean', 'Std']"
39,Lennart Luettgau,sadatMSciNLIDiverseBenchmark2024,MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference,Include,"This paper introduces MSCINLI, a new dataset comprising 132,320 sentence pairs from five diverse scientific domains to enhance the study of scientific Natural Language Inference (NLI). Baseline models, including fine-tuned and prompted LLMs, reveal the dataset's challenging nature, as well as performance degradation due to domain shifts, highlighting the unique characteristics of each domain. Additionally, employing both scientific NLI datasets in intermediate task transfer learning showcases improvements in downstream scientific tasks.",General Science,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"mean and variance, t-tests","['Mean', 'Std', 'Tests']"
40,Lennart Luettgau,dengNewTermBenchmarkingRealtime2024,NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates,Include,"This paper introduces NewTerm, an adaptive benchmark designed for the real-time evaluation of new terms in large language models (LLMs) to address their struggle with real-time information due to knowledge cutoffs. The benchmark is constructed using a highly automated method allowing flexible and minimal human effort updates, revealing a performance reduction of over 20% on various LLMs with new terms and highlighting difficulties in generalizing to distant new terms. Annual updates to NewTerm, starting with 2022 and 2023, are planned to continuously assess and analyze the evolving challenge of new terms in LLMs.",Language Modelling,Updating,,"Real task examples (e.g. GitHub issues), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Procedurally-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean/sum,['Mean']
41,Lennart Luettgau,changLocalizationMethodsActually2024,Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks,Include,"This paper introduces two complementary benchmarks, INJ and DEL, to systematically evaluate localization methods in large language models (LLMs) by examining their ability to identify model components responsible for memorized data. The INJ Benchmark measures whether methods can pinpoint modified weights after actively injecting new information, while the DEL Benchmark assesses the impact of dropping identified neurons on deleting memorized sequences. Results indicate consistent rankings among five localization methods, with adapted network pruning techniques performing well, but even successful methods fail to identify neurons specific to a single memorized sequence.",,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Procedurally-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"mean and variance, t-tests","['Mean', 'Std', 'Tests']"
42,Cornelius Emde,yeRoTBenchMultilevelBenchmark2024,RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning,Include,"LLMs are increasingly deployedin settings where they can use tools, e.g. call functions to retrieve real-time information on weather. This paper proposes benchmark measuring the robustness of LLMs in selecting tools when these are specified under noise (e.g. the function name is perturbed).",Agents,Tool Use,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],"Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)","['Random', 'Convenience']","Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
43,Kaili Liu,maMMLONGBENCHDOCBenchmarkingLongcontext2024,MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations,Include,"The paper presents a long-context multimodal benchmark dataset of more than 1k expert annotated questions over long PDFs which require aggregating evidence across multiple locations and evidence formats (text, image, charts, etc.) to answer. MMLongBench-Doc presents a challenge for strong models such as GPT-4o and other large vision language models (LVLMs), demonstrating the need for improved long-context LVLM capabilities.",NLP,Long Context,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
44,Kaili Liu,kuratovBABILongTestingLimits2024,BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack,Include,"The BABILong benchmark tests language models’ ability to reason across facts distributed in extremely long documents in the reasoning setting, scattering relevant facts among less relevant natural text. The paper finds LLMs only effectively use less than 20% of the context in such settings, with reasoning complexity negatively impacting performance. Multiple methods including in-context reasoning, retrieval augmented generation, and context extension are applied to profile model capabilities in these long-context tasks.",NLP,Long Context,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
45,Kaili Liu,wangAdaLEvalEvaluatingLongcontext2024,Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks,Include,"Ada-LEval presents a length-adaptable benchmark for long-context understanding capabilities of LLMs, involving challenging questions for reliable evaluation and context lengths extending to the ultra-long setting. SOTA open and closed models are evaluated to demonstrate current limitations of LLMs in such settings.",NLP,Long Context,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Free response (e.g. summary paragarph)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation), instruction following rate","['Exact match', 'Distribution', 'Exact match']",simple mean,['Mean']
46,Kaili Liu,zhangAnalyzingTemporalComplex2024,"Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding",Include,TCELongBench assess LLMs’ ability to leverage temporal dynamics when understanding extensive texts. Experiments find that retrieval augmented generation and long-context modeling are fairly effective to handle such tasks.,NLP,Long Context,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",Simple mean,['Mean']
47,Kaili Liu,liLooGLECanLongcontext2024,LooGLE: Can Long-Context Language Models Understand Long Contexts?,Include,The paper presents a long-context benchmark over recent (post-2022) documents with new questions in diverse domains. LooGLE assesses LLMs’ long dependency capabilities and finds poor performance even with long context window LLMs.,NLP,Long Context,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Crowd-sourced', 'Another benchmark', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Human accuracy evaluation","['Exact match', 'Soft match', 'LLM-as-a-Judge', 'Human ratings']",Simple mean,['Mean']
48,Kaili Liu,wangLeaveNoDocument2024,Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA,Include,"Loong is a long-context benchmark which aims to boost the realism of long-context capability evaluation by ensuring each document is relevant to the final answer, covering a range of context lengths and tasks. Various models are assessed on the benchmark, with RAG proving poor for improving performance.",NLP,Long Context,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Short free response', 'Free response', 'Structured']","LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)","['LLM-as-a-Judge', 'LLM post-processing']",simple mean,['Mean']
49,Kaili Liu,senelCoDA21EvaluatingLanguage2022,CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment,Include,CoDA21 is a challenging benchmark to assess NLU capabilities of pretrained language models (PLMs). Performance of PLMs is assessed versus humans.,NLP,Understanding,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), cosine similarity, log generation probability","['Exact match', 'Distribution']",simple mean,['Mean']
50,Kaili Liu,anLevalInstitutingStandardized2024,L-Eval: Instituting Standardized Evaluation for Long Context Language Models,Include,L-Eval presents a standardize evaluation suite for long-context language models consisting of 20 subtasks over long documents up to 200K tokens in length with diverse human-labeled query-response pairs. Evaluation metrics for long-context LLMs are compared for alignment with human judgment. Commercial and open-source LLMs are benchmarked.,NLP,Long Context,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Extended interaction (e.g. conversation, calling an API and processing the response)","['Multiple choice', 'Short free response', 'Free response', 'Interaction']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Soft match', 'Human ratings', 'LLM-as-a-Judge']",simple mean,['Mean']
51,Kaili Liu,zhangMarathonRaceRealm2024,Marathon: A Race Through the Realm of Long Context with Large Language Models,Include,The paper presents the Marathon benchmark to evaluate comprehension and reasoning capabilities of LLMs over long texts. Marathon is used to assess SOTA LLMs and the efficacy of several existing long-context generation strategies.,NLP,Long Context,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
52,Kaili Liu,zhangBenchExtendingLong2024,\inftyBench: Extending Long Context Evaluation Beyond 100K Tokens,Include,"The paper presents InfiniteBench, a new benchmark to evaluate LLMs’ ability to process, understand, and reason over ultra-long contexts over 100k tokens in length. InfiniteBench contains both real and synthetic tasks which present notable challenge to existing SOTA LLMs.",NLP,Long Context,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Extended interaction (e.g. conversation, calling an API and processing the response)","['Multiple choice', 'Short free response', 'Free response', 'Interaction']","LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)","['LLM-as-a-Judge', 'LLM post-processing']",simple mean,['Mean']
53,Kaili Liu,xuStresstestingLongcontextLanguage2024,Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack,Include,"The paper introduces lifelong ICL as a new long-context problem setting for LLMs and the Test Haystack evaluation suite to understand how LLMs utilize contexts for the lifelong ICL task. Many long-context LMs are benchmarked, and contributors to failure cases are identified.",Language Modelling,In-context Learning,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), pass rate","['Exact match', 'Exact match']",,
54,Kaili Liu,kwanM4LEMultiabilityMultirange2024,M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models,Include,"The paper introduces a comprehensive multi-range, multi-ability, multi-task, multi-domain benchmark for long context processing in LLMs. Analysis confirms LLMs struggle to handle long contexts, especially when multiple input spans are involved. Several long context methods are compared.",NLP,Long Context,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'Procedurally-generated', 'LLM-generated']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Normalized score relative to GPT-3.5-Turbo-16K performance","['Exact match', '']",,
55,Kaili Liu,baiLongBenchBilingualMultitask2024,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",Include,"LongBench is the first bilingual multi-task benchmark for long-context understanding. Benchmarking of open and closed source models suggests notable challenges for LLMs, with fine-tuning and scaled position embedding helping to improve long-context capabilities.",NLP,Long Context,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt), Expert-annotated task examples (PhD students)","['Real task', 'Another benchmark', 'Procedurally-generated', 'LLM-generated', 'Expert-crafted']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'Soft match', 'LLM-as-a-Judge', 'LLM post-processing']",simple mean,['Mean']
56,Kaili Liu,mahbubUnveilingEssencePoetry2023,Unveiling the Essence of Poetry: Introducing a Comprehensive Dataset and Benchmark for Poem Summarization,Include,"The paper proposes the task of poem summarization for LLMs and presents the first benchmark, PoemSum, to evaluate such capability. SOTA summarization models are benchmarked and limitations of current models on the poem summarization task are discussed.",NLP,Summarization,,Real task examples (e.g. GitHub issues),['Real task'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",simple mean,['Mean']
57,Kaili Liu,fernandezSyllabusQACourseLogistics2024,SyllabusQA: A Course Logistics Question Answering Dataset,Include,"The paper introduces a new dataset consisting of real-world syllabi for question-answering. Strong LLMs are benchmarked on the dataset, SyllabusQA.",Retrieval,,,"Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Crowd-sourced', 'Procedurally-generated']",Unknown,['Unknown'],"Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'Soft match', 'LLM-as-a-Judge', 'LLM post-processing']",simple mean,['Mean']
58,Kaili Liu,suLivingMomentCan2024,Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?,Include,"This paper addresses the task of reasoning across intricate temporal interconnections and introduces CoTempQA as a comprehensive co-temporal question answering benchmark. Current LLMs exhibit significant deficiencies versus humans in co-temporal comprehension and reasoning, even with Chain of Thought. Mathematical reasoning is found to play a notable role in handling co-temporal events, and a strategy to boost co-temporal reasoning in LLMs which leverages this insight is proposed.",Reasoning,Temporal,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), Wikidata","['Author-crafted', 'Procedurally-generated', 'Crowd-sourced']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
59,Kaili Liu,krojerImageRetrievalContextual2022,Image Retrieval from Contextual Descriptions,Include,"The paper proposes a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe), to assess vision-and-language models’ ability to integrate context cues into interpretation of linguistic utterances. Models such as ViLBERT and CLIP are evaluated and found to lag significantly behind human performance.",Retrieval,,,"Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Crowd-sourced', 'Another benchmark', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
60,Kaili Liu,rayColaBenchmarkCompositional2023,Cola: A Benchmark for Compositional Text-to-image Retrieval,Include,"This paper looks at compositional visual reasoning in LLMs, presenting the COLA benchmark which targets text-to-image retrieval to compose objects with localized attributes. Strategies to adapt pre-trained vision-language models for compositional reasoning are assessed, and the authors find training with multimodal layers to be highly promising. COLA is compared to the CREPE benchmark, demonstrating greater difficulty than this contemporary counterpart.",Reasoning,Compositional,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Crowd-sourced', 'Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
61,Anna Gausen,bhaskarBenchmarkingImprovingTexttoSQL2023,Benchmarking and Improving Text-to-SQL Generation under Ambiguity,Include,"Previous research on Text-to-SQL conversions has relied on datasets with unambiguous mappings, despite real-world queries frequently having multiple valid SQL interpretations due to schema overlaps and confusing relationships. To address this gap, the authors created AmbiQT, a benchmark featuring 3000+ examples with dual valid SQL interpretations. This  reveals that even SOTA LLMs struggle to generate all valid interpretations— because beam search algorithms produce token-level diversity rather than semantic alternatives.",Code Generation,Natural Lanuage,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean (as percentage),['Mean']
62,Anna Sotnikova,xuPEERComprehensiveMultitask2022,PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding,Include,"A benchmark called PEER (a
comprehensive and multi-task benchmark for Protein sEquence undERstanding).
PEER provides a set of diverse protein understanding tasks including protein
function prediction, protein localization prediction, protein structure prediction,
protein-protein interaction prediction, and protein-ligand interaction prediction. ",Biology,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), Spearman’s ρ, L/5 precision, RMSE","['Exact match', 'Correlation']","simple mean, std","['Mean', 'Std']"
63,Anna Gausen,jangTemporalWikiLifelongBenchmark2022,"TemporalWiki: A Lifelong Benchmark for Training and Evaluating
Ever-Evolving Language Models",Include,"Most LLM benchmarks are static yet real factutal knowledge changes, increases and depreciates. TemporalWiki addresses language models' temporal misalignment by providing a benchmark derived from consecutive Wikipedia snapshots to assess how well models adapt to evolving knowledge. The findings demonstrate that updating models using only the differences between snapshots achieves comparable or better perplexity than retraining on entire snapshots, while reducing computational costs by 12x.",Language Modelling,Updating,,Real task examples (e.g. GitHub issues),['Real task'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']",Simple average of perplexity for different snapshots of the wikipedia data.,['Mean']
64,Jonathan Rystrøm,liuAgentBenchEvaluatingLLMs2024,AGENTBENCH: EVALUATING LLMS AS AGENTS,Include,"AgentBench presents a holistic benchmark for evaluating LLMs as agents. It is structured across three domains (code, game, and web) and aims to evaluate a wide range of abilities. ",Agents,General,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']",Aggregated scores (no additional stats),['Mean']
65,Jonathan Rystrøm,huangMetaToolBenchmarkLarge2024,"METATOOL BENCHMARK FOR LARGE LANGUAGE
MODELS: DECIDING WHETHER TO USE TOOLS AND
WHICH TO USE",Include,"MetaTool proposes a benchmark for tool selection. It encompasses a diverse set of scenarios and four different settings (Similar tools, multi-tool, scenario, and reliability). The benchmark only focuses on tool selection and not actual execution.",Agents,Tool Use,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],Random sample (creators defined a task space and sampled from it),['Random'],Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
66,Jonathan Rystrøm,huangMLAgentBenchEvaluatingLanguage2024,MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation,Include,MLAgentBench benchmarks the ability of LLM agents to perform machine learning experiments. The benchmark comprises different tasks from canonical classification to code optimization. A success is beating the baseline by more than 10%,Agents,Coding,,Real task examples (e.g. GitHub issues),['Real task'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"functioning code (i.e., a .py script or model artifacts)",['Free response'],Score improvement of script,['Reward'],mean over 8 runs. ,['Mean']
67,Fangru Lin,yeGlobeSummChallengingBenchmark2024,"GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization",Include,Propose GLOBESUMM and introduce prompting method for silver summary annotation. Validate the quality and difficulty of the dataset.,NLP,Summarization,Multilinguality,Real task examples (e.g. GitHub issues),['Real task'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Free response (e.g. summary paragarph),['Free response'],"n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)","['Soft match', 'LLM post-processing']",simple mean,['Mean']
68,Jonathan Rystrøm,huSportsMetricsBlendingText2024,SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs,Include,"SportsMetrics evaluates LLMs' numerical reasoning abilities within a sports domain. Specifically, it tasks LLMs with filling in information based on play-by-play descriptions from different games. SportsMetrics also include adversarial examples with scrambled rules. ",Reasoning,Mathematical,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Random sample (creators defined a task space and sampled from it),['Random'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple summary stats. ,['Mean']
69,Jonathan Rystrøm,choiLoTabenchBenchmarkingLanguageoriented2024,LOTA-BENCH: BENCHMARKING LANGUAGE-ORIENTED TASK PLANNERS FOR EMBODIED AGENTS,Include,LoTa-Bench is a benchmark for task planning for home-service agents. It proposes a quantitative and automated evaluation framework for language-based agents to complete different home-making tasks like placing an apple in a micro-wave.,Reasoning,Planning,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Crowd-sourced', 'Another benchmark']",Random sample (creators defined a task space and sampled from it),['Random'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Success rate,['Mean']
70,Anna Gausen,songSLINGSinoLinguistic2022,SLING: Sino Linguistic Evaluation of Large Language Models,Include,"The SLING benchmark is introduced to evaluate the linguistic knowledge of pretrained Chinese language models, featuring 38,000 minimal sentence pairs in Mandarin Chinese that highlight syntactic and semantic phenomena. These sentences are naturally-occuring and annotated, from the Chinese Treebank 9.0. Evaluating 18 LMs, the study found that their average accuracy is significantly lower than human performance (69.7% vs. 97.1%), with BERT-base-zh achieving the highest accuracy at 84.8%.",Multilinguality,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Author-crafted', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']",Choice of one input sentence,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Human ratings']",,
71,Harry Mayne,athiwaratkunMultilingualEvaluationCode2023,Multi-lingual Evaluation of Code Generation Models,Include,"Measures code generation capabilities across 10 programming languages (Java, JavaScript, TypeScript, Go, Ruby, Kotlin, PHP, C#, Scala, C++, Swift, and Perl). Transforms existing Python benchmarks into other languages.",Code Generation,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],Accuracy when the generated function is executed.,['Reward'],Simple mean,['Mean']
72,Anna Gausen,pengCOPENProbingConceptual2022,COPEN: Probing Conceptual Knowledge in Pre-trained Language Models,Include,"The paper introduces COPEN, a benchmark designed to probe conceptual knowledge in pre-trained language models (PLMs). It includes three tasks evaluating whether PLMs can group entities by concepts, understand concept properties, and identify concepts in context. Results show that PLMs struggle with conceptual reasoning and often rely on spurious correlations.",NLP,Understanding,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Crowd-sourced', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
73,Anna Sotnikova,hardalovBgGLUEBulgarianGeneral2023,bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark,Include," bgGLUE (Bulgarian General Language
Understanding Evaluation), a benchmark
for evaluating language models on Natural Language
Understanding (NLU) tasks in Bulgarian.
The benchmark includes NLU tasks targeting
a variety of NLP problems (e.g., natural language
inference, fact-checking, named entity
recognition, sentiment analysis, question answering,
etc.) and machine learning tasks (sequence
labeling, document-level classification,
and regression). ",NLP,Understanding,Multilinguality,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Human exams', 'Real task', 'Author-crafted', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Pear./Spear. Corr , Avg. Precision","['Exact match', 'Correlation']","simple mean, for tasks with more than one metric (like Pearson and Spearman correlation for sentiment regression), scores are averaged to get a single task score",['Mean']
74,Negar Foroutan,kwanMTevalMultiturnCapabilities2024,MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models,Include,"Thie paper introduces MT-Eval, a benchmark to evaluate the multiturn conversational abilities of LLMs. Paper's analysis of interactions in LMSYS-Chat1M reveals four predominant patterns when users interact with AI assistants: Recollection, where the assistant must recall information from earlier turns; Expansion, involving the exploration of varied topics within the main subject; Refinement, where initial instructions are clarified
or revised; and Follow-up, consisting of questions based on the assistant’s previous responses. They then construct evaluation sets for each interaction type by augmenting existing datasets or creating new ones to cover real-world applications.",User Interaction,,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']",Random sample (creators defined a task space and sampled from it),['Random'],Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']",simple mean,['Mean']
75,Emma Beharry,naousReadMeBenchmarkingMultilingual2024,"README++: Benchmarking Multilingual Language Models for
Multi-Domain Readability Assessment",Include,"ReadMe++ is a multilingual and multi-domain dataset for readability assessment according to the Common European Framework of Reference for Languages (CEFR) scale in Arabic, English, French, Hindi, and Russian. The dataset is human-annotated and publicly available. The dataset can benchmark supervised, unsupervised, and few-shot approaches, and is measured by the Pearson Correlation between predictions and ground-truth labels (supervised, few-shot) or the Ranged Sentence Readability Score (unsupervised). ",NLP,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Human exams', 'Real task', 'Author-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Distribution (perplexity, calibration, correlation)",['Distribution'],"Min, max, average","['Mean', 'Other']"
76,Emma Beharry,hengleStillNotQuite2024,Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis,Include,"ANGST is a benchmark for depression-anxiety comorbidity classification from social media posts. The dataset has multi-class labeling for anxiety, depression, both, or none, and the samples are neutrally seeded from Reddit and human-annotated by expert psychologists. Additionally, the paper presents ANGST-SILVER, a more extensive and silver-labeled dataset by GPT-3.5-turbo to support few-shot learning or supervised fine-tuning. ",Mental Health,,,Real task examples (e.g. GitHub issues),['Real task'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Weighted Precision, Recall, F1 scores, and macro-F1 scores for binary and multi-class classification. Hamming loss is also reported for multi-class classification. ","['Mean', 'Other']"
77,Emma Beharry,tanDevBenchMultimodalDevelopmental2024,DevBench: A multimodal developmental benchmark for language learning,Include,"DevBench is a multimodal benchmark for assessing how LLMs compare to human language development across seven language evaluation tasks spanning lexical, syntactic, and semantic domains. Each task contains item-level human baseline data to facilitate human-model language development comparison using a novel metric: softmax-optimized Kullback-Leibler divergence. The goal of the benchmark is to measure whether developmentally realistic data leads to human-like learning in LLMs. ",Language Modelling,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Real task', 'Author-crafted']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Distribution (perplexity, calibration, correlation)",['Distribution'],"Visual semantic tasks were measured with representational similarity analysis (RSA), while the other tasks were measured with a novel metric: softmax-optimized Kullback-Leibler divergence",['Other']
78,Anna Sotnikova,shavrinaRussianSuperGLUERussianLanguage2020,RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark,Include,"In this paper, we introduce an advanced Russian general language understanding evaluation benchmark – RussianGLUE. This benchmark consists of nine tasks, collected and organised analogically to the SuperGLUE methodology (Wang et al., 2019),  it was developed from scratch for the Russian language. We provide baselines, human level evaluation, an open- source framework for evaluating models and an overall leaderboard of transformer models for the Russian language.",NLP,Understanding,Multilinguality,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Crowd-sourced', 'Another benchmark', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), exact match, MCC (Matthews Correlation Coefficient)","['Exact match', 'Exact match']",simple mean,['Mean']
79,Anna Sotnikova,taktashevaRuBLiMPRussianBenchmark2024,RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs,Include,"Minimal pairs are a well-established approach
to evaluating the grammatical knowledge of language
models. This paper introduces
the Russian Benchmark of Linguistic
Minimal Pairs (RuBLiMP), which includes
45k pairs of sentences that differ in grammaticality
and isolate a morphological, syntactic,
or semantic phenomenon. In contrast to existing
benchmarks of linguistic minimal pairs,
RuBLiMP is created by applying linguistic
perturbations to automatically annotated sentences
from open text corpora and decontaminating
test data. ",NLP,,Multilinguality,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Another benchmark', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']","simple mean,  inter-annotator agreement with WAWA and the Dawid-Skene method for vote aggregation.  delta-scores to measure performance differences between models under different dataset filtering conditions","['Mean', 'Other']"
80,Harry Mayne,liInfiBenchEvaluatingQuestionanswering2024,InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models,Include,Freeform question-answering (QA) benchmark for code across 15 programming languages.,Code Generation,,,Real task examples (e.g. GitHub issues),['Real task'],"Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Targeted', 'Criterion']",Free response (e.g. summary paragarph),['Free response'],"n-gram (BLEU, ROUGE, chrF), Also consider unit tests for some questions.","['Soft match', 'Reward']","Mean, standard deviation.","['Mean', 'Std']"
81,Harry Mayne,duMercuryCodeEfficiency2024,Mercury: A Code Efficiency Benchmark for Code Large Language Models,Include,Introduces the first code efficiency benchmark for Code LLMs. Benchmark functional correctness and code efficiency simultaneously,Code Generation,,,Real task examples (e.g. GitHub issues),['Real task'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Structured response (e.g. valid JSON, API call alone)",['Structured'],runtime percentile of the LLM-generated code on the runtime distribution supported by corresponding solutions (the Leetcode solutions),['Reward'],,
82,Oishi Deb,linghuMultimodalSituatedReasoning2024,Multi-modal Situated Reasoning in 3D Scenes,Include,"Introduces MSQA, a large-scale dataset (251K pairs) for multi-modal situated reasoning in 3D scenes, and two corresponding benchmarks: Multi-modal Situated Question Answering (MSQA) and Multi-modal Situated Next-step Navigation (MSNN). The MSQA dataset was collected scalably using 3D scene graphs and vision-language models, while the benchmarks use a novel interleaved input setting (text, image, point cloud) to improve situation awareness and resolve ambiguity present in single-modality approaches.",Reasoning,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']","Simple mean/average scores (MSQA Correctness Score C, MSNN Accuracy) are used to aggregate results. Different models or settings are compared directly based on these mean scores presented in tables.",['Mean']
83,Oishi Deb,wuSTaRKBenchmarkingLLM2024,STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases,Include,"STaRK is a large-scale benchmark for evaluating LLM-based retrieval systems on semi-structured knowledge bases (SKBs) that integrate textual and relational information. It covers product search, academic paper search, and precision medicine domains. A novel pipeline synthesizes realistic queries and ground truth answers, supplemented by human-generated queries, revealing significant challenges for current retrieval systems.",Retrieval,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'Crowd-sourced', 'Procedurally-generated', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring), Distribution (perplexity, calibration, correlation)","['Exact match', 'LLM post-processing', 'Distribution']","Simple mean/average of Hit@k, Recall@k, and MRR over the test sets.",['Mean']
84,Jabez Magomere,krumdickBizBenchQuantitativeReasoning2024,BizBench: A Quantitative Reasoning Benchmark for Business and Finance,Include,"This paper introduces BizBench, a benchmark for evaluating models’ ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question answering (QA) over financial data via program synthesis.",Finance,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'Real task', 'Author-crafted', 'Procedurally-generated', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
85,Oishi Deb,ghoshEPiCEmployingProverbs2022,ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding,Include,"This paper introduces ePiC, a high-quality crowdsourced dataset designed to benchmark abstract language understanding and analogical reasoning in LLMs. The dataset pairs narratives with proverbs, featuring fine-grained span alignments and minimal lexical overlap. Three tasks are proposed: proverb recommendation/alignment, narrative generation, and identifying similar narrative motifs. Experiments show that current LLMs struggle with these tasks compared to humans, indicating significant challenges in abstract reasoning.",Reasoning,Logical,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Author-crafted', 'Crowd-sourced']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Multiple choice, Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), Distribution (perplexity, calibration, correlation)","['Exact match', 'Soft match', 'Human ratings', 'Distribution']","Accuracy, MRR, Precision, Recall, F1, BLEU, ROUGE-L, Keyword Recall, Mean Likert scores.","['Mean', 'Other']"
86,Oishi Deb,yuanUnlockingMarketsMultilingual2024,Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering,Include,"The paper introduces Multilingual Cross-market Product-based Question Answering (MCPQA), a novel task where information from a resource-rich market (e.g., US) is used to answer product questions in a resource-scarce market, potentially in a different language. It presents a large-scale dataset derived from 17 Amazon marketplaces (11 languages), with a translated subset for Electronics called McMarket. Experiments on review-based answer generation (AG) and question ranking (QR) benchmark various models, demonstrating that leveraging cross-market information significantly boosts performance.",Retrieval,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Free response (e.g. summary paragarph),['Free response'],"n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), Distribution (perplexity, calibration, correlation)","['Soft match', 'Human ratings', 'Distribution']","BLEU-4, ROUGE-L, MRR, Precision@3. Mean scores are reported, sometimes with standard deviation (e.g., for text lengths in Table 2 ).","['Mean', 'Std']"
87,Anna Sotnikova,berdicevskisSuperlimSwedishLanguage2023,Superlim: A Swedish Language Understanding Evaluation Benchmark,Include,"We present Superlim, a multi-task NLP bench- mark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. From the set of experiments, it is quite challenging to the models.",NLP,Understanding,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Human exams', 'Real task', 'Author-crafted', 'Another benchmark', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph), predicted label","['Multiple choice', 'Short free response', 'Free response', 'Multiple choice']",Krippendorff’s α,['Correlation'],"simple mean, std","['Mean', 'Std']"
88,Oishi Deb,wangMAVENARGCompletingPuzzle2024,MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation.,Include,"This paper introduces MAVEN-ARG, an augmentation of the MAVEN dataset with event argument annotations, creating the first large-scale, all-in-one resource for event detection, argument extraction (EAE), and relation extraction. MAVEN-ARG features a comprehensive schema (162 event types, 612 argument roles), substantial data scale (over 290k annotated arguments), and exhaustive annotations (document-level, entity & non-entity args). Experiments show MAVEN-ARG poses significant challenges for existing EAE models and LLMs.",NLP,Extraction,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Author-crafted', 'Crowd-sourced', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Precision, Recall, F1 score, Exact Match (EM)","['Mean', 'Other']"
89,Anna Gausen,jiangFollowBenchMultilevelFinegrained2024,"FollowBench: A Multi-level Fine-grained Constraints Following
Benchmark for Large Language Models",Include,"The paper presents a benchmark called FollowBench for multi fine-grained constraint following evaluations. It asses five different constraint types (e.g. content, situation, style, format and example). The paper evaluated 13 LLMs with FollowBench which highlights weaknesses in LLMs instruction following capabilities.",Instruction Following,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']",,
90,Oishi Deb,romanouCRABAssessingStrength2023,CRAB: Assessing the Strength of Causal Relationships Between Real-World Events.,Include,"This paper introduces CRAB, a new benchmark to evaluate the causal reasoning abilities of language models on real-world events presented in news narratives. It contains approximately 2,700 event pairs derived from 20 news stories, annotated with fine-grained causality scores (0-100) based on context. Experiments using large language models reveal poor performance, particularly when reasoning about complex causal structures (like causal frames and chains) versus simple ones.",Reasoning,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'Crowd-sourced', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Macro F1 score, Exact Match (EM)",['Mean']
91,Jabez Magomere,zhaoFinanceMATHKnowledgeintensiveMath2024,FinanceMATH: Knowledge-Intensive Math Reasoning in Finance Domains,Include,This paper introduces FinanceMath; a novel benchmark designed to evaluate LLMs’ capabilities in solving knowledge-intensive math reasoning problems. These problems require college-level knowledge in the finance domain for effective resolution.,Reasoning,Mathematics,Finance,Crowd-sourced task examples (e.g. Prolific-created tasks),['Crowd-sourced'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",Simple Mean,['Mean']
92,Jabez Magomere,zhaoFinDVerExplainableClaim2024,FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents,Include,"A comprehensive benchmark designed to evaluate the explainable claim verification capabilities of LLMs in the context of understanding and analyzing long, hybrid-content financial documents. FINDVER contains 2,400 expertannotated examples, divided into three subsets: information extraction, numerical reasoning, and knowledge-intensive reasoning—each addressing common scenarios encountered in realworld financial contexts.",Finance,,,"Real task examples (e.g. GitHub issues), Domain expert annotators","['Real task', 'Expert-crafted']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",Simple mean,['Mean']
93,Lennart Luettgau,magnussonPalomaBenchmarkEvaluating2024,Paloma: A Benchmark for Evaluating Language Model Fit,Include,"Evaluations of language models typically use a single dataset for measuring perplexity, but this dataset comprises various domains with different language distributions. PALOMA introduces a new benchmark to assess language model performance across distinct English and code domains, including two new datasets from top subreddits and popular programming languages, providing a more detailed and domain-specific analysis of model fit.",Domain Applications,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Free response (e.g. summary paragarph),['Free response'],"Distribution (perplexity, calibration, correlation)",['Distribution'],simple mean/sum,['Mean']
94,Fangru Lin,tangTofuEvalEvaluatingHallucinations2024,TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization,Include,"Propose a summarization dataset generated by LLMs and human annotations of factual consistencies. Show that LLMs hallucinate and have diverse errors, and that non-LLM evaluators can capture these errors better than LLMs.",NLP,Summarization,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Another benchmark', 'LLM-generated']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']","Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)","['Human ratings', 'LLM-as-a-Judge', 'LLM post-processing']",simple mean,['Mean']
95,Lennart Luettgau,casolaMultiPICoMultilingualPerspectivist2024,MultiPICo: Multilingual Perspectivist Irony Corpus,Include,"Perspectivism in NLP models different individual perspectives by leveraging data annotated with subjective opinions. The proposed MultiPICo corpus includes multilingual ironic short conversations from Twitter and Reddit, along with annotator sociodemographic information, allowing for the analysis of demographic influences on irony perception and the benchmarking of large language models' ability to recognize irony across different groups and languages.",NLP,,,Crowd-sourced task examples (e.g. Prolific-created tasks),['Crowd-sourced'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean/sum,['Mean']
96,Lennart Luettgau,jinRWKUBenchmarkingRealworld2024,RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models,Include,"Large language models often memorize sensitive or harmful information from their training data, necessitating methods to erase this knowledge. The Real-World Knowledge Unlearning (RWKU) benchmark is proposed to address this challenge by evaluating the ability of LLMs to forget specific knowledge without access to the original training data, using real-world famous people as unlearning targets, and employing rigorous evaluation methods for both forgetting and retaining relevant information in various applications.",Language Modelling,Unlearning,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'], simple mean/sum,['Mean']
97,Lennart Luettgau,jiangXFACTRMultilingualFactual2020,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,Include,"Language models have effectively captured factual knowledge through cloze-style fill-in-the-blank questions, but evaluations have mostly focused on English. To assess factual knowledge retrieval across different languages, a multilingual benchmark for cloze-style probes covering 23 diverse languages is created, along with expanded methods and decoding algorithms for multi-word entities. The study also introduces a code-switching method to enhance multilingual models' knowledge access, demonstrating its effectiveness across several languages.",Knowledge,General,Multilinguality,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean/sum,['Mean']
98,Jabez Magomere,yuKoLACarefullyBenchmarking2024,KoLA: Carefully Benchmarking World Knowledge of Large Language Models,Include,"This paper introduced Knowledge-oriented LLM Assessment benchmark (KoLA), which aims at carefully benchmarking the world knowledge of LLMs by undertaking meticulous designs considering the aforementioned three factors: ability modeling, known and evolving data sources and contrastive evaluation system.",Knowledge,General,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Crowd-sourced', 'Another benchmark', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",,
99,Fangru Lin,subbiahSTORYSUMMEvaluatingFaithfulness2024,STORYSUMM: Evaluating Faithfulness in Story Summarization,Include,"Propose a dataset, show that one human annotation protocol is likely to miss inconsistencies, and recent automatic metrics do not perform well either",NLP,Summarization,,"Real task examples (e.g. GitHub issues), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)","['Human ratings', 'LLM-as-a-Judge', 'LLM post-processing']",,
100,Lennart Luettgau,zhengNEOBENCHEvaluatingRobustness2024,NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms,Include,"The performance of Large Language Models (LLMs) declines due to the temporal drift between the training data and newer texts, notably impacted by the emergence of neologisms. A resource of recent English neologisms is created and analyzed, revealing that introducing new words significantly reduces model performance in tasks like machine translation. To address this, a benchmark is constructed to evaluate LLMs' ability to handle neologisms across various natural language understanding tasks, showing that models trained on more recent data perform better and highlighting the complexity neologisms pose for static LLMs",Language Modelling,Updating,,Crowd-sourced task examples (e.g. Prolific-created tasks),['Crowd-sourced'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",simple mean/sum,['Mean']
101,Anna Sotnikova,pfisterSuperGLEBerGermanLanguage2024,SuperGLEBer: German Language Understanding Evaluation Benchmark,Include,"This is a broad NLU benchmark suite for the German language. The benchmark consists of 29 different tasks ranging over different types such as document classification, sequence tagging, sentence similarity, and question answering, on which 10 different German-pretrained models are evaluated.",NLP,Understanding,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Author-crafted', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],"simple mean, mean and std, averaging across multiple metrics","['Mean', 'Std']"
102,Lennart Luettgau,asthanaEvaluatingLLMsTargeted2024,Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts,Include,"NLP models are useful for aiding comprehension of complex texts from unfamiliar domains, but simplifying entire texts can remove important details. Targeted concept simplification helps readers understand difficult concepts within context, enhancing vocabulary and knowledge. The new WIKIDOMAINS dataset and preliminary benchmarks show human judges prefer explanations over simplifications for difficult concepts, with no single model excelling across all quality dimensions, highlighting the need for personalized reading comprehension support.",NLP,Summarization,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Author-crafted', 'Crowd-sourced']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Free response (e.g. summary paragarph),['Free response'],"Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Human ratings', 'LLM-as-a-Judge']","simple mean/sum, t-tests","['Mean', 'Tests']"
103,Lennart Luettgau,karpinskaOneThousandOne2024,One Thousand and One Pairs: A “novel” challenge for long-context language models,Include,"While synthetic long-context LLM benchmarks typically test surface-level retrieval, the NOCHA dataset assesses models' abilities to retrieve, synthesize, and reason over book-length texts. The dataset consists of true and false claim pairs about 67 recently-published English fictional books, requiring global reasoning for verification. Experiments show that human readers excel at this task, but long-context LLMs struggle significantly, with the highest accuracy from GPT-4O at 55.8%, indicating a need for improved models and methodologies for handling extensive world-building and complex narratives.",NLP,Long Context,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Author-crafted', 'Crowd-sourced']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],"simple mean/sum, GLMs","['Mean', 'Tests']"
104,Fangru Lin,zhaoQTSummQueryfocusedSummarization2023,QTSumm: Query-Focused Summarization over Tabular Data,Include,Propose a dataset. Show that the task is challenging. Propose a method to improve model performance.,Code Generation,,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Crowd-sourced', 'Another benchmark']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Free response (e.g. summary paragarph),['Free response'],"n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)","['Soft match', 'Human ratings', 'LLM post-processing']",,
105,Lennart Luettgau,suTextttConflictBankBenchmarkEvaluating2024,CONFLICTBANK: A Benchmark for Evaluating Knowledge Conflicts in Large Language Models,Include,"Large language models (LLMs) have made significant progress, but the issue of knowledge conflicts, which can lead to hallucinations, remains underexplored. To address this, CONFLICTBANK, a large benchmark of claim-evidence and QA pairs, is introduced to study conflicts arising from misinformation, temporal discrepancies, and semantic divergences. Through comprehensive experiments on various LLMs, the study provides insights into conflicts in retrieved and encoded knowledge, highlighting the importance of resolving these conflicts for developing trustworthy AI.",Factuality,,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']",Memorization score,['Exact match'],simple mean/sum,['Mean']
106,Lennart Luettgau,yangCRAGComprehensiveRAG2024,CRAG - Comprehensive RAG Benchmark,Include,"Retrieval-Augmented Generation (RAG) aims to improve Large Language Models (LLMs) by supplementing them with external knowledge, but existing datasets fail to capture the diverse and dynamic nature of real-world Question Answering (QA) tasks. The Comprehensive RAG Benchmark (CRAG) is introduced to address this gap, featuring 4,409 question-answer pairs and mock APIs for web and Knowledge Graph search, covering a wide range of domains and question types. Evaluation on CRAG shows that adding RAG improves LLM accuracy but still falls short of trustworthy QA, particularly with questions involving high dynamism, low popularity, or complexity, indicating key areas for future research.",Retrieval,,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Human ratings', 'LLM-as-a-Judge']",simple mean/sum,['Mean']
107,Harry Mayne,zhangSelenePioneeringAutomated2024,Selene: Pioneering Automated Proof in Software Verification,Include,A benchmark for automated proof generation in software verification.,Code Generation,,,Real task examples (e.g. GitHub issues),['Real task'],"Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)","['Random', 'Convenience']","Structured response (e.g. valid JSON, API call alone)",['Structured'],Generated proof verified by an independent prover system.,['Reward'],"Mean, ",['Mean']
108,Emma Beharry,gharaeeBIOSCAN5MMultimodalDataset2024,BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity,Include,"BIOSCAN-5M is a multimodal benchmark for insect classification and contains images, taxonomic labels, raw nucleotide barcode sequences, barcode index numbers, geographic location, and size metadata. The dataset is publicly available and includes data from novel species. The benchmark supports classification, zero-shot transfer learning, and retrieval learning.",Biology,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']","Accuracy is reported for classification in both open and closed-world settings. Fine-tuned accuracy and linear probing accuracy are reported in a closed-world setting, while 1NN-genus probing accuracy is reported in an open-world setting. AMI is reported for zero-shot transfer learning, and in multimodal retrieval learning, micro and macro top-1 accuracy is reported. ","['Mean', 'Other']"
109,Anna Gausen,coda-fornoCogBenchLargeLanguage2024,CogBench: a large language model walks into a psychology lab,Include,"CogBench is a benchmark that uses seven cognitive psychology experiments to evaluate LLMs by assessing their behavioral characteristics. CogBench provides ten behavioral metrics to phenotype LLM behavior e.g. model-based reasoning, exploration strategies, metacognition, and risk-taking tendencies. The benchmark is applied to 40 different LLMs.",Psychology,,,"Real task examples (e.g. GitHub issues), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Procedurally-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Free response (e.g. summary paragarph)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']",The metrics are averaged and normalized against human performance,['Mean']
110,Fangru Lin,liTEGDBComprehensiveDataset2024,TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs,Include,Present a large-scale dataset. Develop a pipeline for relevant research. Benchmark existing models on the dataset.,NLP,Extraction,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
111,Emma Beharry,mitaStrikingGoldAdvertising2024,Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation,Include,"CAMERA is a multimodal benchmark for automatic ad text generation (ATG)  in Japanese. The paper presents the first standardization and formalization of the ATG task, and the first ATG benchmark. The dataset was manually annotated, and the benchmark contains automatic and human evaluations. ",Business,,,Real task examples (e.g. GitHub issues),['Real task'],"Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)","['Random', 'Convenience']",Free response (e.g. summary paragarph),['Free response'],"n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM post-processing (extracting answers, reformatting for automated scoring), Distribution (perplexity, calibration, correlation)","['Soft match', 'Human ratings', 'LLM post-processing', 'Distribution']","BLEU-4, Rouge-1, BERTScore, Keyword Insertion Rates (KWD), Sentence Length Regulation Compliance Rates (REG), Pearson and Spearman Correlation for Human Evaluation",['Other']
112,Oishi Deb,jacoviChainofthoughtStrongIts2024,A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains,Include,"This paper introduces REVEAL, a benchmark dataset created to evaluate automatic methods for verifying step-by-step reasoning chains, specifically Chain-of-Thought (CoT) answers from language models in open-domain QA. REVEAL provides fine-grained annotations for each reasoning step, assessing its relevance, type (attribution or logic), factual correctness against evidence (attribution), and logical consistency with previous steps. The benchmark aims to support research in improving the reliability and correctness of LLM reasoning.",Reasoning,Logical,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'Crowd-sourced', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match']," Macro F1 score, per-class F1 score",['Mean']
113,Negar Foroutan,tanzerBenchmarkLearningTranslate2024,A Benchmark for Learning to Translate a New Language from One Grammar Book,Include,"This paper introduces MTOB, a benchmark for learning to translate between English and
Kalamang—a language with less than 200 speakers and therefore virtually no presence on the web—using several hundred pages of grammatical reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data. While LLM baselines do not yet match human performance, their experiments show a clear trend that increasing LLM quality and context window size improves translation quality.",Language Modelling,In-context Learning,,The examples are created by a linguist ,['Expert-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Free response (e.g. summary paragarph),['Free response'],"n-gram (BLEU, ROUGE, chrF)",['Soft match'],Unknown,['Unknown']
114,Oishi Deb,ribeiroSTREETMULTITASKSTRUCTURED2023,STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK,Include,"This paper introduces STREET, a unified multi-task benchmark designed to evaluate natural language reasoning and explanation capabilities. Unlike typical QA datasets, STREET requires models not only to answer questions but also to generate structured, step-by-step explanations (reasoning graphs) detailing the derivation process. Evaluations using T5 and GPT-3 indicate that current models struggle to produce accurate reasoning graphs, lagging behind human performance.",Reasoning,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Human exams', 'Real task', 'Author-crafted', 'Crowd-sourced', 'Another benchmark', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Convenience', 'Targeted']",Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall), Reasoning Graph Accuracy and Reasoning Graph Similarity based on graph edit distance and textual similarity e.g. BLEURT.","['Exact match', 'Soft match']","Answer Accuracy (Exact Match %), Reasoning Graph Accuracy (%), Reasoning Graph Similarity (%).",['Mean']
115,Emma Beharry,yangDataTalesBenchmarkRealworld2024,DataTales: A Benchmark for Real-World Intelligent Data Narration,Include,"DataTales is a novel benchmark designed to assess data narration of market movement data. It contains a human baseline and is publicly available. Specifically, DataTales assesses the proficiency of LLMs at performing lookups, comparisons, subtraction, rate of change, causal analysis, trend analysis, and predictive analysis to craft a financial report based upon market data. ",Data Analysis,,,Real task examples (e.g. GitHub issues),['Real task'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Soft match', 'Human ratings']","Factuality is calculated with Named Entity Recognition (NER) empowered accuracy, described in the paper. Style is measured with BLEU. Insightfulness is measured by human assessments based on impact (breadth of claim), and significance (magnitude of changes) on a 5 point Likert scale, and the average of the human review is reported. ",['Mean']
116,Oishi Deb,guptaTempTabQATemporalQuestion2023,TempTabQA: Temporal Question Answering for Semi-Structured Tables,Include,"This paper introduces TempTabQA, a new dataset designed for evaluating temporal question answering capabilities on semi-structured Wikipedia Infobox tables. The dataset includes over 11k QA pairs covering more than 90 domains. Experiments demonstrate that state-of-the-art models, including large language models, significantly underperform compared to humans, indicating the benchmark's difficulty and its potential to drive improvements in temporal reasoning.",Language Modelling,Updating,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Real task', 'Author-crafted', 'Crowd-sourced']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Meteor","['Exact match', 'Soft match', 'Soft match']","F1, EM, R1, R2, MET",['Mean']
117,Oishi Deb,liangSceMQAScientificCollege2024,SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark,Include,"This paper introduces SceMQA, a multimodal question answering benchmark focused on science subjects (Math, Physics, Chemistry, Biology) at the college entrance level. It aims to fill the difficulty gap between primary/middle school and college-level benchmarks. SceMQA includes multiple-choice and free-response questions, detailed solution explanations, and knowledge point labels. Evaluation of current MLLMs shows performance around 50-60% accuracy, indicating the benchmark's challenge.",General Science,,,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Human exams', 'Author-crafted']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Accuracy (%). Kappa score used for error analysis inter-rater reliability.,"['Mean', 'Other']"
118,Emma Beharry,zhangHumorAIMassive2024,Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning,Include,"The paper presents a New Yorker Caption Ranking Dataset, a novel multimodal human preference dataset for generating humorous cartoon captions. The paper presents additional novel evaluation methods to perform group comparisons between AI and human-generated cartoon captures, and leverages data from The New Yorker Caption Contest. The benchmark can be used to assess model-generated captions and support preference-based fine-tuning algorithms. ",NLP,Understanding,,"Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Real task', 'Crowd-sourced']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Free response (e.g. summary paragarph),['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Soft match', 'Human ratings', 'LLM-as-a-Judge']","Simple mean and variance on accuracy are used to assess the overall and best pick comparisons for cartoons, and expectation adjusted distinct N-grams (EAD) and Sentence-BERT embedding cosine similarity (SBERT) are used to assess caption diversity. ","['Mean', 'Std', 'Other']"
119,Oishi Deb,liMEQABenchmarkMultihop2024,MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations,Include,"This paper introduces MEQA, the first benchmark for multi-hop event-centric question answering, designed to evaluate reasoning over both events and entities. Using a novel semi-automatic strategy based on composing event structures from information extraction datasets, it created 2,243 challenging questions. Each question is paired with a multi-step QA-format explanation. Experiments show that MEQA is challenging for state-of-the-art models, including LLMs, which struggle with both answer accuracy and generating faithful explanations.",Reasoning,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Author-crafted', 'Crowd-sourced', 'Another benchmark', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Explanation Completeness P/R/F1, Explanation Logical Consistency %","['Exact match', 'Exact match']","Precision, Recall, F1 score, Completeness (P/R/F1), Logical Consistency (%).",['Mean']
120,Oishi Deb,hoWikiWhyAnsweringExplaining2023,WikiWhy: Answering and Explaining Cause-and-Effect Questions,Include,"This paper introduces WikiWhy, a question-answering dataset focused on evaluating LLM reasoning by requiring models to answer ""why"" questions and provide explicit natural language rationales explaining cause-and-effect relationships. Grounded in Wikipedia facts across 11 diverse topics, the dataset contains over 9,000 question-answer-rationale triples. Experiments with GPT-3 baselines show low correctness (38.7% human eval) for end-to-end answer and explanation generation, indicating significant room for improvement.",Reasoning,Commonsense,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Real task', 'Author-crafted', 'Crowd-sourced']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Convenience', 'Targeted']","Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Short free response', 'Free response']","n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), Unordered/Ordered BERT-F1 using DeBERTa-based BERTScore","['Soft match', 'Human ratings', 'LLM-as-a-Judge']","BERT-F1, ROUGE-L F1, Human Judgement Proportions (%), Pearson Correlation (r) for metric validation.",['Mean']
121,Oishi Deb,sunRevealingPersonalityTraits2024,Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues,Include,"This paper introduces Explainable Personality Recognition, a novel task requiring models to identify Big-Five personality traits from dialogues and provide supporting evidence. It proposes the Chain-of-Personality-Evidence (COPE) framework, reasoning from dialogue context to short-term states to long-term traits. Based on COPE, the PersonalityEvd dataset is constructed from dialogues, featuring annotated state/trait labels and detailed reasoning evidence. Experiments with LLMs show the task is challenging.",Psychology,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'Crowd-sourced', 'Another benchmark', 'LLM-generated']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Binary F1 for Evidence IDs","['Exact match', 'Soft match', 'Human ratings', 'LLM-as-a-Judge', 'Exact match']","Accuracy, F1 score, BERTScore F1, Average score (1-5 scale).",['Mean']
122,Ryan Kearns,zhangMultimodalSelfinstructSynthetic2024,Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model,Include,"The authors devise a synthetic data generation pipeline to generate a visual QA dataset on abstract images, like charts, dashboards, and 2D layouts. They find that LMMs struggle on basic QA tasks, like reading analog clocks. However, finetuning on their synthetic dataset yields minor improvements, including some transferred improvements to related benchmarks like ChartQA and MathVista.",Reasoning,Compositional,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Landmark Coverage Rate (LCR(%)) for route-planning","['Exact match', 'Soft match', 'Exact match']","simple mean/sum, percentage point improvements",['Mean']
123,Ryan Kearns,maExaminationCompositionalityLarge2024,An Examination of the Compositionality of Large Generative Vision-Language Models,Include,"The authors explore the different failure modes across evaluation methods for image composition understanding in multimodal models. They find via ablation that a popular metric, VisualGPTScore, is biased towards syntactical correctness in the caption over image contents. They compose a new benchmark, SADE, by combining debiased subsets of existing composition understanding benchmarks.",Reasoning,Compositional,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Free response (e.g. summary paragarph), Log-likelihood of a given free response","['Free response', 'Logits']","Distribution (perplexity, calibration, correlation), recall@1","['Distribution', 'Exact match']",simple mean/sum,['Mean']
124,Oishi Deb,huangMetaLogicLogicalReasoning2022,MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure,Include,"This paper proposes MetaLogic, a benchmark designed to evaluate models' logical reasoning by generating detailed explanations called ""logic metagraphs"". These metagraphs extend typical reasoning chains by including rebuttal conditions, internal logical formulae based on modal logic, and degrees of certainty for each statement. Based on 1,000 logical passages from the ReClor dataset, MetaLogic challenges models to produce these fine-grained structures. Experimental results show current models struggle significantly with this task.",Reasoning,Logical,,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Human exams', 'Author-crafted', 'Crowd-sourced', 'Another benchmark']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Convenience', 'Targeted', 'Criterion']","Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), Macro-F1 for the multi-class certainty prediction","['Exact match', 'Exact match']","F1 score, AllCorrect (Exact Match), Accuracy, Macro F1",['Mean']
125,Oishi Deb,leeQASAAdvancedQuestion2023,QASA: Advanced Question Answering on Scientific Articles,Include,"This paper introduces QASA, a benchmark for advanced question answering in scientific articles, motivated by the dual process theory of human reasoning. It proposes a three-stage approach (associative selection, evidential rationale-generation, systematic composition) to tackle ""full-stack reasoning"". The dataset contains 1798 QA pairs on AI/ML papers, featuring diverse question types (surface, testing, deep) and requiring answers composed from multiple evidential rationales. Experiments show the proposed approach outperforms InstructGPT, emphasising the importance of the rationale generation step.",Reasoning,Logical,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Real task', 'Author-crafted', 'Crowd-sourced']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Free response (e.g. summary paragarph),['Free response'],"n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Soft match', 'Human ratings']","Precision, Recall, F1, ROUGE-1, ROUGE-2, ROUGE-L, Human evaluation win/tie/lose rates (%).",['Mean']
126,Oishi Deb,mirzaeeSPARTQATextualQuestion2021,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,Include,"This paper introduces SPARTQA, a textual QA benchmark designed to evaluate spatial reasoning in language models, addressing limitations of prior datasets like bAbI Task 17. It includes SPARTQA-HUMAN, a set annotated by humans with more natural language and complex scenes (based on NLVR images), and SPARTQA-AUTO, a larger, automatically generated dataset using novel context-free grammar and spatial reasoning rules. Experiments show LMs perform poorly on SPARTQA-HUMAN but improve significantly after further pretraining on SPARTQA-AUTO.",Reasoning,Spatial,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Author-crafted', 'Crowd-sourced', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Convenience', 'Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Accuracy (%), F1 Score (%)",['Mean']
127,Oishi Deb,bhargavaDiscoSenseCommonsenseReasoning2022,DiscoSense: Commonsense Reasoning with Discourse Connectives,Include,"This paper introduces DISCOSENSE, a benchmark for commonsense reasoning that focuses on understanding various discourse connectives. The task requires selecting the most plausible sentence ending given a preceding context sentence and a specific discourse connective. The benchmark uses Conditional Adversarial Filtering, an extension of Adversarial Filtering, to generate difficult distractor options. Evaluations demonstrate that state-of-the-art language models find DISCOSENSE challenging, suggesting it's a valuable tool for assessing commonsense reasoning.",Reasoning,Commonsense,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Crowd-sourced', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Accuracy (%), Standard Deviation, Error Rate (%)","['Mean', 'Std']"
128,Emma Beharry,hsiehSugarCrepeFixingHackable2023,SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,Include,"SugarCrepe is a multimodal benchmark for multimodal compositional understanding. The benchmark specifically ensures that all hard negative (incorrect) descriptions in multiple-choice image-to-text retrieval tasks are fluent and plausible. The benchmark is publicly available, and the data was manually reviewed for quality control. ",Reasoning,Compositional,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Multiple choice,['Multiple choice'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],Reports average scores for commonsense Vera score gap and Grammar score gap. The paper also reports the pairwise better ratio between SugarCrepe and ARO+CREPE. ,['Mean']
129,Anna Sotnikova,renBEACONBenchmarkComprehensive2024,BEACON: Benchmark for Comprehensive RNA Tasks and Language Models,Include,"This paper presents BEACON, the first comprehensive benchmark for evaluating RNA language models across 13 tasks related to RNA structure, function, and engineering. It analyzes various models and components, highlighting the benefits of single nucleotide tokenization and ALiBi positional encoding, and introduces BEACON-B, a strong, resource-efficient baseline model.",Biology,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), Top L Precision, Top-k ACC, R^2, AUC, MCRMSE, Spearmann core","['Exact match', 'Correlation']",Mean and std,"['Mean', 'Std']"
130,Harry Mayne,maSpreadsheetBenchChallengingReal2024,SPREADSHEETBENCH: Towards Challenging Real World Spreadsheet Manipulation,Include,Agentic benchmark measuring whether LLMs can do real-world spreadsheet manipulation.,Code Generation,,,"Real task examples (e.g. GitHub issues), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Structured response (e.g. valid JSON, API call alone)",['Structured'],Execute the code and evaluate exact match of table vs ground truth table.,['Exact match'],Mean,['Mean']
131,Anna Sotnikova,guptaBiphoneModelingInter2023,Bi-Phone: Modeling Inter Language Phonetic Influences in Text,Include,"Many users are forced to use the web in a language they’re not fluent in (the second language (L2) ), often resulting in text errors influenced by their native language (L1). This work introduces Bi-Phone, a model that uses phoneme confusions between L1 and L2 to generate realistic corrupted text, evaluates its impact on language models with the new FunGLUE benchmark, and proposes a phoneme prediction task to improve model robustness.",NLP,Understanding,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
132,Jabez Magomere,zhuAreLargeLanguage2024,Are Large Language Models Good Statisticians?,Include,"This paper introduces the StatQA benchmark designed to evaluate LLMs’ proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods.",Data Analysis,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'Procedurally-generated', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
133,Lennart Luettgau,houWikiContradictBenchmarkEvaluating2024,WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia,Include,"Retrieval-augmented generation (RAG) helps mitigate limitations in large language models (LLMs), but how LLMs handle knowledge conflicts from equally trustworthy sources remains unclear. The WikiContradict benchmark, consisting of 253 high-quality, human-annotated instances, evaluates LLM responses to contradictory passages from Wikipedia. Evaluations reveal that while LLMs struggle to generate answers reflecting the conflicting nature of contexts, especially with implicit conflicts, an automated model achieves an F-score of 0.8 in estimating LLM performance, highlighting areas for further improvement.",Knowledge,Conflicts,,Real task examples (e.g. GitHub issues),['Real task'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Human ratings', 'LLM-as-a-Judge']",simple mean/sum,['Mean']
134,Harry Mayne,konIaCevalCodeGeneration2024,IaC-Eval: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs,Include,Evaluating LLMs ability to generate Infrastructure-as-Code (IaC) code (part of cloud computing),Code Generation,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Unknown,['Unknown'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],Functional correctness checks. Evaluated by (1) producing a dependency graph from the code (2) using an IaC policy engine to check whether the instruction specification are in the program.,['Reward'],"Mean, ",['Mean']
135,Harry Mayne,waghjaleECCOCanWe2024,ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?,Include,Evaluating the efficiency of LLM generated code.,Code Generation,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues)","['Human exams', 'Real task']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Code ""Speedup"" and ""Memory Reduction"" versus reference solutions.",['Reward'],"Mean, variance","['Mean', 'Std']"
136,Lennart Luettgau,wuClashEvalQuantifyingTugofwar2024,ClashEval: Quantifying the tug-of-war between an LLM√¢‚Ç¨‚Ñ¢s internal prior and external evidence,Include,"Retrieval-augmented generation (RAG) aims to reduce hallucinations and update knowledge in large language models (LLMs). A study with over 1,200 questions across six domains examines how LLMs handle correctly and incorrectly retrieved content. Findings show LLMs often adopt wrong retrieved information, especially if they lack confidence in their initial response, but are less likely to accept highly unrealistic content, presenting a significant challenge and benchmark for improving LLM accuracy when faced with conflicting information.",Knowledge,Conflicts,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Crowd-sourced', 'Another benchmark']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean/sum,['Mean']
137,Lennart Luettgau,pressCiteMECanLanguage2024,CiteME: Can Language Models Accurately Cite Scientific Claims?,Include,"With thousands of new scientific papers published monthly, staying updated and accurately attributing claims is challenging. The CiteME benchmark evaluates the ability of large language models (LLMs) to identify cited papers in text excerpts from recent machine learning papers, highlighting a significant gap between human performance (69.7% accuracy) and LLMs (4.2-18.5% accuracy). Introducing CiteAgent, an autonomous system built on GPT-4o that searches and reads papers, bridges this gap by achieving 35.3% accuracy, moving towards better automatic verification of claims made by LMs.",Agents,General,,Real task examples (e.g. GitHub issues),['Real task'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean/sum,['Mean']
138,Anna Gausen,jinShoppingMMLUMassive2024,Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models,Include," Shopping MMLU is a comprehensive benchmark for evaluating how large language models (LLMs) perform on online shopping tasks.  The authors transformed online shopping tasks into a text-to-text format suitable for LLMs, evaluated over 20 different models, and analyzed performance patterns.",Agents,Web Agent,,Real task examples (e.g. GitHub issues),['Real task'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",simple average ,['Mean']
139,Jabez Magomere,zhuangToolQADatasetLLM2023,ToolQA: A Dataset for LLM Question Answering with External Tools,Include,"This paper introduces ToolQA, which is designed to faithfully evaluate LLMs’ ability to use external tools for question answering as compared to just retrieving from memorization. ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions.",Agents,Tool Use,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
140,Lennart Luettgau,chenCopyBenchMeasuringLiteral2024,CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation,Include,"COPYBENCH is introduced to evaluate both literal and non-literal reproduction of copyrighted content by language models (LMs), addressing a gap where previous research only considered literal similarities. Using copyrighted fiction books, COPYBENCH assesses literal and non-literal copying, finding that while literal copying is rare, non-literal copying, such as event and character copying, is more prevalent, especially in larger models. The benchmark reveals that training-time alignment can reduce literal copying but may increase non-literal copying, and current inference-time methods are more effective for literal copying than for non-literal copying, highlighting areas for improvement in copyright mitigation strategies.",Language Modelling,Copyright,,Text snippets from books,['Author-crafted'],,['Unknown'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Human ratings', 'LLM-as-a-Judge']",simple mean/sum,['Mean']
141,Lennart Luettgau,ajithLitSearchRetrievalBenchmark2024,LitSearch: A Retrieval Benchmark for Scientific Literature Search,Include,"iterature search questions often require deep understanding and reasoning across research articles, posing challenges for modern search engines. LitSearch, a new benchmark with 597 literature search queries about recent ML and NLP papers, is introduced to address these challenges. The benchmark, constructed from GPT-4 generated and manually written questions, reveals a significant performance gap between traditional retrieval models like BM25 and state-of-the-art dense retrievers, with LLM-based reranking further improving retrieval performance, highlighting the limitations of commercial search engines on these complex queries.",General Science,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Human ratings', 'LLM-as-a-Judge']",simple mean/sum,['Mean']
142,Lujain Ibrahim,heMedEvalMultilevelMultitask2023,"MEDEVAL: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation",Include,"MEDEVAL is a multi-level, multi-task, and multi-domain medical benchmark. The paper collects data from several healthcare systems and annotations from experts. It evaluates generic and domain-specific language models under zero-shot and fine-tuned settings.",Medicine,,,,['Unknown'],Random sample (creators defined a task space and sampled from it),['Random'],"Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), They use an ambiguity classifier as well from previous work","['Exact match', 'Soft match', 'LLM-as-a-Judge']",simple mean/sum,['Mean']
143,Fangru Lin,duPAGEDBenchmarkProcedural2024,PAGED: A Benchmark for Procedural Graphs Extraction from Documents,Include,"Propose a dataset. Find that baseline models cannot extract optimal procedural graphs well, and that LLMs have advantages in building relevant structures.",NLP,Extraction,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",,
144,Jabez Magomere,zhangToolBeHonestMultilevelHallucination2024,ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models,Include,"This paper presents ToolBH, a benchmark designed to diagnose hallucinations in tool-augmented large language models (LLMs). Hallucinations are evaluated from two dimensions: depth, using a multi-level evaluation framework, and breadth, encompassing three distinct scenarios that are likely to induce hallucinations. The authors developed seven tasks and curated 700 evaluation samples through multiple rounds of manual annotation.",Agents,Tool Use,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
145,Anna Gausen,yeBenchmarkingLlmsUncertainty2024,Benchmarking LLMs via Uncertainty Quantification,Include,This paper introduces a new benchmarking approach for Large Language Models that incorporates uncertainty quantification using conformal prediction across five NLP tasks. ,Language Modelling,Calibration,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Multiple choice, ","['Multiple choice', '']","Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']",mean,['Mean']
146,Anna Gausen,guoWhatCanLarge2023,What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks,Include," The paper develops a benchmark to  assess the capabilities of five LLMs on chemistry, using eight chemistry tasks requiring understanding, reasoning, and explanation abilities.",Chemistry,,,Real task examples (e.g. GitHub issues),['Real task'],"Random sample (creators defined a task space and sampled from it), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",mean and standard dev,"['Mean', 'Std']"
147,Fangru Lin,wangCanLanguageModels2023,Can Language Models Solve Graph Problems in Natural Language?,Include,Propose a dataset. Evaluate LLMs with different prompting approaches. Propose new approaches to boost LLM performance.,NLP,Extraction,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), partial credit","['Exact match', 'Soft match']",,
148,Anna Gausen,wuStreamBenchBenchmarkingContinuous2024,StreamBench: Towards Benchmarking Continuous Improvement of Language Agents,Include,"StreamBench is a benchmark designed to evaluate language agents' ability to improve over time through feedback. The authors propose a novel evaluation setting where language models must continuously learn from an input-feedback sequence, with the goal of maximizing accuracy across a range of tasks.",NLP,Updating,,Real task examples (e.g. GitHub issues),['Real task'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",mean,['Mean']
149,Angelika Romanou,piUOUOUncontextualizedUncommon2024,UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models,Include,"VLMs ability to handle rare objects, which fall into the long tail of data distributions, is less studied in the current literature. . To rigorously evaluate this aspect, the authors introduce the ""Uncontextualized Uncommon Objects"" (UOUO) benchmark which focuses on systematically testing VLMs on rare and specialized objects.",Grounding,,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']",Random sample (creators defined a task space and sampled from it),['Random'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Mean IoU (Intersection over Union)","['Exact match', 'Soft match']",simple mean,['Mean']
150,Lujain Ibrahim,billahnagoudiJASMINEArabicGPT2023,JASMINE: Arabic GPT Models for Few-Shot Learning,Include,"The paper introduces a suite of Arabic autoregressive Transformer language models ranging in size and pre-trained on a large and diverse dataset. It also introduces a benchmark for automated and human evaluation of Arabic autoregressive models, with coverage of social biases, harms, and toxicity.",NLP,,Multilinguality,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Convenience', 'Targeted']","Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), Distribution (perplexity, calibration, correlation)","['Exact match', 'Human ratings', 'Distribution']",simple mean/sum,['Mean']
151,Cornelius Emde,changDrspiderDiagnosticEvaluation2023,Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness,Include,The paper proposes Dr Spider a text-to-SQL robustness benchmark. The authors adapt the Spider benchmark by introducing various perturbations and measuring drop in model performance. ,Code Generation,Natural Language,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Crowd-sourced', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)","['Random', 'Convenience']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), The code is executed and results are verified against ground truth results","['Exact match', 'Reward']",No statistical methods used. just simple mean and differences in means.,['Mean']
152,Anna Gausen,zengEvaluatingLargeLanguage2024,Evaluating Large Language Models at Evaluating Instruction Following,Include,"This paper introduces LLMBAR, a benchmark specifically designed to evaluate how well LLM evaluators can assess instruction following in LLM outputs. This benchmark tries to evaluate whether ""LLM evaluators"" themselves can reliably judge how well models follow instructions.",Instruction Following,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"simple mean and for rating-based evaluations they measure ""hedging rate""",['Mean']
153,Anna Gausen,xuMAgICInvestigationLarge2024,"MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration",Include,The paper presents a benchmark called MAGIC  that is designed to evaluate Large Language Models (LLMs) in multi-agent settings. It evaluates LLMs' capabilities in multi-agent environments through competition-based scenarios and defines seven metrics to measure.,Agents,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response)","['Free response', 'Interaction']","Exact Match (accuracy, F1, precision, recall), Win rate","['Exact match', 'Reward']",simple mean to aggregate performance over scenarios and roles,['Mean']
154,Fangru Lin,yanComprehensiveStudyTextattributed2023,A Comprehensive Study on Text-attributed Graphs: Benchmarking and Rethinking,Include,Propose a dataset. Conduct extensive benchmarking experiments on a wide range of models. Propose topological training.,NLP,Extraction,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
155,Fangru Lin,liCanLargeLanguage2024,"Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models",Include,Propose datasets. Show that LLMs cannot process graphs well. Use the dataset to boost LLM performance.,NLP,Extraction,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Crowd-sourced', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']",,
156,Fangru Lin,zhangDTGBComprehensiveBenchmark2024,DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs,Include,Propose a dataset. Benchmark popular algorithms with this dataset and showcase the limitations of current models in handling dynamic text-attributed graphs.,NLP,Extraction,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
157,Fangru Lin,huangEmbraceDivergenceRicher2024,Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles,Include,Present a text summarization dataset for articles of diverse opinions towards same events and schema to find them. Present LLM-based evaluation methods for this dataset. Show that LLMs can well summarize single documents but fail to do so for multiple.,NLP,Summarization,,"Real task examples (e.g. GitHub issues), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Human ratings (text quality, preference, NOT manual scoring of other metrics)",['Human ratings'],,
158,Fangru Lin,amarOpenAspBenchmarkMultidocument2023,OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization,Include,Present a dataset for multi-document open aspect-based summarization. Show the dataset is of high quality and it presents challenge to LLMs.,NLP,Summarization,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Crowd-sourced', 'Another benchmark']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"n-gram (BLEU, ROUGE, chrF)",['Soft match'],,
159,Ryan Kearns,luMathVistaEvaluatingMathematical2024,MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts,Include,"The paper introduces MathVista, a benchmark for mathematical reasoning in visual contexts. This includes algebraic/arithmetic/geometric reasoning as well as interpreting functional plots and chart data. MathVista combines math questions from 28 existing multimodal datasets, plus 3 new datasets hand-annotated from internet sources.",Reasoning,Mathematics,,"Human exam questions (e.g. GRE questions), Modified from another benchmark (e.g. translation into another language)","['Human exams', 'Another benchmark']","Random sample (creators defined a task space and sampled from it), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Criterion']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",simple mean/sum,['Mean']
160,Fangru Lin,liuRevisitingGoldStandard2023,Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,Include,"Propose a modified summarization salience protocol, curate the Robust Summarization Evaluation (RoSE) benchmark, conduct a comparative study of human evaluation protocols. Evaluate 50 automatic metrics and their variants and demonstrate how the benchmark leads to more statistically stable and significant results.",NLP,Summarization,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Crowd-sourced', 'Another benchmark']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
161,Fangru Lin,cheangCanLMsGeneralize2023,Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization,Include,Propose a novel benchmark. Show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Discuss recommendations to the research community. ,NLP,Summarization,,Real task examples (e.g. GitHub issues),['Real task'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Human ratings', 'LLM-as-a-Judge']",,
162,Lujain Ibrahim,liNewsBenchSystematicEvaluation2024,NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism,Include,"The paper introduces a benchmark to evaluate LLM capabilities in Chinese journalism, with a focus on writing proficiency and safety adherence. It also proposes several GPT-4 based automated evaluation protocols and uses the benchmark to evaluate popular LLMs that can handle Chinese.",NLP,,,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']",,
163,Anna Gausen,dengMobilebenchEvaluationBenchmark2024,Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents,Include,"Mobile-Bench is a novel benchmark for evaluating LLM agents' capabilities in mobile device interactions. It  creates a more realistic environment for benchmarking that combines API and UI operations, evaluates multi-app coordination, and introduces more nuanced evaluation metrics.",Agents,Web,,"Real task examples (e.g. GitHub issues), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)","['Interaction', 'Structured']","Exact Match (accuracy, F1, precision, recall), checkpoint coverage","['Exact match', 'Exact match']",simple mean,['Mean']
164,Anna Sotnikova,romeroCVQACulturallydiverseMultilingual2024,CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark,Include,"The paper introduces CVQA2, a culturally diverse multilingual Visual Question Answering benchmark that includes 10,000 questions across 31 languages and 30 countries, incorporating input from native speakers and cultural experts.  The benchmark measures cultural understanding and multilingual visual question answering.",Knowledge,Cultural,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Expert-crafted', 'Crowd-sourced', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
165,Anna Gausen,shaoNYUCTFBench2024,NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security,Include,The benchmark aims to assess the capability of LLMs in solving CTF challenges autonomously.  The NYU CTF Bench includes  CTF challenges from NYU’s CSAW cybersecurity events.,Agents,Web,,"Human exam questions (e.g. GRE questions), Expert-crafted task examples (e.g. hand-written examples)","['Human exams', 'Expert-crafted']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Interaction', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
166,Ryan Kearns,chenAreWeRight2024,Are We on the Right Way for Evaluating Large Vision-Language Models?,Include,"The authors review general multi-modal capability benchmarks and finds problems to do with data leakage and questions answerable without any visual input. They automatically and then manually filter instances from these benchmarks, resulting in MMStar, a ""vision-indispensible"" multi-modal benchmark. Evaluation and ablation studies show that MMStar mitigates leakage better than existing benchmarks.",VQA,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), Custom metrics: multi-modal gain, multi-modal leakage","['Exact match', 'Exact match']","simple mean/sum, plus comparisons to scores from the base LLMs comprising the multi-modal models (called ""multi-modal gain"" and ""multi-modal leakage"" statistics)",['Mean']
167,Emma Beharry,dumpalaSUGARCREPEDatasetVisionlanguage2024,SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations,Include,"SugarCrepe++ is a multimodal benchmark for evaluating semantic and lexical understanding. The benchmark improves upon prior compositional reasoning tasks by having the model choose between two semantically equivalent but lexically dissimilar correct captions, and one lexically similar but semantically dissimilar hard negative caption for an image. The benchmark is publicly available, human-validated, and can be used to evaluate multi-modal and unimodal LLMs. ",Language Modelling,Robustness,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
168,Ryan Kearns,jiLargeLanguageModels2024,Large Language Models as Automated Aligners for benchmarking Vision-Language Models,Include,"The authors utilise LLMs to produce question-answer-reasoning triplets from COCO images. The result is Auto-Bench, a general multi-modal capability and value alignment benchmark dataset.",Alignment,Alignment,,"Crowd-sourced task examples (e.g. Prolific-created tasks), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Crowd-sourced', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']",simple mean/sum,['Mean']
169,Maria Grandury,caoWorstPromptPerformance2024,On the Worst Prompt Performance of Large Language Models,Include,"This paper introduces RobustAlpacaEval, a benchmark to evaluate the worst-case prompt performance of LLMs across semantically equivalent real-world queries. It shows that ChatGPT and six open-source LLMs from the Llama, Mistral, and Gemma families are highly sensitive to prompt phrasing, that characterizing the worst prompt is difficult, and that common techniques for improving prompt robustness offer limited success.",Language Modelling,Robustness,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],"Mean, worst and best out of 11",['Mean']
170,Angelika Romanou,liFIREDatasetFeedback2024,FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models,Include,"This paper introduces FIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn conversations that are derived from 27 source datasets, empowering VLMs to spontaneously refine their responses based on user feedback across diverse tasks. The authors also develop the FIRELLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, and they show remarkable feedback-refining capability, outperforming untrained VLMs by 50%.",Language Modelling,Updating,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response)","['Multiple choice', 'Short free response', 'Free response', 'Interaction']","n-gram (BLEU, ROUGE, chrF), For dialogue assessment, they introduce four metrics: average turn (AT), average dialogue refinement (ADR), average turn refinement (ATR), and refinement ratio (RR).","['Soft match', 'Reward']",simple mean,['Mean']
171,Ryan Kearns,huangEffiBenchBenchmarkingEfficiency2024,EffiBench: Benchmarking the Efficiency of Automatically Generated Code,Include,"The paper introduces EffiBench, a benchmark of LeetCode problems designed to assess the time and memory efficiency of LLM-written programs. Problems are filtered from HuggingFace to problems corresponding to conventional algorithmic problem types (DFS, binary search, greedy, ...).",Code Generation,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues)","['Human exams', 'Real task']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']",Execution time and memory usage efficiency; unit test correctness,['Reward'],simple mean/sum,['Mean']
172,Cornelius Emde,chaoJailbreakBenchOpenRobustness2024,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Include,"The paper proposes a benchmark for jailbreaking LLMs (i.e. eliciting harmful content through adversarial attacks). They provide a dataset, python package and leaderboard. Each score of the benchmark is a combination of Model + Defense + Thread Model.",Alignment,Alignment,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Expert-crafted', 'Another benchmark']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],simple mean. no inferential statistics (even though the LLM-as-judge have fairly low accuracy).,['Mean']
173,Ryan Kearns,wangPictureWorthThousand2024,Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models,Include,"This paper introduces SpatialEval, a multimodal spatial reasoning dataset with four subtasks. SpatialEval tasks include map reading, maze navigation, locating objects on a grid, and QA from captioned images. Ablation studies show that LVLMs primarily use text over visual cues when provided.",Reasoning,Spatial,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Unclear","['Author-crafted', 'Unknown']",Unknown,['Unknown'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"mean with ""error bars from 3 runs at temperature 0.2"" (unsure if this is a standard error or just the range in scores) (17)","['Mean', 'Std']"
174,Oishi Deb,kasaiRealTimeQAWhats2023,RealTime QA: What's the Answer Right Now?,Include,"This paper introduces REALTIME QA, a dynamic question answering platform that evaluates systems' ability to answer questions about the current world. New questions requiring up-to-date information are released weekly. The paper presents the platform and evaluates strong baselines built on large language models (like GPT-3 and T5) combined with information retrieval (web search, DPR). Results highlight the importance of timely retrieval but also show models may provide outdated answers when retrieval is insufficient.",NLP,Updating,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Real task', 'Author-crafted']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Exact Match (EM), F1 Score (%)",['Mean']
175,Maria Grandury,akhbariSETLEXSEMCHALLENGEUsing2024,SETLEXSEM CHALLENGE: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models,Include,"This paper introduces a synthetic benchmark designed to evaluate the robustness of large language models (LLMs) in performing set operations under lexical and semantic variation.
The benchmark systematically alters input features like token type, length, frequency, and semantic similarity to test LLMs' ability to generalize across incidental variations, i.e. their System 2 robustness.",Language Modelling,Robustness,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Convenience', 'Targeted']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Mean and standard deviation
",['Mean']
176,Oishi Deb,tanBenchmarkingImprovingTemporal2023,Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models,Include,"This paper introduces TEMPREASON, a comprehensive probing dataset designed to evaluate the temporal reasoning capabilities of LLMs across three hierarchical levels (Basic, Advanced, Complex) grounded in Allen's interval algebra. The paper also proposes a novel learning framework involving temporal span extraction and time-sensitive reinforcement learning to enhance LLM temporal reasoning. Experiments show TEMPREASON is challenging for current LLMs, and the proposed framework effectively improves performance.",Reasoning,Temporal,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Crowd-sourced', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Accuracy (%),['Mean']
177,Ryan Kearns,wangJourneyBenchChallengingOnestop2024,JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images,Include,JourneyBench is a diverse vision-language understanding benchmark using AI-generated images from the Midjourney platform. These images are paired with text prompts for QA tasks through various extensive human annotation and human-machine-in-the-loop filtering systems. The resulting benchmark is substantially harder than multimodal benchmarks that use common images from COCO or Flickr.,Retrieval,,,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'Author-crafted', 'Expert-crafted', 'Crowd-sourced', 'Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'Soft match', 'LLM post-processing']",simple mean/sum,['Mean']
178,Oishi Deb,luLearnExplainMultimodal2022,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,Include,"This paper introduces SCIENCEQA, a large-scale multimodal benchmark featuring ~21k multiple-choice science questions (grades 1-12) across diverse topics. Questions can involve text, images, or both. Uniquely, each question is annotated with a detailed explanation comprising relevant background knowledge (lecture) and step-by-step reasoning (solution). The authors propose using language models to generate these explanations as Chains-of-Thought (CoT) and demonstrate that this process significantly improves answer accuracy.",Reasoning,,,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Human exams', 'Author-crafted', 'Crowd-sourced']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Convenience', 'Targeted']","Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']","Accuracy (%), BLEU-4, ROUGE-L",['Mean']
179,Maria Grandury,chenDrAcademyBenchmarkEvaluating2024,Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models,Include,"This paper introduces Dr.Academy, a benchmark for evaluating the question generation capabilities of LLMs in educational contexts. It evaluates questions generated by LLMs across general, monodisciplinary, and interdisciplinary domains using a cognitive framework based on Anderson and Krathwohl’s taxonomy. The quality of LLM's output is evaluated by automatic metrics which correlate with human scores.",Education,,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],"Simple mean to aggregate automatic scores, Pearson and Spearman correlation between human and automatic ratings​, and Krippendorff’s Alpha inter-rater agreement for human ratings.","['Mean', 'Other']"
180,Ryan Kearns,lyuMMScanMultimodal3D2024,MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations,Include,"The paper develops MMScan, a benchmark of 3D scenes that tests spatial and attribute understanding via visual grounding and QA tasks. 3D scene data from an existing dataset is annotated with a human-machine-in-the-loop setup, and human annotators create questions from these annotations.",Grounding,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Crowd-sourced', 'Another benchmark']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring), 3D IoU-based Average Precision","['Exact match', 'Soft match', 'LLM post-processing', 'Soft match']",simple mean/sum,['Mean']
181,Simeng Han,heExploringCapacityPretrained2023,Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change,Include,"The paper introduces four core Reasoning about Actions and Change (RAC) tasks as a unified textual benchmark, carefully designed to minimize confounding linguistic factors (e.g., grounding) and maintain a sharp focus on RAC. The resulting benchmark, TRAC (Textual Reasoning about Actions and Change), includes problems of varying complexity and enables more fine-grained evaluation of language models, with an emphasis on assessing their structural generalization capabilitie — crucial for effective RAC.",Reasoning,Planning,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
182,Maria Grandury,edmanCUTEMeasuringLLMs2024,CUTE: Measuring LLMsâ€™ Understanding of Their Tokens,Include,"The paper introduces CUTE, a benchmark designed to test the orthographic knowledge of large language models (LLMs), specifically their understanding of the character composition of tokens. It evaluates multiple LLMs on tasks requiring spelling, character-level similarity, and text manipulation.",NLP,,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
183,Ryan Kearns,tianSciCodeResearchCoding2024,SciCode: A Research Coding Benchmark Curated by Scientists,Include,"SciCode is a benchmark consisting of multi-step scientific code generation problems. Scientists curate code implementations from published research in their field and write test cases for python implementations of these problems. Frontier reasoning models evaluated on SciCode struggle to achieve double-digit accuracy on the most ""realistic"" evaluation setup.",Code Generation,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples)","['Author-crafted', 'Expert-crafted']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), Unit test cases","['Exact match', 'Reward']","mean/sum, where problem correct means all subproblems must be correct",['Mean']
184,Ryan Kearns,liWhenLlmsMeet2024,When LLMs Meet Cunning Texts: A Fallacy Understanding Benchmark for Large Language Models,Include,"FaLlacy Understanding Benchmark (FLUB) contains multiple choice, classification, and explanation questions about ""cunning texts."" These are snippets from posts on a Chinese online forum, which human annotators filter and then annotate with multiple choice questions, a ""cunning type"" classification, and an explanation.",Reasoning,Logical,,Crowd-sourced task examples (e.g. Prolific-created tasks),['Crowd-sourced'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']",simple mean/sum,['Mean']
185,Ryan Kearns,wangNeedleMultimodalHaystack2024,Needle In A Multimodal Haystack,Include,"MM-NIAH is a multimodal (image+text) variant of the conventional ""needle in a haystack"" task in NLP. Authors concatenate documents from OBELICS to produce long-context documents, embed sentences into text or artifacts into images, and prompt MLLMs to answer questions about these ""needles.""",NLP,Long Context,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), Soft Accuracy for counting task","['Exact match', 'Soft match']",simple mean/sum,['Mean']
186,,yuMMvetEvaluatingLarge2024,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,Include,"MM-Vet (MultiModal Veterinarian) is a general benchmark for vision-language capabilities, emphasising the integration of multiple capabilities per problem. Questions are sourced from ""various online sources"" and authors-hand annotate most of the answers. As questions are often open-ended and diverse, the benchmark uses LLM-scoring with GPT-4 as a judge.",VQA,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Author-crafted', 'Crowd-sourced']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],mean and variance,"['Mean', 'Std']"
187,Ryan Kearns,zhangMIntRec20LargescaleBenchmark2024,MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations,Include,"MIntRec2.0 is a multimodal dataset (image, text, and audio) to assess intent recognition. Scenes are sourced from TV shows with corresponding subtitles, and models must match one of 30 defined intent classes.",NLP,Understanding,,Produced media (TV sitcom scenes),['Author-crafted'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean/sum,['Mean']
188,Anna Gausen,shenTaskBenchBenchmarkingLarge2024,TaskBench: Benchmarking Large Language Models for Task Automation,Include,"TaskBench is a framework for evaluating how well large language models (LLMs) can automate complex tasks. It addresses three stages of task automation: task decomposition, tool selection and parameter prediction. It introduces Tool Graph - a novel representation of tools and their dependencies.",Agents,Tool Use,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Correlation (Matthew's correlation, Pearson's r)","['Exact match', 'Soft match', 'Correlation']",simple means to report F1 scores and ROUGE metrics,['Mean']
189,Harry Mayne,huangDAcodeAgentData2024,DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models,Include,A code generation benchmark specifically for agent-based data science tasks.,Code Generation,,Data Science,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Real task', 'Author-crafted']","Convenience sample (creators found a set of tasks that was readily accessible), Unknown","['Convenience', 'Unknown']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), Execution-based evaluation. e.g. run the agent's code and see if it matches the ground-truth results. Plus different rubrics for each task.","['Exact match', 'Reward']","Mean,",['Mean']
190,Angelika Romanou,parcalabescuVALSETaskindependentBenchmark2022,VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena,Include,"This paper proposes VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. The authors cover a broad spectrum of basic linguistic phenomena affecting the linguistic and visual modalities.  The overall weak performance of these models indicates that there is a need for a reliable foiling dataset targeting the visual grounding capabilities of V&L models through the lens of linguistic constructs.",VQA,,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
191,Yushi Yang,huangConMeRethinkingEvaluation2024,ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs	,Include,"We introduce ConMe – a compositional reasoning (CR) benchmark and a novel data generation pipeline leveraging VLMs to produce ‘hard CR Q&A’.

Our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark. 

Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.",Reasoning,Compositional,,LLM- and VLM- generated task examples,['LLM-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
192,Harry Mayne,liEvoCodeBenchEvolvingCode2024,EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations,Include,Code generation benchmark with evolving questions (updated every 6 months),Code Generation,,,Real task examples (e.g. GitHub issues),['Real task'],"Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']","Structured response (e.g. valid JSON, API call alone)",['Structured'],Execution-based / functional correctness. Pass unit tests.,['Reward'],"Mean, ",['Mean']
193,Harry Mayne,gongEvaluationLLMsSyntaxaware2024,Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks,Include,"Create a ""Fill-in-the-Middle"" code benchmark for LLMs and uses it to make claims about effective pretraining in code LLMs.",Code Generation,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues)","['Human exams', 'Real task']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],Execution-Based Evaluation (unit tests),['Reward'],"Mean, ",['Mean']
194,Angelika Romanou,bhatiaLocalConceptsUniversals2024,From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models,Include,"Current benchmarks often overlook a crucial aspect of cultural diversity: how universal concepts are represented across cultures. GlobalRG addresses this gap with two tasks inspired by popular vision-and-language benchmarks, image-text retrieval and visual grounding. Extensive evaluations reveal notable cross-cultural discrepancies. In particular, they show that even when models retrieve or ground images that appear culturally diverse, those images frequently share underlying Western-centric elements.",Knowledge,Cultural,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), Diversity@k: that measures the cultural diversity among the retrieved images, helping to identify models’ bias towards specific countries or regions.","['Exact match', 'Distribution']",simple mean,['Mean']
195,Anna Sotnikova,kannenAestheticsCulturalCompetence2024,Beyond Aesthetics: Cultural Competence in Text-to-Image Models,Include,"This work introduces CUBE, the first benchmark designed to evaluate the cultural competence of Text-to-Image (T2I) models through the lenses of cultural awareness and cultural diversity. Using structured knowledge bases and large language models, the authors construct a scalable framework and dataset spanning 8 countries and 3 cultural domains (cuisine, landmarks, and art), revealing major cultural representation gaps in current T2I systems.",Knowledge,Cultural,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Expert-crafted', 'Procedurally-generated', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Human ratings', 'LLM-as-a-Judge']",simple mean + std,"['Mean', 'Std']"
196,Harry Mayne,yangInterCodeStandardizingBenchmarking2023,InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback,Include,A standard coding benchmark with an interactive environment. An early agentic coding benchmark.,Agents,Coding,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)","['Interaction', 'Structured']",Execution-based evaluation (unit tests),['Reward'],"Mean, standard errors","['Mean', 'Std']"
197,Harry Mayne,yanCodeScopeExecutionbasedMultilingual2024,CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,Include,"Evaluation of code understanding and generation capacities. ""An execution-based, multilingual, multitask, multidimensional evaluation benchmark""",Code Generation,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Human exams', 'Real task', 'Another benchmark']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Execution-based metrics.","['Exact match', 'Soft match', 'Reward']","Mean, standard deviation","['Mean', 'Std']"
198,Hazel Kim,zhangCarefulExaminationLarge2024,A Careful Examination of Large Language Model Performance on Grade School Arithmetic,Include,"The paper introduces GSM1k, that mirros GSM8K-style but guaranteed absent from model pre-training to show the LLM genuine reasoning capabilities rather than memorization. LLMs stumble on it compared with GSM8k, exposing memorization in many, though frontier models still generalize well.",Reasoning,Mathematics,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Expert-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'Human ratings', 'LLM post-processing']","Mean, Spearman/Pearson Correlations
(For completeness, they also report the standard Pearson but also mention that Pearson is not the ideal metric due to the curve-of-best-fit not appearing linear.)","['Mean', 'Other']"
199,Yilun Zhao,wuEvaluatingAnalyzingRelationship2024,Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models,Include,"This work introduces R-Bench, a new benchmark designed to evaluate hallucinations in inter-object relationships. It identifies three key sources of hallucination and reveals that LVLMs often ignore visual input, depend too heavily on language priors, and struggle with spatial reasoning due to long-tail distribution biases in training data.",VQA,,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
200,Angelika Romanou,liNaturalBenchEvaluatingVisionlanguage2024,NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples,Include,"This paper introduces NaturalBench to evaluate vision-language models on their natural adversarial samples, i.e. samples that challenge models significantly more than humans. NaturalBench offers comprehensive skill tags to assess compositional reasoning abilities and highlights model biases in VLMs.",Language Modelling,Robustness,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
201,Lujain Ibrahim,yinSafeWorldGeodiverseSafety2024,SafeWorld: Geo-Diverse Safety Alignment,Include,"The paper introduces a benchmark to evaluate LLMs' ability to generate culturally sensitive and legally compliant responses across diverse global contexts. It also proposes a multi-dimensional automatic safety evaluation framework for assessing the contextual appropriateness, accuracy, and comprehensiveness of responses. ",Alignment,Safety,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)","['LLM-as-a-Judge', 'LLM post-processing']",simple mean,['Mean']
202,Harry Mayne,liuRepoBenchBenchmarkingRepositorylevel2024,RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems,Include,Evaluating whether models can do repository-level code generation.,Code Generation,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Real task', 'Author-crafted']","Random sample (creators defined a task space and sampled from it), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Criterion']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']","mean,",['Mean']
203,Jabez Magomere,zhangBenchmarkingDataScience2024,Benchmarking Data Science Agents,Include,"This paper introduces DSEval, a novel bencmark for evaluating LLMs as data science agents.",Code Generation,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
204,Hazel Kim,kimFANToMBenchmarkStresstesting2023,FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,Include,"FANTOM is a comprehensive benchmark with 10,000 questions designed to evaluate theory-of-mind reasoning capabilities in LLMs by tracking beliefs across multi-party conversations with information asymmetry. The benchmark reveals that even leading LLMs struggle significantly with belief tracking compared to humans, demonstrating only ""illusory"" theory-of-mind abilities that break down when tested systematically.",Theory of Mind,,,"Crowd-sourced task examples (e.g. Prolific-created tasks), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Crowd-sourced', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']","Means, comparisons with percentage point gaps.",['Mean']
205,Jabez Magomere,huInfiAgentDABenchEvaluatingAgents2024,InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks,Include,"This paper introduces InfiAgent-DABench, a benchmark designed to evaluate LLM-based agents on data analysis tasks. These tasks require agents to solve the tasks end-to-end by interacting with an execution environment. ",Code Generation,,,"Real task examples (e.g. GitHub issues), ","['Real task', 'Unknown']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
206,Hazel Kim,fanNPHardEvalDynamicBenchmark2024,NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,Include,"NPHardEval is a comprehensive benchmark consisting of 900 algorithmic tasks across P, NP-Complete, and NP-Hard complexity classes, designed specifically to evaluate large language models' reasoning capabilities. It targets to accurately assess LLMs' algorithmic problem-solving abilities across varying computational complexity levels.",Reasoning,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']","Wilcoxon tests, variance analysis",['Other']
207,Negar Foroutan,wangCanLanguageModels2024,Can Language Models Serve as Text-Based World Simulators?,Include,"This paper introduces a new benchmark, containing a dataset of text game state transitions and accompanying game tasks. They use this to directly quantify how well LLMs can serve as text-based world simulators. They test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations.",Grounding,,,"Modified from another benchmark (e.g. translation into another language), The dataset is derived from the open BYTESIZED32 corpus.","['Another benchmark', 'Another benchmark']",Random sample (creators defined a task space and sampled from it),['Random'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
208,Hazel Kim,liGSMplusComprehensiveBenchmark2024,GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers,Include,"The authors create GSM‑PLUS, an adversarial extension of GSM8K that perturbs each seed question in eight different ways to test the robustness of LLM mathematical reasoning. They observe the huge performance drops from models on critical‑thinking and arithmetic variations with four prompting strategies.",Reasoning,Mathematical,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Crowd-sourced', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring), Distribution (perplexity, calibration, correlation)","['Exact match', 'LLM post-processing', 'Distribution']",Means and percentage differences,['Mean']
209,Lujain Ibrahim,zhangMultiTrustComprehensiveBenchmark2024,MULTITRUST: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models,Include,"The paper introduces a benchmark on the trustworthiness of MLLMs across five primarcy aspects: truthfulness, safety, robustness, fairness, and privacy. It benchmarks 20+ MLMMs and highlights the complexities introduced by multi-modality. ",Alignment,Safety,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Unknown","['Targeted', 'Unknown']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r)","['Exact match', 'LLM-as-a-Judge', 'Distribution', 'Correlation']","simple mean/sum, correlation between overall rankings and general capabilities based on MMBench","['Mean', 'Other']"
210,Angelika Romanou,bittonVisITbenchDynamicBenchmark2023,VisIT-Bench: A Dynamic Benchmark for Evaluating Instruction-Following Vision-and-Language Models,Include,VisIT-Bench (Visual InsTruction Benchmark) is a benchmark for evaluating instruction-following vision-language models for real-world use. The authors curated 70 “instruction families” that they believe instruction-tuned vision-language models should be able to address.  We conduct a large-scale empirical comparison of multimodal instruction-following models using their benchmark.,Instruction Following,,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Crowd-sourced', 'Another benchmark', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Elo ratings, Win rate","['Soft match', 'Human ratings', 'LLM-as-a-Judge', 'Reward']",simple mean,['Mean']
211,Negar Foroutan,singhIndicGenBenchMultilingualBenchmark2024,IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages,Include,"This benchmark introduces INDICGENBENCH; a benchmark for evaluating LLMs
on user-facing generation tasks across a diverse set 29 of Indic languages covering 13
scripts and 4 language families. INDICGENBENCH is composed of diverse generation
tasks like cross-lingual summarization, machine translation, and cross-lingual question
answering. INDICGENBENCH extends existing benchmarks to many Indic languages through
human curation providing multi-way parallel evaluation data for many under-represented Indic languages.",NLP,,Multilinguality,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Random sample (creators defined a task space and sampled from it),['Random'],"Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",Simple mean,['Mean']
212,Jabez Magomere,castillo-boladoPromptsDynamicConversational2024,Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models,Include,"This paper introduces the LTM benchmark designed to evaluate the Long-Term Memory (LTM) and Continual Learning (CL) capabilities of conversational agents. The LTM Benchmark engages agents in a single, prolonged conversation, incorporating multiple tasks and distractions to simulate realistic and meaningful interactions.",NLP,Long Context,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'Soft match', 'LLM post-processing']",,
213,Hazel Kim,chuTimeBenchComprehensiveEvaluation2024,TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models,Include,"TIMEBENCH is a comprehensive multi-task benchmark for evaluating large language models' temporal reasoning capabilities across symbolic, commonsense, and event-level reasoning. The benchmark evaluates various LLMs under different prompting conditions, revealing significant performance gaps compared to humans while providing detailed analyses of errors and scaling behaviors.",Reasoning,Temporal,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Author-crafted', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']","Mean, percentage",['Mean']
214,Anna Sotnikova,srinivasanCLiMBContinualLearning2022,CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks,Include,"The paper introduces CLiMB, a benchmark designed to evaluate continual learning (CL) for multimodal tasks, addressing the challenges of learning both new multimodal and unimodal tasks over time. It shows that while common CL methods can reduce forgetting in multimodal learning.",Language Modelling,Updating,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), VQAScore","['Exact match', 'Exact match']","simple mean, std, relative performance changes","['Mean', 'Std']"
215,Angelika Romanou,liVRSBenchVersatileVisionlanguage2024,VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding,Include,"VRSBench is a versatile vision-language dataset and benchmark for remote sensing image understanding. This comprehensive dataset not only addresses the limitations of previous datasets that either ignore detailed object information or suffer from quality control issues but also enriches the field by providing a diverse range of annotations including detailed captions, object referring, and visual question answering with rich object information and verified by human annotators.",VQA,,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Crowd-sourced', 'Another benchmark', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Intersection over Union (IoU)","['Exact match', 'Soft match', 'LLM-as-a-Judge', 'Soft match']",simple mean,['Mean']
216,Jan Batzner,linTruthfulQAMeasuringHow2022,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Include,"Introduces TruthfulQA, a benchmark to evaluate the generation of false statements that imitate common human misconceptions across e.g. health, law, finance and politics. Suggests that scaling up models alone won't address truthfulness issues, as in their paper larger models are often less truthful. ",Alignment,Alignment,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Human ratings', 'LLM-as-a-Judge']","Simple Means, the percentage of questions answered correctly.",['Mean']
217,Hazel Kim,samdarshiConnectingDotsEvaluating2024,Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game,Include,"The paper introduces a dataset of New York Times Connections puzzles with custom metrics to evaluate top language models against human players of varying skill levels. Results show that the best models significantly underperform both novice and expert humans. Based on the empirical analysis, the work develops a knowledge taxonomy to analyze model limitations in word categorization tasks.",Reasoning,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Real task', 'Author-crafted']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']","simple mean, weighted and unweighted clustering scores, frequency counts, Fleiss Kappa","['Mean', 'Other']"
218,Anna Sotnikova,herediaXNLIeuDatasetCrosslingual2024,XNLIeu: a dataset for cross-lingual NLI in Basque,Include,"XNLIeu is an expanded version of the XNLI benchmark that includes Basque, created by machine-translating and then manually post-editing the original English data to support cross-lingual NLI research in low-resource languages. Experiments with various LLMs show that post-editing significantly improves performance and that the translate-train strategy is most effective, though its advantage lessens when applied to natively created datasets.",NLP,Understanding,Multilinguality,"Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Expert-crafted', 'Crowd-sourced', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean and std,"['Mean', 'Std']"
219,Jabez Magomere,alamCTIBenchBenchmarkEvaluating2024,CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence,Include,"This paper introduces CTIBench, a benchmark designed to assess LLMs’ performance in cyner threat intelligence (CTI) applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape.",Alignment,Safety,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'LLM-generated']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Convenience', 'Targeted']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
220,Cornelius Emde,jinJailbreakingLargeLanguage2024,Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters,Include,"The paper proposes JAMBench - a benchmark for LLM jailbreaks against content moderation classifiers used as guardrails. The dataset is specifically designed to not trigger input-level harm classifiers, but trigger output-level harm classifiers to enable the study of evading output-level detection through jailbreaks.",Alignment,Safety,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), A non-defined ""jailbreak success rate"". likely LLM-as-a-Judge but unclear.","['Exact match', 'Reward']",Experiments are repeated 5 times but resulting information onf uncertainty are not reported.,['Unknown']
221,Angelika Romanou,zhouVLUEMultitaskMultidimension2022,VLUE: A Multi-Task Multi-Dimension Benchmark for Evaluating Vision-Language Pre-training,Include,"VLUE is a vision-language benchmark consisting of 4 representative VL tasks, each equipped with a private test set annotated on images from wild distribution. The authors evaluate the efficiency-performance trade-off of representative VLP models and build a Pareto SOTA landscape for current VLP research. Additionally they provide an extensive analysis of the generalization ability and the efficiency-performance trade-off of representative VLP models.",Language Modelling,Adaptability,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Crowd-sourced', 'Another benchmark', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",simple mean,['Mean']
222,Hazel Kim,liuExposingAttentionGlitches2023,Exposing Attention Glitches with Flip-Flop Language Modeling,Include,"This paper introduces FFLM, a synthetic benchmark designed to investigate ""attention glitches"" that cause Transformer-based language models to make sporadic reasoning errors.The benchmark tests models' ability to copy binary symbols across long distances while ignoring intervening tokens. The paper with this benchmark further explores regularisation and architectural tweaks to mitigate them.",NLP,Long Context,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Random sample (creators defined a task space and sampled from it),['Random'],"Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Mean error, scatter plots, attention heatmaps",['Mean']
223,Jan Batzner,merdjanovskaNoiseBenchBenchmarkingImpact2024,NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition,Include,"NOISEBENCH evaluates label noise in Named Entity Recognition (NER) models. It provides multiple variants of the same dataset with different types of real noise (expert errors, crowd- sourcing errors, automatic annotation errors and LLM errors).",NLP,Extraction,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Micro-averaged entity-level F1 score reported as means across 3 runs with standard deviations. Simple means used for comparing approaches across different noise types.,"['Mean', 'Std']"
224,Hazel Kim,jinMMToMQAMultimodalTheory2024,MMToM-QA: Multimodal Theory of Mind Question Answering,Include,"The paper introduces MMToM-QA, the first benchmark that evaluates Theory-of-Mind (ToM) reasoning across multimodal inputs (video and text), containing diverse test questions and synthetic training videos. The authors propose BIP-ALM, a novel approach that combines Bayesian inverse planning with language models to extract unified representations from multimodal data, demonstrating that while current large language and multimodal models lack robust ToM capacity, their method narrows the gap to human-level performance.",Theory of Mind,,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"simple mean, statistical tests","['Mean', 'Tests']"
225,Jan Batzner,dengCOLDBenchmarkChinese2022,COLD: A Benchmark for Chinese Offensive Language Detection,Include,"COLDATASET is dataset of Chinese sentences with binary offensive/non-offensive labels covering topics of race, gender, and region. COLDETECTOR is a baseline detector trained on this dataset.",Alignment,Safety,,Real task examples (e.g. GitHub issues),['Real task'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Reporting accuracy, precision, recall, and F1 scores, both as macro averages across all categories and separately for offensive and non-offensive classes.",['Mean']
226,Emma Beharry,krojerAreDiffusionModels2023,Are Diffusion Models Vision-And-Language Reasoners?,Include,"GDBench is a benchmark designed to assess vision-and-language reasoning in diffusion-based models using image-text matching. GDBench aggregates 8 existing datasets/benchmarks to measure text retrieval, image retrieval, and bias towards religious groups, national identity, and sexual orientation. The code and benchmark setup are publicly available. ",Grounding,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']",Generated image,['Free response'],"Distribution (perplexity, calibration, correlation)",['Distribution'],"Image retrieval error, effect score bias",['Mean']
227,Angelika Romanou,tsurutaSARSCoV2InteractionDataset2024,A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models,Include,"AVIDa-SARS-CoV-2 is a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins. AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and Omicron variants.  The authors report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-specific pre-trained language models.",Biology,,,Real task examples (e.g. GitHub issues),['Real task'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
228,Jabez Magomere,maAgentBoardAnalyticalEvaluation2024,AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents,Include,"This paper introduces AGENTBOARD, a benchmark designed to evaluate LLM agents capabilities in partially observable environments, multi-round interactions and diverse tasks through a unified evaluation framework and fine-grained metric analysis. ",Reasoning,Planning,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
229,Jan Batzner,maruNibblingHardCore2022,Nibbling at the Hard Core of Word Sense Disambiguation,Include,The authors introduce new challenging test sets for Word Sense Disambiguation evaluation  specifically designed to evaluate model resilience on rare word senses and present a more rigorous evaluation framework.,NLP,Understanding,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],F1 scores (micro-averaged and macro-averaged) as the primary statistical method.,['Mean']
230,Emma Beharry,huangConMeRethinkingEvaluation2024,ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs,Include,"ConMe is a multimodal compositional reasoning benchmark that presents a novel automatic hard negative generation pipeline using VLMs. It is publicly-available, manually verification, and presents a complementary automatic analysis and error categorization pipeline. ",Reasoning,Compositional,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']",Simple mean/accuracy,['Mean']
231,Angelika Romanou,chenCurriculumBroadcoverageBenchmark2022,Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,Include,"Current models do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. In this paper, authors introduce CURRICULUM as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. CURRICULUM contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. ",NLP,Understanding,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Correlation (Matthew's correlation, Pearson's r)","['Exact match', 'Correlation']",simple mean,['Mean']
232,Karolina Korgul,xieOSWorldBenchmarkingMultimodal2024,OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments,Include,"OSWorld introduces a scalable, executable computer environment supporting real OSs (Ubuntu, Windows, macOS) to evaluate multimodal agents on 369 open-ended real-world tasks. It includes complex setups, execution-based evaluation, and detailed analysis of LLM/VLM agents' capabilities and deficiencies.",Agents,Tool Use,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language)","['Human exams', 'Real task', 'Author-crafted', 'Expert-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response)","['Multiple choice', 'Short free response', 'Free response', 'Interaction']","Exact Match (accuracy, F1, precision, recall), Execution-based evaluation scripts","['Exact match', 'Reward']","Just simple mean, with occasional reporting of variance or distribution plots.","['Mean', 'Std']"
233,Angelika Romanou,dasEXAMSVMultidisciplineMultilingual2024,EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models,Include,"EXAMS-V is a multi-discipline multimodal multilingual exam benchmark for evaluating vision language models.  The questions come in 11 languages across 20 school disciplines. The evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision–text models such as GPT-4V and Gemini.",VQA,,,"Human exam questions (e.g. GRE questions), Modified from another benchmark (e.g. translation into another language)","['Human exams', 'Another benchmark']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
234,Hazel Kim,jiangBRAINTEASERLateralThinking2023,BRAINTEASER: Lateral Thinking Puzzles for Large Language Models,Include,"The paper introduces BRAINTEASER, a multiple‑choice benchmark that probes large language models’ ability for lateral thinking, a creative, non‑linear reasoning that overrides default commonsense associations. It describes a three‑step construction pipeline (web crawling and filtering of puzzles, semi‑automatic distractor generation, and semantic + contextual reconstructions) that yields high‑quality items while controlling for memorization. ",Reasoning,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Simple proportion, human/model comparisons",['Mean']
235,Jan Batzner,ushioGenerativeLanguageModels2022,Generative Language Models for Paragraph-Level Question Generation,Include,"QG-Bench, a comprehensive benchmark for paragraph-level question generation (QG) that unifies existing question answering datasets into a standard format. The authors fine-tune LMs for the QG task and evaluate them using both automatic metrics and human evaluation.",NLP,Extraction,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), Correlation (Matthew's correlation, Pearson's r)","['Soft match', 'Human ratings', 'Correlation']",Simple mean scores for each metric. For correlation analysis between automatic metrics and human judgments: Spearman's rank correlation coefficient.,"['Mean', 'Other']"
236,Angelika Romanou,gingOpenendedVQABenchmarking2024,Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy,Include,"This paper propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, the authors suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. ",VQA,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), - Contains: A less restrictive option is to consider a response correct if the prediction contains the true class name after preprocessing - ClipMatch: matching the prediction and label using cosine similarity in a vector embedding space","['Exact match', 'Soft match']",simple mean and variance,"['Mean', 'Std']"
237,Anna Sotnikova,hwangMultitaskBenchmarkKorean2022,A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction,Include,"This work introduces LBOX OPEN, the first large-scale benchmark of Korean legal AI datasets, comprising a legal corpus of 147k precedents and multiple tasks including classification, legal judgment prediction, and summarization. It also presents LCUBE, the first Korean legal language model trained on this corpus.",Law,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples)","['Real task', 'Author-crafted', 'Expert-crafted']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",simple mean and std,"['Mean', 'Std']"
238,Hazel Kim,shiLargeLanguageModels2023,Large Language Models Can Be Easily Distracted by Irrelevant Context,Include,"The paper introduces GSM-IC, a variant of the GSM8K arithmetic reasoning dataset that includes irrelevant sentences to test large language models' distractibility. The authors evaluate several prompting strategies on LLMs, revealing significant performance drops when irrelevant information is present, and explore mitigation strategies including self-consistency decoding, exemplar design, and explicit instructions that partially restore performance.",Reasoning,Mathematical,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Another benchmark', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Percentage, comparison",['Mean']
239,Angelika Romanou,leeVHELMHolisticEvaluation2024,VHELM: A Holistic Evaluation of Vision Language Models,Include,"The paper's main contributions are three-fold. First, the authors identify the aspects that are both applicable to VLMs and important to evaluate from either a technological or societal perspective. Second, they assemble 21 existing VLM benchmark datasets, which are sets of image-text prompts and expected output, and map to the aspects to ensure complete coverage. Third, they standardize the evaluation procedures so that apple-to-apple comparisons can be made across the models.",VQA,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring), Win rate","['Exact match', 'Soft match', 'LLM-as-a-Judge', 'LLM post-processing', 'Reward']",simple mean,['Mean']
240,Jan Batzner,wangM4GTbenchEvaluationBenchmark2024,M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection,Include,"M4GT-Bench, a multilingual, multi-domain benchmark for detecting machine-generated text (MGT). The benchmark extends the previous dataset and contains three distinct tasks: Binary MGT Detection (classifying text as human-written or machine-generated), multi-way Generator Detection (which LLM generated text), change Point Detection (text transitions from human-written to machine-generated).",NLP,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), Mean Absolute Error","['Exact match', 'Soft match']",Simple means for the main metrics,['Mean']
241,Simeng Han,comsaBenchmarkReasoningSpatial2023,A Benchmark for Reasoning with Spatial Prepositions,Include,"The paper introduces a new benchmark designed to evaluate the inferential capabilities of statements involving spatial prepositions. Featuring original datasets in both English and Romanian, the benchmark explores the boundaries of large language models’ reasoning about spatial relations.",Reasoning,Spatial,,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
242,Yilun Zhao,luoCODISBenchmarkingContextdependent2024,CODIS: Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models,Include,This paper introduces the CODIS benchmark for evaluating MLLMs on their ability to incorporate free-form textual context to improve image understanding. The authors show that current MLLMs underperform compared to humans and struggle to effectively extract and utilize contextual information to improve their understanding of images.,VQA,,,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
243,Hazel Kim,lalCaTbenchBenchmarkingLanguage2024,,Include,,Reasoning,Planning,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Real task', 'Author-crafted']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Human ratings']","Means, standard deviations, weighted Fleiss‑k","['Mean', 'Std']"
244,Yilun Zhao,suActPlan1KBenchmarkingProcedural2024,ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities,Include,"This paper introduces ActPlan-1K for evaluating VLMs on procedural and counterfactual reasoning tasks. By combining natural language descriptions with simulated environment images, the benchmark assesses the ability of VLMs to generate coherent action plans.",Reasoning,Planning,,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Soft match', 'Human ratings']",,
245,Simeng Han,yinGeoMLAMAGeodiverseCommonsense2022,GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models,Include,"In this paper, a benchmark dataset is introduced, Geo-diverse Commonsense Multilingual Language Models Analysis (GEOMLAMA), for probing the diversity of the relational knowledge in multilingual PLMs. ",Knowledge,Cultural,,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],Random sample (creators defined a task space and sampled from it),['Random'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']",,
246,Angelika Romanou,kesenViLMAZeroshotBenchmark2024,ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models,Include,"This paper introduces VILMA, a zero-shot benchmark for evaluating VidLMs, designed to require strong temporal understanding. The authros adopt the following methodology: (i) they harvest high-quality examples from existing video-language datasets; (ii) they create counterfactual examples or ‘foils’, so that a test requires distinguishing correct from counterfactual video+text pairs; (iii) they create a proficiency test to gauge if a model learns the capabilities we deem necessary to solve the main test; (iv) they apply automatic and manual validation of the examples and their counterfactuals to control for biases and to ensure a high-quality evaluation benchmark.",Grounding,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
247,Hazel Kim,kurticMathadorLMDynamicBenchmark2024,Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models,Include,"The paper introduces Mathador-LM, a arithmetic reasoning benchmark where models must reach a target number using five input numbers and basic arithmetic operations. Evaluations across various LLMs show that even top models perform far below the level of 3rd-grade students, revealing significant gaps in mathematical reasoning while avoiding test-data contamination issues.",Reasoning,Mathematical,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],"Random sample (creators defined a task space and sampled from it), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"simple mean, 95% confidence interval, percentage point of performance gains over baselines","['Mean', 'Tests']"
248,Emma Beharry,zhangCABComprehensiveAttention2023,CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling,Include,"CAB is a multimodal benchmark assessing long-range modeling in transformers across computer vision, natural language processing, speech processing, and time-series forecasting. It is publicly available and composed of 7 tasks, spanning 9 datasets, to measure noncausal self, causal self, noncausal cross, and causal cross attention with a custom metric. ",NLP,Long Context,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']",Unknown,['Unknown'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring), Distribution (perplexity, calibration, correlation), Mel-Cepstral Distortion is a measure of audio quality for TTS. A custom index is defined to balance all the evaluation metrics. ","['Exact match', 'Soft match', 'LLM post-processing', 'Distribution', '']","Simple mean/sum, custom normalized aggregate metric","['Mean', 'Other']"
249,Yilun Zhao,chenMLLMasajudgeAssessingMultimodal2024,MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark,Include,"This paper introduces the MLLM-as-a-Judge benchmark to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking.",LLM as a Judge,,,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Correlation (Matthew's correlation, Pearson's r)",['Correlation'],,
250,Hazel Kim,gandhiUnderstandingSocialReasoning2023,Understanding Social Reasoning in Language Models with Language Models,Include,"The paper introduces BigToM, a comprehensive benchmark containing 5,000 Theory-of-Mind scenarios created through procedural generation using GPT-4-populated causal templates and validated by human raters. This benchmark addresses key limitations in existing ToM evaluations of large language models, specifically the inconsistent results across previous studies and methodological concerns about evaluation validity. This benchmark aims to provide a more rigorous framework for assessing how well AI systems can understand human mental states.",Theory of Mind,,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Mean with 95% Confidence Interval ,"['Mean', 'Tests']"
251,Jan Batzner,bandarkarBelebeleBenchmarkParallel2024,The BELEBELE Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants,Include,"BELEBELE is a multiple-choice machine reading comprehension benchmark designed to evaluate language models' multilingual capabilities. It's covering diverse languages and scripts from high-, medium-, to low-resource languages. ",NLP,Understanding,Multilinguality,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Crowd-sourced', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean accuracy,['Mean']
252,Angelika Romanou,panchalWhatSayWhen2024,What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction,Include,"Open-ended, asynchronous interactions, where an AI model may proactively deliver timely responses or feedback based on the unfolding situation in real-time, are an open challenge. This work presents the QEVD-FIT-COACH benchmark and dataset, which explores human-AI interaction in the challenging, yet controlled, real-world domain of fitness coaching – a task which intrinsically requires monitoring live user activity and providing immediate feedback.",User Interaction,,,"Real task examples (e.g. GitHub issues), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'LLM-generated']",Random sample (creators defined a task space and sampled from it),['Random'],"Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response)","['Free response', 'Interaction']","n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Soft match', 'LLM-as-a-Judge']",simple mean,['Mean']
253,Yushi Yang,zhangCABComprehensiveAttention2023,CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling	,Include,"Current benchmarks testing different attention architectures for long-term modelling only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions. 

In this paper, we propose Comprehensive Attention Benchmark (CAB) with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. In seven tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures.",NLP,Long Context,,Real task examples (e.g. GitHub issues),['Real task'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Distribution (perplexity, calibration, correlation), MCD, MSD, PSNR, SSIM","['Exact match', 'Soft match', 'Distribution', 'Soft match']",,
254,Hazel Kim,tianDiagnosingFirstorderLogical2021,Diagnosing the First‑Order Logical Reasoning Ability Through LogicNLI,Include,"The paper introduces LogicNLI, a NLI‑style benchmark crafted to diagnose large language models’ first‑order logic (FOL) reasoning abilities. It disentangles logic from commonsense by procedurally generating facts, rules and statements covering seven FOL operators, and evaluates models along four axes: accuracy, robustness, generalization and proof‑based traceability. Experiment results show substantial gaps to human performance and highlight weaknesses in negation and universal quantification.",Reasoning,Logical,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
255,Hazel Kim,zhouRICAEvaluatingRobust2021,RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms,Include,"RICA is a benchmark for evaluating language models' ability to make robust commonsense inferences despite linguistic variations and logical perturbations. Starting from first‑order‑logic templates that encode commonsense axioms, the authors automatically crawl ConceptNet and ATOMIC, then apply 24 perturbation types (negation, antonym, paraphrase, etc.). Experiments show that pre-trained language models perform poorly on robust inference tasks even after fine-tuning, highlighting a significant gap between current AI systems and human-level understanding.",Reasoning,Logical,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template)","['Crowd-sourced', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean + standard deviations,"['Mean', 'Std']"
256,Jan Batzner,agrawalLargeLanguageModels2022,Large Language Models are Few-Shot Clinical Information Extractors,Include,"Datasets for benchmarking LLMs on five different clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. They show that LLMs perform well on these tasks despite not being specifically trained for the clinical domain.",NLP,Extraction,Medicine,"Real task examples (e.g. GitHub issues), Expert-crafted task examples (e.g. hand-written examples)","['Real task', 'Expert-crafted']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Primary metrics used are F1 scores, accuracy, recall, and precision. Partially, micro and macro averages are reported.",['Mean']
257,Hazel Kim,guhaLegalBenchCollaborativelyBuilt2023,LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models,Include,"LEGALBENCH is a comprehensive benchmark for evaluating language models' legal reasoning capabilities, consisting of 162 tasks spanning six distinct categories of legal reasoning that were designed and hand-crafted by legal professionals. The benchmark provides evaluation code, prompts, and a common vocabulary bridging legal frameworks and AI development, enabling rigorous assessment of both open-source and commercial language models on practically useful legal reasoning skills.",Law,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Mean and Standard deviation,"['Mean', 'Std']"
258,Hazel Kim,chenToMBenchBenchmarkingTheory2024,ToMBench: Benchmarking Theory of Mind in Large Language Models,Include,"ToMBENCH is a bilingual benchmark that evaluates language models' Theory of Mind (ToM) capabilities through classic psychology tasks measuring distinct social cognition abilities in a multiple-choice format. Built entirely from scratch to avoid data contamination, the benchmark reveals that even advanced models like GPT-4 still significantly underperform humans in understanding and attributing mental states, particularly when subjected to coherence stress tests. Their aim with ToMBENCH is to enable an efficient and effective evaluation of LLMs’ ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.",Theory of Mind,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Author-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean with percentage point,['Mean']
259,Jan Batzner,wuDetectRLBenchmarkingLLMgenerated2024,DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios,Include,"Evaluate LLM-generated text in realistic scenarios. The benchmark collects human-written texts from high-risk domains, generates comparable texts using popular LLMs, and applies various attack methods to simulate real-world conditions. ",NLP,Detection,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']","Simple mean to aggregate across different settings. For each detector, they report AUROC and F1 Score values for each specific condition and the average across those conditions. ",['Mean']
260,Hazel Kim,patelMultiLogiEvalEvaluatingMultistep2024,Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models,Include,"Multi-LogiEval evaluates language models' multi-step logical reasoning across propositional, first-order, and non-monotonic logic with varied inference rules and depths. Tests on leading models reveal substantial performance drops as reasoning complexity increases, exposing critical gaps in logical reasoning capabilities.",Reasoning,Logical,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean ,['Mean']
261,Karolina Korgul,luWebLINXRealworldWebsite2024,WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue,Include,WebLinx introduces a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Authors develop a multimodal agent able of interpreting both visual and textual input to complete web-based tasks with long context understanding and planning capabilities.,Agents,Web,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Author-crafted', 'Expert-crafted', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Short free response (e.g. single word or number), Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Interaction', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
262,Jan Batzner,zhangMuCGECMultireferenceMultisource2022,MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction,Include,"The paper introduces MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction. This dataset contains different Chinese-as-a-Second-Language (CSL) learner sources, with each sentence corrected by three independent annotators and reviewed by a senior annotator.",NLP,Understanding,,Real task examples (e.g. GitHub issues),['Real task'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']","Simple mean, char-based F0.5 scores for overall performance, along with precision and recall.",['Mean']
263,Hazel Kim,spragueMuSRTestingLimits2024,MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning,Include,"The paper introduces MuSR, a long‑form natural‑language problems that require multistep “soft” reasoning combining commonsense, deductive and theory‑of‑mind inference. Generated through a neurosymbolic pipeline, these long-form problems reveal critical gaps in current models' reasoning capabilities despite being consistently solvable by humans.",Reasoning,Commonsense,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"mean + standard deviatin, significance test. proportion","['Mean', 'Std']"
264,Chris Schmitz,caoWenMindComprehensiveBenchmark2024,"WenMind: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts
",Include,"WenMind is a benchmark for Chinese Classical Literature and Language Arts (CCLLA). It spans 42 tasks across three sub-domains (Ancient Prose, Ancient Poetry, Ancient Literary Culture), in both domain- and capability-oriented formats.
",Knowledge,Cultural,,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'Author-crafted', 'Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Soft match', 'LLM-as-a-Judge']","Stratified human agreement evaluation on LLM-graded items; comparisons to BLEU/F1 for scoring validity.
",['Mean']
265,Chris Schmitz,ouyangCliMedBenchLargeScaleChinese2024,"CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios
",Include,"CliMedBench is a Chinese clinical medical benchmark with 33.7k QA items across 14 core scenarios derived from real-world medical records and exams, measuring LLMs’ clinical reasoning and language abilities. It includes evaluations of 11 LLMs.
",Medicine,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'Real task', 'Author-crafted', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Free response (e.g. summary paragraph, executable code), Sequencing","['Multiple choice', 'Free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Soft match', 'Human ratings']",Simple mean,['Mean']
266,Yilun Zhao,huangRAVELEvaluatingInterpretability2024,RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations,Include,"This paper introduces the RAVEL benchmark to evaluate how well interpretability methods disentangle multiple high-level concepts in language models. The authors propose a new method, Multi-task Distributed Alignment Search, which identifies distributed representations that meet multiple causal criteria, outperforming existing approaches on RAVEL.",,,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Distribution (perplexity, calibration, correlation)",['Distribution'],,
267,Chris Schmitz,sabourEmoBenchEvaluatingEmotional2024,"EmoBench: Evaluating the Emotional Intelligence of Large Language Models
",Include,"EmoBench evaluates emotional intelligence (EI) covering emotional understanding and application with 400 questions in English and Chinese.
",Psychology,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Unknown,['Unknown'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Accuracy, Fleiss’ k for human agreement
","['Mean', 'Other']"
268,Simeng Han,garcia-ferreroThisNotDataset2023,This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models,Include,"The paper introduces a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. ",NLP,Understanding,,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
269,Chris Schmitz,josephFactPICOFactualityEvaluation2024,"FACTPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence
",Include,"FACTPICO evaluates the factuality of LLM-generated plain language summaries of medical randomized controlled trials (RCTs). It features fine-grained expert annotations across five key dimensions and includes both human and LLM-generated rationales.
",NLP,Summarization,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Human ratings (text quality, preference, NOT manual scoring of other metrics)",['Human ratings'],"Flesch Kincaid, Rouge-L, Kendalls, Spearmans
",['Other']
270,Chris Schmitz,liuRevisitingDeIdentificationElectronic2023,"Revisiting De-Identification of Electronic Medical Records: Evaluation of Within- and Cross-Hospital Generalization
",Include,"Benchmark for de-identification of protected health information (PHI) in Chinese electronic medical records, with a focus on cross-hospital generalization. Constructs a multi-hospital dataset and evaluates various models and domain generalization (DG) techniques to assess performance under domain shift.
",Medicine,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Real task', 'Author-crafted']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)","['Random', 'Convenience']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
271,Yilun Zhao,liuConvBenchMultiturnConversation2024,ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models,Include,"This paper introduces the ConvBench to evaluate LVLMs across hierarchical capabilities such as perception, reasoning, and creativity. It enables fine-grained error attribution and includes an automatic evaluation framework.",User Interaction,,,"Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Expert-crafted', 'Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],simple mean,['Mean']
272,Chris Schmitz,dinhSciExBenchmarkingLarge2024,"SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading
",Include,"SciEx is a multilingual, multimodal benchmark of university-level computer science exams. It includes freeform questions with expert grading that contain both text and images. It compares to a baseline of human students, as all questions are real exam questions.
",General Science,,,Human exam questions (e.g. GRE questions),['Human exams'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Human ratings (text quality, preference, NOT manual scoring of other metrics)",['Human ratings'],"Pearson correlation, RMSE, differences to student baselines
",['Other']
273,Yilun Zhao,zhangUnveilingTapestryConsistency2024,Unveiling the Tapestry of Consistency in Large Vision-Language Models,Include,This paper introduces the ConBench benchmark to evaluate the consistency of LVLMs across prompts with varying solution spaces centered on the same knowledge point. The authors reveal key patterns in LVLM behavior and propose a trigger-based diagnostic refinement method to improve consistency and indirectly enhance captioning performance.,Language Modelling,Robustness,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
274,Simeng Han,fierroMuLanStudyFact2024,MULAN : A Study of Fact Mutability in Language Models,Include,"The authors create MULAN , a benchmark for evaluating the ability of English language models to anticipate time-contingency, covering both 1:1 and 1:N relations. ",Reasoning,Temporal,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
275,Yilun Zhao,bitton-guettaVisualRiddlesCommonsense2024,Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models,Include,"This paper introduces Visual Riddles, a benchmark designed to evaluate LLMs on complex visual reasoning tasks that require commonsense and world knowledge. The dataset includes 400 carefully crafted riddles, each combining images, questions, and textual hints, revealing significant performance gaps between current models and human reasoning abilities.",VQA,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples)","['Author-crafted', 'Expert-crafted']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Human ratings', 'LLM-as-a-Judge']",simple mean,['Mean']
276,Emma Beharry,liQuantifyingAdaptabilityPretrained2022,Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,Include,"TaskBench500 is a benchmark designed to systematically measure how LLMs adapt to new tasks. It comprises ""500 procedurally generated sequence modeling tasks"" spanning ""lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge"" (4696). The benchmark is publicly available. ",Language Modelling,In-context Learning,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r)","['Exact match', 'Distribution', 'Correlation']","Simple mean, average",['Mean']
277,Yilun Zhao,duEmbSpatialbenchBenchmarkingSpatial2024,EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models,Include,"This paper introduces the EmbSpatial-Bench benchmark to evaluate the spatial understanding capabilities of LVLMs in embodied environments. The authors also propose EmbSpatial-SFT, an instruction-tuning dataset aimed at enhancing LVLMs' spatial reasoning from an egocentric perspective",VQA,,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']",Random sample (creators defined a task space and sampled from it),['Random'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
278,Yilun Zhao,chungCanVisualLanguage2024,Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!,Include,"This paper introduces the UNPIE benchmark to evaluate multimodal understanding in machines using puns. By pairing puns with explanatory images, the study tests models on tasks like grounding, disambiguation, and reconstruction, showing that visual context significantly enhances performance over text-only approaches.",Grounding,,,"Expert-crafted task examples (e.g. hand-written examples), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Expert-crafted', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],image,['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean/sum,['Mean']
279,Emma Beharry,kumarVisionlanguageModelsUnderstand2024,Do Vision-Language Models Understand Compound Nouns?,Include,"Compun is a multimodal benchmark to assess how models understand compound nouns using text-to-image retrieval tasks. The dataset is publicly available, manually curated, and focuses on noun+noun compound nouns. ",NLP,Understanding,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Average,['Mean']
280,Emma Beharry,flachsGrammaticalErrorCorrection2020,Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,Include,"CWEB is a benchmark for grammatical error correction that is publicly available and manually annotated by experts. It contains website data from Common Crawl, and includes sentences with low and high error density. ",NLP,Understanding,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Random sample (creators defined a task space and sampled from it),['Random'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']",Simple mean,['Mean']
281,Anna Sotnikova,joshiILTURBenchmarkIndian2024,IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning,Include,"This paper introduces IL-TUR, a benchmark designed to evaluate NLP models for legal text understanding and reasoning in the Indian legal context. The benchmark covers both monolingual (English, Hindi) and multilingual tasks across 9 Indian languages. It provides baseline models and a leaderboard with comparison in automating legal document processing.",Law,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Author-crafted', 'Expert-crafted', 'Crowd-sourced', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), BERTScore, GLEU","['Exact match', 'Soft match', 'Soft match']",simple mean,['Mean']
282,Jan Batzner,royBenchCLAMPBenchmarkEvaluating2023,BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing,Include,BenchCLAMP evaluates Constrained LAnguage Model Parsing on syntactic and semantic parsing tasks. It provides context-free grammars using both prompt-based learning and fine-tuning across different data resource settings.,NLP,Understanding,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Mean scores across different data splits and standard deviation for low-resource settings,"['Mean', 'Std']"
283,Jan Batzner,ryanRevisitingNonEnglishText2023,Revisiting non-English Text Simplification: A Unified Multilingual Benchmark,Include,"MULTI-SIM, benchmark for multilingual text simplification, containing complex-simple sentence pairs across 12 languages. They show improvements from multilingual training for non-English languages and strong performance of Russian for cross-lingual transfer.",NLP,Summarization,,"Real task examples (e.g. GitHub issues), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Expert-crafted', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Soft match', 'Human ratings']",Automatic Evaluation Metrics (SARI; BLEU); Measure inter-annotator agreement using Krippendorff's alpha,['Mean']
284,Jan Batzner,siREADINChineseMultitask2023,READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises,Include,READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input.,Language Modelling,Robustness,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Crowd-sourced', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']","Micro-Average, Worst-Average",['Mean']
285,Harry Mayne,schwettmannFINDFunctionDescription2023,FIND: A Function Description Benchmark for Evaluating Interpretability Methods,Include,Evaluating the ability of LLMs as interpretability agents as a proxy for how well they might perform in automated interpretability pipelines. i.e. can LLMs recover the functions from input/output data alone.,Reasoning,,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],"Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Execution-based (unit tests)","['Exact match', 'LLM-as-a-Judge', 'Reward']",Mean,['Mean']
286,Jan Batzner,zuoPatentEvalUnderstandingErrors2024,PatentEval: Understanding Errors in Patent Generation,Include,"PatentEval, a benchmark annotated by human experts, tailored for assessing language models of different sizes and capacities. This includes pairwise comparisons and detailed analysis of error types in each output.",Law,,,Real task examples (e.g. GitHub issues),['Real task'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Human ratings (text quality, preference, NOT manual scoring of other metrics)",['Human ratings'],Win and draw rates from pairwise comparisons. For automated metrics and human judgment evaluation: Kendall's Tau ,"['Mean', 'Other']"
287,Harry Mayne,yinNaturalLanguageCode2023,Natural Language to Code Generation in Interactive Data Science Notebooks,Include,"A benchmark for data science tasks (natural language to code) in a Jupyter notebook. A benchmark with ""realistic NL intents, rich notebook context, and a series of interrelated problems""",Code Generation,,,"Real task examples (e.g. GitHub issues), Expert-crafted task examples (e.g. hand-written examples)","['Real task', 'Expert-crafted']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), Execution-based evaluation (unit tests)","['Exact match', 'Reward']","Mean, error bars on figures in appendix.","['Mean', 'Std']"
288,Yushi Yang,zhangCABComprehensiveAttention2023,Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,Include,"We present a large-scale empirical study on LM adaptability using TASKBENCH500, a benchmark of 500 procedurally generated sequence modeling tasks. 

We evaluate three facets of adaptability, finding that: (1) adaptation methods vary in memorizing small datasets; (2) some show compositional adaptability to complex tasks; and (3) label distribution mismatches arise from differences in intrinsic label difficulty. 

Our results show that adaptability to new tasks can be systematically analyzed, similar to generalization.",NLP,Long Context,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],"Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
289,Yushi Yang,kumarVisionlanguageModelsUnderstand2024,Do Vision-Language Models Understand Compound Nouns?,Include,"We curate Compun, a novel benchmark with 400 unique and commonly used Compound Nouns (CN), to evaluate the effectiveness of VLMs in interpreting CNs. 

We perform an in-depth analysis to highlight CLIPs’ limited understanding of certain types of CNs. 

We present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25% on Compun.",NLP,Understanding,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Random sample (creators defined a task space and sampled from it),['Random'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
290,Jan Batzner,asaiBUFFETBenchmarkingLarge2024,BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer,Include,,Language Modelling,In-context Learning,Multilinguality,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",Dataset score is calculated as a macro-average of the per-language score.,['Mean']
291,Yushi Yang,hareshClevrSkillsCompositionalLanguage2024,ClevrSkills: Compositional Language And Visual Reasoning in Robotics,Include,"We ask the question: if the models are taught the low-level capabilities, can they compose them in novel ways to achieve high-level tasks like cleaning the table without having to be explicitly taught so? 

To this end, we present ClevrSkills - a benchmark suite for compositional reasoning in robotics. The dataset contains trajectories generated on a range of robotics tasks with language and visual annotations as well as multi-modal prompts as task specification. 

We benchmark multiple different VLM baselines on ClevrSkills and show that even after being pre-trained on many tasks, these models fail on compositional reasoning in robotics tasks.",Reasoning,Compositional,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Extended interaction (e.g. conversation, calling an API and processing the response)",['Interaction'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
292,Lujain Ibrahim,hanMedSafetyBenchEvaluatingImproving2024,MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models,Include,The paper introduces the first benchmark dataset designed to measure the medical safety of LLMs. It uses the dataset to evaluate and improve the medical safety of LLMs using fine-tuning. ,Medicine,,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],Random sample (creators defined a task space and sampled from it),['Random'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],"simple mean, standard error of the mean",['Mean']
293,Jan Batzner,shivagundeLargerProbesTell2023,Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning,Include,"Psycholinguistic datasets for negation and role reversal, which extend existing smaller benchmarks using GPT-3. Evaluation of multiple LMs on these extended benchmarks reveals performance drops.",NLP,Understanding,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']","Simple means, McNemar test, Minimum detectable effect (MDE)","['Mean', 'Other']"
294,Jabez Magomere,wuSmartPlayBenchmarkLLMs2024,SMARTPLAY : A BENCHMARK FOR LLMS AS INTELLIGENT AGENTS,Include,"This paper introduces SmartPlay, a benchmark for assessing LLMs as intelliget agents using 6 different games including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness.",Agents,,,Real task examples (e.g. GitHub issues),['Real task'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Extended interaction (e.g. conversation, calling an API and processing the response)",['Interaction'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Scores are normalised relative to human performance.,['Other']
295,Lujain Ibrahim,liWMDPBenchmarkMeasuring2024,The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning,Include,"The paper introduces a benchmark with questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. It also introduces a state-of-the-art unlearning method to reduce model performance on the benchmark.",Alignment,Safety,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples)","['Author-crafted', 'Expert-crafted']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
296,Jabez Magomere,renValueBenchComprehensivelyEvaluating2024,ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models,Include,"This paper introduces ValueBench, a benchmark for evaluating value orientations and value understanding in LLMs, grounded in realistic human-AI interactions to test for value orientations, along with new tasks for evaluating value understanding in an open-ended value space.",Alignment,Alignment,,Real task examples (e.g. GitHub issues),['Real task'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",,
297,Lujain Ibrahim,wangDecodingTrustComprehensiveAssessment2023,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Include,"The paper introduces a comprehensive trustworthiness evaluation for large language
models with a focus on GPT-4 and GPT-3.5. The benchmark introduced considers diverse perspectives including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. ",Alignment,Alignment,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']","simple mean, weighted mean",['Mean']
298,Jan Batzner,chakrabortyCounterTuringTest2023,Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index,Include,Counter Turing Test (CT2) is a benchmark to evaluate the robustness of AI-generated text detection techniques. The AI Detectability Index (ADI) is a metric to rank LLMs according to how detectable their outputs are as machine-generated versus human-written.,NLP,Detection,,"Real task examples (e.g. GitHub issues), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']","mean, standard deviation, entropy calculations, z-scores, p-values, bootstrapping, Le Cam's lemma, multiplicative damping factors","['Mean', 'Std', 'Tests', 'Other']"
299,Emma Beharry,tuWaterBenchHolisticEvaluation2024,WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models,Include,"WaterBench is a benchmark for evaluating LLM watermarks across detection and generation quality. The paper also presents a hyper-parameter search method to control watermarking strength, and automatic evaluation using GPT4-Judge. The dataset is publicly available and human-validated. ",NLP,Detection,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Generation Metric and Generation Quality Drop are never explicitly defined in the paper. ","['Exact match', 'Human ratings', 'LLM-as-a-Judge', 'LLM-as-a-Judge']","True Positive Rate, True Negative Rate, Generation Metric and Generation Quality Drop",['Mean']
300,Emma Beharry,luoMMMRSMultimodalMultiGSD2024,"MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation",Include,"MMM-RS is a large, multi-modal multi-GSD, and multi-scene remote sensing text-to-image generation benchmark. It is publicly available, aggregated and filtered from existing datasets, and contains information-rich captions. ",Grounding,,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Image,['Free response'],"Distribution (perplexity, calibration, correlation)",['Distribution'],Simple mean,['Mean']
301,Jan Batzner,devriesDUMBBenchmarkSmart2023,DUMB: A Benchmark for Smart Evaluation of Dutch Models,Include,"DUMB, a benchmark for evaluating Dutch language models across nine tasks. The authors propose Relative Error Reduction (RER) for better cross-task comparison and evaluate pre-trained models, finding that current Dutch models underperform while identifying strategies for future model improvements.",NLP,Understanding,Multilinguality,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Binomial mixed effects regression models,['Tests']
302,Yushi Yang,hsiehSugarCrepeFixingHackable2023,SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,Include,"We introduce SUGARCREPE, a new benchmark for vision-language compositionality evaluation. 

We employ large language models, instead of rule-based templates used in previous benchmarks, to generate fluent and sensical hard negatives, and utilize an adversarial refinement mechanism to maximally reduce biases.' (abstract)",Reasoning,Compositional,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
303,Cornelius Emde,toyerTensorTrustInterpretable2024,Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game,Include,The authors propose a benchmark testing model's robustness to prompt injection attacks. The authors collect multi-step attacks and defenses though red-teaming in the form of an online LLM-driven adversarial game in which the adversarial goal is for the LLM to leak secret string set by the defender. They then compare multiple SOTA LLMs on their robustness to leaking the secrets.,Alignment,Safety,,Crowd-sourced task examples (e.g. Prolific-created tasks),['Crowd-sourced'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
304,Jan Batzner,mackoMULTITuDELargescaleMultilingual2023,MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark,Include,"MULTITuDE, a benchmark dataset for multilingual machine-generated text detection. It contains human-written and machine-generated text across languages from multilingual LLMs. The authors evaluate how zero-shot and fine-tuned detectors generalize across languages and LLMs.",NLP,Detection,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r)","['Exact match', 'Distribution', 'Correlation']","Mean, ANOVA, post-hoc pairwise tests","['Mean', 'Tests']"
305,Negar Foroutan,ahujaMEGAVERSEBenchmarkingLarge2024,"MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks",Include," This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. This benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. ",NLP,,Multilinguality,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Random sample (creators defined a task space and sampled from it),['Random'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",simple mean,['Mean']
306,Emma Beharry,hareshClevrSkillsCompositionalLanguage2024,ClevrSkills: Compositional Language And Visual Reasoning in Robotics,Include,ClevrSkills is a multi-modal benchmark for evaluating compositional reasoning in robotics. It is composed of 33 tasks spanning three cumulative different levels of difficulty. The benchmark is publicly available and built on top of ManiSkill 2. ,Reasoning,Compositional,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Movement trajectory to complete task,['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
307,Yilun Zhao,shiriEmpiricalAnalysisSpatial2024,An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models,Include,"This paper introduces the Spatial-MM benchmark to evaluate the spatial reasoning capabilities of LMMs. The authors demonstrate that incorporating structured information like bounding boxes and scene graphs improves performance, while also revealing limitations in human-perspective reasoning and the ineffectiveness of chain-of-thought prompting for complex spatial tasks.",Reasoning,Spatial,,"Expert-crafted task examples (e.g. hand-written examples), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Expert-crafted', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
308,Yilun Zhao,yingMMTbenchComprehensiveMultimodal2024,MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI,Include,"This paper introduces the MMT-Bench benchmark for evaluating LVLMs across different complex, expert-level multimodal tasks",Reasoning,Multimodal,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Crowd-sourced', 'Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",simple mean,['Mean']
309,Yilun Zhao,chenM3CoTNovelBenchmark2024,M^3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought,Include,"This paper introduces the M3CoT benchmark to address current benchmarks' limitations such as lack of visual reasoning, single-step reasoning, and limited domain diversity. It focuses on the evaluation of multi-domain, multi-step, and multi-modal reasoning.",Reasoning,Multimodal,,"Human exam questions (e.g. GRE questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'Crowd-sourced', 'Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
310,Chris Schmitz,ohERBenchEntityRelationshipBased2024,"ERBench: An Entity-Relationship-Based Automatically Verifiable Hallucination Benchmark for Large Language Models
",Include,"ERBench introduces a benchmark to measure factual hallucinations in LLM-generated text by extracting entity-relation (ER) tuples and verifying them against Wikipedia. The benchmark spans four domains, allows for automatic verification, and includes multi-hop reasoning tasks.
",Factuality,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Means, standard deviations, comparisons","['Mean', 'Std']"
311,Yilun Zhao,liuMMDUMultiturnMultiimage2024,MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs,Include,"This paper introduces MMDU, a new benchmark, and MMDU-45k, a large-scale instruction tuning dataset, to evaluate and enhance the performance of LVLMs in realistic, multi-turn, multi-image conversation scenarios. The authors use a clustering-based method to curate diverse image-text contexts from Wikipedia and construct question-answer pairs with human and GPT-4o collaboration.",User Interaction,,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],simple mean/sum,['Mean']
312,Chris Schmitz,zhangM3ExamMultilingualMultimodal2023,"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models
",Include,"M3Exam evaluates LLMs on 12,317 multilingual, multimodal, multilevel multiple-choice exam questions from 9 languages and 3 educational stages.
",Language Modelling,,Multilinguality,Human exam questions (e.g. GRE questions),['Human exams'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Descriptive accuracy only
",['Mean']
313,Chris Schmitz,khandekarMedCalcBenchEvaluatingLarge2024,"MEDCALC-BENCH: Evaluating Large Language Models for Medical Calculations
",Include,"MedCalc-Bench tests LLMs on 55 tasks emulating real-world ""medical calculators"" using patient notes; tasks require knowledge recall, attribute extraction, and computation.
",Medicine,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']","Binomial standard error reported
",['Tests']
314,Yilun Zhao,liCanLanguageModels2023,Can Language Models Understand Physical Concepts?,Include,"This paper introduces the VEC benchmark to evaluate language models' understanding of visual and embodied physical concepts. The authors also propose a knowledge distillation method that transfers embodied understanding from vision-language models to LMs, achieving significant gains without massive model scaling.",Grounding,,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Convenience', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']",mean and variance,"['Mean', 'Std']"
315,Chris Schmitz,liMediQQuestionAskingLLMs2024,"MediQ: Question Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning
",Include,"Medi-Q evaluates LLMs' ability to ask clarifying questions for improved clinical reasoning. It provides expert-authored diagnostic cases and assesses interactive decision-making.
",Medicine,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Extended interaction (e.g. conversation, calling an API and processing the response)",['Interaction'],"Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Human ratings', 'LLM-as-a-Judge']",,
316,Chris Schmitz,wuMedJourneyBenchmarkEvaluation2024,"MedJourney: Benchmark and Evaluation of Large Language Models Over Patient Clinical Journey
",Include,"MedJourney evaluates LLMs on tasks across the clinical journey of a patient. Its 12 task datasets are partially collected from other benchmarks (7) and partially newly constructed (5).
",Medicine,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Author-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Human ratings', 'LLM-as-a-Judge']",,
317,Valentin Hoffman,fenogenovaMERAComprehensiveLLM2024,MERA: A Comprehensive LLM Evaluation in Russian,Include,"The paper proposes a new instruction benchmark, MERA, for measuring LM capabilities in Russian. MERA comprises 21 tasks that have been (i) specifically created for MERA, (ii) translated from English tasks, or (iii) taken from existing Russian resources. The authors tried to culturally adapt MERA to the Russian context, for example by replacing historical concepts in translated English tasks with Russian ones. The paper also provides model baselines (evaluation of 19 LMs) as well as a human baseline for MERA.",NLP,,Multilinguality,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Human exams', 'Author-crafted', 'Expert-crafted', 'Crowd-sourced', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Correlation (Matthew's correlation, Pearson's r), (school) grade","['Exact match', 'LLM-as-a-Judge', 'Correlation', 'Soft match']",simple mean,['Mean']
318,Chris Schmitz,zambranochavesRaLEsBenchmarkRadiology2023,"RaLEs: a Benchmark for Radiology Language Evaluations
",Include,"RaLEs is a benchmark for evaluating language understanding and generation on radiology reports, which it defines as a distinct domain. It covers six datasets across four tasks. It compares general, biomedical, clinical, and radiology-specific models and includes new datasets and metrics.
",Medicine,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Distribution (perplexity, calibration, correlation)","['Exact match', 'Soft match', 'Distribution']",,
319,Chris Schmitz,sivasubramaniamSM3TexttoQuerySyntheticMultiModel2024,"SM3: Synthetic Multi-modal Medical Text-to-Query Benchmark
",Include,"SM3 is a synthetic benchmark for evaluating medical retrieval capabilities in LLMs. It focuses on converting diverse multimodal medical inputs into structured queries.
",Medicine,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Human ratings']",,
320,Chris Schmitz,liuBenchmarkingLargeLanguage2023,"Benchmarking Large Language Models on CMExam: A Comprehensive Chinese Medical Exam Dataset
",Include,"CMExam is a benchmark based on Chinese medical licensing exam questions, to evaluate LLMs on domain-specific knowledge and reasoning. It benchmarks several general and medical domain LLMs on multiple-choice and explanation tasks.
",Medicine,,,Human exam questions (e.g. GRE questions),['Human exams'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",,
321,Chris Schmitz,xiaCARESComprehensiveBenchmark2024,"CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision-Language Models
",Include,"CARES is a benchmark for evaluating the ""trustworthiness"" of medical vision-language models (Med-LVLMs). It assesses five dimensions: trustfulness, fairness, safety, privacy, and robustness. Some of these are via the QA benchmark, others are via supplemental testing eg. attempted jailbreaks. 41K QA pairs from 18K images across 16 medical modalities and 27 anatomical regions.
",Medicine,,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']","Inter-rater agreement (Krippendorff’s alpha), statistical comparisons
",['Other']
322,Chris Schmitz,wangCMBComprehensiveMedical2024," CMB: A Comprehensive Medical Benchmark in Chinese 
",Include,"CMB (Chinese Medical Benchmark) is a large-scale Chinese-native benchmark designed to evaluate LLMs on both theoretical knowledge (via multiple-choice exams) and practical reasoning (via multi-turn dialogues on clinical cases). 
",Medicine,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues)","['Human exams', 'Real task']","Random sample (creators defined a task space and sampled from it), Specific criteria (items were taken from a larger set based on specified rules)","['Random', 'Criterion']","Multiple choice, Extended interaction (e.g. conversation, calling an API and processing the response)","['Multiple choice', 'Interaction']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Human ratings', 'LLM-as-a-Judge']","Means, standard deviations, and Spearman/Pearson correlations with expert rankings.
","['Mean', 'Std', 'Other']"
323,Chris Schmitz,kweonEHRNoteQALLMBenchmark2024,"EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries
",Include,"EHRNoteQA is a benchmark designed to evaluate LLM performance in answering clinical questions grounded in electronic health records (EHRs). The benchmark includes questions hand-reviewed by physicians across four clinical specialties. It evaluates LLMs’ capabilities under different reasoning requirements, providing both answer annotations and evidence spans.
",Medicine,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Human ratings', 'LLM-as-a-Judge']",,
324,Karolina Korgul,trivediAppWorldControllableWorld2024,"AppWorld: A Controllable World of Apps and People
for Benchmarking Interactive Coding Agents",Include,"AppWorld introduces a benchmark and simulator for evaluating LLM-based agents in performing complex, real-world digital tasks across multiple apps via API calls.",Agents,Coding,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Interaction', 'Structured']","Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",,
325,Chris Schmitz,liuLargeLanguageModels2024,"Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark
",Include,"ClinicBench is a benchmark for evaluating large language models (LLMs) on complex clinical decision-making tasks. It includes 17 datasets, 11 tasks, and evaluates 22 LLMs under both zero- and few-shot settings using automatic and human evaluations.
",Medicine,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']","Multiple choice, Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Soft match', 'Human ratings']","Averages and win/tie percentages in human eval; no advanced statistics reported
",['Mean']
326,Valentin Hoffman,sunMeasuringEffectInfluential2023,Measuring the Effect of Influential Messages on Varying Personas,Include,"The authors examine the task of predicting how social media users will react to a news event. To this aim, they collect a Twitter-based benchmark consisting of (i) headlines, (ii) user information (referred to as ""persona""), and (iii) user reactions, represented as sentiment polarity, sentiment intensity, and response text. They define the task of ""Response Forecasting on Personas for News Media"" as predicting the user reaction from the headline and user information, and they evaluate several LMs on this task.",NLP,Understanding,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Crowd-sourced', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Correlation (Matthew's correlation, Pearson's r)","['Exact match', 'Soft match', 'Human ratings', 'LLM-as-a-Judge', 'Correlation']",,
327,Chris Schmitz,liLexEvalComprehensiveChinese2024,"LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models
",Include,"LexEval is a benchmark for evaluating LLMs' legal capabilities in Chinese. It includes 23 tasks spanning six cognitive levels and tests both general and legal-specific models.
",Law,,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language)","['Human exams', 'Real task', 'Author-crafted', 'Expert-crafted', 'Another benchmark']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']","Basic comparisons across models; no significance tests
",['Mean']
328,Karolina Korgul,kohVisualWebArenaEvaluatingMultimodal2024,"VisualWebArena: Evaluating Multimodal Agents on Realistic Visually
Grounded Web Tasks",Include,"VisualWebArena is a benchmark designed to assess the performance of multimodal agents on diverse and complex web-based tasks (requiring reasoning and visual understanding) that evaluate various capabilities of autonomous multimodal agents in three different environments (Classifieds, Shopping, and Reddit). Paper also introduces new agent design using Set-of-Marks (SoM) prompting to simplify the visual action space and improve performance.",Agents,Web,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Expert-crafted', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Interaction', 'Structured']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Custom reward functions (e.g. must_include, eval_vqa, eval_fuzzy_image_match)","['Exact match', 'LLM-as-a-Judge', 'Exact match']",Simple mean success rates across tasks and subsets. There is no formal hypothesis testing or statistical significance tests.,['Mean']
329,Harry Mayne,yinNaturalLanguageCode2023,Natural Language to Code Generation in Interactive Data Science Notebooks,Include,"A benchmark for natural language to code, specifically for data science tasks in computational notebooks, e.g. Jupyter notebooks.",Code Generation,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Real task', 'Author-crafted']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), Execution-based scoring.","['Exact match', 'Reward']","Mean, error bars on some plots","['Mean', 'Std']"
330,Harry Mayne,khanXCodeEvalExecutionbasedLarge2024,"XCODEEVAL: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",Include,"A large multilingual, multitask coding benchmark. Includes classification, generation, translation and retrival tasks.",Code Generation,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],"Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)","['Random', 'Convenience']","Multiple choice, Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Structured']","Exact Match (accuracy, F1, precision, recall), Execution-based evaluation. Fairly comprehensive.","['Exact match', 'Reward']",Mean,['Mean']
331,Karolina Korgul,yoranAssistantBenchCanWeb2024,ASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?,Include,"This paper introduces AssistantBench - benchmark designed to evaluate whether web agents can solve realistic and time-consuming web-based tasks, such as finding gym schedules or real-estate prices. It also presents SPA (See-Plan-Act), a new web agent for more effective open-web navigation.",Agents,Web,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Real task', 'Author-crafted', 'Expert-crafted', 'Crowd-sourced']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Interaction', 'Structured']","Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",Simple mean and standard error of the mean (for plots like accuracy vs. steps),"['Mean', 'Std']"
332,Harry Mayne,guoRedCodeRiskyCode2024,RedCode: Risky Code Execution and Generation Benchmark for Code Agents,Include,Benchmark for unsafe code recognition and generation for LLM-based agents.,Agents,Coding,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Multiple choice, Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Structured']","LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Execution-based evaluation","['LLM-as-a-Judge', 'Reward']",Mean,['Mean']
333,Cornelius Emde,liEvaluatingInstructionfollowingRobustness2024,Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection,Include,"The paper tests the robustness of instruction following LMMs to prompt injection when the context maybe injected by adversaries, i.e. LLM driven web search.",Instruction Following,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Unknown","['Convenience', 'Unknown']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"simple mean across samples, then the relative decline in accuracy due to injection is computed.",['Mean']
334,Karolina Korgul,caoSpider2vHowFar2024,Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?,Include,"This paper introduces Spider2-V, a benchmark that evaluates multimodal agents on end-to-end data science and engineering workflows by requiring them to write code and perform GUI actions across professional enterprise applications. Tasks are implemented in a real-time, executable virtual environment and derived from real-world tutorials, with custom setup and evaluation functions. It tests agents’ ability to ground actions in visual and textual observations and leverage documentation through retrieval-augmented generation.",Agents,Coding,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Author-crafted', 'Expert-crafted', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Interaction', 'Structured']","Exact Match (accuracy, F1, precision, recall), execution-based verification, file-based comparison, information-based validation","['Exact match', 'Reward']",Simple mean/sum (Success Rate %),['Mean']
335,Valentin Hoffman,sheScoNeBenchmarkingNegation2023,ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning,Include,"This paper examines how well LMs handle natural language negation. To this end, the authors introduce two benchmarks: ScoNE-NLI, derived from the existing benchmark MoNLI, contains contrast sets of six examples with up to two negations where either zero, one, or  both negative morphemes affect the NLI label; ScoNE-NLG is specifically designed for generative models like InstructGPT and contains contrasting triplets of half-completed narratives that have different completions depending on the presence and scope of negation.",NLP,Understanding,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
336,Valentin Hoffman,liuWe`reAfraidLanguage2023,We're Afraid Language Models Aren't Modeling Ambiguity,Include,"This paper examines the ability of LMs to handle ambiguity. The authors model ambiguity via its effects on entailment relations and collect a dataset of NLI examples, each annotated with a set of labels (potentially reflecting underlying ambiguity) and disambiguating rewrites. LMs perform substantially worse on AmbiEnt than humans.",NLP,Understanding,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r), edit-f1","['Exact match', 'Distribution', 'Correlation', 'Soft match']",simple mean,['Mean']
337,Karolina Korgul,zhouWebArenaRealisticWeb2024,WebArena: A Realistic Web Environment for Building Autonomous Agents,Include,"WebArena introduces an open‑domain benchmark and environment where language‑guided agents must navigate fully functional websites (e.g. e‑commerce stores, forums, GitLab, maps) to satisfy natural‑language task intents in a similar to real setting.",Agents,Web,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Author-crafted', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Extended interaction (e.g. conversation, calling an API and processing the response)",['Interaction'],"Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), must_include', 'fuzzy_match', and programmatic checks which don't fit standard categories","['Exact match', 'LLM-as-a-Judge', 'Soft match']",simple mean,['Mean']
338,Valentin Hoffman,yinALCUNALargeLanguage2023,ALCUNA: Large Language Models Meet New Knowledge,Include,"This paper aims to evaluate how well LMs can deal with new knowledge not seen during pretraining. The authors devise KnowGen, a method to create new knowledge based on existing resources, and use it to construct AlCuna, a dataset of questions about new biological species. Experiments show that LMs have substantial difficulties handling new knowledge.",Language Modelling,Updating,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'Procedurally-generated', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
339,Karolina Korgul,drouinWorkArenaHowCapable2024,WorkArena: How Capable are Web Agents at Solving Common Knowledge Work Tasks?,Include,"WorkArena introduces a benchmark of 33 enterprise software tasks using the ServiceNow platform to evaluate LLM-powered web agents. Alongside it, the authors present BrowserGym - a multimodal environment for training and assessing agents on web-based workflows.",Agents,Web,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Expert-crafted', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response)","['Free response', 'Interaction']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Mean and standard error, with bootstrap confidence intervals","['Mean', 'Std']"
340,Valentin Hoffman,hallVisoGenderDatasetBenchmarking2023,VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution,Include,"This paper examines occupation-related gender bias in vision-language models. The authors introduce VisoGender, a new benchmark of images associated with captions containing pronoun relationships of the depicted subjects and objects. They use VisoGender to probe resolution bias (difference between pronoun resolution accuracies for image subjects with masculine versus feminine gender presentations) and retrieval bias (bias in ratios of professionals with masculine and feminine gender presentations retrieved for a gender-neutral search query). Experiments on VisoGender provide evidence for bias in state-of-the-art vision-language models.",Alignment,Bias,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Ranking of images","['Multiple choice', 'Free response']","Resolution task: accuracy gap. Retrieval bias: Bias@K, Skew@K, NDKL.",['Distribution'],simple mean,['Mean']
341,Valentin Hoffman,hanInstinctiveBiasSpurious2024,The Instinctive Bias: Spurious Images lead to Illusion in MLLMs,Include,"This paper examines the extent to which multimodal LMs are distracted by spurious information provided via images in the context of commonsense question answering, which the authors refer to as ""instinctive bias."" The authors create CorrelationQA, a benchmark consisting of (i) commonsense questions, (ii) answer choices, and (iii) images in various forms that either support the correct or one of the incorrect answer choices. Applying CorrelationQA to a series of mulitmodal LMs, the authors find that all of them struggle if the information in the image does not support the correct answer choice.",Alignment,Bias,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), They also report accuracy drop between cases where the image supports the correct answer choice and the cases where it supports one of the incorrect answer choices.","['Exact match', '']",simple mean,['Mean']
342,Maria Grandury,wattsPARIKSHALargescaleInvestigation2024,PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data,Include,"This paper presents Pariksha, a benchmark of culturally-nuanced prompts in 10 Indic languages created by native speakers. This benchmark is used to evaluate the agreement between human and LLM evaluators in two settings, pairwise comparisons and direct scoring.",LLM as a Judge,,Multilinguality,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples)","['Author-crafted', 'Expert-crafted']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Human ratings', 'LLM-as-a-Judge']","Simple mean (for direct assessment scores), Elo rating via Maximum Likelihood Estimation (MLE) (for pairwise comparisons), Fleiss’ Kappa and Percentage Agreement (for inter-annotator agreement and human-LLM agreement), and Kendall’s Tau (for human-LLM leaderboard rank correlation).","['Mean', 'Other']"
343,Valentin Hoffman,esiobuROBBIERobustBias2023,ROBBIE: Robust Bias Evaluation of Large Generative Language Models,Include,"This paper attempts to make bias evaluation more robust by examining several bias benchmarks at the same time, two of which are new contributions of the paper, an approach referred to as ROBBIE (Robust Bias Evaluation). The authors apply ROBBIE to several LMs and also examine the effects of different bias mitigation strategies.",Alignment,Bias,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"BiasScore: percentage of demographic groups in a dataset for which the LM continuations are more negative (e.g., toxic) than the average percentage of negative generations across demographic groups",['Distribution'],,
344,Karolina Korgul,boisvertWorkArenaCompositionalPlanning2024,WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks,Include,"WorkArena++ is a benchmark building on top of WorkArena with 682 tasks that simulate realistic workflows for knowledge workers using the ServiceNow platform. It aims to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding of web agents, highlighting current challenges for AI in workplace automation compared to human performance.  The benchmark also provides a method for generating large datasets of observation/action traces for fine-tuning models.",Agents,Web,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Another benchmark', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Extended interaction (e.g. conversation, calling an API and processing the response)",['Interaction'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean and standard error,"['Mean', 'Std']"
345,Karolina Korgul,wangGTABenchmarkGeneral2024,,Include,"GTA is a benchmark designed to evaluate LLM-based tool agents in realistic settings using multimodal written-by-human queries and executable tools. It features 229 tasks requiring reasoning, planning, and real-world tool use, spanning perception, operation, logic, and creativity.",Agents,Tool Use,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Real task', 'Author-crafted', 'Expert-crafted', 'Crowd-sourced']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r)","['Exact match', 'LLM post-processing', 'Distribution', 'Correlation']","Simple mean/sum, correlation (Pearson's r)","['Mean', 'Other']"
346,Maria Grandury,liuNLEBench+NorGLMComprehensiveEmpirical2024,NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian,Include,"This paper introduces NLEBench, a benchmark designed to evaluate generative language models in Norwegian. It also presents NorGLM, a series of open-source generative language models trained specifically for Norwegian, and evaluates them on the benchmark.",NLP,,Multilinguality,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Expert-crafted', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Distribution (perplexity, calibration, correlation), Toxicity Score, Entailment Score","['Exact match', 'Soft match', 'Distribution', 'LLM-as-a-Judge']",simple mean,['Mean']
347,Lujain Ibrahim,levySafeTextBenchmarkExploring2022,SafeText: A Benchmark for Exploring Physical Safety in Language Models,Include,"The paper introduces a benchmark dataset, SAFETEXT, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. It evaluates several state-of-the-art models on the dataset.",Alignment,Safety,,Real task examples (e.g. GitHub issues),['Real task'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Distribution (perplexity, calibration, correlation)","['Exact match', 'Human ratings', 'LLM-as-a-Judge', 'Distribution']",,
348,Lujain Ibrahim,mireshghallahCanLLMsKeep2024,Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory,Include,"The paper introduces a benchmark grounded in the theory of contextual integrity and designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. It consists of four tiers, gradually increasing in complexity, with the final tier evaluating contextual privacy reasoning and theory of mind capabilities.",Alignment,Alignment,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM post-processing (extracting answers, reformatting for automated scoring), Correlation (Matthew's correlation, Pearson's r)","['Exact match', 'Human ratings', 'LLM post-processing', 'Correlation']",simple mean,['Mean']
349,Maria Grandury,parkOpenKoLLMLeaderboard2024,Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark,Include,"This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark to evaluate large language models (LLMs) in Korean. It includes human and machine translations of the English Open LLM Leaderboard benchmarks and adds a benchmark curated from scratch. They keep private test sets to prevent data leakage, offering empirical insights through correlation and temporal analyses.",NLP,,Multilinguality,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), Ko-H5 score","['Exact match', 'Unknown']","simple mean, correlation","['Mean', 'Other']"
350,Karolina Korgul,fanR2HBuildingMultimodal2023,R2H: Building Multimodal Navigation Helpers that Respond to Help Requests,Include,"R2H introduces benchmark that evaluates navigation‑helper agents which ""see and respond"". It defines two tasks: 
1. Respond to Dialog History (RDH) 
2. Respond‑during‑Interaction (RdI) 
They are built on three vision‑and‑dialog navigation corpora, and supply automatic metrics + human studies to judge how well a helper’s natural‑language responses help a (fixed) performer agent complete navigation. ",Agents,General,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), The primary metrics (GP, SR, SPL, PWSR) measure the task performer's navigation success when guided by the helper. Helper effectiveness is inferred from these outcomes, with SPL and PWSR combining success and path efficiency.","['Exact match', 'Reward']",Simple means with error bars,"['Mean', 'Std']"
351,Negar Foroutan,chiPLUELanguageUnderstanding2023,PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English,Include,"The paper introduces the Privacy Policy Language Understanding Evaluation (PLUE) benchmark, a multi-task benchmark for evaluating the privacy policy language understanding across various tasks. They also collect a large corpus of privacy policies
to enable privacy policy domain-specific language model pre-training. They evaluate several
generic pre-trained language models and continue pre-training them on the collected corpus.",Law,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Unknown,['Unknown'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
352,Negar Foroutan,aggarwalIndicXNLIEvaluatingMultilingual2022,IndicXNLI: Evaluating Multilingual Inference for Indian Languages,Include,"The paper introduces INDICXNLI, an NLI dataset for 11 Indic languages. It has been created by high-quality machine translation of the original English XNLI dataset and the analysis
attests to the quality of INDICXNLI. By finetuning different pre-trained LMs on this INDICXNLI, they analyze various cross-lingual transfer techniques with respect to the impact of the choice of language models, languages, multi-linguality, mix-language input,
etc.",NLP,Understanding,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Unknown,['Unknown'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple Mean,['Mean']
353,Negar Foroutan,sanchetiAgentspecificDeonticModality2022,Agent-Specific Deontic Modality Detection in Legal Language,Include,"This paper introduces, LEXDEMOD, a corpus of English contracts annotated with deontic modality expressed with respect to a contracting party or agent along with the modal triggers. We benchmark this dataset on two tasks: (i) agent-specific multi-label deontic modality classification, and (ii) agent-specific deontic modality and trigger span detection using Transformer-based language models. Transfer learning experiments show that the linguistic diversity of modal expressions in LEXDEMOD generalizes reasonably from lease to employment and rental agreements.",Law,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Author-crafted', 'Crowd-sourced']",Unknown,['Unknown'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean and standard deviation,"['Mean', 'Std']"
354,Maria Grandury,kotoLargeLanguageModels2023,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,Include,"The paper introduces IndoMMLU, a multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia created by professional teachers.",NLP,Understanding,Multilinguality,Human exam questions (e.g. GRE questions),['Human exams'],"Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']","simple mean, correlation, calibration","['Mean', 'Other']"
355,Negar Foroutan,braunAGBDECorpusAutomated2024,AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts,Include,"In this paper, the authors introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts. Together with the data, they present a first baseline for the task of detecting potentially void
clauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5. The results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54. ",Law,,,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],Unknown,['Unknown'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Unkown ,['Unknown']
356,Thom Foster,aroraHaveLLMsAdvanced2023,"Have LLMs Advanced Enough?
A Challenging Problem Solving Benchmark For Large Language Models",Include,"Curate a set of 515 challenging engineering, mathematics, physics and chemistry questions from the entrance exams to top Indian universities.",Reasoning,Mathematical,,Human exam questions (e.g. GRE questions),['Human exams'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall), For multiple correct multiple choice questions, if there were 4 correct answers and the taker selects 3, they score 0.75. If they selected 4 correct and an additional 5th incorrect, they score 0. This is to mimic actual IEE exam scoring. ","['Exact match', 'Soft match']","simple mean, optionally the negative scoring as above",['Mean']
357,Negar Foroutan,doddapaneniLeavingNoIndic2023,"Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",Include,"In this work, the authors aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, they curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, they create a humansupervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. ",NLP,Understanding,Multilinguality,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Random sample (creators defined a task space and sampled from it),['Random'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Simple mean, standard deviation","['Mean', 'Std']"
358,Yushi Yang,gharaeeBIOSCAN5MMultimodalDataset2024,BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity,Include,"BIOSCAN-5M is a comprehensive dataset containing multi-modal information for over 5 million insect specimens, and it significantly expands existing image-based biological datasets by including taxonomic labels, raw nucleotide barcode sequences, assigned barcode index numbers, geographical, and size information.",Biology,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
359,Karolina Korgul,zhouHAZARDChallengeEmbodied2024,HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments,Include,"The HAZARD Challenge introduces a benchmark for evaluating embodied agents' decision-making in dynamically changing environments such as fires, floods, and wind. It supports reinforcement learning, rule-based, search-based, and LLM-driven agents, using realistic simulations built on the ThreeDWorld platform to assess object rescue performance in disaster scenarios.",Agents,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), rescued value rate, averaged rescue step, averaged damaged rate","['Exact match', 'Reward']",simple mean,['Mean']
360,Maria Grandury,yangCanLargeLanguage2024,Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?,Include,"The paper introduces ConsisEval, a benchmark to evaluate the ""hard-to-easy"" inconsistency in large language models (LLMs), i.e., when they can correctly solve harder problems but fail on easier counterparts. The benchmark consists in pairs of questions with a strict order of difficulty, generated by an LLM and annotated by humans. They propose an evaluation metric rooted in probability theory.",Language Modelling,Robustness,,"Expert-crafted task examples (e.g. hand-written examples), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Expert-crafted', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation), Consistency Score, Relative Consistency Score","['Exact match', 'Distribution', 'Distribution']",simple mean,['Mean']
361,Thom Foster,guoCanLlmsSolve2024,Can LLMs Solve Molecule Puzzles? A Multimodal Benchmark for Molecular Structure Elucidation,Include,"217 examples of chemical structure elucidation. ie given a count of the number of each type of atom e.g C6 H22 O2, work out the molecular structure in SMILES notation. This is a long and challenging process that requires running and understanding many types of chemistry tests (H-NMR, C-NMR etc). They break down this task into sub-tasks with 20,000 examples of each sub-task. ",Biology,,,Real task examples (e.g. GitHub issues),['Real task'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), Many chemistry specific metrics, such as molecule validity, Fingerprint Tanimoto Similarity etc.","['Exact match', 'Soft match']",,
362,Valentin Hoffman,felknerWinoQueerCommunityintheloopBenchmark2023,WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,Include,"This paper examines biases in LMs that harm the LGBTQ+ community. Specifically, the authors introduce WinoQueer, a community-sourced benchmark for measuring anti-queer and anti-trans bias in LMs. They evaluate several LMs on WinoQueer, finding substantial evidence of anti-LGBTQ+ bias.",Alignment,Bias,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Crowd-sourced', 'Another benchmark', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],This task is not based on model responses; it exclusively relies on the probability assigned to input tokens.,['Logits'],Bias Score: percentage of examples for which the likelihood of the more stereotypical sentence is higher than the likelihood of the less stereotypical sentence.,['Distribution'],simple mean,['Mean']
363,Karolina Korgul,mathaiKGymPlatformDataset2024,kGym: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution,Include,"The paper introduces KGYM, a platform, and KBENCHSYZ, a dataset, to benchmark Large Language Models on the task of resolving real-world Linux kernel crashes. The KGYM platform provides a software engineering environment for large-scale experiments on the Linux kernel, including compiling, running, detecting crashes, inspecting logs, and querying/patching the codebase. KBENCHSYZ is a benchmark dataset derived from real-world Linux kernel bugs, containing crash reports, reproducers, and developer-written fixes. ",Agents,Coding,,"Real task examples (e.g. GitHub issues), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Procedurally-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple sum/mean - raw percentages,['Mean']
364,Thom Foster,liFRoGEvaluatingFuzzy2024,FRoG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models,Include,"The benchmark aims to measure the ability of LLMs to understand “general quantifiers”, words like “all”, “most”, “some”. The take GSM8K+MathQA questions and mask out occurrences of “x%”. Given the masked question and true answer the model has to decide which quantifier (out of a multiple choice selection) best entails the answer",Reasoning,Mathematical,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],"Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
365,Karolina Korgul,dingEasy2HardbenchStandardizedDifficulty2024,Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization,Include,"Easy2Hard-Bench is a benchmark comprising six datasets across diverse domains like mathematics, programming, chess puzzles, and reasoning. Its main contribution is providing datasets with fine-grained, continuous numerical difficulty scores for each problem. The difficulty scores are systematically estimated using established difficulty ranking systems such as Item Response Theory (IRT) and Glicko-2 models, leveraging abundant performance data from humans and LLMs.",Reasoning,Logical,,"Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Human exams', 'Real task', 'Author-crafted', 'Expert-crafted', 'Another benchmark', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Multiple choice', 'Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']","Mean, standard deviation, Spearman correlation for difficulty alignment (IRT vs human/GPT4)","['Mean', 'Std']"
366,Karolina Korgul,maLargeLanguageModels2024,Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach,Include,"TextStarCraft II turns the full StarCraft II video‑game into a purely text interface so that large language‑model (LLM) agents can play the game through natural‑language commands.
The paper also introduces “Chain of Summarisation” (CoS), a cache‑like, multi‑frame prompting procedure that lets an LLM compress streams of observations and issue batches of actions fast enough to keep up with real‑time play. ​It also provided a benchmark, which enables the evaluation of LLMs’ real-time strategic decision-making and long-term planning abilities through natural language. ",Reasoning,Planning,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Procedurally-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), Correlation (Matthew's correlation, Pearson's r), Win Rate, Population Block Ratio (PBR), Resource Utilisation Ratio (RUR), Average Population Utilization (APU), Technology Rate (TR)","['Exact match', 'Correlation', 'Reward']",Simple mean and standard deviation,"['Mean', 'Std']"
367,Karolina Korgul,liAPIbankComprehensiveBenchmark2023,API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs,Include,"The paper introduces API-Bank, a benchmark designed to evaluate and enhance LLMs in utilising external tools. It addresses questions regarding the effectiveness of current LLMs in tool use, methods for improvement, and remaining obstacles. The benchmark includes a runnable evaluation system with 73 API tools and a comprehensive training set of tool-use dialogues, used to train a tool-augmented LLM called Lynx.",Agents,Tool Use,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Expert-crafted', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",Simple mean,['Mean']
368,Karolina Korgul,wenBenchmarkingComplexInstructionfollowing2024,Benchmarking Complex Instruction-Following with Multiple Constraints Composition,Include,"COMPLEXBENCH is a new benchmark designed to evaluate the complex instruction-following ability of large language models, focusing specifically on the composition of multiple constraints within instructions. The authors introduce a rule-augmented LLM-based evaluation method to reliably assess model performance on complex instructions with composed constraints.",Instruction Following,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Author-crafted', 'Crowd-sourced', 'Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Mean and pairwise agreement figures;,"['Mean', 'Other']"
369,Karolina Korgul,zhengJudgingLLMasajudgeMTbench2023,Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,Include,"The paper introduces MT‑Bench, a set of 80 hand‑crafted multi‑turn prompts, and Chatbot Arena, a crowdsourced head‑to‑head battle platform, to evaluate LLM assistants on open‑ended dialogue. It shows that strong LLMs such as GPT‑4 can act as reliable automatic judges of other models’ answers, matching human preferences with around 80 % agreement, and discusses biases and mitigation strategies for the “LLM‑as‑a‑judge” paradigm.

",Alignment,Alignment,User Interaction,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)","['Author-crafted', 'Crowd-sourced', 'Another benchmark']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'Human ratings', 'LLM-as-a-Judge']","Simple mean scores, simple sum scores, inter‑annotator agreement percentages.",['Mean']
370,Simeng Han,tamkinTaskAmbiguityHumans2023,Task Ambiguity in Humans and Language Models,Include,"The paper investigates how both humans and models behave in the face of such task ambiguity by proposing AmbiBench, a new benchmark of six ambiguously-specified classification tasks. The paper also evaluates humans and models on AmbiBench by seeing how well they identify the intended task using 1) instructions with varying degrees of ambiguity, and 2) different num-
bers of labeled examples.",Language Modelling,Robustness,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
371,Simeng Han,heOlympiadBenchChallengingBenchmark2024,OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,Include,"In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Notably, the best-performing model, GPT-4V, attains a low average score on OlympiadBench, with an even lower school in physics, highlighting the benchmark rigor and the intricacy of physical reasoning.",Reasoning,Mathematical,,Human exam questions (e.g. GRE questions),['Human exams'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],Post-processing with heuristics,['Soft match'],,
372,Simeng Han,shiLanguageModelsAre2023,Language models are multilingual chain-of-thought reasoners,Include,"We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. ",Reasoning,Mathematical,Multilinguality,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
373,Hunar Batra,liHaluEvalLargescaleHallucination2023,"HaluEval: A Large-Scale Hallucination Evaluation Benchmark
for Large Language Models",Include,"HaluEval introduces a large-scale benchmark to evaluate the hallucination behaviour of large language models (LLMs) across question answering, dialogue, summarization, and general queries. It seeks to understand what types of content and to which extent LLMs can generate hallucinated response. It combines 30,000 automatically generated and 5,000 human-annotated samples using a two-step “sampling-then-filtering” framework. Results show that LLMs, including ChatGPT, frequently produce hallucinations (~19.5%) and struggle to recognise them, but performance improves with external knowledge or reasoning steps.",Language Modelling,Hallucination,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Human ratings']",Mean accuracy is used to aggregate model performance across tasks and conditions,['Mean']
374,Simeng Han,mialonGAIABenchmarkGeneral2024,GAIA: A Benchmark for General AI Assistant,Include,"Create a benchmark which is conceptually simple questions for humans. It contains 466 quesitons from realistic questions and annotated by humans. The questions require fundamental abilities: reasoning, code, tool use et al.",Agents,,,Real task examples (e.g. GitHub issues),['Real task'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
375,Hunar Batra,sunHeadtotailHowKnowledgeable2024,Head-to-Tail: How knowledgeable are LLMs? aka will LLMs replace knowledge graphs?,Include,"Head‑to‑Tail introduces an 18 k‑example question–answer benchmark that probes how much factual knowledge large language models really store. Questions are grouped by the popularity of the subject entity (head, torso, tail) and span four domains (IMDb movies, Goodreads books, academic entities from MAG and DBLP, and DBpedia open‑domain facts). Across 16 public LLMs, even GPT‑4 answers only ≈ 31 % correctly overall, and performance drops sharply from head to tail entities—indicating that current LLMs are far from replacing structured knowledge graphs.",Language Modelling,Hallucination,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Procedurally-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']","They report simple percentage means, and compute Spearman’s ρ and Pearson’s r between rule‑ and LLM‑based scores to show consistency; and give a 98 % human‑vs‑ChatGPT agreement figure (percent agreement, not kappa).","['Mean', 'Other']"
376,Hunar Batra,monteiroRepLiQAQuestionansweringDataset2024,RepLiQA: A question-answering dataset for benchmarking LLMs on unseen reference content,Include,"REPLIQA introduces a five‑split, entirely synthetic question‑answering benchmark (~90 K QA pairs over ~18 K reference docs) whose documents, people, places and events do not exist on the public web.  Because the knowledge is novel, the dataset isolates a model’s reading‑with‑context ability from any memorised pre‑training knowledge.  The paper also reports a large‑scale benchmark of 18 production LLMs, showing sharp performance drops relative to TRIVIAQA and highlighting that larger models often lean on memory rather than reading.",Retrieval,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Author-crafted', 'Crowd-sourced']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Mean, recall, F1 with standard error comparisons","['Mean', 'Std']"
377,Maria Grandury,salemiLaMPWhenLarge2024,LaMP: When Large Language Models Meet Personalization,Include,"The paper introduces LaMP, a benchmark designed to train and evaluate large language models (LLMs) for personalized output generation. It includes personalized text classification and generation tasks, along with retrieval-augmented personalization methods that significantly improve performance in both zero-shot and fine-tuned settings.",Alignment,Alignment,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Real task', 'Author-crafted', 'Another benchmark', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",simple mean,['Mean']
378,Hunar Batra,zhangCLAMBERBenchmarkIdentifying2024,CLAMBER: A benchmark for identifying and clarifying ambiguous information needs in LLMs,Include,"CLAMBER is a 12 K‑example benchmark that probes two complementary abilities of large language models (LLMs): (1) detecting whether a user query is ambiguous and (2) asking a helpful clarifying question when ambiguity is present. The dataset is organised around an eight‑category taxonomy that spans epistemic misalignment, linguistic ambiguity, and aleatoric output‑related ambiguities, enabling fine‑grained evaluation of current open and closed LLMs under several prompting regimes.",,Robustness,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Soft match', 'Human ratings']",Mean and standard error; calibration (ECE) and AUROC for confidence analyses.,"['Mean', 'Std']"
379,Hunar Batra,chenExploringPotentialLarge2024,Exploring the potential of LLMs in Computational Argumentation,Include,"The authors systematically evaluate large language models (LLMs) on a broad suite of computational argumentation tasks spanning both argument mining (claim, evidence, stance, type) and argument generation (argument construction, summarization). They unify 14 public datasets into six task categories, introduce a new document‑level Counter‑Speech Generation benchmark that jointly exercises mining + generation, and report zero‑/few‑shot results for GPT‑3.5‑Turbo, Flan‑T5/UL2, and Llama‑2 models.",Reasoning,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Real task', 'Author-crafted']",Random sample (creators defined a task space and sampled from it),['Random'],"Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics)","['Exact match', 'Soft match', 'Human ratings']",Mean; McNemar x^2 test for mining,"['Mean', 'Tests']"
380,Hunar Batra,abdinKITABEvaluatingLLMs2024,KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval,Include,"The paper introduces KITAB, a large‑scale benchmark that evaluates how well large language models (LLMs) can satisfy multi‑way constraint‑based information‑retrieval queries (e.g., “list all books by Toni Morrison published between 1970‑1980”). KITAB contains > 600 authors and 12 989 natural‑language queries with automatically verified ground‑truth answer sets, plus an evaluation protocol that separates failure modes such as irrelevance, constraint violations and incompleteness.",Instruction Following,,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
381,Simeng Han,xiaSportQABenchmarkSports2024,SportQA: A Benchmark for Sports Understanding in Large Language Models,Include,"We introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. ",Sports,,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
382,Simeng Han,xiongTRIGOBenchmarkingFormal2023,TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models,Include,"The paper proposes TRIGO, a benchmark evaluating the ability of generative language models to formally reduce trigonometric expressions using the Lean proof assistant. It includes manually annotated real-world problems and procedurally generated problems of controlled difficulty.",Reasoning,Mathematical,,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Human exams', 'Author-crafted', 'Procedurally-generated']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
383,Simeng Han,chenTheoremQATheoremdrivenQuestion2023,TheoremQA: A Theorem-driven Question Answering Dataset,Include,"The paper introduces TheoremQA, a benchmark designed to evaluate LLMs’ abilities to apply university-level science theorems across Math, Physics, EE&CS, and Finance. It collects 800 high-quality questions over 354 theorems and evaluates 16 LLMs under chain-of-thought and program-of-thought prompting.",Reasoning,Logical,,"Human exam questions (e.g. GRE questions), Expert-crafted task examples (e.g. hand-written examples), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'Expert-crafted', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
384,Simeng Han,huangOlympicArenaBenchmarkingMultidiscipline2024,OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI,Include,"OlympicArena is a bilingual, multimodal benchmark with 11,163 problems from 62 Olympic competitions across 7 disciplines. It evaluates LLMs’ and LMMs’ cognitive reasoning abilities using both answer-level and process-level metrics, highlighting major limitations even in state-of-the-art models like GPT-4o.",Reasoning,,,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)","['Human exams', 'Author-crafted']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
385,Simeng Han,friederMathematicalCapabilitiesChatGPT2023,Mathematical Capabilities of ChatGPT,Include,"This paper introduces GHOSTS and miniGHOSTS, the first graduate-level natural language benchmarks designed to evaluate the mathematical reasoning abilities of ChatGPT and GPT-4. It covers a broad spectrum of tasks, including proof completion, symbolic integration, and Olympiad problem solving, with human-annotated fine-grained evaluation of errors and failures.",Reasoning,Mathematical,,"Human exam questions (e.g. GRE questions), Expert-crafted task examples (e.g. hand-written examples), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'Expert-crafted', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Human ratings (text quality, preference, NOT manual scoring of other metrics)",['Human ratings'],,
386,Maria Grandury,aroraCausalGymBenchmarkingCausal2024,CausalGym: Benchmarking causal interpretability methods on linguistic tasks,Include,"This paper introduces CausalGym, a benchmark adapted from SyntaxGym for evaluating causal interpretability methods on their ability to find linear features in LMs that, when subject to intervention, causally influence linguistic behaviours. The authors show that the causal mechanisms require multi-step movement of information, and that they emerge in discrete stages (not gradually) early in training.",,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Another benchmark', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Targeted', 'Criterion']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']",simple mean,['Mean']
387,Simeng Han,mishraNumGLUESuiteFundamental2022,NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,Include,"NUMGLUE is a multi-task benchmark designed to test AI systems' ability to perform simple arithmetic reasoning across eight diverse tasks. It introduces four new datasets and shows that even state-of-the-art models underperform compared to humans, emphasizing the need for robust, generalizable mathematical reasoning.",Reasoning,Mathematical,,"Human exam questions (e.g. GRE questions), Expert-crafted task examples (e.g. hand-written examples)","['Human exams', 'Expert-crafted']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
388,Simeng Han,kazemiBoardgameQADatasetNatural2023,BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information,Include,"BoardgameQA is a synthetic dataset designed to test language models’ ability to reason with contradictory and incomplete information. It models defeasible reasoning scenarios where conflicts must be resolved via predefined source preferences, combining multi-hop reasoning and missing knowledge challenges.",Reasoning,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
389,Maria Grandury,leiterPrExMeLargeScale2024,PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation,Include,"This paper introduces PrExMe, a benchmark of prompt templates to evaluate LLMs as metrics for machine translation and summarization tasks. It systematically explores how prompt variation impacts metric performance and stability, revealing both robust patterns and significant sensitivities in prompt design.",NLP,Summarization,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Correlation (Matthew's correlation, Pearson's r)","['Exact match', 'Correlation']","simple mean, significance clusters",['Mean']
390,Valentin Hoffman,sahooIndiBiasBenchmarkDataset2024,IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context,Include,"This paper examines LMs' biases and stereotypes in the Indian context. Specifically, the authors introduce IndiBias, a benchmark consisting of an adapted, translated, and augmented version of CrowS-Pairs. The authors also conduct a variant of SEAT to measure intersectional biases in LMs. Using both their Indian variant of CrowS-Pairs and SEAT, the authors test a series of LMs, finding varying levels of social biases.",Alignment,Bias,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",This task is not based on LM responses; it solely relies on measuring the probabilities assigned to tokens in the sentence pairs.,['Logits'],Bias Percentage: percentage of sentence pairs for which the more stereotypical sentence has a higher probability than the less stereotypical sentence.,['Distribution'],simple mean,['Mean']
391,Valentin Hoffman,marchiorimanerbaSocialBiasProbing2024,Social Bias Probing: Fairness Benchmarking for Language Models,Include,"This paper examines social biases in LMs, focusing on disparate treatment. The authors start by observing a limitation of prior benchmarks (e.g., CrowS-Pairs), specifically that they are limited to binary association tests. As a remedy, the authors collect SoFa (Social Fairness), a benchmark that makes it possible to compare stereotypical associations across several identities. They show in their experiments that the resulting rankings of LMs differ from prior benchmarks.",Alignment,Bias,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",This task is not based on model responses; it relies solely on perplexity measurements.,['Logits'],"SoFa Score: variance in normalized log perplexity across grouped sentences (i.e., sentences with the same stereotype and different identities)",['Distribution'],simple mean,['Mean']
392,Karolina Korgul,xieTravelPlannerBenchmarkRealworld2024,TravelPlanner: A Benchmark for Real-World Planning with Language Agents,Include,"TravelPlanner is a benchmark created to evaluate the real-world planning capabilities of language agents, particularly in the context of multi-day travel planning under complex constraints. It provides a static sandbox environment with tools and includes 1,225 annotated user queries with reference plans. Experiments reveal that current LLMs struggle significantly with the complexity of these tasks, highlighting major limitations in current agent planning capabilities.",Reasoning,Planning,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Expert-crafted', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",Mean and breakdown by category,['Mean']
393,Lujain Ibrahim,marraffiniGreatestGoodBenchmark2024,The Greatest Good Benchmark: Measuring LLMs√¢‚Ç¨‚Ñ¢ Alignment with Utilitarian Moral Dilemmas,Include,The paper introduces the Greatest Good Benchmark (GGB) to evaluate LLMs moral judgments using utilitarian dilemmas. It analyzes a diverse set of models uncovering consistent moral preferences that diverge from established moral theories and lay population moral standards.,Alignment,Alignment,,"Human exam questions (e.g. GRE questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'LLM-generated']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Multiple choice,['Multiple choice'],"Human ratings (text quality, preference, NOT manual scoring of other metrics)",['Human ratings'],"simple mean + standard errors, standard deviations","['Mean', 'Std']"
394,Lujain Ibrahim,zhangSafetyBenchEvaluatingSafety2024,SafetyBench: Evaluating the Safety of Large Language Models,Include,"The paper introduces SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs through MCQs spanning a range of safety concerns. It incorporates both Chinese and English data and tests both Chinese and English LLMs. ",Alignment,Safety,,"Human exam questions (e.g. GRE questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean,['Mean']
395,Jonathan Rystrøm,sunFevalAsssessingFundamental2024,F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods,Include,"F-Eval is a bilingual (Chinese/English) benchmark for ""fundamental"" LLM abilities across expression, commonsense and logic. The benchmark works for both base models and instruction tuned models.",NLP,,Multilinguality,"Human exam questions (e.g. GRE questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Human exams', 'Another benchmark', 'LLM-generated']",Unknown,['Unknown'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Distribution (perplexity, calibration, correlation)","['Exact match', 'LLM-as-a-Judge', 'Distribution']",The models are primarily ordered by rank.,['Mean']
396,Hannah Kirk,laineMeMyselfAI2024,"Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",Include,"The paper introduces the Situational Awareness Dataset (SAD), a benchmark comprising 7 task categories and >13,000 questions to quantify situational awareness in Large Language Models (LLMs). SAD evaluates models' ability to recognize their own generated text, predict their own behavior, determine whether a prompt is from evaluation or deployment, and follow instructions that depend on self-knowledge.",Agents,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Expert-crafted', 'Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Exact match', 'LLM-as-a-Judge']","In the main table, they only reported the weighted mean. In the appendix, they say ""Where plotted, the error bars show what the standard error would be if estimated using bootstrapping. To estimate the standard error of our reported model scores, we analytically estimate the standard error.""","['Mean', 'Std']"
397,Jonathan Rystrøm,basuAPIBLENDComprehensiveCorpora2024,API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs,Include,"API-Blend introduces a benchmark and training dataset for constructing API calls with LLMs. It especially focuses on enhancing existing datasets to mimic real-world use cases for the tasks of tool detection, slot filling, and sequencing.",Agents,Tool Use,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)","['Exact match', 'Soft match']",Weighted mean based on test samples.,['Mean']
398,Hannah Kirk,chevalierLanguageModelsScience2024,Language Models as Science Tutors,Include,"TUTOREVAL is a benchmark for evaluating language models as science tutors, consisting of questions about long chapters from STEM textbooks written by experts. The benchmark combines long contexts, free-form generation, and scientific knowledge across multiple disciplines. They use GPT-4 as an evaluator to mark the answers against key points provided by experts.",User Interaction,,General Science,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],"Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Free response (e.g. summary paragraph, executable code)",['Free response'],"Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)","['Human ratings', 'LLM-as-a-Judge']","Simple mean across questions for each category. They also compute correlation metrics between human and GPT-4 evaluations: Pearson, Spearman, and Kendall-τ coefficients to validate GPT-4 as a reliable evaluator.","['Mean', 'Other']"
399,Thom Foster,bhuiyaSeeminglyPlausibleDistractors2024,Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?,Include,"Takes HotpotQA makes it harder by adding ""distractor"" paragraphs. ",Reasoning,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
400,Jonathan Rystrøm,pratoLargeLanguageModels2024,Do Large Language Models Know How Much They Know?,Include,"The paper evaluates how well smalll LLMs can recollect material in their fine-tuning data. Specifically, they are fine-tuned on fictive diary entries and asked to reproduce this training material. The benchmarked models are not instruction-tuned but rather base models.",Language Modelling,Hallucination,,Procedurally-generated task examples (e.g. Creating instances from a template),['Procedurally-generated'],Random sample (creators defined a task space and sampled from it),['Random'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean over examples (accuracy),['Mean']
401,Hannah Kirk,huangFlamesBenchmarkingValue2024,Flames: Benchmarking Value Alignment of LLMs in Chinese,Include,"FLAMES is an adversarial benchmark in Chinese for evaluating value alignment of LLMs, encompassing five dimensions of human values (Fairness, Legality, Data Protection, Morality, Safety). The benchmark includes 2,251 manually designed adversarial prompts,  annotated responses from 17 mainstream LLMs, and a specified scorer trained to evaluate model responses accurately.",Alignment,Alignment,Multilinguality,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Author-crafted', 'Crowd-sourced']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM post-processing (extracting answers, reformatting for automated scoring)",['LLM post-processing'],The paper reports simple means for calculating the overall accuracy as a macro average across all five dimensions,['Mean']
402,Jonathan Rystrøm,baiMTbench101FinegrainedBenchmark2024,MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues,Include,"MT-Bench-101 proposes a detailed taxonomy of multi-turn chat abilities with an associated benchmark. The overarching abilities are perceptivity, adaptibility, and interactivity. These abilities are broken down into further subskills and 13 total subtasks. ",User Interaction,,,LLM-generated task examples (e.g. Filtered from responses to a prompt),['LLM-generated'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],Simple mean,['Mean']
403,Jonathan Rystrøm,boginSUPEREvaluatingAgents2024,SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories,Include,"SUPER evaluates LLM agents ability to reproduce ML experiments. They provide both end-to-end examples as well as smaller masked examples. The models are tested both on the outcome (e.g., reaching a score) and reaching different landmarks.",Agents,,,"Expert-crafted task examples (e.g. hand-written examples), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Expert-crafted', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],mostly mean - variance only reported for best model. ,"['Mean', 'Std']"
404,Jonathan Rystrøm,wangAppBenchPlanningMultiple2024,AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction,Include,AppBench is a benchmark for complex API interaction with interweaving dependencies across multiple Apps. It leverages graph relationships between apps to construct more complex test cases than previous works. ,Agents,Tool Use,,"Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],simple mean. ,['Mean']
405,Jonathan Rystrøm,chenLLMArenaAssessingCapabilities2024,LLMARENA: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments,Include,LLMArena is a comprehensive benchmark for multi-agent LLM games. It proposes seven game environments that test a wide array of capabilities. The games range from adversarial games like Poker to collaborative games like Hanabi (a personal favourite of this reviewer).,Reasoning,Planning,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Unknown,['Unknown'],"Extended interaction (e.g. conversation, calling an API and processing the response)",['Interaction'],"TrueScore, win rates, reward (game specific)",['Reward'],,
406,Valentin Hoffman,nangiaCrowSpairsChallengeDataset2020,CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,Include,"This paper examines social biases in LMs against protected demographic groups in the United States. The authors introduce a benchmark, Crowdsourced Stereotype Pairs (CrowS-Pairs), that consists of sentences pairs, where one is more stereotyping, and the other one is less stereotyping. Measuring the probability that LMs assign to these sentence pairs, the authors find that all evaluated LMs manifest substantial social biases across all tested categories.",Alignment,Bias,,Crowd-sourced task examples (e.g. Prolific-created tasks),['Crowd-sourced'],"Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",The task is not based on model responses; it solely relies on the probabilities assigned to the tokens in the two sentences.,['Logits'],"Percentage of items (i.e., sentence pairs) for which an LM assigns a higher (psuedo-)likelihood to the stereotyping sentence over the less stereotyping sentence",['Distribution'],simple mean,['Mean']
407,Hunar Batra,gaoEnablingLargeLanguage2023,Enabling LLMs to generate text with citations,Include,"The paper introduces ALCE (Automatic LLMs’ Citation Evaluation), the first fully‑reproducible benchmark that evaluates how well large language‑model systems answer open questions while providing sentence‑level citations to supporting passages.  ALCE includes three citation‑focused QA datasets (ASQA, QAMPARI, ELI5), automatic metrics for fluency, factual correctness, and citation quality, and extensive experiments showing that even GPT‑4‑based systems remain citation‑incomplete roughly half the time.",Retrieval,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'Soft match', 'LLM post-processing']","Mean, and human–automatic correlation (Cohen’s Kappa coefficient) for validation.","['Mean', 'Other']"
408,Yushi Yang,dumpalaSUGARCREPEDatasetVisionlanguage2024,SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations	,Include,"In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations.

We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. 

Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly to object attributes and spatial relations.",Language Modelling,Robustness,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
409,Yushi Yang,tanDevBenchMultimodalDevelopmental2024,DevBench: A multimodal developmental benchmark for language learning,Include,"We introduce DEVBENCH, a multimodal benchmark comprising seven language evaluation tasks spanning the domains of lexical, syntactic, and semantic ability, with behavioral data from both children and adults. '

'We evaluate a set of vision–language models on these tasks, comparing models and humans on their response patterns. '

'Across tasks, models exhibit variation in their closeness to human response patterns, and models that perform better on a task also more closely resemble human behavioral responses. DEVBENCH thus provides a benchmark for comparing models to human language development.' (abstract)",Language Modelling,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples)","['Author-crafted', 'Expert-crafted']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
410,Valentin Hoffman,morabitoSTOPBenchmarkingLarge2024,STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions,Include,"This paper examines social biases in LMs as they unfold in situations that exhibit gradually increasing levels of offensiveness. To this aim, the authors introduce STOP (Sensitivity Testing on Offensive Progressions), a benchmark containing sentences that describe situations escalating from less to more explicitly offensive. They find that all examined LMs are inconsistent at detecting explicitly offensive bias in STOP.",Alignment,Bias,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Procedurally-generated', 'LLM-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"simple mean, standard deviation","['Mean', 'Std']"
411,Hunar Batra,liangUHGEvalBenchmarkingHallucination2024,UHGEval: Benchmarking the Hallucination of Chinese LLMs via Unconstrained Generation,Include,"UHGEval introduces a 5k samples benchmark for evaluating hallucination in Chinese large‑language models.  The authors collect 2015‑2017 Chinese news articles, ask five different Chinese LLMs to continue each “news beginning” without any restrictive prompts, then automatically rank, label (keyword‑level), and human‑verify hallucinations. The paper also ships a modular evaluation framework supporting three task forms: discriminative, selective, and generative.",Language Modelling,Hallucination,,"Real task examples (e.g. GitHub issues), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Real task', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), BertScore, kwPrec","['Exact match', 'Soft match', 'Soft match']",Mean,['Mean']
412,Hunar Batra,diaoDoolittleBenchmarksCorpora2023,Doolittle: Benchmarks and Corpora for Academic Writing Formalization,Include,"The paper introduces Academic Writing Formalization (AWF), a paragraph‑level text‑refinement task that converts informal‑academic prose into formal‑academic prose, going beyond grammatical error correction to include word choice and structural improvements. To support the task, the authors release DOOLITTLE, a 68K‑paragraph corpus (55.6 K formal, 13.0 K informal) with expert rewrites for 930 test/dev paragraphs, and they benchmark nine systems, proposing metric‑oriented reinforcement learning (MORL) that lets smaller PLMs approach ChatGPT quality while still trailing human rewrites.",NLP,,,"Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)","['Real task', 'Author-crafted', 'Crowd-sourced']",Random sample (creators defined a task space and sampled from it),['Random'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Distribution (perplexity, calibration, correlation), Semantic Similarity, BARTScore, Char-level edit distance","['Exact match', 'Soft match', 'LLM-as-a-Judge', 'Distribution', 'Soft match']","Simple mean, and for annotation agreement Cohen’s Kappa coefficient was used.","['Mean', 'Other']"
413,Simeng Han,liCanLLMAlready2023,Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,Include,"BIRD is a large-scale benchmark for text-to-SQL generation that focuses on realistic, noisy, and large databases. It introduces 12,751 text-to-SQL pairs over 95 databases (33.4 GB) across 37 domains, emphasizing challenges in database value comprehension, external knowledge reasoning, and SQL execution efficiency.",Code Generation,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Structured response (e.g. valid JSON, API call alone)",['Structured'],Execution Accuracy (EX) and Valid Efficiency Score (VES),['Reward'],,
414,Simeng Han,bittonWinoGAViLGamifiedAssociation2022,WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models,Include,"WinoGAViL introduces a gamified benchmark where humans create vision-language association tasks that are easy for humans but challenging for AI models. Inspired by Codenames, it evaluates models’ abilities to reason about commonsense associations between textual cues and visual candidates.",Reasoning,Commonsense,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",['Author-crafted'],Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],Jaccard Index between model predictions and human-labeled associations,['Human ratings'],,
415,Hunar Batra,zhaoCould`veAskedThat2024,I Could’ve Asked That: Reformulating Unanswerable Questions,Include,"The paper introduces CouldAsk, a document‑grounded QA benchmark that first asks a model to detect when a user’s question is unanswerable from a given document and then reformulate that question so it becomes answerable while staying relevant to the user’s intent. COULDASK pools 6 sub‑datasets (3 existing Wikipedia‑based sets and 3 new GPT‑4‑generated–then–human‑verified sets from news, Reddit, and Yelp) and proposes reference‑free automatic metrics to score both the detection (F1) and reformulation (“success rate”) stages, revealing that today’s best LLMs still succeed less than 30 % of the time.",,NLP,Understanding,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Crowd-sourced', 'Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'LLM post-processing']",Simple mean is used for aggregation.,['Mean']
416,Hunar Batra,moneaGlitchMatrixLocating2024,A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia,Include,"The paper introduces Fakepedia, a large synthetic dataset of counter‑factual Wikipedia‑style paragraphs that intentionally contradict models’ stored factual knowledge.  Using this dataset, the authors benchmark several LLMs on their ability to ground answers in the prompt rather than in parametric memory and propose Masked Grouped Causal Tracing (MGCT) which is a fast, robust causal‑intervention method to reveal the internal computations that differentiate grounded from ungrounded responses.",,Grounding,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Multiple choice, Short free response (e.g. single word or number)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],Mean; authors additionally report t‑tests for MGCT effect differences and classification accuracy for the detector.,"['Mean', 'Tests']"
417,Valentin Hoffman,halevyFlexTapeCan`t2024,"""Flex Tape Can't Fix That"": Bias and Misinformation in Edited Language Models",Include,"This paper examines the extent to which model edits amplify social biases in LMs. To this end, the authors introduce Seesaw-cf, a benchmark of edits with accompanying prompts that aim to detect any bias-related effects of the edits. Using Seesaw-cf with several LMs and editing methods , the authors find that edits can amplify social biases in LMs.",Alignment,Bias,,"Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Another benchmark', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Output probability change of attribute","['Exact match', 'Human ratings', 'LLM-as-a-Judge', '']",simple mean,['Mean']
418,Hunar Batra,labanSummEditsMeasuringLLM2023,SummEdits: Measuring LLM Ability at Factual Reasoning Through the Lens of Summarization,Include,"SUMMEDITS introduces a 10‑domain benchmark to test whether language models can detect factual inconsistencies in summaries. The authors create a low‑cost, highly reproducible protocol in which seed summaries are lightly edited by an LLM and then labeled by humans as factually consistent or not; most LLMs perform barely above chance, with GPT‑4 still 8 pp below human accuracy.",NLP,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Mean,['Mean']
419,Hunar Batra,xiangCAREMIChineseBenchmark2023,CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care,Include,"CARE‑MI introduces a 1,612‑item Chinese benchmark that tests large‑language‑model misinformation in long‑form answers on the sensitive domain of maternity and infant care. Items are derived from biomedical KGs and medical‑licensing MCQ banks, converted mostly with LLM + rule pipelines into true/false and open‑ended questions, paired with retrieved evidence, and vetted by medical experts. The authors evaluate several Chinese LLMs, provide a human baseline, and release a fine‑tuned LLaMA‑13B “judge” model to automate scoring.",Knowledge,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Procedurally-generated', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
420,Hunar Batra,buchmannAttributeAbstainLarge2024,Attribute or Abstain: LLMs as Long Document Assistants,Include,"The authors introduce LAB, a 6‑task benchmark that evaluates whether LLMs reading single long documents can (i) answer or classify correctly, (ii) attribute each claim to explicit evidence spans, or (iii) abstain when the answer is absent. They compare five LLMs and five retrieval strategies, showing that “citation” (one‑shot answer + evidence generation) works best for large or fine‑tuned models, while post‑hoc evidence retrieval can help small models.",Retrieval,,,"Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)","['Real task', 'Another benchmark']",Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Short free response', 'Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)","['Exact match', 'Soft match', 'LLM post-processing']","‑ Per‑metric means & confidence via single runs

‑ Spearman correlation (response/evidence vs position)

‑ Cohen’s κ for human IAA (0.74‑0.77)","['Mean', 'Other']"
421,Hunar Batra,pratoEpiKevalEvaluationLanguage2023,EpiK-Eval: Evaluation for Language Models as Epistemic Models,Include,"EpiK‑Eval is a synthetic QA benchmark that tests whether language models can consolidate facts that are scattered across multiple training documents, rather than stored inside a single context window. The authors generate 18 templated story‑based tasks (counting, temporal, causal, etc.), create both unsegmented and segmented versions of each story, fine‑tune several LLMs on each setting, and compare performance.",Retrieval,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],"Free response (e.g. summary paragraph, executable code)",['Free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],Simple mean,['Mean']
422,Hunar Batra,liuAlignBenchBenchmarkingChinese2024,AlignBench: Benchmarking Chinese Alignment of LLMs,Include,"AlignBench is a 683‑query, eight‑category benchmark that tests how well Chinese‑supported LLMs satisfy user intent (“alignment”) in realistic, open‑ended settings. The authors supply a human‑in‑the‑loop curation pipeline, reference answers with evidence links, and a rule‑calibrated, multi‑dimensional GPT‑4‑as‑judge evaluation scheme, then benchmark 17 popular LLMs.",Alignment,Alignment,Multilinguality,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code)",['Free response'],"LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",['LLM-as-a-Judge'],"Mean, sample‑level Pearson r, system‑level Pearson r, pairwise win‑rate % (for agreement studies).","['Mean', 'Other']"
423,Hunar Batra,ramprasadAnalyzingLLMBehavior2024,Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends,Include,"This work releases a span‑level benchmark that labels factual inconsistencies (“hallucinations”) in dialogue summaries produced by GPT‑4, Alpaca‑13B, and several fine‑tuned BART‑style models on SAMSum and DialogSum. It introduces a refined error taxonomy, most notably the new class Circumstantial Inference, and shows that existing automatic factuality metrics miss many of these subtle errors; two prompt‑based detectors they propose perform better.",NLP,Summarization,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],"Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Mean, F1, balanced accuracy; 95% CIs via bootstrap.","['Mean', 'Std']"
424,Hunar Batra,chenFELMBenchmarkingFactuality2023,FELM: Benchmarking Factuality Evaluation of LLMs,Include,"FELM is a meta‑benchmark that measures how well factuality evaluators (usually LLM‑based) can spot factual errors in long‑form answers produced by ChatGPT.
It contains 817 prompts spanning five domains (world knowledge, science/tech, writing & recommendation, math, reasoning). The ChatGPT answers are split into 3,948 text‑segments; each segment is human‑labelled as correct or incorrect and, if incorrect, annotated with an error‑type, explanation and supporting / contradicting reference links.​",Language Modelling,Hallucination,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Structured response (e.g. valid JSON, API call alone)",['Structured'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],"Mean, Precision/Recall/F1, Balanced Accuracy; inter‑annotator agreement (Cohen’s κ / raw %).","['Mean', 'Other']"
425,Hunar Batra,lanCriticEvalEvaluatingLargescale2024,CriticEval: Evaluating Large-scale Language Model as Critic,Include,"CriticEval is a benchmark that measures the critique ability of large language models (LLMs) along four sub‑skills: feedback, comparison, correction (refinement), and meta‑feedback, across nine diverse task types. It supplies 3.6K human‑vetted items spanning low/medium/high/correct response qualities, provides both scalar and textual critique targets, and offers objective (correlation / accuracy / pass‑rate) and subjective (GPT‑4‑with‑reference) scoring pipelines.",LLM as a Judge,,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Author-crafted', 'Another benchmark', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)","['Free response', 'Structured']","Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)","['Exact match', 'Distribution']","Simple mean, Spearman correlation (with p‑value < 0.05)","['Mean', 'Other']"
426,Valentin Hoffman,chenCrosscareAssessingHealthcare2024,Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias,Include,"This paper examines how LMs associate disease prevalence with different demographic groups. The authors introduce Cross-Care, a benchmark probing this association across 89 diseases and nine demographic groups. Applying Cross-Care to a series of LMs, the authors find substantial misalignment between LM representation of disease prevalence and real disease prevalence rates across demographic groups.",Alignment,Bias,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)","['Author-crafted', 'Another benchmark', 'Procedurally-generated']",Targeted items (creators defined a task space and chose tasks within it strategically),['Targeted'],The task is not based on responses; it relies solely on the probability assigned to the tokens in the sentence.,['Logits'],Mean of the output logits,['Distribution'],simple mean,['Mean']
427,Valentin Hoffman,wanFactualityTaxDiversityintervened2024,The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention,Include,"This paper examines the question of whether prompt-based diversity interventions for text-to-image models result in non-factual demographic distribution. The authors introduce DoFaiR, a benchmark to systematically analyze this question, finding that diversity-oriented instructions indeed lead to historically less accurate demographic distributions. They also propose a method to mitigate this factuality tax.",Alignment,Bias,,"Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Procedurally-generated', 'LLM-generated']","Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)","['Random', 'Targeted']",image,['Free response'],"Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Factual Diversity Divergence (quantifies the divergence in the level of demographic diversity in model generations compared with the factual ground truth)","['Exact match', 'LLM-as-a-Judge', 'Distribution']",simple mean,['Mean']
428,Thom Foster,zengMRbenMetareasoningBenchmark2024,MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs,Include,"Dataset of question,answer pairs in which answers have mistakes. Models are evaluated on ability to spot mistake and give a reason for why it is wrong. Extends existing work like MR-GSM8K to physics, chemistry, logic, coding etc",Reasoning,,,Modified from another benchmark (e.g. translation into another language),['Another benchmark'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Multiple choice, Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Free response']","Exact Match (accuracy, F1, precision, recall), Correlation (Matthew's correlation, Pearson's r), Define MR-Score, a weighted combination of matthews correlation, and two variants of accuracy","['Exact match', 'Correlation', 'Correlation']",,
429,Hannah Kirk,maharanaEvaluatingVeryLongterm2024,Evaluating Very Long-Term Conversational Memory of LLM Agents,Include,"The paper introduces LOCOMO, a dataset created through a machine-human pipeline that generates high-quality, very long-term dialogues by grounded LLM-generators in personas and temporal event graphs. Across 10 conversations (each averaging 600 turns and 16K tokens across up to 32 sessions), they present an evaluation benchmark measuring long-term memory in models through question answering, event summarization, and multi-modal dialogue generation tasks.",NLP,Long Context,,"Crowd-sourced task examples (e.g. Prolific-created tasks), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Crowd-sourced', 'LLM-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']","Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)","['Multiple choice', 'Short free response', 'Free response']","Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), FactScore (Min et al., 2023), a method that evaluates the factuality of generated text by decomposing both the reference and hypothesis into atomic facts; MMRelevance","['Exact match', 'Soft match', 'Exact match']",,
430,Valentin Hoffman,jhaSeeGULLStereotypeBenchmark2023,SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models,Include,"This paper introduces SeeGULL, a broad-coverage dataset of stereotypes spanning 178 countries across six continents. SeeGULL is built using the generative capabilities of LMs, and it also includes offensiveness scores for the stereotypes as well as human annotations.",Alignment,Bias,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)","['Crowd-sourced', 'Procedurally-generated', 'LLM-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']",Multiple choice,['Multiple choice'],Mean entailment,['Distribution'],simple mean,['Mean']
431,Hannah Kirk,hendrycksAligningAIShared2020,Aligning AI With Shared Human Values,Include,"The paper introduces the ETHICS dataset, a benchmark for assessing language models' understanding of basic concepts in morality in text-based scenarios across five dimensions based in normative ethics: justice, well-being, duties, virtues, and commonsense morality.",Alignment,Alignment,,"Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Scraped from social media (Reddit)","['Author-crafted', 'Crowd-sourced', 'Procedurally-generated']","Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)","['Targeted', 'Criterion']","Multiple choice, Numeric response (for utilitarian task)","['Multiple choice', 'Short free response']","Exact Match (accuracy, F1, precision, recall)",['Exact match'],"The paper uses simple means for the primary evaluation metric. For each task, they report the percentage of correct predictions. For the overall score, they take a simple average across the five ethical categories. They also test whether models can distinguish ambiguous scenarios from clear-cut scenarios by using predictive uncertainty estimates (Area Under the Receiver Operating Characteristic curve).",['Mean']
432,Hannah Kirk,panRewardsJustifyMeans2023,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark,Include,"MACHIAVELLI is a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million diverse scenarios centered on social decision-making. It measures AI agents' tendencies to be power-seeking, cause disutility, and commit ethical violations when pursuing rewards, revealing tensions between maximizing rewards and behaving ethically. ",Alignment,Alignment,,"LLM-generated task examples (e.g. Filtered from responses to a prompt), Human-crafted task examples from an existing human game (Choose-Your-Own-Adventure)","['LLM-generated', 'Procedurally-generated']","Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)","['Convenience', 'Criterion']",Multiple choice,['Multiple choice'],"Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring), A key metric is: Score = # harms committed by agent / # harms committed by random baseline (aka a normalised ratio relative to random baseline of 1000 random trajectories)","['Exact match', 'LLM post-processing', 'Reward']",,
433,Thom Foster,wangSciBenchEvaluatingCollegelevel2024,SCIBENCH: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models,Include,"SciBench is a dataset of ~1000 college-level scientific questions from maths, physics and chemistry",Reasoning,,,"Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples)","['Human exams', 'Author-crafted', 'Expert-crafted']","Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)","['Convenience', 'Targeted']",Short free response (e.g. single word or number),['Short free response'],"Exact Match (accuracy, F1, precision, recall)",['Exact match'],,
434,Thom Foster,chenWeakevalstrongEvaluatingEliciting2024,Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles,Include,"Multi-turn puzzle game in which an agent in given a crazy scenario ""The man's car lights were broken, and the fox was in the middle of the road, but he didn't hit him"" and has to work on a reasonable explanation for why the situation isn't in fact, crazy. The agent gets to ask yes/no questions to an LLM overseeer, before submitting a guess at the final answer. ",Reasoning,,,Expert-crafted task examples (e.g. hand-written examples),['Expert-crafted'],Convenience sample (creators found a set of tasks that was readily accessible),['Convenience'],"Extended interaction (e.g. conversation, calling an API and processing the response)",['Interaction'],"Exact Match (accuracy, F1, precision, recall), Define 2 new metrics, RND and OCC which handle intricacies of the mutli-turn evaluation","['Exact match', 'Reward']",,
435,Adel Bibi,chiyah-garciaRepairsBlockWorld2024,Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models,Include,Paper proposes a dataset and a benchmark measuring LLMs ability to respond/correct/repair ambiguous questions/requests and how they recover from them. Benchmark is built based on a simulator that simulates boxes on a table at various locations where the VLM needs to respond to questions about box positions and where they should be moved to in which such questions might be vague.,NLP,Understanding,,"Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt), Original benchmark modified through an agent automatically and through crowdsourcing it was filtered for quality.","['Crowd-sourced', 'Another benchmark', 'LLM-generated', 'Crowd-sourced']",Specific criteria (items were taken from a larger set based on specified rules),['Criterion'],Short free response (e.g. single word or number),['Short free response'],IOU,['Soft match'],"mean, std","['Mean', 'Std']"
