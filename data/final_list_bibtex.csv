title,summary,link,year,venue,inclusion,modality,contribution,bibtex,bibkey
Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark,"In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow in size, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by (Malladi et al., 2023). Unlike traditional ZO-SGD methods, ou让work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families, three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments will be made public.",https://proceedings.mlr.press/v235/zhang24ad.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-zhang24ad,
  title = 	 {Revisiting Zeroth-Order Optimization for Memory-Efficient {LLM} Fine-Tuning: A Benchmark},
  author =       {Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D. and Yin, Wotao and Hong, Mingyi and Wang, Zhangyang and Liu, Sijia and Chen, Tianlong},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {59173--59190},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhang24ad/zhang24ad.pdf},
  url = 	 {https://proceedings.mlr.press/v235/zhang24ad.html},
  abstract = 	 {In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow in size, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by (Malladi et al., 2023). Unlike traditional ZO-SGD methods, ou让work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families, three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments will be made public.}
}",zhangRevisitingZerothorderOptimization2024
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,"We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models.",https://proceedings.mlr.press/v235/yu24o.html,2024,ICML,Yes,Multimodal,Benchmark,"@InProceedings{pmlr-v235-yu24o,
  title = 	 {{MM}-Vet: Evaluating Large Multimodal Models for Integrated Capabilities},
  author =       {Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {57730--57754},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/yu24o/yu24o.pdf},
  url = 	 {https://proceedings.mlr.press/v235/yu24o.html},
  abstract = 	 {We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models.}
}",yuMMvetEvaluatingLarge2024
MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI,"Large Vision-Language Models (LVLMs) show significant strides in general-propose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, and reasoning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $20$ publicly available LVLMs such as the proprietary GeminiProVision model, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.",https://proceedings.mlr.press/v235/ying24a.html,2024,ICML,Yes,Multimodal,Benchmark,"@InProceedings{pmlr-v235-ying24a,
  title = 	 {{MMT}-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask {AGI}},
  author =       {Ying, Kaining and Meng, Fanqing and Wang, Jin and Li, Zhiqian and Lin, Han and Yang, Yue and Zhang, Hao and Zhang, Wenbo and Lin, Yuqi and Liu, Shuo and Lei, Jiayi and Lu, Quanfeng and Chen, Runjian and Xu, Peng and Zhang, Renrui and Zhang, Haozhe and Gao, Peng and Wang, Yali and Qiao, Yu and Luo, Ping and Zhang, Kaipeng and Shao, Wenqi},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {57116--57198},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/ying24a/ying24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/ying24a.html},
  abstract = 	 {Large Vision-Language Models (LVLMs) show significant strides in general-propose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, and reasoning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $20$ publicly available LVLMs such as the proprietary GeminiProVision model, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.}
}",yingMMTbenchComprehensiveMultimodal2024
Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study,"Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions.",https://proceedings.mlr.press/v235/xu24h.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-xu24h,
  title = 	 {Is {DPO} Superior to {PPO} for {LLM} Alignment? {A} Comprehensive Study},
  author =       {Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {54983--54998},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/xu24h/xu24h.pdf},
  url = 	 {https://proceedings.mlr.press/v235/xu24h.html},
  abstract = 	 {Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions.}
}",xuDPOSuperiorPPO2024
TravelPlanner: A Benchmark for Real-World Planning with Language Agents,"Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks—even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.",https://proceedings.mlr.press/v235/xie24j.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-xie24j,
  title = 	 {{T}ravel{P}lanner: A Benchmark for Real-World Planning with Language Agents},
  author =       {Xie, Jian and Zhang, Kai and Chen, Jiangjie and Zhu, Tinghui and Lou, Renze and Tian, Yuandong and Xiao, Yanghua and Su, Yu},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {54590--54613},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/xie24j/xie24j.pdf},
  url = 	 {https://proceedings.mlr.press/v235/xie24j.html},
  abstract = 	 {Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks—even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.}
}",xieTravelPlannerBenchmarkRealworld2024
Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models,"The issue of hallucinations is a prevalent concern in existing Large Vision-Language Models (LVLMs). Previous efforts have primarily focused on investigating object hallucinations, which can be easily alleviated by introducing object detectors. However, these efforts neglect hallucinations in inter-object relationships, which is essential for visual comprehension. In this work, we introduce R-Bench, a novel benchmark for evaluating Vision Relationship Hallucination. R-Bench features image-level questions that focus on the existence of relationships and instance-level questions that assess local visual comprehension. We identify three types of relationship co-occurrences that lead to hallucinations: relationship-relationship, subject-relationship, and relationship-object. The visual instruction tuning dataset’s long-tail distribution significantly impacts LVLMs’ understanding of visual relationships. Additionally, our analysis reveals that current LVLMs tend to overlook visual content, overly rely on the common sense knowledge of Large Language Models (LLMs), and struggle with spatial relationship reasoning based on contextual information.",https://proceedings.mlr.press/v235/wu24l.html,2024,ICML,Yes,Multimodal,Benchmark,"@InProceedings{pmlr-v235-wu24l,
  title = 	 {Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models},
  author =       {Wu, Mingrui and Ji, Jiayi and Huang, Oucheng and Li, Jiale and Wu, Yuhang and Sun, Xiaoshuai and Ji, Rongrong},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {53553--53570},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/wu24l/wu24l.pdf},
  url = 	 {https://proceedings.mlr.press/v235/wu24l.html},
  abstract = 	 {The issue of hallucinations is a prevalent concern in existing Large Vision-Language Models (LVLMs). Previous efforts have primarily focused on investigating object hallucinations, which can be easily alleviated by introducing object detectors. However, these efforts neglect hallucinations in inter-object relationships, which is essential for visual comprehension. In this work, we introduce R-Bench, a novel benchmark for evaluating Vision Relationship Hallucination. R-Bench features image-level questions that focus on the existence of relationships and instance-level questions that assess local visual comprehension. We identify three types of relationship co-occurrences that lead to hallucinations: relationship-relationship, subject-relationship, and relationship-object. The visual instruction tuning dataset’s long-tail distribution significantly impacts LVLMs’ understanding of visual relationships. Additionally, our analysis reveals that current LVLMs tend to overlook visual content, overly rely on the common sense knowledge of Large Language Models (LLMs), and struggle with spatial relationship reasoning based on contextual information.}
}",wuEvaluatingAnalyzingRelationship2024a
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models,"Most existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.",https://proceedings.mlr.press/v235/wang24z.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-wang24z,
  title = 	 {{S}ci{B}ench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models},
  author =       {Wang, Xiaoxuan and Hu, Ziniu and Lu, Pan and Zhu, Yanqiao and Zhang, Jieyu and Subramaniam, Satyen and Loomba, Arjun R and Zhang, Shichang and Sun, Yizhou and Wang, Wei},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {50622--50649},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24z/wang24z.pdf},
  url = 	 {https://proceedings.mlr.press/v235/wang24z.html},
  abstract = 	 {Most existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.}
}",wangSciBenchEvaluatingCollegelevel2024
Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs,"Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter settings and difficult to optimize. We build on these results to offer insights into improving debating strategies, such as adjusting agent agreement levels, which can significantly enhance performance and even surpass all other non-debate protocols we evaluated. We provide an open-source repository to the community with several state-of-the-art protocols together with evaluation scripts to benchmark across popular research datasets.",https://proceedings.mlr.press/v235/smit24a.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-smit24a,
  title = 	 {Should we be going {MAD}? {A} Look at Multi-Agent Debate Strategies for {LLM}s},
  author =       {Smit, Andries Petrus and Grinsztajn, Nathan and Duckworth, Paul and Barrett, Thomas D and Pretorius, Arnu},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {45883--45905},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/smit24a/smit24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/smit24a.html},
  abstract = 	 {Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter settings and difficult to optimize. We build on these results to offer insights into improving debating strategies, such as adjusting agent agreement levels, which can significantly enhance performance and even surpass all other non-debate protocols we evaluated. We provide an open-source repository to the community with several state-of-the-art protocols together with evaluation scripts to benchmark across popular research datasets.}
}",smitShouldWeBe2024
Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT,"Retrieval pipelines are an integral component of many machine learning systems. However, they perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to finetune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scaling to documents up to 32K tokens long. We describe a pretraining data mixture which allows this encoder to process both short and long context sequences, and a finetuning approach that adapts this base model to retrieval with only single-sample batches. Finally, we validate the M2-BERT retrieval encoder on LoCoV1, finding that it outperforms competitive Transformer-based models by at least 22.2 points, despite containing 90× fewer parameters.",https://proceedings.mlr.press/v235/saad-falcon24a.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-saad-falcon24a,
  title = 	 {Benchmarking and Building Long-Context Retrieval Models with {L}o{C}o and M2-{BERT}},
  author =       {Saad-Falcon, Jon and Fu, Daniel Y and Arora, Simran and Guha, Neel and Re, Christopher},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {42918--42946},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/saad-falcon24a/saad-falcon24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/saad-falcon24a.html},
  abstract = 	 {Retrieval pipelines are an integral component of many machine learning systems. However, they perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to finetune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scaling to documents up to 32K tokens long. We describe a pretraining data mixture which allows this encoder to process both short and long context sequences, and a finetuning approach that adapts this base model to retrieval with only single-sample batches. Finally, we validate the M2-BERT retrieval encoder on LoCoV1, finding that it outperforms competitive Transformer-based models by at least 22.2 points, despite containing 90× fewer parameters.}
}",saad-falconBenchmarkingBuildingLongcontext2024
tinyBenchmarks: evaluating LLMs with fewer examples,"The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models’ abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.",https://proceedings.mlr.press/v235/maia-polo24a.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-maia-polo24a,
  title = 	 {tiny{B}enchmarks: evaluating {LLM}s with fewer examples},
  author =       {Maia Polo, Felipe and Weber, Lucas and Choshen, Leshem and Sun, Yuekai and Xu, Gongjun and Yurochkin, Mikhail},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {34303--34326},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/maia-polo24a/maia-polo24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/maia-polo24a.html},
  abstract = 	 {The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models’ abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.}
}",maiapoloTinyBenchmarksEvaluatingLLMs2024
WebLINX: Real-World Website Navigation with Multi-Turn Dialogue,"We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings.",https://proceedings.mlr.press/v235/lu24e.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-lu24e,
  title = 	 {{W}eb{LINX}: Real-World Website Navigation with Multi-Turn Dialogue},
  author =       {Lu, Xing Han and Kasner, Zden\v{e}k and Reddy, Siva},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {33007--33056},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/lu24e/lu24e.pdf},
  url = 	 {https://proceedings.mlr.press/v235/lu24e.html},
  abstract = 	 {We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings.}
}",luWebLINXRealworldWebsite2024
The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning,"The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private and restricted to a narrow range of malicious use scenarios, which limits further research into reducing malicious use. To fill these gaps, we release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai.",https://proceedings.mlr.press/v235/li24bc.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-li24bc,
  title = 	 {The {WMDP} Benchmark: Measuring and Reducing Malicious Use with Unlearning},
  author =       {Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D. and Dombrowski, Ann-Kathrin and Goel, Shashwat and Mukobi, Gabriel and Helm-Burger, Nathan and Lababidi, Rassin and Justen, Lennart and Liu, Andrew Bo and Chen, Michael and Barrass, Isabelle and Zhang, Oliver and Zhu, Xiaoyuan and Tamirisa, Rishub and Bharathi, Bhrugu and Herbert-Voss, Ariel and Breuer, Cort B and Zou, Andy and Mazeika, Mantas and Wang, Zifan and Oswal, Palash and Lin, Weiran and Hunt, Adam Alfred and Tienken-Harder, Justin and Shih, Kevin Y. and Talley, Kemper and Guan, John and Steneker, Ian and Campbell, David and Jokubaitis, Brad and Basart, Steven and Fitz, Stephen and Kumaraguru, Ponnurangam and Karmakar, Kallol Krishna and Tupakula, Uday and Varadharajan, Vijay and Shoshitaishvili, Yan and Ba, Jimmy and Esvelt, Kevin M. and Wang, Alexandr and Hendrycks, Dan},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {28525--28550},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24bc/li24bc.pdf},
  url = 	 {https://proceedings.mlr.press/v235/li24bc.html},
  abstract = 	 {The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private and restricted to a narrow range of malicious use scenarios, which limits further research into reducing malicious use. To fill these gaps, we release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai.}
}",liWMDPBenchmarkMeasuring2024
R2E: Turning any Github Repository into a Programming Agent Environment,"While Large Language Models’ (LLMs) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI coding agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R2E code is available at https://r2e.dev/",https://proceedings.mlr.press/v235/jain24c.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-jain24c,
  title = 	 {{R}2{E}: Turning any Github Repository into a Programming Agent Environment},
  author =       {Jain, Naman and Shetty, Manish and Zhang, Tianjun and Han, King and Sen, Koushik and Stoica, Ion},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {21196--21224},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/jain24c/jain24c.pdf},
  url = 	 {https://proceedings.mlr.press/v235/jain24c.html},
  abstract = 	 {While Large Language Models’ (LLMs) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI coding agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R2E code is available at https://r2e.dev/}
}",jainR2ETurningAny2024
MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation,"A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with 37.5% average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from 100% on well-established older datasets to as low as 0% on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination.",https://proceedings.mlr.press/v235/huang24y.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-huang24y,
  title = 	 {{MLA}gent{B}ench: Evaluating Language Agents on Machine Learning Experimentation},
  author =       {Huang, Qian and Vora, Jian and Liang, Percy and Leskovec, Jure},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {20271--20309},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/huang24y/huang24y.pdf},
  url = 	 {https://proceedings.mlr.press/v235/huang24y.html},
  abstract = 	 {A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with 37.5% average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from 100% on well-established older datasets to as low as 0% on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination.}
}",huangMLAgentBenchEvaluatingLanguage2024
Position: TrustLLM: Trustworthiness in Large Language Models,"Large language models (LLMs) have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and capability (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones, suggesting that open-source models can achieve high levels of trustworthiness without additional mechanisms like <em>moderator</em>, offering valuable insights for developers in this field. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Besides these observations, we’ve uncovered key insights into the multifaceted trustworthiness in LLMs. We emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. We advocate that the establishment of an AI alliance between industry, academia, the open-source community to foster collaboration is imperative to advance the trustworthiness of LLMs.",https://proceedings.mlr.press/v235/huang24x.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-huang24x,
  title = 	 {Position: {T}rust{LLM}: Trustworthiness in Large Language Models},
  author =       {Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and Sun, Hanchi and Liu, Zhengliang and Liu, Yixin and Wang, Yijue and Zhang, Zhikun and Vidgen, Bertie and Kailkhura, Bhavya and Xiong, Caiming and Xiao, Chaowei and Li, Chunyuan and Xing, Eric P. and Huang, Furong and Liu, Hao and Ji, Heng and Wang, Hongyi and Zhang, Huan and Yao, Huaxiu and Kellis, Manolis and Zitnik, Marinka and Jiang, Meng and Bansal, Mohit and Zou, James and Pei, Jian and Liu, Jian and Gao, Jianfeng and Han, Jiawei and Zhao, Jieyu and Tang, Jiliang and Wang, Jindong and Vanschoren, Joaquin and Mitchell, John and Shu, Kai and Xu, Kaidi and Chang, Kai-Wei and He, Lifang and Huang, Lifu and Backes, Michael and Gong, Neil Zhenqiang and Yu, Philip S. and Chen, Pin-Yu and Gu, Quanquan and Xu, Ran and Ying, Rex and Ji, Shuiwang and Jana, Suman and Chen, Tianlong and Liu, Tianming and Zhou, Tianyi and Wang, William Yang and Li, Xiang and Zhang, Xiangliang and Wang, Xiao and Xie, Xing and Chen, Xun and Wang, Xuyu and Liu, Yan and Ye, Yanfang and Cao, Yinzhi and Chen, Yong and Zhao, Yue},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {20166--20270},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/huang24x/huang24x.pdf},
  url = 	 {https://proceedings.mlr.press/v235/huang24x.html},
  abstract = 	 {Large language models (LLMs) have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and capability (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones, suggesting that open-source models can achieve high levels of trustworthiness without additional mechanisms like moderator, offering valuable insights for developers in this field. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Besides these observations, we’ve uncovered key insights into the multifaceted trustworthiness in LLMs. We emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. We advocate that the establishment of an AI alliance between industry, academia, the open-source community to foster collaboration is imperative to advance the trustworthiness of LLMs.}
}",huangPositionTrustLLMTrustworthiness2024
InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks,"In this paper, we introduce InfiAgent-DABench, the first benchmark specifically designed to evaluate LLM-based agents on data analysis tasks. Agents need to solve these tasks end-to-end by interacting with an execution environment. This benchmark contains DAEval, a dataset consisting of 603 data analysis questions derived from 124 CSV files, and an agent framework which incorporates LLMs to serve as data analysis agents for both serving and evaluating. Since data analysis questions are often open-ended and hard to evaluate without human supervision, we adopt a format-prompting technique to convert each question into a closed-form format so that they can be automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the current challenges encountered in data analysis tasks. In addition, building upon our agent framework, we develop a specialized agent, DAAgent, which surpasses GPT-3.5 by 3.9% on DABench. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.",https://proceedings.mlr.press/v235/hu24s.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-hu24s,
  title = 	 {{I}nfi{A}gent-{DAB}ench: Evaluating Agents on Data Analysis Tasks},
  author =       {Hu, Xueyu and Zhao, Ziyu and Wei, Shuang and Chai, Ziwei and Ma, Qianli and Wang, Guoyin and Wang, Xuwu and Su, Jing and Xu, Jingjing and Zhu, Ming and Cheng, Yao and Yuan, Jianbo and Li, Jiwei and Kuang, Kun and Yang, Yang and Yang, Hongxia and Wu, Fei},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {19544--19572},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/hu24s/hu24s.pdf},
  url = 	 {https://proceedings.mlr.press/v235/hu24s.html},
  abstract = 	 {In this paper, we introduce InfiAgent-DABench, the first benchmark specifically designed to evaluate LLM-based agents on data analysis tasks. Agents need to solve these tasks end-to-end by interacting with an execution environment. This benchmark contains DAEval, a dataset consisting of 603 data analysis questions derived from 124 CSV files, and an agent framework which incorporates LLMs to serve as data analysis agents for both serving and evaluating. Since data analysis questions are often open-ended and hard to evaluate without human supervision, we adopt a format-prompting technique to convert each question into a closed-form format so that they can be automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the current challenges encountered in data analysis tasks. In addition, building upon our agent framework, we develop a specialized agent, DAAgent, which surpasses GPT-3.5 by 3.9% on DABench. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.}
}",huInfiAgentDABenchEvaluatingAgents2024
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks,"We introduce <b>S</b>yntax-<b>A</b>ware <b>F</b>ill-<b>i</b>n-the-<b>M</b>iddle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com.",https://proceedings.mlr.press/v235/gong24f.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-gong24f,
  title = 	 {Evaluation of {LLM}s on Syntax-Aware Code Fill-in-the-Middle Tasks},
  author =       {Gong, Linyuan and Wang, Sida and Elhoushi, Mostafa and Cheung, Alvin},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {15907--15928},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/gong24f/gong24f.pdf},
  url = 	 {https://proceedings.mlr.press/v235/gong24f.html},
  abstract = 	 {We introduce Syntax-Aware Fill-in-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com.}
}",gongEvaluationLLMsSyntaxaware2024
WorkArena: How Capable are Web Agents at Solving Common Knowledge Work Tasks?,"We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents’ ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 33 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.",https://proceedings.mlr.press/v235/drouin24a.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-drouin24a,
  title = 	 {{W}ork{A}rena: How Capable are Web Agents at Solving Common Knowledge Work Tasks?},
  author =       {Drouin, Alexandre and Gasse, Maxime and Caccia, Massimo and Laradji, Issam H. and Del Verme, Manuel and Marty, Tom and Vazquez, David and Chapados, Nicolas and Lacoste, Alexandre},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {11642--11662},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/drouin24a/drouin24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/drouin24a.html},
  abstract = 	 {We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents’ ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 33 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.}
}",drouinWorkArenaHowCapable2024
CogBench: a large language model walks into a psychology lab,"Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces <em>CogBench</em>, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs’ behavior. We apply <em>CogBench</em> to 40 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs’ behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.",https://proceedings.mlr.press/v235/coda-forno24a.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-coda-forno24a,
  title = 	 {{C}og{B}ench: a large language model walks into a psychology lab},
  author =       {Coda-Forno, Julian and Binz, Marcel and Wang, Jane X and Schulz, Eric},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {9076--9108},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/coda-forno24a/coda-forno24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/coda-forno24a.html},
  abstract = 	 {Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs’ behavior. We apply CogBench to 40 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs’ behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.}
}",coda-fornoCogBenchLargeLanguage2024
Language Models as Science Tutors,"NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized in math have a 32K-token context window, and they excel at TutorEval while performing strongly on GSM8K and MATH. Our datasets build on open-source materials, and we release our models, data, and evaluations publicly.",https://proceedings.mlr.press/v235/chevalier24a.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-chevalier24a,
  title = 	 {Language Models as Science Tutors},
  author =       {Chevalier, Alexis and Geng, Jiayi and Wettig, Alexander and Chen, Howard and Mizera, Sebastian and Annala, Toni and Aragon, Max and Fanlo, Arturo Rodriguez and Frieder, Simon and Machado, Simon and Prabhakar, Akshara and Thieu, Ellie and Wang, Jiachen T. and Wang, Zirui and Wu, Xindi and Xia, Mengzhou and Xia, Wenhan and Yu, Jiatong and Zhu, Junjie and Ren, Zhiyong and Arora, Sanjeev and Chen, Danqi},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {8310--8335},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/chevalier24a/chevalier24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/chevalier24a.html},
  abstract = 	 {NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized in math have a 32K-token context window, and they excel at TutorEval while performing strongly on GSM8K and MATH. Our datasets build on open-source materials, and we release our models, data, and evaluations publicly.}
}",chevalierLanguageModelsScience2024
Premise Order Matters in Reasoning with Large Language Models,"Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model’s accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that even if the model performance is decent on the optimal order, permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.",https://proceedings.mlr.press/v235/chen24i.html,2024,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v235-chen24i,
  title = 	 {Premise Order Matters in Reasoning with Large Language Models},
  author =       {Chen, Xinyun and Chi, Ryan Andrew and Wang, Xuezhi and Zhou, Denny},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {6596--6620},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/chen24i/chen24i.pdf},
  url = 	 {https://proceedings.mlr.press/v235/chen24i.html},
  abstract = 	 {Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model’s accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that even if the model performance is decent on the optimal order, permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.}
}",chenPremiseOrderMatters2024
MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark,"Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, a closer examination reveals persistent challenges in the evaluative capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: https://mllm-judge.github.io/.",https://proceedings.mlr.press/v235/chen24h.html,2024,ICML,Yes,Multimodal,Benchmark,"@InProceedings{pmlr-v235-chen24h,
  title = 	 {{MLLM}-as-a-Judge: Assessing Multimodal {LLM}-as-a-Judge with Vision-Language Benchmark},
  author =       {Chen, Dongping and Chen, Ruoxi and Zhang, Shilin and Wang, Yaochen and Liu, Yinuo and Zhou, Huichi and Zhang, Qihui and Wan, Yao and Zhou, Pan and Sun, Lichao},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {6562--6595},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/chen24h/chen24h.pdf},
  url = 	 {https://proceedings.mlr.press/v235/chen24h.html},
  abstract = 	 {Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, a closer examination reveals persistent challenges in the evaluative capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: https://mllm-judge.github.io/.}
}",chenMLLMasajudgeAssessingMultimodal2024
CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling,"Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer’s efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods’ capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.",https://proceedings.mlr.press/v202/zhang23r.html,2023,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v202-zhang23r,
  title = 	 {{CAB}: Comprehensive Attention Benchmarking on Long Sequence Modeling},
  author =       {Zhang, Jun and Jiang, Shuyang and Feng, Jiangtao and Zheng, Lin and Kong, Lingpeng},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {41194--41218},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/zhang23r/zhang23r.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zhang23r.html},
  abstract = 	 {Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer’s efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods’ capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.}
}",zhangCABComprehensiveAttention2023
Large Language Models Can Be Easily Distracted by Irrelevant Context,"Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the <em>distractibility</em> of large language models, i.e., how the model prediction can be distracted by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of different prompting techniques for large language models, and find that the model is easily distracted by irrelevant information. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.",https://proceedings.mlr.press/v202/shi23a.html,2023,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v202-shi23a,
  title = 	 {Large Language Models Can Be Easily Distracted by Irrelevant Context},
  author =       {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H. and Sch\""{a}rli, Nathanael and Zhou, Denny},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {31210--31227},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/shi23a/shi23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/shi23a.html},
  abstract = 	 {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model prediction can be distracted by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of different prompting techniques for large language models, and find that the model is easily distracted by irrelevant information. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.}
}",shiLargeLanguageModels2023
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark,"Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce Machiavelli, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents’ tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics–designing agents that are Pareto improvements in both safety and capabilities.",https://proceedings.mlr.press/v202/pan23a.html,2023,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v202-pan23a,
  title = 	 {Do the Rewards Justify the Means? {M}easuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark},
  author =       {Pan, Alexander and Chan, Jun Shern and Zou, Andy and Li, Nathaniel and Basart, Steven and Woodside, Thomas and Zhang, Hanlin and Emmons, Scott and Hendrycks, Dan},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {26837--26867},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/pan23a/pan23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/pan23a.html},
  abstract = 	 {Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce Machiavelli, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents’ tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics–designing agents that are Pareto improvements in both safety and capabilities.}
}",panRewardsJustifyMeans2023
QASA: Advanced Question Answering on Scientific Articles,"Reasoning is the crux of intellectual thinking. While question answering (QA) tasks are prolific with various computational models and benchmark datasets, they mostly tackle factoid or shallow QA without asking deeper understanding. Dual process theory asserts that human reasoning consists of associative thinking to collect relevant pieces of knowledge and logical reasoning to consciously conclude grounding on evidential rationale. Based on our intensive think-aloud study that revealed the three types of questions: surface, testing, and deep questions, we first propose the QASA benchmark that consists of 1798 novel question answering pairs that require full-stack reasoning on scientific articles in AI and ML fields. Then we propose the QASA approach that tackles the full-stack reasoning with large language models via associative selection, evidential rationale-generation, and systematic composition. Our experimental results show that QASA’s full-stack inference outperforms the state-of-the-art InstructGPT by a big margin. We also find that rationale-generation is critical for the performance gain, claiming how we should rethink advanced question answering. The dataset is available at https://github.com/lgresearch/QASA.",https://proceedings.mlr.press/v202/lee23n.html,2023,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v202-lee23n,
  title = 	 {{QASA}: Advanced Question Answering on Scientific Articles},
  author =       {Lee, Yoonjoo and Lee, Kyungjae and Park, Sunghyun and Hwang, Dasol and Kim, Jaehyeon and Lee, Hong-In and Lee, Moontae},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {19036--19052},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/lee23n/lee23n.pdf},
  url = 	 {https://proceedings.mlr.press/v202/lee23n.html},
  abstract = 	 {Reasoning is the crux of intellectual thinking. While question answering (QA) tasks are prolific with various computational models and benchmark datasets, they mostly tackle factoid or shallow QA without asking deeper understanding. Dual process theory asserts that human reasoning consists of associative thinking to collect relevant pieces of knowledge and logical reasoning to consciously conclude grounding on evidential rationale. Based on our intensive think-aloud study that revealed the three types of questions: surface, testing, and deep questions, we first propose the QASA benchmark that consists of 1798 novel question answering pairs that require full-stack reasoning on scientific articles in AI and ML fields. Then we propose the QASA approach that tackles the full-stack reasoning with large language models via associative selection, evidential rationale-generation, and systematic composition. Our experimental results show that QASA’s full-stack inference outperforms the state-of-the-art InstructGPT by a big margin. We also find that rationale-generation is critical for the performance gain, claiming how we should rethink advanced question answering. The dataset is available at https://github.com/lgresearch/QASA.}
}",leeQASAAdvancedQuestion2023
VLUE: A Multi-Task Multi-Dimension Benchmark for Evaluating Vision-Language Pre-training,"Recent advances in vision-language pre-training (VLP) have demonstrated impressive performance in a range of vision-language (VL) tasks. However, there exist several challenges for measuring the community’s progress in building general multi-modal intelligence. First, most of the downstream VL datasets are annotated using raw images that are already seen during pre-training, which may result in an overestimation of current VLP models’ generalization ability. Second, recent VLP work mainly focuses on absolute performance but overlooks the efficiency-performance trade-off, which is also an important indicator for measuring progress. To this end, we introduce the Vision-Language Understanding Evaluation (VLUE) benchmark, a multi-task multi-dimension benchmark for evaluating the generalization capabilities and the efficiency-performance trade-off (“Pareto SOTA”) of VLP models. We demonstrate that there is a sizable generalization gap for all VLP models when testing on out-of-distribution test sets annotated on images from a more diverse distribution that spreads across cultures. Moreover, we find that measuring the efficiency-performance trade-off of VLP models leads to complementary insights for several design choices of VLP. We release the VLUE benchmark to promote research on building vision-language models that generalize well to images unseen during pre-training and are practical in terms of efficiency-performance trade-off.",https://proceedings.mlr.press/v162/zhou22n.html,2022,ICML,Yes,Multimodal,Benchmark,"@InProceedings{pmlr-v162-zhou22n,
  title = 	 {{VLUE}: A Multi-Task Multi-Dimension Benchmark for Evaluating Vision-Language Pre-training},
  author =       {Zhou, Wangchunshu and Zeng, Yan and Diao, Shizhe and Zhang, Xinsong},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27395--27411},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zhou22n/zhou22n.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zhou22n.html},
  abstract = 	 {Recent advances in vision-language pre-training (VLP) have demonstrated impressive performance in a range of vision-language (VL) tasks. However, there exist several challenges for measuring the community’s progress in building general multi-modal intelligence. First, most of the downstream VL datasets are annotated using raw images that are already seen during pre-training, which may result in an overestimation of current VLP models’ generalization ability. Second, recent VLP work mainly focuses on absolute performance but overlooks the efficiency-performance trade-off, which is also an important indicator for measuring progress. To this end, we introduce the Vision-Language Understanding Evaluation (VLUE) benchmark, a multi-task multi-dimension benchmark for evaluating the generalization capabilities and the efficiency-performance trade-off (“Pareto SOTA”) of VLP models. We demonstrate that there is a sizable generalization gap for all VLP models when testing on out-of-distribution test sets annotated on images from a more diverse distribution that spreads across cultures. Moreover, we find that measuring the efficiency-performance trade-off of VLP models leads to complementary insights for several design choices of VLP. We release the VLUE benchmark to promote research on building vision-language models that generalize well to images unseen during pre-training and are practical in terms of efficiency-performance trade-off.}
}",zhouVLUEMultitaskMultidimension2022
StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models,"Knowledge and language understanding of models evaluated through question answering (QA) has been usually studied on static snapshots of knowledge, like Wikipedia. However, our world is dynamic, evolves over time, and our models’ knowledge becomes outdated. To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM under-perform those with a retrained LM. For questions about higher-frequency named entities, parametric updates are particularly beneficial. In our dynamic world, the StreamingQA dataset enables a more realistic evaluation of QA models, and our experiments highlight several promising directions for future research.",https://proceedings.mlr.press/v162/liska22a.html,2022,ICML,Yes,Language,Benchmark,"@InProceedings{pmlr-v162-liska22a,
  title = 	 {{S}treaming{QA}: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models},
  author =       {Liska, Adam and Kocisky, Tomas and Gribovskaya, Elena and Terzi, Tayfun and Sezener, Eren and Agrawal, Devang and De Masson D'Autume, Cyprien and Scholtes, Tim and Zaheer, Manzil and Young, Susannah and Gilsenan-Mcmahon, Ellen and Austin, Sophia and Blunsom, Phil and Lazaridou, Angeliki},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {13604--13622},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/liska22a/liska22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/liska22a.html},
  abstract = 	 {Knowledge and language understanding of models evaluated through question answering (QA) has been usually studied on static snapshots of knowledge, like Wikipedia. However, our world is dynamic, evolves over time, and our models’ knowledge becomes outdated. To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM under-perform those with a retrained LM. For questions about higher-frequency named entities, parametric updates are particularly beneficial. In our dynamic world, the StreamingQA dataset enables a more realistic evaluation of QA models, and our experiments highlight several promising directions for future research.}
}",liskaStreamingQABenchmarkAdaptation2022
Aligning AI With Shared Human Values,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",https://iclr.cc//virtual/2021/poster/2960,2021,ICLR,Yes,Language,Benchmark,"@inproceedings{hendrycksAligningAIShared2020,
  title = {Aligning {{AI With Shared Human Values}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
  year = {2020},
  month = oct,
  urldate = {2025-04-11},
  abstract = {We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.},
  langid = {english},
    }",hendrycksAligningAIShared2020a
Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness,"Neural text-to-SQL models have achieved remarkable performance in translating natural language questions into SQL queries. However, recent studies reveal that text-to-SQL models are vulnerable to task-specific perturbations. Previous curated robustness test sets usually focus on individual phenomena. In this paper, we propose a comprehensive robustness benchmark based on Spider, a cross-domain text-to-SQL benchmark, to diagnose the model robustness. We design 17 perturbations on databases, natural language questions, and SQL queries to measure the robustness from different angles. In order to collect more diversified natural question perturbations, we utilize large pretrained language models (PLMs) to simulate human behaviors in creating natural questions. We conduct a diagnostic study of the state-of-the-art models on the robustness set. Experimental results reveal that even the most robust model suffers from a 14.0% performance drop overall and a 50.7% performance drop on the most challenging perturbation. We also present a breakdown analysis regarding text-to-SQL model designs and provide insights for improving model robustness.",https://iclr.cc//virtual/2023/poster/11467,2023,ICLR,Yes,Language,Benchmark,"@inproceedings{
chang2023drspider,
title={Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-{SQL} Robustness},
author={Shuaichen Chang and Jun Wang and Mingwen Dong and Lin Pan and Henghui Zhu and Alexander Hanbo Li and Wuwei Lan and Sheng Zhang and Jiarong Jiang and Joseph Lilien and Steve Ash and William Yang Wang and Zhiguo Wang and Vittorio Castelli and Patrick Ng and Bing Xiang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Wc5bmZZU9cy}
}",changDrspiderDiagnosticEvaluation2023
"Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization","We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?To help answer this, we first introduce an open-source modular library, $RL4LMs$ (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the $GRUE$ (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, $NLPO$ (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.",https://iclr.cc//virtual/2023/poster/10970,2023,ICLR,Yes,Language,Benchmark,"@inproceedings{
ramamurthy2023is,
title={Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization},
author={Rajkumar Ramamurthy and Prithviraj Ammanabrolu and Kiant{\'e} Brantley and Jack Hessel and Rafet Sifa and Christian Bauckhage and Hannaneh Hajishirzi and Yejin Choi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8aHzds2uUyB}
}",ramamurthyReinforcementLearningNot2023
Language models are multilingual chain-of-thought reasoners,"We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at AnonymousLink and the supplementary material.",https://iclr.cc//virtual/2023/poster/10960,2023,ICLR,Yes,Language,Benchmark,"@inproceedings{
shi2023language,
title={Language models are multilingual chain-of-thought reasoners},
author={Freda Shi and Mirac Suzgun and Markus Freitag and Xuezhi Wang and Suraj Srivats and Soroush Vosoughi and Hyung Won Chung and Yi Tay and Sebastian Ruder and Denny Zhou and Dipanjan Das and Jason Wei},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=fR3wGCk-IXp}
}",shiLanguageModelsAre2023
Multi-lingual Evaluation of Code Generation Models,"We present two new benchmarks, MBXP and Multilingual HumanEval, designed to evaluate code completion models in over 10 programming languages. These datasets are generated using a conversion framework that transpiles prompts and test cases from the original MBPP and HumanEval datasets into the corresponding data in the target language. By using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of  few-shot prompting to teach the model new languages, and zero-shot translation abilities. In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks.",https://iclr.cc//virtual/2023/poster/12102,2023,ICLR,Yes,Language,Benchmark,"@inproceedings{
athiwaratkun2023multilingual,
title={Multi-lingual Evaluation of Code Generation Models},
author={Ben Athiwaratkun and Sanjay Krishna Gouda and Zijian Wang and Xiaopeng Li and Yuchen Tian and Ming Tan and Wasi Uddin Ahmad and Shiqi Wang and Qing Sun and Mingyue Shang and Sujan Kumar Gonugondla and Hantian Ding and Varun Kumar and Nathan Fulton and Arash Farahani and Siddhartha Jain and Robert Giaquinto and Haifeng Qian and Murali Krishna Ramanathan and Ramesh Nallapati and Baishakhi Ray and Parminder Bhatia and Sudipta Sengupta and Dan Roth and Bing Xiang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Bo7eeXm6An8}
}",athiwaratkunMultilingualEvaluationCode2023
On Pre-training Language Model for Antibody,"Antibodies are vital proteins offering robust protection for the human body from pathogens. The development of general protein and antibody-specific pre-trained language models both facilitate antibody prediction tasks. However, there have been limited studies that comprehensively explore the representation capability of distinct pre-trained language models on different antibody tasks. To investigate the problem, we aim to answer several key questions in this paper, such as how pre-trained language models perform in antibody tasks with different specificity and how introducing specific biological mechanisms to the pre-training process can benefit the model. Additionally, we evaluate if the learned antibody pre-trained representations can be applied to real-world antibody problems, like drug discovery and immune process understanding. Previously, no benchmark available largely hindered the study to answer these questions. To aid in our investigation, we provide an AnTibody Understanding Evaluation (ATUE) benchmark. We comprehensively evaluate the performance of protein pre-trained language models by empirical study along with conclusions and new insights. Our ATUE and code are released at https://github.com/dqwang122/EATLM.",https://iclr.cc//virtual/2023/poster/10766,2023,ICLR,Yes,Language,Benchmark,"@inproceedings{
wang2023on,
title={On Pre-training Language Model for Antibody},
author={Danqing Wang and Fei YE and Hao Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=zaq4LV55xHl}
}",wangPretrainingLanguageModel2023
STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK,"We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the question are used to produce intermediate conclusions that can prove the correctness of a certain answer. We perform extensive evaluation with popular language models such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models still lag behind human performance when producing such structured reasoning steps. We believe this work will provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.",https://iclr.cc//virtual/2023/poster/12169,2023,ICLR,Yes,Language,Benchmark,"@inproceedings{
ribeiro2023street,
title={{STREET}: A {MULTI}-{TASK} {STRUCTURED} {REASONING} {AND} {EXPLANATION} {BENCHMARK}},
author={Danilo Neves Ribeiro and Shen Wang and Xiaofei Ma and Henghui Zhu and Rui Dong and Deguang Kong and Juliette Burger and Anjelica Ramos and zhiheng huang and William Yang Wang and George Karypis and Bing Xiang and Dan Roth},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=1C_kSW1-k0}
}",ribeiroSTREETMULTITASKSTRUCTURED2023
Task Ambiguity in Humans and Language Models,"Language models have recently achieved strong performance across a wide range of NLP benchmarks. However, real world tasks are often poorly specified, and agents must deduce the intended behavior from a combination of context, instructions, and examples. We investigate how both humans and models behave in the face of such task ambiguity by proposing AmbiBench, a new benchmark of six ambiguously-specified classification tasks. We evaluate humans and models on AmbiBench by seeing how well they identify the intended task using 1) instructions with varying degrees of ambiguity, and 2) different numbers of labeled examples. We find that the combination of model scaling (to 175B parameters) and reinforcement learning from human feedback (RLHF) enables models to approach or exceed the accuracy of human participants across tasks, but that either one of these alone is not sufficient. In addition, we show how to dramatically improve the accuracy of language models trained without RLHF by finetuning on a small number of ambiguous in-context examples, providing a promising direction for teaching models to generalize well in the face of ambiguity.",https://iclr.cc//virtual/2023/poster/11306,2023,ICLR,Yes,Language,Benchmark,"@inproceedings{
tamkin2023task,
title={Task Ambiguity in Humans and Language Models},
author={Alex Tamkin and Kunal Handa and Avash Shrestha and Noah Goodman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=QrnDe_9ZFd8}
}",tamkinTaskAmbiguityHumans2023
"When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?","Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode the compositional relationships between objects and attributes. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of \emph{Visual Genome Attribution}, to test the understanding of objects' properties; \emph{Visual Genome Relation}, to test for relational understanding; and \emph{COCO-Order \& Flickr30k-Order}, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We present the settings where  state-of-the-art VLMs behave like bags-of-words---i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large scale datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on image-text retrieval over existing datasets without using the composition and order information. This further motivates the value of using ARO to benchmark VLMs. Given that contrastive pretraining optimizes for retrieval on large datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality. ",https://iclr.cc//virtual/2023/poster/10875,2023,ICLR,Yes,Multimodal,Benchmark,"@inproceedings{
yuksekgonul2023when,
title={When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?},
author={Mert Yuksekgonul and Federico Bianchi and Pratyusha Kalluri and Dan Jurafsky and James Zou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=KRLUvxh8uaX}
}",yuksekgonulWhenWhyVisionlanguage2023
WikiWhy: Answering and Explaining Cause-and-Effect Questions,"As large language models (LLMs) grow larger and more sophisticated, assessing their ""reasoning"" capabilities in natural language grows more challenging. Recent question answering (QA) benchmarks that attempt to assess reasoning are often limited by a narrow scope of covered situations and subject matters. We introduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining why an answer is true in natural language. WikiWhy contains over 9,000 ""why"" question-answer-rationale triples, grounded on Wikipedia facts across a diverse set of topics. Each rationale is a set of supporting statements connecting the question to the answer. WikiWhy serves as a benchmark for the reasoning capabilities of LLMs because it demands rigorous explicit rationales for each answer to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized. GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements.",https://iclr.cc//virtual/2023/poster/10792,2023,ICLR,Yes,Language,Benchmark,"@inproceedings{
ho2023wikiwhy,
title={WikiWhy: Answering and Explaining Cause-and-Effect Questions},
author={Matthew Ho and Aditya Sharma and Justin Chang and Michael Saxon and Sharon Levy and Yujie Lu and William Yang Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=vaxnu-Utr4l}
}",hoWikiWhyAnsweringExplaining2023
A Benchmark for Learning to Translate a New Language from One Grammar Book,"Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang—a language with less than 200 speakers and therefore virtually no presence on the web—using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 language learning than L1 language acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials. We hope that MTOB will help measure LLM capabilities along a new dimension, and that the methods developed to solve it could help expand access to language technology for underserved communities by leveraging qualitatively different kinds of data than traditional machine translation.",https://iclr.cc//virtual/2024/poster/17609,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
tanzer2024a,
title={A Benchmark for Learning to Translate a New Language from One Grammar Book},
author={Garrett Tanzer and Mirac Suzgun and Eline Visser and Dan Jurafsky and Luke Melas-Kyriazi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=tbVWug9f2h}
}",tanzerBenchmarkLearningTranslate2024
AgentBench: Evaluating LLMs as Agents,"The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.Thus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.Our extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.Improving instruction following and training on high quality multi-round alignment data could improve agent performance.And different from existing assumptions, training on code present ambivalent impacts on different agent tasks.Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.",https://iclr.cc//virtual/2024/poster/17388,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
liu2024agentbench,
title={AgentBench: Evaluating {LLM}s as Agents},
author={Xiao Liu and Hao Yu and Hanchen Zhang and Yifan Xu and Xuanyu Lei and Hanyu Lai and Yu Gu and Hangliang Ding and Kaiwen Men and Kejuan Yang and Shudan Zhang and Xiang Deng and Aohan Zeng and Zhengxiao Du and Chenhui Zhang and Sheng Shen and Tianjun Zhang and Yu Su and Huan Sun and Minlie Huang and Yuxiao Dong and Jie Tang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=zAdUB0aCTQ}
}",liuAgentBenchEvaluatingLLMs2024
BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks,"The genome sequence contains the blueprint for governing cellular processes.   While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data.   Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce **BEND**, a **BEN**chmark for **D**NA language models, featuring  a collection of realistic and biologically meaningful downstream tasks defined on the human genome.  We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features.  BEND is available at https://github.com/frederikkemarin/BEND.",https://iclr.cc//virtual/2024/poster/17578,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
marin2024bend,
title={{BEND}: Benchmarking {DNA} Language Models on Biologically Meaningful Tasks},
author={Frederikke Isa Marin and Felix Teufel and Marc Horlacher and Dennis Madsen and Dennis Pultz and Ole Winther and Wouter Boomsma},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=uKB4cFNQFg}
}",marinBENDBenchmarkingDNA2024
Can Large Language Models Infer Causation from Correlation?,"Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize – they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and can be helpful in guiding future research on improving LLMs’ pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.",https://iclr.cc//virtual/2024/poster/17518,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
jin2024can,
title={Can Large Language Models Infer Causation from Correlation?},
author={Zhijing Jin and Jiarui Liu and Zhiheng LYU and Spencer Poff and Mrinmaya Sachan and Rada Mihalcea and Mona T. Diab and Bernhard Sch{\""o}lkopf},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=vqIH0ObdqL}
}",jinCanLargeLanguage2024
Can LLMs Keep a Secret? Testing  Privacy  Implications of Language Models  via Contextual Integrity Theory,"Existing efforts on quantifying privacy implications for large language models (LLMs) solely focus on measuring leakage of training data. In this work, we shed light on the often-overlooked interactive settings where an LLM receives information from multiple sources and generates an output to be shared with other entities, creating the potential of exposing sensitive input data in inappropriate contexts. In these scenarios, humans nat- urally uphold privacy by choosing whether or not to disclose information depending on the context. We ask the question “Can LLMs demonstrate an equivalent discernment and reasoning capability when considering privacy in context?” We propose CONFAIDE, a benchmark grounded in the theory of contextual integrity and designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. CONFAIDE consists of four tiers, gradually increasing in complexity, with the final tier evaluating contextual privacy reasoning and theory of mind capabilities. Our experiments show that even commercial models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively, highlighting the urgent need for a new direction of privacy-preserving approaches as we demonstrate a larger underlying problem stemmed in the models’ lack of reasoning capabilities.",https://iclr.cc//virtual/2024/poster/18131,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
mireshghallah2024can,
title={Can {LLM}s Keep a Secret? Testing  Privacy  Implications of Language Models  via Contextual Integrity Theory},
author={Niloofar Mireshghallah and Hyunwoo Kim and Xuhui Zhou and Yulia Tsvetkov and Maarten Sap and Reza Shokri and Yejin Choi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gmg7t8b4s0}
}",mireshghallahCanLLMsKeep2024
Evaluating Language Model Agency Through Negotiations,"We introduce an approach to evaluate language model (LM) agency using negotiation games. This approach better reflects real-world use cases and addresses some of the shortcomings of alternative LM benchmarks. Negotiation games enable us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental evaluation data leakage. We use our approach to test six widely used and publicly accessible LMs, evaluating performance and alignment in both self-play and cross-play settings. Noteworthy findings include: (i) only closed-source models tested here were able to complete these tasks; (ii) cooperative bargaining games proved to be most challenging to the models; and (iii) even the most powerful models sometimes ""lose"" to weaker opponents.",https://iclr.cc//virtual/2024/poster/19505,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
davidson2024evaluating,
title={Evaluating Language Model Agency Through Negotiations},
author={Tim Ruben Davidson and Veniamin Veselovsky and Michal Kosinski and Robert West},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=3ZqKxMHcAg}
}",davidsonEvaluatingLanguageModel2024
Evaluating Large Language Models at Evaluating Instruction Following,"As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these “LLM evaluators”, particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.",https://iclr.cc//virtual/2024/poster/17598,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
zeng2024evaluating,
title={Evaluating Large Language Models at Evaluating Instruction Following},
author={Zhiyuan Zeng and Jiatong Yu and Tianyu Gao and Yu Meng and Tanya Goyal and Danqi Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=tr0KidwPLc}
}",zengEvaluatingLargeLanguage2024
GAIA: a benchmark for General AI Assistants,"We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA’s philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system’s capability to exhibit similar robustness as the average human does on such questions. Using GAIA’s methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board accessible at https://huggingface.co/gaia-benchmark.",https://iclr.cc//virtual/2024/poster/18176,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
mialon2024gaia,
title={{GAIA}: a benchmark for General {AI} Assistants},
author={Gr{\'e}goire Mialon and Cl{\'e}mentine Fourrier and Thomas Wolf and Yann LeCun and Thomas Scialom},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=fibxvahvs3}
}",mialonGAIABenchmarkGeneral2024
HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments,"Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.",https://iclr.cc//virtual/2024/poster/17872,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
zhou2024hazard,
title={{HAZARD} Challenge: Embodied Decision Making in Dynamically Changing Environments},
author={Qinhong Zhou and Sunli Chen and Yisong Wang and Haozhe Xu and Weihua Du and Hongxin Zhang and Yilun Du and Joshua B. Tenenbaum and Chuang Gan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=n6mLhaBahJ}
}",zhouHAZARDChallengeEmbodied2024
KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval,"We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., “a list of ice cream shops in San Diego”). In the past, such queries were considered as tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes across dimensions such as information popularity, constraint types, and context availability. Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases. While context availability mitigates irrelevant information, it is not helpful for satisfying constraints, identifying fundamental barriers to constraint satisfaction. We open source our contributions to foster further research on improving constraint satisfaction abilities of future models.",https://iclr.cc//virtual/2024/poster/18346,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
abdin2024kitab,
title={{KITAB}: Evaluating {LLM}s on Constraint Satisfaction for Information Retrieval},
author={Marah I Abdin and Suriya Gunasekar and Varun Chandrasekaran and Jerry Li and Mert Yuksekgonul and Rahee Ghosh Peshawaria and Ranjita Naik and Besmira Nushi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=b3kDP3IytM}
}",abdinKITABEvaluatingLLMs2024
KoLA: Carefully Benchmarking World Knowledge of Large Language Models,"The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models, and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate 21 open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset will be updated every three months to provide timely references for developing LLMs and knowledge-related systems.",https://iclr.cc//virtual/2024/poster/19238,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
yu2024kola,
title={Ko{LA}: Carefully Benchmarking World Knowledge of Large Language Models},
author={Jifan Yu and Xiaozhi Wang and Shangqing Tu and Shulin Cao and Daniel Zhang-Li and Xin Lv and Hao Peng and Zijun Yao and Xiaohan Zhang and Hanming Li and Chunyang Li and Zheyuan Zhang and Yushi Bai and Yantao Liu and Amy Xin and Kaifeng Yun and Linlu GONG and Nianyi Lin and Jianhui Chen and Zhili Wu and Yunjia Qi and Weikai Li and Yong Guan and Kaisheng Zeng and Ji Qi and Hailong Jin and Jinxin Liu and Yu Gu and Yuan Yao and Ning Ding and Lei Hou and Zhiyuan Liu and Xu Bin and Jie Tang and Juanzi Li},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=AqN23oqraW}
}",yuKoLACarefullyBenchmarking2024
Large Language Models as Automated Aligners for  benchmarking  Vision-Language Models,"With the advancements in Large Language Models (LLMs), Vision-Language Models (VLMs) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence. In this work, we address the limitations via Auto-Bench, which delves into exploring LLMs as proficient aligners, measuring the alignment between VLMs and human intelligence and value through automatic data curation and assessment. Specifically, for data curation, Auto-Bench utilizes LLMs (e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning triplets via prompting on visual symbolic representations (e.g., captions, object locations, instance relationships, and etc. The curated data closely matches human intent, owing to the extensive world knowledge embedded in LLMs. Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered question-answer-reasoning triplets have been curated, covering 4 primary abilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to serve as judges, implementing the quantitative and qualitative automated assessments to facilitate a comprehensive evaluation of VLMs. Our validation results reveal that LLMs are proficient in both evaluation data curation and model assessment, achieving an average agreement rate of 85%. We envision Auto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating the evolving sophisticated VLMs.",https://iclr.cc//virtual/2024/poster/17968,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
ji2024large,
title={Large Language Models as Automated Aligners for  benchmarking  Vision-Language Models},
author={Yuanfeng Ji and Chongjian GE and Weikai Kong and Enze Xie and Zhengying Liu and Zhenguo Li and Ping Luo},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=kZEXgtMNNo}
}",jiLargeLanguageModels2024
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset,"Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",https://iclr.cc//virtual/2024/poster/19219,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
zheng2024lmsyschatm,
title={{LMSYS}-Chat-1M: A Large-Scale Real-World {LLM} Conversation Dataset},
author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Tianle Li and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zhuohan Li and Zi Lin and Eric Xing and Joseph E. Gonzalez and Ion Stoica and Hao Zhang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=BOfDKxfwt0}
}",zhengLMSYSchat1MLargescaleRealworld2024
LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents,"Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.",https://iclr.cc//virtual/2024/poster/19271,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
choi2024lotabench,
title={LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents},
author={Jae-Woo Choi and Youngwoo Yoon and Hyobin Ong and Jaehong Kim and Minsu Jang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ADSxCpCu9s}
}",choiLoTabenchBenchmarkingLanguageoriented2024
MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts,"Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",https://iclr.cc//virtual/2024/poster/18900,2024,ICLR,Yes,Multimodal,Benchmark,"@inproceedings{
lu2024mathvista,
title={MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
author={Pan Lu and Hritik Bansal and Tony Xia and Jiacheng Liu and Chunyuan Li and Hannaneh Hajishirzi and Hao Cheng and Kai-Wei Chang and Michel Galley and Jianfeng Gao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KUNzEQMWU7}
}",luMathVistaEvaluatingMathematical2024
MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use,"Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers -- we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to.",https://iclr.cc//virtual/2024/poster/18657,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
huang2024metatool,
title={MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use},
author={Yue Huang and Jiawen Shi and Yuan Li and Chenrui Fan and Siyuan Wu and Qihui Zhang and Yixin Liu and Pan Zhou and Yao Wan and Neil Zhenqiang Gong and Lichao Sun},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=R0c2qtalgG}
}",huangMetaToolBenchmarkLarge2024
MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback,"To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools.However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases.We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback.To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4.We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation.Our analysis of 20 open- and closed-source LLMs offers intriguing findings.(a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1--8% for each turn of tool use and 2--17% with natural language feedback.(b) Better single-turn performance does not guarantee better multi-turn performance.(c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities.We expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.",https://iclr.cc//virtual/2024/poster/18006,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
wang2024mint,
title={{MINT}: Evaluating {LLM}s in Multi-turn Interaction with Tools and Language Feedback},
author={Xingyao Wang and Zihan Wang and Jiateng Liu and Yangyi Chen and Lifan Yuan and Hao Peng and Heng Ji},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jp3gWrMuIZ}
}",wangMINTEvaluatingLLMs2024
MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations,"Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. However, most existing multimodal intent benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. In this paper, we introduce MIntRec2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 high-quality dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes, across text, video, and audio modalities. In addition to more than 9,300 in-scope samples, it also includes over 5,700 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world open scenarios, enhancing its practical applicability. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the organization of single-turn and multi-turn dialogue data, modality feature extraction, multimodal fusion, as well as in-scope classification and out-of-scope detection. Evaluation benchmarks are built using classic multimodal fusion methods, ChatGPT, and human evaluators. While existing methods incorporating nonverbal information yield improvements, effectively leveraging context information and detecting out-of-scope samples remains a substantial challenge. Notably, powerful large language models exhibit a significant performance gap compared to humans, highlighting the limitations of machine learning methods in the advanced cognitive intent understanding task. We believe that MIntRec2.0 will serve as a valuable resource, providing a pioneering foundation for research in human-machine conversational interactions, and significantly facilitating related applications.The full dataset and codes are available for use at https://github.com/thuiar/MIntRec2.0.",https://iclr.cc//virtual/2024/poster/17855,2024,ICLR,Yes,Multimodal,Benchmark,"@inproceedings{
zhang2024mintrec,
title={{MI}ntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations},
author={Hanlei Zhang and Xin Wang and Hua Xu and Qianrui Zhou and Kai Gao and Jianhua Su and jinyue Zhao and Wenrui Li and Yanting Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=nY9nITZQjc}
}",zhangMIntRec20LargescaleBenchmark2024
MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning,"While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our data instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.",https://iclr.cc//virtual/2024/poster/18015,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
sprague2024musr,
title={Mu{SR}: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning},
author={Zayne Rea Sprague and Xi Ye and Kaj Bostrom and Swarat Chaudhuri and Greg Durrett},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jenyYQzue1}
}",spragueMuSRTestingLimits2024
On the generalization capacity of neural networks during generic multimodal reasoning,"The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",https://iclr.cc//virtual/2024/poster/17366,2024,ICLR,Yes,Multimodal,Benchmark,"@inproceedings{
ito2024on,
title={On the generalization capacity of neural networks during generic multimodal reasoning},
author={Takuya Ito and Soham Dan and Mattia Rigotti and James Kozloski and Murray Campbell},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=zyBJodMrn5}
}",itoGeneralizationCapacityNeural2024
Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy,"The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models’ capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our benchmark to a suite of vision-language models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of vision-language modeling.",https://iclr.cc//virtual/2024/poster/19102,2024,ICLR,Yes,Multimodal,Benchmark,"@inproceedings{
ging2024openended,
title={Open-ended {VQA} benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy},
author={Simon Ging and Maria Alejandra Bravo and Thomas Brox},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=EXitynZhYn}
}",gingOpenendedVQABenchmarking2024
PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization,"Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our findings reveal that PandaLM-7B offers a performance comparable to both GPT-3.5 and GPT-4. Impressively, PandaLM-70B surpasses their performance. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage.",https://iclr.cc//virtual/2024/poster/19427,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
wang2024pandalm,
title={Panda{LM}: An Automatic Evaluation Benchmark for {LLM} Instruction Tuning Optimization},
author={Yidong Wang and Zhuohao Yu and Wenjin Yao and Zhengran Zeng and Linyi Yang and Cunxiang Wang and Hao Chen and Chaoya Jiang and Rui Xie and Jindong Wang and Xing Xie and Wei Ye and Shikun Zhang and Yue Zhang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=5Nn2BLV7SB}
}",wangPandaLMAutomaticEvaluation2024
RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems,"Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is actively maintained with the latest code, serving as a live benchmark publicly available at https://github.com/Leolty/repobench.",https://iclr.cc//virtual/2024/poster/17776,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
liu2024repobench,
title={RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems},
author={Tianyang Liu and Canwen Xu and Julian McAuley},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=pPjZIOuQuF}
}",liuRepoBenchBenchmarkingRepositorylevel2024
SmartPlay : A Benchmark for LLMs as Intelligent Agents,"Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at https://github.com/microsoft/SmartPlay",https://iclr.cc//virtual/2024/poster/18620,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
wu2024smartplay,
title={SmartPlay : A Benchmark for {LLM}s as Intelligent Agents},
author={Yue Wu and Xuan Tang and Tom Mitchell and Yuanzhi Li},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=S2oTVrlcp3}
}",wuSmartPlayBenchmarkLLMs2024
Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game,"While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to *prompt injection attacks*: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 563,000 prompt injection attacks and 118,000 prompt-based ""defenses"" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is the first dataset that includes both human-generated attacks and defenses for instruction-following LLMs. The attacks in our dataset have easily interpretable structure, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as *prompt extraction* and *prompt hijacking*. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release data and code at [tensortrust.ai/paper](https://tensortrust.ai/paper)",https://iclr.cc//virtual/2024/poster/18168,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
toyer2024tensor,
title={Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game},
author={Sam Toyer and Olivia Watkins and Ethan Adrian Mendes and Justin Svegliato and Luke Bailey and Tiffany Wang and Isaac Ong and Karim Elmaaroufi and Pieter Abbeel and Trevor Darrell and Alan Ritter and Stuart Russell},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=fsW7wJGLBd}
}",toyerTensorTrustInterpretable2024
Towards Understanding Factual Knowledge of Large Language Models,"Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to explore the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs can compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes are publicly available at: https://github.com/THU-BPM/Pinocchio.",https://iclr.cc//virtual/2024/poster/19298,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
hu2024towards,
title={Towards Understanding Factual Knowledge of Large Language Models},
author={Xuming Hu and Junzhe Chen and Xiaochuan Li and Yufei Guo and Lijie Wen and Philip S. Yu and Zhijiang Guo},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=9OevMUdods}
}",huUnderstandingFactualKnowledge2024
ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models,"With the ever-increasing popularity of pretrained Video-Language Models (VidLMs), there is a pressing need to develop robust evaluation methodologies that delve deeper into their visio-linguistic capabilities. To address this challenge, we present ViLMA (Video Language Model Assessment), a task-agnostic benchmark that places the assessment of fine-grained capabilities of these models on a firm footing. Task-based evaluations, while valuable, fail to capture the complexities and specific temporal aspects of moving images that VidLMs need to process. Through carefully curated counterfactuals, ViLMA offers a controlled evaluation suite that sheds light on the true potential of these models, as well as their performance gaps compared to human-level understanding. ViLMA also includes proficiency tests, which assess basic capabilities deemed essential to solving the main counterfactual tests. We show that current VidLMs’ grounding abilities are no better than those of vision-language models which use static images. This is especially striking once the performance on proficiency tests is factored in. Our benchmark serves as a catalyst for future research on VidLMs, helping to highlight areas that still need to be explored.",https://iclr.cc//virtual/2024/poster/17922,2024,ICLR,Yes,Multimodal,Benchmark,"@inproceedings{
kesen2024vilma,
title={Vi{LMA}: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models},
author={Ilker Kesen and Andrea Pedrotti and Mustafa Dogan and Michele Cafagna and Emre Can Acikgoz and Letitia Parcalabescu and Iacer Calixto and Anette Frank and Albert Gatt and Aykut Erdem and Erkut Erdem},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=liuqDwmbQJ}
}",kesenViLMAZeroshotBenchmark2024
WebArena: A Realistic Web Environment for Building Autonomous Agents,"With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.  The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \ours can be used to measure such progress.\footnote{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.}",https://iclr.cc//virtual/2024/poster/17826,2024,ICLR,Yes,Language,Benchmark,"@inproceedings{
zhou2024webarena,
title={WebArena: A Realistic Web Environment for Building Autonomous Agents},
author={Shuyan Zhou and Frank F. Xu and Hao Zhu and Xuhui Zhou and Robert Lo and Abishek Sridhar and Xianyi Cheng and Tianyue Ou and Yonatan Bisk and Daniel Fried and Uri Alon and Graham Neubig},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=oKn9c6ytLx}
}",zhouWebArenaRealisticWeb2024
A Large Scale Search Dataset for Unbiased Learning to Rank,"The unbiased learning to rank (ULTR) problem has been greatly advanced by recent deep learning techniques and well-designed debias algorithms. However, promising results on the existing benchmark datasets may not be extended to the practical scenario due to some limitations of existing datasets. First, their semantic feature extractions are outdated while state-of-the-art large-scale pre-trained language models like BERT cannot be utilized due to the lack of original text. Second, display features are incomplete; thus in-depth study on ULTR is impossible such as the displayed abstract for analyzing the click necessary bias. Third, synthetic user feedback has been adopted by most existing datasets and real-world user feedback is greatly missing. To overcome these disadvantages, we introduce the Baidu-ULTR dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008 expert annotated queries(397,572 query document pairs). Baidu-ULTR is the first billion-level dataset for ULTR. Particularly, it offers: (1)the original semantic features and pre-trained language models of different sizes; (2)sufficient display information such as position, displayed height, and displayed abstract, enabling the comprehensive study of multiple displayed biases; and (3)rich user feedback on search result pages (SERPs) like dwelling time, allowing for user engagement optimization and promoting the exploration of multi-task learning in ULTR. Furthermore, we present the design principle of Baidu-ULTR and the performance of representative ULTR algorithms on Baidu-ULTR. The Baidu-ULTR dataset and corresponding baseline implementations are available at https://github.com/ChuXiaokai/baidu_ultr_dataset. The dataset homepage is available at https://searchscience.baidu.com/dataset.html.",https://neurips.cc//virtual/2022/poster/55768,2022,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2022_07f56009,
 author = {Zou, Lixin and Mao, Haitao and Chu, Xiaokai and Tang, Jiliang and Ye, Wenwen and Wang, Shuaiqiang and Yin, Dawei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {1127--1139},
 publisher = {Curran Associates, Inc.},
 title = {A Large Scale Search Dataset for Unbiased Learning to Rank},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/07f560092a0edceabf55af32a40eaee3-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}",zouLargeScaleSearch2022
A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction,"The recent advances of deep learning have dramatically changed how machine learning, especially in the domain of natural language processing, can be applied to legal domain. However, this shift to the data-driven approaches calls for larger and more diverse datasets, which are nevertheless still small in number, especially in non-English languages. Here we present the first large-scale benchmark of Korean legal AI datasets, LBOX OPEN, that consists of one legal corpus, two classification tasks, two legal judgement prediction (LJP) tasks, and one summarization task. The legal corpus consists of 147k Korean precedents (259M tokens), of which 63k are sentenced in last 4 years and 96k are from the first and the second level courts in which factual issues are reviewed. The two classification tasks are case names (11.3k) and statutes (2.8k) prediction from the factual description of individual cases. The LJP tasks consist of (1) 10.5k criminal examples where the model is asked to predict fine amount, imprisonment with labor, and imprisonment without labor ranges for the given facts, and (2) 4.7k civil examples where the inputs are facts and claim for relief and outputs are the degrees of claim acceptance. The summarization task consists of the Supreme Court precedents and the corresponding summaries (20k). We also release realistic variants of the datasets by extending the domain (1) to infrequent case categories in case name (31k examples) and statute (17.7k) classification tasks, and (2) to long input sequences in the summarization task (51k). Finally, we release LCUBE, the first Korean legal language model trained on the legal corpus from this study. Given the uniqueness of the Law of South Korea and the diversity of the legal tasks covered in this work, we believe that LBOX OPEN contributes to the multilinguality of global legal research. LBOX OPEN and LCUBE will be publicly available.",https://neurips.cc//virtual/2022/poster/55740,2022,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2022_d15abd14,
 author = {Hwang, Wonseok and Lee, Dongjun and Cho, Kyoungyeon and Lee, Hanuhl and Seo, Minjoon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {32537--32551},
 publisher = {Curran Associates, Inc.},
 title = {A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/d15abd14d5894eebd185b756541d420e-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}",hwangMultitaskBenchmarkKorean2022
CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks,"Current state-of-the-art vision-and-language models are evaluated on tasks either individually or in a multi-task setting, overlooking the challenges of continually learning (CL) tasks as they arrive. Existing CL benchmarks have facilitated research on task adaptation and mitigating ""catastrophic forgetting"", but are limited to vision-only and language-only tasks. We present CLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL setting, and to systematically evaluate how upstream continual learning can rapidly generalize to new multimodal and unimodal tasks. CLiMB includes implementations of several CL algorithms and a modified Vision-Language Transformer (ViLT) model that can be deployed on both multimodal and unimodal tasks. We find that common CL methods can help mitigate forgetting during multimodal task learning, but do not enable cross-task knowledge transfer. We envision that CLiMB will facilitate research on a new class of CL algorithms for this challenging multimodal setting.",https://neurips.cc//virtual/2022/poster/55711,2022,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2022_bd361197,
 author = {Srinivasan, Tejas and Chang, Ting-Yun and Pinto Alva, Leticia and Chochlakis, Georgios and Rostami, Mohammad and Thomason, Jesse},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {29440--29453},
 publisher = {Curran Associates, Inc.},
 title = {CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/bd3611971089d466ab4ca96a20f7ab13-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}",srinivasanCLiMBContinualLearning2022
Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,"When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io.",https://neurips.cc//virtual/2022/poster/54469,2022,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2022_11332b6b,
 author = {Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {2507--2521},
 publisher = {Curran Associates, Inc.},
 title = {Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}",luLearnExplainMultimodal2022
PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding,"We are now witnessing significant progress of deep learning methods in a variety of tasks (or datasets) of proteins. However, there is a lack of a standard benchmark to evaluate the performance of different methods, which hinders the progress of deep learning in this field. In this paper, we propose such a benchmark called PEER, a comprehensive and multi-task benchmark for Protein sEquence undERstanding. PEER provides a set of diverse protein understanding tasks including protein function prediction, protein localization prediction, protein structure prediction, protein-protein interaction prediction, and protein-ligand interaction prediction. We evaluate different types of sequence-based methods for each task including traditional feature engineering approaches, different sequence encoding methods as well as large-scale pre-trained protein language models. In addition, we also investigate the performance of these methods under the multi-task learning setting. Experimental results show that large-scale pre-trained protein language models achieve the best performance for most individual tasks, and jointly training multiple tasks further boosts the performance. The datasets and source codes of this benchmark will be open-sourced soon.",https://neurips.cc//virtual/2022/poster/55752,2022,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2022_e467582d,
 author = {Xu, Minghao and Zhang, Zuobai and Lu, Jiarui and Zhu, Zhaocheng and Zhang, Yangtian and Chang, Ma and Liu, Runcheng and Tang, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {35156--35173},
 publisher = {Curran Associates, Inc.},
 title = {PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e467582d42d9c13fa9603df16f31de6d-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}",xuPEERComprehensiveMultitask2022
Robustness Analysis of Video-Language Models Against Visual and Language Perturbations,"Joint visual and language modeling on large-scale datasets has recently shown good progress in multi-modal tasks when compared to single modal learning. However, robustness of these  approaches against real-world perturbations has not been studied. In this work, we perform the first extensive robustness study of video-language models against various real-world perturbations. We focus on text-to-video retrieval and propose two large-scale benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different text perturbations. The study reveals some interesting initial findings from the studied models: 1) models are more robust when text is perturbed versus when video is perturbed, 2) models that are pre-trained are more robust than those trained from scratch, 3) models attend more to scene and objects rather than motion and action. We hope this study will serve as a benchmark and guide future research in robust video-language learning. The benchmark introduced in this study along with the code and datasets is available at https://bit.ly/3CNOly4.",https://neurips.cc//virtual/2022/poster/55743,2022,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2022_de6ff07c,
 author = {Schiappa, Madeline and Vyas, Shruti and Palangi, Hamid and Rawat, Yogesh and Vineet, Vibhav},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {34405--34420},
 publisher = {Curran Associates, Inc.},
 title = {Robustness Analysis of Video-Language Models Against Visual and Language Perturbations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/de6ff07cbd222c10d694c2b2f732aceb-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}",schiappaRobustnessAnalysisVideolanguage2022
TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models,"In order to diagnostically analyze and improve the capability of pretrained language models (PLMs) in text generation, we propose TGEA 2.0, to date the largest dataset built on machine-authored texts by PLMs with fine-grained semantic annotations on a wide variety of pathological generation errors. We collect 170K nominal, phrasal and sentential prompts from 6M natural sentences in 3 domains. These prompts are fed into 4 generative PLMs with their best decoding strategy to generate paragraphs. 195,629 sentences are extracted from these generated paragraphs for manual annotation, where 36K erroneous sentences are detected, 42K erroneous spans are located and categorized into an error type defined in a two-level error taxonomy. We define a \textbf{Mi}nimal \textbf{S}et of \textbf{E}rror-related \textbf{W}ords (MiSEW) for each erroneous span, which not only provides error-associated words but also rationalizes the reasoning behind the error. Quality control with a pre-annotation and feedback loop is performed before and during the entire annotation process. With the diagnostically annotated dataset, we propose 5 diagnosis benchmark tasks (i.e., erroneous text detection, MiSEW extraction, erroneous span location and correction together with error type classification) and 2 pathology mitigation benchmark tasks (pairwise comparison and word prediction). Experiment results on these benchmark tasks demonstrate that TGEA 2.0 is a challenging dataset that could facilitate further research on automatic diagnosis and pathology mitigation over machine texts. The dataset will be publicly available at https://github.com/tjunlp-lab/TGEA/.",https://neurips.cc//virtual/2022/poster/55660,2022,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2022_cd556f38,
 author = {Ge, Huibin and Zhao, Xiaohu and Liu, Chuang and Zeng, Yulong and Liu, Qun and Xiong, Deyi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {31612--31626},
 publisher = {Curran Associates, Inc.},
 title = {TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/cd556f38dba3a6c367c42fa85fc0801c-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}",geTGEA20Largescale2022
"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish","The availability of compute and data to train larger and larger language models increases the demand for robust methods of benchmarking the true progress of LM training. Recent years witnessed significant progress in standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or KILT have become a de facto standard tools to compare large language models. Following the trend to replicate GLUE for other languages, the KLEJ benchmark\ (klej is the word for glue in Polish) has been released for Polish. In this paper, we evaluate the progress in benchmarking for low-resourced languages. We note that only a handful of languages have such comprehensive benchmarks. We also note the gap in the number of tasks being evaluated by benchmarks for resource-rich English/Chinese and the rest of the world.In this paper, we introduce LEPISZCZE (lepiszcze is the Polish word for glew, the Middle English predecessor of glue), a new, comprehensive benchmark for Polish NLP with a large variety of tasks and high-quality operationalization of the benchmark.We design LEPISZCZE with flexibility in mind. Including new models, datasets, and tasks is as simple as possible while still offering data versioning and model tracking. In the first run of the benchmark, we test 13 experiments (task and dataset pairs) based on the five most recent LMs for Polish. We use five datasets from the Polish benchmark and add eight novel datasets. As the paper's main contribution, apart from LEPISZCZE, we provide insights and experiences learned while creating the benchmark for Polish as the blueprint to design similar benchmarks for other low-resourced languages.",https://neurips.cc//virtual/2022/poster/55618,2022,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2022_890b206e,
 author = {Augustyniak, Lukasz and Tagowski, Kamil and Sawczyn, Albert and Janiak, Denis and Bartusiak, Roman and Szymczak, Adrian and Janz, Arkadiusz and Szyma\'{n}ski, Piotr and W\k{a}troba, Marcin and Morzy, Miko\l aj and Kajdanowicz, Tomasz and Piasecki, Maciej},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {21805--21818},
 publisher = {Curran Associates, Inc.},
 title = {This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/890b206ebb79e550f3988cb8db936f42-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}",augustyniakThisWayDesigning2022
WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents,"Most existing benchmarks for grounding language in interactive environments either lack realistic linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. We develop WebShop – a simulated e-commerce website environment with 1.18 million real-world products and 12,087 crowd-sourced text instructions. In this environment, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase a product given an instruction. WebShop provides several challenges including understanding compositional instructions, query (re-)formulation, dealing with noisy text in webpages, and performing strategic exploration. We collect over 1,600 human trajectories to first validate the benchmark, then train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of 29%, which significantly outperforms rule heuristics but is far lower than expert human performance (59%). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show our agent trained on WebShop exhibits non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of our benchmark for developing practical web agents that can operate in the wild.",https://neurips.cc//virtual/2022/poster/52872,2022,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2022_82ad13ec,
 author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {20744--20757},
 publisher = {Curran Associates, Inc.},
 title = {WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}",yaoWebShopScalableRealworld2022a
WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models,"While vision-and-language models perform well on tasks such as visual question answering, they struggle when it comes to basic human commonsense reasoning skills. In this work, we introduce WinoGAViL: an online game of vision-and-language associations (e.g., between werewolves and a full moon), used as a dynamic evaluation benchmark. Inspired by the popular card game Codenames, a spymaster gives a textual cue related to several visual candidates, and another player tries to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (>90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient. Our analysis as well as the feedback we collect from players indicate that the collected associations require diverse reasoning skills, including general knowledge, common sense, abstraction, and more. We release the dataset, the code and the interactive game, allowing future data collection that can be used to develop models with better association abilities.",https://neurips.cc//virtual/2022/poster/55689,2022,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2022_a96fe863,
 author = {Bitton, Yonatan and Bitton Guetta, Nitzan and Yosef, Ron and Elovici, Yuval and Bansal, Mohit and Stanovsky, Gabriel and Schwartz, Roy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {26549--26564},
 publisher = {Curran Associates, Inc.},
 title = {WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a96fe863f85c59789bba63588a9557b4-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}",bittonWinoGAViLGamifiedAssociation2022a
A Comprehensive Study on Text-attributed Graphs: Benchmarking and Rethinking,"Text-attributed graphs (TAGs) are prevalent in various real-world scenarios, where each node is associated with a text description. The cornerstone of representation learning on TAGs lies in the seamless integration of textual semantics within individual nodes and the topological connections across nodes. Recent advancements in pre-trained language models (PLMs) and graph neural networks (GNNs) have facilitated effective learning on TAGs, garnering increased research interest. However, the absence of meaningful benchmark datasets and standardized evaluation procedures for TAGs has impeded progress in this field. In this paper, we propose CS-TAG, a comprehensive and diverse collection of challenging benchmark datasets for TAGs. The CS-TAG datasets are notably large in scale and encompass a wide range of domains, spanning from citation networks to purchase graphs. In addition to building the datasets,  we conduct extensive benchmark experiments over CS-TAG with various learning paradigms, including PLMs, GNNs, PLM-GNN co-training methods, and the proposed novel topological pre-training of language models. In a nutshell, we provide an overview of the CS-TAG datasets, standardized evaluation procedures, and present baseline experiments. The entire CS-TAG project is publicly accessible at \url{https://github.com/sktsherlock/TAG-Benchmark}.",https://neurips.cc//virtual/2023/poster/73479,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_37d00f56,
 author = {Yan, Hao and Li, Chaozhuo and Long, Ruosong and Yan, Chao and Zhao, Jianan and Zhuang, Wenwen and Yin, Jun and Zhang, Peiyan and Han, Weihao and Sun, Hao and Deng, Weiwei and Zhang, Qi and Sun, Lichao and Xie, Xing and Wang, Senzhang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {17238--17264},
 publisher = {Curran Associates, Inc.},
 title = {A Comprehensive Study on Text-attributed Graphs: Benchmarking and Rethinking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/37d00f567a18b478065f1a91b95622a0-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",yanComprehensiveStudyTextattributed2023
Are Diffusion Models Vision-And-Language Reasoners?,"Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality.Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM.Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis.We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground.We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining generative capabilities. We also measure the stereotypical bias in diffusion models, and find that Stable Diffusion 2.1 is, for the most part, less biased than Stable Diffusion 1.5.Overall, our results point in an exciting direction bringing discriminative and generative model evaluation closer. We will release code and benchmark setup soon.",https://neurips.cc//virtual/2023/poster/70890,2023,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2023_1a675d80,
 author = {Krojer, Benno and Poole-Dayan, Elinor and Voleti, Vikram and Pal, Chris and Reddy, Siva},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {8385--8405},
 publisher = {Curran Associates, Inc.},
 title = {Are Diffusion Models Vision-And-Language Reasoners?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1a675d804f50509b8e21d0d3ca709d03-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}",krojerAreDiffusionModels2023a
BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing,"Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output meaning representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark seven language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or even surpass state-of-the-art methods for both syntactic and semantic parsing when the model output is constrained to be valid.",https://neurips.cc//virtual/2023/poster/73488,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_9c1535a0,
 author = {Roy, Subhro and Thomson, Samuel and Chen, Tongfei and Shin, Richard and Pauls, Adam and Eisner, Jason and Van Durme, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {49814--49829},
 publisher = {Curran Associates, Inc.},
 title = {BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9c1535a02f0ce079433344e14d910597-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",royBenchCLAMPBenchmarkEvaluating2023
Benchmarking Large Language Models on CMExam - A comprehensive Chinese Medical Exam Dataset,"Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could generate relevant reasoning and demonstrate improved performance after finetuning, they fall short of a desired standard, indicating ample room for improvement. To the best of our knowledge, CMExam is the first Chinese medical exam dataset to provide comprehensive medical annotations. The experiments and findings of LLM evaluation also provide valuable insights into the challenges and potential solutions in developing Chinese medical QA systems and LLM evaluation pipelines.",https://neurips.cc//virtual/2023/poster/73638,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_a48ad12d,
 author = {Liu, Junling and Zhou, Peilin and Hua, Yining and Chong, Dading and Tian, Zhongyu and Liu, Andrew and Wang, Helin and You, Chenyu and Guo, Zhenhua and ZHU, LEI and Li, Michael Lingzhi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {52430--52452},
 publisher = {Curran Associates, Inc.},
 title = {Benchmarking Large Language Models on CMExam - A comprehensive Chinese Medical Exam Dataset},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a48ad12d588c597f4725a8b84af647b5-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",liuBenchmarkingLargeLanguage2023a
Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models,"Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. As test samples in real-world applications usually differ from adaptation data, the robustness of these adaptation methods against distribution shifts are essential. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead, it results in even lower robustness. We hope this study could benefit future research in the development of robust multimodal adaptation methods. The benchmark, code, and dataset used in this study can be accessed at https://adarobustness.github.io.",https://neurips.cc//virtual/2023/poster/73702,2023,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2023_a2a544e4,
 author = {Chen, Shuo and Gu, Jindong and Han, Zhen and Ma, Yunpu and Torr, Philip and Tresp, Volker},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {51758--51777},
 publisher = {Curran Associates, Inc.},
 title = {Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a2a544e43acb8b954dc5846ff0d77ad5-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",chenBenchmarkingRobustnessAdaptation2023
BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information,"Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor.",https://neurips.cc//virtual/2023/poster/73665,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_7adce80e,
 author = {Kazemi, Mehran and Yuan, Quan and Bhatia, Deepti and Kim, Najoung and Xu, Xin and Imbrasaite, Vaiva and Ramachandran, Deepak},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {39052--39074},
 publisher = {Curran Associates, Inc.},
 title = {BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/7adce80e86aa841490e6307109094de5-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",kazemiBoardgameQADatasetNatural2023a
Can Language Models Solve Graph Problems in Natural Language?,"Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question.",https://neurips.cc//virtual/2023/poster/71520,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_622afc4e,
 author = {Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {30840--30861},
 publisher = {Curran Associates, Inc.},
 title = {Can Language Models Solve Graph Problems in Natural Language?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/622afc4edf2824a1b6aaf5afe153fa93-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}",wangCanLanguageModels2023
Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,"Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years.  In particular, GPT-4 and Claude-2 have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present BIRD, a BIg benchmark for laRge-scale Database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents,  and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most popular and effective text-to-SQL models, i.e. GPT-4, only achieve 54.89% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. We also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research.The leaderboard and source code are available: https://bird-bench.github.io/.",https://neurips.cc//virtual/2023/poster/73529,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_83fc8fab,
 author = {Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Geng, Ruiying and Huo, Nan and Zhou, Xuanhe and Chenhao, Ma and Li, Guoliang and Chang, Kevin and Huang, Fei and Cheng, Reynold and Li, Yongbin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {42330--42357},
 publisher = {Curran Associates, Inc.},
 title = {Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/83fc8fab1710363050bbd1d4b8cc0021-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",liCanLLMAlready2023a
CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care,"The recent advances in natural language processing (NLP), have led to a new trend of applying large language models (LLMs) to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form (LF) generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building LF generation evaluation benchmarks that can be transferred to other knowledge-intensive domains and low-resourced languages. Our proposed benchmark fills the gap between the extensive usage of LLMs and the lack of datasets for assessing the misinformation generated by these models. It contains 1,612 expert-checked questions, accompanied with human-selected references. Using our benchmark, we conduct extensive experiments and found that current Chinese LLMs are far from perfect in the topic of maternity and infant care. In an effort to minimize the reliance on human resources for performance evaluation, we offer off-the-shelf judgment models for automatically assessing the LF output of LLMs given benchmark questions. Moreover, we compare potential solutions for LF generation evaluation and provide insights for building better automated metrics.",https://neurips.cc//virtual/2023/poster/73657,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_84062fe5,
 author = {Xiang, Tong and Li, Liangzhi and Li, Wangyue and Bai, Mingbai and Wei, Lu and Wang, Bowen and Garcia, Noa},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {42358--42381},
 publisher = {Curran Associates, Inc.},
 title = {CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/84062fe53d23e0791c6dbb456783e4a9-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",xiangCAREMIChineseBenchmark2023a
C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models,"New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.",https://neurips.cc//virtual/2023/poster/73516,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_c6ec1844,
 author = {Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and lei, jiayi and Fu, Yao and Sun, Maosong and He, Junxian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {62991--63010},
 publisher = {Curran Associates, Inc.},
 title = {C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/c6ec1844bec96d6d32ae95ae694e23d8-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",huangCevalMultilevelMultidiscipline2023a
Cola: A Benchmark for Compositional Text-to-image Retrieval,"Compositional reasoning is a hallmark of human visual intelligence. Yet, despite the size of large vision-language models, they struggle to represent simple compositions by combining objects with their attributes. To measure this lack of compositional capability, we design Cola, a text-to-image retrieval benchmark to Compose Objects Localized with Attributes. To solve Cola, a model must retrieve images with the correct configuration of attributes and objects and avoid choosing a distractor image with the same objects and attributes but in the wrong configuration. Cola contains about 1.2k composed queries of 168 objects and 197 attributes on around 30K images. Our human evaluation finds that Cola is 83.33% accurate, similar to contemporary compositionality benchmarks. Using Cola as a testbed, we explore empirical modeling designs to adapt pre-trained vision-language models to reason compositionally. We explore 6 adaptation strategies on 2 seminal vision-language models, using compositionality-centric test benchmarks - Cola and CREPE. We find the optimal adaptation strategy is to train a multi-modal attention layer that jointly attends over the frozen pre-trained image and language features. Surprisingly, training multimodal layers on CLIP performs better than tuning a larger FLAVA model with already pre-trained multimodal layers. Furthermore, our adaptation strategy improves CLIP and FLAVA to comparable levels, suggesting that training multimodal layers using contrastive attribute-object data is key, as opposed to using them pre-trained. Lastly, we show that Cola is harder than a closely related contemporary benchmark, CREPE, since simpler fine-tuning strategies without multimodal layers suffice on CREPE, but not on Cola. However, we still see a significant gap between our best adaptation and human accuracy, suggesting considerable room for further research. Project page: https://cs-people.bu.edu/array/research/cola/",https://neurips.cc//virtual/2023/poster/73493,2023,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2023_917cd410,
 author = {Ray, Arijit and Radenovic, Filip and Dubey, Abhimanyu and Plummer, Bryan and Krishna, Ranjay and Saenko, Kate},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {46433--46445},
 publisher = {Curran Associates, Inc.},
 title = {Cola: A Benchmark for Compositional Text-to-image Retrieval},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/917cd410aa55b61594fa2a6f6e5a9e94-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",rayColaBenchmarkCompositional2023
DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,"Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications to healthcare and finance – where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives – including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially due to the reason that GPT-4 follows the (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/.",https://neurips.cc//virtual/2023/poster/73486,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_63cb9921,
 author = {Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and Truong, Sang and Arora, Simran and Mazeika, Mantas and Hendrycks, Dan and Lin, Zinan and Cheng, Yu and Koyejo, Sanmi and Song, Dawn and Li, Bo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {31232--31339},
 publisher = {Curran Associates, Inc.},
 title = {DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/63cb9921eecf51bfad27a99b2c53dd6d-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",wangDecodingTrustComprehensiveAssessment2023
Diplomat: A Dialogue Dataset for Situated PragMATic Reasoning,"The ability to discern and comprehend pragmatic meanings is a cornerstone of social and emotional intelligence, referred to as pragmatic reasoning. Despite the strides made in the development of Large Language Models (LLMs), such as ChatGPT, these models grapple with capturing the nuanced and ambiguous facets of language, falling short of the aspiration to build human-like conversational agents. In this work, we introduce a novel benchmark, the **DiPlomat**, which delves into the fundamental components of conversational pragmatic reasoning, encompassing situational context reasoning, open-world knowledge acquisition, and unified figurative language understanding. We start by collecting a new human-annotated dialogue dataset, composed of 4,177 multi-turn dialogues and a vocabulary of 48,900 words. Along with the dataset, two tasks are proposed to evaluate machines' pragmatic reasoning capabilities, namely, Pragmatic Reasoning and Identification(PIR) and Conversational Question Answering (CQA). Furthermore, we probe into a zero-shot natural language inference task, where the significance of context in pragmatic reasoning is underscored. Experimental findings illustrate the existing limitations of current prevailing LLMs in the realm of pragmatic reasoning, shedding light on the pressing need for further research to facilitate the emergence of emotional intelligence within human-like conversational agents.",https://neurips.cc//virtual/2023/poster/73411,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_924303c6,
 author = {Li, Hengli and Zhu, Song-Chun and Zheng, Zilong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {46856--46884},
 publisher = {Curran Associates, Inc.},
 title = {Diplomat: A Dialogue Dataset for Situated PragMATic Reasoning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/924303c6a45685510877ee018cdc8f80-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",liDiplomatDialogueDataset2023
Exposing Attention Glitches with Flip-Flop Language Modeling,"Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of _attention glitches_, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce _flip-flop language modeling_ (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs.",https://neurips.cc//virtual/2023/poster/71426,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_510ad301,
 author = {Liu, Bingbin and Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {25549--25583},
 publisher = {Curran Associates, Inc.},
 title = {Exposing Attention Glitches with Flip-Flop Language Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/510ad3018bbdc5b6e3b10646e2e35771-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}",liuExposingAttentionGlitches2023
FELM: Benchmarking Factuality Evaluation of Large Language Models,"Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in substantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as FELM. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g. information from Wikipedia), FELM focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on FELM, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.",https://neurips.cc//virtual/2023/poster/73491,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_8b8a7960,
 author = {chen, shiqi and Zhao, Yiran and Zhang, Jinghan and Chern, I-Chun and Gao, Siyang and Liu, Pengfei and He, Junxian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {44502--44523},
 publisher = {Curran Associates, Inc.},
 title = {FELM: Benchmarking Factuality Evaluation of Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/8b8a7960d343e023a6a0afe37eee6022-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",chenFELMBenchmarkingFactuality2023a
FIND: A Function Description Benchmark for Evaluating Interpretability Methods,"Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate methods that use pretrained language models (LMs) to produce code-based and natural language descriptions of function behavior. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (AIA) generates function descriptions. We find that an AIA, built with an off-the-shelf LM augmented with black-box access to functions, can sometimes infer function structure—acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, FIND also reveals that LM-based descriptions capture global function behavior while missing local details. These results suggest that FIND will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.",https://neurips.cc//virtual/2023/poster/73478,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_ef0164c1,
 author = {Schwettmann, Sarah and Shaham, Tamar and Materzynska, Joanna and Chowdhury, Neil and Li, Shuang and Andreas, Jacob and Bau, David and Torralba, Antonio},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {75688--75715},
 publisher = {Curran Associates, Inc.},
 title = {FIND: A Function Description Benchmark for Evaluating Interpretability Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ef0164c1112f56246224af540857348f-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",schwettmannFINDFunctionDescription2023
InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback,"Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash, Spider, and MBPP datasets. We demonstrate InterCode’s viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan & Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages.",https://neurips.cc//virtual/2023/poster/73513,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_4b175d84,
 author = {Yang, John and Prabhakar, Akshara and Narasimhan, Karthik and Yao, Shunyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {23826--23854},
 publisher = {Curran Associates, Inc.},
 title = {InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4b175d846fb008d540d233c188379ff9-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",yangInterCodeStandardizingBenchmarking2023
Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,"Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences.To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them.We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans.Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna.The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",https://neurips.cc//virtual/2023/poster/73434,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_91f18a12,
 author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {46595--46623},
 publisher = {Curran Associates, Inc.},
 title = {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",zhengJudgingLLMasajudgeMTbench2023
"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","Large language models have emerged as a promising approach towards achieving general-purpose AI agents. The thriving open-source LLM community has greatly accelerated the development of agents that support human-machine dialogue interaction through natural language processing. However, human interaction with the world extends beyond only text as a modality, and other modalities such as vision are also crucial. Recent works on multi-modal large language models, such as GPT-4V and Bard, have demonstrated their effectiveness in handling visual modalities. However, the transparency of these works is limited and insufficient to support academic research. To the best of our knowledge, we present one of the very first open-source endeavors in the field, LAMM, encompassing a Language-Assisted Multi-Modal instruction tuning dataset, framework, and benchmark. Our aim is to establish LAMM as a growing ecosystem for training and evaluating MLLMs, with a specific focus on facilitating AI agents capable of bridging the gap between ideas and execution, thereby enabling seamless human-AI interaction. Our main contribution is three-fold: 1) We present a comprehensive dataset and benchmark, which cover a wide range of vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We outline the detailed methodology of constructing multi-modal instruction tuning datasets and benchmarks for MLLMs, enabling rapid scaling and extension of MLLM research to diverse domains, tasks, and modalities. 3) We provide a primary but potential MLLM training framework optimized for modality extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Our baseline model is trained within 24 A100 GPU hours, framework supports training with V100 and RTX3090 is available thanks to the open-source society. Codes and data are now available at https://openlamm.github.io.",https://neurips.cc//virtual/2023/poster/73519,2023,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2023_548a41b9,
 author = {Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Huang, Xiaoshui and Wang, Zhiyong and Sheng, Lu and BAI, LEI and Shao, Jing and Ouyang, Wanli},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {26650--26685},
 publisher = {Curran Associates, Inc.},
 title = {LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/548a41b9cac6f50dccf7e63e9e1b1b9b-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",yinLAMMLanguageassistedMultimodal2023
LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models,"The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning—which distinguish between its many forms—correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LegalBench, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LegalBench enables.",https://neurips.cc//virtual/2023/poster/73565,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_89e44582,
 author = {Guha, Neel and Nyarko, Julian and Ho, Daniel and R\'{e}, Christopher and Chilton, Adam and K, Aditya and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and Talisman, Dmitry and Hoque, Enam and Surani, Faiz and Fagan, Frank and Sarfaty, Galit and Dickinson, Gregory and Porat, Haggai and Hegland, Jason and Wu, Jessica and Nudell, Joe and Niklaus, Joel and Nay, John and Choi, Jonathan and Tobia, Kevin and Hagan, Margaret and Ma, Megan and Livermore, Michael and Rasumov-Rahe, Nikon and Holzenberger, Nils and Kolt, Noam and Henderson, Peter and Rehaag, Sean and Goel, Sharad and Gao, Shang and Williams, Spencer and Gandhi, Sunny and Zur, Tom and Iyer, Varun and Li, Zehua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {44123--44279},
 publisher = {Curran Associates, Inc.},
 title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/89e44582fd28ddfea1ea4dcb0ebbf4b0-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",guhaLegalBenchCollaborativelyBuilt2023
LOVM: Language-Only Vision Model Selection,"Pre-trained multi-modal vision-language models (VLMs) are becoming increasingly popular due to their exceptional performance on downstream vision applications, particularly in the few- and zero-shot settings. However, selecting the best-performing VLM for some downstream applications is non-trivial, as it is dataset and task-dependent. Meanwhile, the exhaustive evaluation of all available VLMs on a novel application is not only time and  computationally demanding but also necessitates the collection of a labeled dataset for evaluation. As the number of open-source VLM variants increases, there is a need for an efficient model selection strategy that does not require access to a curated evaluation dataset. This paper proposes a novel task and benchmark for efficiently evaluating VLMs' zero-shot performance on downstream applications without access to the downstream task dataset. Specifically, we introduce a new task LOVM: **L**anguage-**O**nly  **V**ision  **M**odel Selection , where methods are expected to perform both model selection and performance prediction based solely on a text description of the desired downstream application. We then introduced an extensive LOVM benchmark consisting of ground-truth evaluations of 35 pre-trained VLMs and 23 datasets, where methods are expected to rank the pre-trained VLMs and predict their zero-shot performance.",https://neurips.cc//virtual/2023/poster/73618,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_68c33c4e,
 author = {Zohar, Orr and Huang, Shih-Cheng and Wang, Kuan-Chieh and Yeung, Serena},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {33120--33132},
 publisher = {Curran Associates, Inc.},
 title = {LOVM: Language-Only Vision Model Selection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/68c33c4e6fc97f7b31c964dc83303a28-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",zoharLOVMLanguageonlyVision2023a
"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models","Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.",https://neurips.cc//virtual/2023/poster/73506,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_117c5c86,
 author = {Zhang, Wenxuan and Aljunied, Mahani and Gao, Chang and Chia, Yew Ken and Bing, Lidong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {5484--5505},
 publisher = {Curran Associates, Inc.},
 title = {M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/117c5c8622b0d539f74f6d1fb082a2e9-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",zhangM3ExamMultilingualMultimodal2023a
Mathematical Capabilities of ChatGPT,"We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., mathlib, the Lean Mathematical Library), current datasets of natural-language mathematics used to benchmark language models either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets test on 1636 human expert evaluations whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulating use cases that arise in the daily professional activities of mathematicians. We benchmark the models on a range of fine-grained performance metrics. For advanced mathematics, this is the most detailed evaluation effort to date. We find that ChatGPT and GPT-4 can be used most successfully as mathematical assistants for querying facts, acting as mathematical search engines and knowledge base interfaces. GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty. Contrary to many positive reports in the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student. Hence, if you aim to use ChatGPT to pass a graduate-level math exam, you would be better off copying from your average peer!",https://neurips.cc//virtual/2023/poster/73421,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_58168e8a,
 author = {Frieder, Simon and Pinchetti, Luca and Chevalier, Chevalier and Griffiths, Ryan-Rhys and Salvatori, Tommaso and Lukasiewicz, Thomas and Petersen, Philipp and Berner, Julius},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {27699--27744},
 publisher = {Curran Associates, Inc.},
 title = {Mathematical Capabilities of ChatGPT},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/58168e8a92994655d6da3939e7cc0918-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",friederMathematicalCapabilitiesChatGPT2023a
"PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance","Although large language models (LLMs) have shown great performance in natural language processing (NLP) in the financial domain, there are no publicly available financially tailored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 128K data samples to support the fine-tuning, and an evaluation benchmark with 8 tasks and 15 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including six financial NLP tasks and two financial prediction tasks. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced to facilitate future research in financial AI.",https://neurips.cc//virtual/2023/poster/73431,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_6a386d70,
 author = {Xie, Qianqian and Han, Weiguang and Zhang, Xiao and Lai, Yanzhao and Peng, Min and Lopez-Lira, Alejandro and Huang, Jimin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {33469--33484},
 publisher = {Curran Associates, Inc.},
 title = {PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6a386d703b50f1cf1f61ab02a15967bb-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",xiePIXIUComprehensiveBenchmark2023
PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,"Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks–where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge.  There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities–including plan generation–LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",https://neurips.cc//virtual/2023/poster/73553,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_7a92bcde,
 author = {Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {38975--38987},
 publisher = {Curran Associates, Inc.},
 title = {PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/7a92bcdede88c7afd108072faf5485c8-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",valmeekamPlanBenchExtensibleBenchmark2023
RaLEs: a Benchmark for Radiology Language Evaluations,"The radiology report is the main form of communication between radiologists and other clinicians. Prior work in natural language processing in radiology reports has shown the value of developing methods tailored for individual tasks such as identifying reports with critical results or disease detection. Meanwhile, English and biomedical natural language understanding benchmarks such as the General Language Understanding and Evaluation as well as Biomedical Language Understanding and Reasoning Benchmark have motivated the development of models that can be easily adapted to address many tasks in those domains. Here, we characterize the radiology report as a distinct domain and introduce RaLEs, the Radiology Language Evaluations, as a benchmark for natural language understanding and generation in radiology. RaLEs is comprised of seven natural language understanding and generation evaluations including the extraction of anatomical and disease entities and their relations, procedure selection, and report summarization. We characterize the performance of models designed for the general, biomedical, clinical and radiology domains across these tasks. We find that advances in the general and biomedical domains do not necessarily translate to radiology, and that improved models from the general domain can perform comparably to smaller clinical-specific models. The limited performance of existing pre-trained models on RaLEs highlights the opportunity to improve domain-specific self-supervised models for natural language processing in radiology. We propose RaLEs as a benchmark to promote and track the development of such domain-specific radiology language models.",https://neurips.cc//virtual/2023/poster/73601,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_eb5683d0,
 author = {Zambrano Chaves, Juanma and Bhaskhar, Nandita and Attias, Maayane and Delbrouck, Jean-Benoit and Rubin, Daniel and Loening, Andreas and Langlotz, Curtis and Chaudhari, Akshay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {74429--74454},
 publisher = {Curran Associates, Inc.},
 title = {RaLEs: a Benchmark for Radiology Language Evaluations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/eb5683d06bdef51ed4dff644908eef4b-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",zambranochavesRaLEsBenchmarkRadiology2023a
RealTime QA: What's the Answer Right Now?,"We introduce RealTime QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). RealTime QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open-domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that RealTime QA will spur progress in instantaneous applications of question answering and beyond.",https://neurips.cc//virtual/2023/poster/73639,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_9941624e,
 author = {Kasai, Jungo and Sakaguchi, Keisuke and takahashi, yoichi and Le Bras, Ronan and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {49025--49043},
 publisher = {Curran Associates, Inc.},
 title = {RealTime QA: What\textquotesingle s the Answer Right Now?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9941624ef7f867a502732b5154d30cb7-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",kasaiRealTimeQAWhats2023
"Revisiting Out-of-distribution Robustness in NLP: Benchmarks, Analysis, and LLMs Evaluations","This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduceBOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pretrained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learningmechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at https://github.com/lifan-yuan/OOD_NLP.",https://neurips.cc//virtual/2023/poster/73407,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_b6b5f50a,
 author = {Yuan, Lifan and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Zou, FangYuan and Cheng, Xingyi and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {58478--58507},
 publisher = {Curran Associates, Inc.},
 title = {Revisiting Out-of-distribution Robustness in NLP: Benchmarks, Analysis, and LLMs Evaluations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/b6b5f50a2001ad1cbccca96e693c4ab4-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",yuanRevisitingOutofdistributionRobustness2023
SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,"In the last year alone, a surge of new benchmarks to measure $\textit{compositional}$ understanding of vision-language models have permeated the machine learning ecosystem.Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors.Surprisingly, we find significant biases in $\textit{all}$ these benchmarks rendering them hackable. This hackability is so dire that blind models with no access to the image outperform state-of-the-art vision-language models.To remedy this rampant vulnerability, we introduce $\textit{SugarCrepe}$, a new benchmark for vision-language compositionality evaluation.We employ large language models, instead of rule-based templates used in previous benchmarks, to generate fluent and sensical hard negatives, and utilize an adversarial refinement mechanism to maximally reduce biases. We re-evaluate state-of-the-art models and recently proposed compositionality inducing strategies, and find that their improvements were hugely overestimated, suggesting that more innovation is needed in this important direction.We release $\textit{SugarCrepe}$ and the code for evaluation at: https://github.com/RAIVNLab/sugar-crepe.",https://neurips.cc//virtual/2023/poster/73628,2023,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2023_63461de0,
 author = {Hsieh, Cheng-Yu and Zhang, Jieyu and Ma, Zixian and Kembhavi, Aniruddha and Krishna, Ranjay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {31096--31116},
 publisher = {Curran Associates, Inc.},
 title = {SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/63461de0b4cb760fc498e85b18a7fe81-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",hsiehSugarCrepeFixingHackable2023
ToolQA: A Dataset for LLM Question Answering with External Tools,"Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available for the broader scientific community on GitHub.",https://neurips.cc//virtual/2023/poster/73466,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_9cb2a749,
 author = {Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {50117--50143},
 publisher = {Curran Associates, Inc.},
 title = {ToolQA: A Dataset for LLM Question Answering with External Tools},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9cb2a7495900f8b602cb10159246a016-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",zhuangToolQADatasetLLM2023
Understanding Social Reasoning in Language Models with Language Models,"As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.",https://neurips.cc//virtual/2023/poster/73680,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_2b9efb08,
 author = {Gandhi, Kanishk and Fraenken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {13518--13529},
 publisher = {Curran Associates, Inc.},
 title = {Understanding Social Reasoning in Language Models with Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/2b9efb085d3829a2aadffab63ba206de-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",gandhiUnderstandingSocialReasoning2023
VisIT-Bench: A Dynamic Benchmark for Evaluating Instruction-Following Vision-and-Language Models,"We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluating instruction-following vision-language models for real-world use. Our starting point is curating 70 ""instruction families"" that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/.",https://neurips.cc//virtual/2023/poster/73556,2023,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2023_5503389d,
 author = {Bitton, Yonatan and Bansal, Hritik and Hessel, Jack and Shao, Rulin and Zhu, Wanrong and Awadalla, Anas and Gardner, Josh and Taori, Rohan and Schmidt, Ludwig},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {26898--26922},
 publisher = {Curran Associates, Inc.},
 title = {VisIT-Bench: A Dynamic Benchmark for Evaluating Instruction-Following Vision-and-Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5503389dbe070cdae9b48086c4996a59-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",bittonVisITbenchDynamicBenchmark2023
VisoGender:  A dataset for benchmarking gender bias in image-text pronoun resolution,"We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related biases within a hegemonic system of binary gender, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene. VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias, where we evaluate the difference between pronoun resolution accuracies for image subjects with gender presentations perceived as masculine versus feminine by human annotators and ii) retrieval bias, where we compare ratios of professionals perceived to have masculine and feminine gender presentations retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they demonstrate bias in resolving binary gender in complex scenes. While the direction and magnitude of gender bias depends on the task and the model being evaluated, captioning models are generally less biased than Vision-Language Encoders.",https://neurips.cc//virtual/2023/poster/73666,2023,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2023_c93f26b1,
 author = {Hall, Siobhan Mackenzie and Gon\c{c}alves Abrantes, Fernanda and Zhu, Hanwen and Sodunke, Grace and Shtedritski, Aleksandar and Kirk, Hannah Rose},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {63687--63723},
 publisher = {Curran Associates, Inc.},
 title = {VisoGender:  A dataset for benchmarking gender bias in image-text pronoun resolution},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/c93f26b1381b17693055a611a513f1e9-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",hallVisoGenderDatasetBenchmarking2023
What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks,"Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4,GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs’ performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.",https://neurips.cc//virtual/2023/poster/73716,2023,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2023_bbb33018,
 author = {Guo, Taicheng and Guo, kehan and Nan, Bozhao and Liang, Zhenwen and Guo, Zhichun and Chawla, Nitesh and Wiest, Olaf and Zhang, Xiangliang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {59662--59688},
 publisher = {Curran Associates, Inc.},
 title = {What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/bbb330189ce02be00cf7346167028ab1-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}",guoWhatCanLarge2023
$\texttt{ConflictBank}$: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLMs,"Large language models (LLMs) have achievedimpressive advancements across numerous disciplines, yet the critical issue of knowledge conflicts, a major source of hallucinations, has rarely been studied. While a few research explored the conflicts between the inherent knowledge of LLMs and the retrieved contextual knowledge, a comprehensive assessment of knowledge conflict in LLMs is still missing. Motivated by this research gap, we firstly propose ConflictBank, the largest benchmark with 7.45M claim-evidence pairs and 553k QA pairs, addressing conflicts from misinformation, temporal discrepancies, and semantic divergences.Using ConflictBank, we conduct the thorough and controlled experiments for a comprehensive understanding of LLM behavior in knowledge conflicts, focusing on three key aspects: (i) conflicts encountered in retrieved knowledge, (ii) conflicts within the models' encoded knowledge, and (iii) the interplay between these conflict forms.Our investigation delves into four model families and twelve LLM instances and provides insights into conflict types, model sizes, and the impact at different stages.We believe that knowledge conflicts represent a critical bottleneck to achieving trustworthy artificial intelligence and hope our work will offer valuable guidance for future model training and development.Resources are available at https://github.com/zhaochen0110/conflictbank.",https://neurips.cc//virtual/2024/poster/97447,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_baf4b960,
 author = {Su, Zhaochen and Zhang, Jun and Qu, Xiaoye and Zhu, Tong and Li, Yanshu and Sun, Jiashuo and Li, Juntao and Zhang, Min and Cheng, Yu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {103242--103268},
 publisher = {Curran Associates, Inc.},
 title = {\textbackslash texttt\lbrace ConflictBank\rbrace : A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/baf4b960d118f838ad0b2c08247a9ebe-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",suTextttConflictBankBenchmarkEvaluating2024
A Careful Examination of Large Language Model Performance on Grade School Arithmetic,"Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning.However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability.To investigate this claim rigorously, we commission Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark,the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more.When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 8%, with several families of models showing evidence of systematic overfitting across almost all model sizes.Further analysis suggests a positive relationship (Spearman's r^2=0.36) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that some models may have partially memorized GSM8k.Nevertheless, many models, especially those on the frontier, show minimal signs of overfitting, and all models broadly demonstrate generalization to novel math problems guaranteed to not be in their training data.",https://neurips.cc//virtual/2024/poster/97687,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_53384f20,
 author = {Zhang, Hugh and Da, Jeff and Lee, Dean and Robinson, Vaughn and Wu, Catherine and Song, Will and Zhao, Tiffany and Raja, Pranav and Zhuang, Charlotte and Slack, Dylan and Lyu, Qin and Hendryx, Sean and Kaplan, Russell and Lunati, Michele and Yue, Summer},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {46819--46836},
 publisher = {Curran Associates, Inc.},
 title = {A Careful Examination of Large Language Model Performance on Grade School Arithmetic},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/53384f2090c6a5cac952c598fd67992f-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",zhangCarefulExaminationLarge2024
AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents,"Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a significant step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.",https://neurips.cc//virtual/2024/poster/97853,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_877b4068,
 author = {Ma, Chang and Zhang, Junlei and Zhu, Zhihao and Yang, Cheng and Yang, Yujiu and Jin, Yaohui and Lan, Zhenzhong and Kong, Lingpeng and He, Junxian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {74325--74362},
 publisher = {Curran Associates, Inc.},
 title = {AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/877b40688e330a0e2a3fc24084208dfa-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",maAgentBoardAnalyticalEvaluation2024
AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries,"Practical semantic parsers are expected to understand user utterances and map them to executable programs, even when these are ambiguous. We introduce a new benchmark, AMBROSIA, which we hope will inform and inspire the development of text-to-SQL parsers capable of recognizing and interpreting ambiguous requests. Our dataset contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries. In each case, the ambiguity persists even when the database context is provided. This is achieved through a novel approach that involves controlled generation of databases from scratch. We benchmark various LLMs on AMBROSIA, revealing that even the most advanced models struggle to identify and interpret ambiguity in questions.",https://neurips.cc//virtual/2024/poster/97762,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_a4c942a8,
 author = {Saparina, Irina and Lapata, Mirella},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {90600--90628},
 publisher = {Curran Associates, Inc.},
 title = {AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a4c942a8405cc910f0a833d28d2573cc-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",saparinaAMBROSIABenchmarkParsing2024a
Are Large Language Models Good Statisticians?,"Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry. Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored. To bridge this gap, we introduce StatQA, a new benchmark designed for statistical analysis tasks. StatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods. We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as GPT-4o achieve a best performance of only 64.83%, indicating significant room for improvement. Notably, while open-source LLMs (e.g. LLaMA-3) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (e.g. GPT-4o). Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors. This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential. Our source code and data are available at https://statqa.github.io/.",https://neurips.cc//virtual/2024/poster/97546,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_72978620,
 author = {Zhu, Yizhang and Du, Shiyin and Li, Boyan and Luo, Yuyu and Tang, Nan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {62697--62731},
 publisher = {Curran Associates, Inc.},
 title = {Are Large Language Models Good Statisticians?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/729786203d330da046dd8091c2d92a66-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",zhuAreLargeLanguage2024
Are We on the Right Way for Evaluating Large Vision-Language Models?,"Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.7% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks near 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.",https://neurips.cc//virtual/2024/poster/94237,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_2f8ee6a3,
 author = {Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and Zhao, Feng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {27056--27087},
 publisher = {Curran Associates, Inc.},
 title = {Are We on the Right Way for Evaluating Large Vision-Language Models?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/2f8ee6a3d766b426d2618e555b5aeb39-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",chenAreWeRight2024
A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models,"Antibodies are crucial proteins produced by the immune system to eliminate harmful foreign substances and have become pivotal therapeutic agents for treating human diseases.To accelerate the discovery of antibody therapeutics, there is growing interest in constructing language models using antibody sequences.However, the applicability of pre-trained language models for antibody discovery has not been thoroughly evaluated due to the scarcity of labeled datasets.To overcome these limitations, we introduce AVIDa-SARS-CoV-2, a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins.AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and Omicron variants.Furthermore, we release VHHCorpus-2M, a pre-training dataset for antibody language models, containing over two million VHH sequences.We report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-specific pre-trained language models.These results confirm that AVIDa-SARS-CoV-2 provides valuable benchmarks for evaluating the representation capabilities of antibody language models for binding prediction, thereby facilitating the development of AI-driven antibody discovery.The datasets are available at https://datasets.cognanous.com.",https://neurips.cc//virtual/2024/poster/97722,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_d2a1e47f,
 author = {Tsuruta, Hirofumi and Yamazaki, Hiroyuki and Maeda, Ryota and Tamura, Ryotaro and Imura, Akihiro},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {116149--116171},
 publisher = {Curran Associates, Inc.},
 title = {A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/d2a1e47f7dc635fac77fbd6e2ec799e4-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",tsurutaSARSCoV2InteractionDataset2024
AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope Prediction,"Epitope identification is vital for antibody design yet challenging due to the inherent variability in antibodies. While many deep learning methods have been developed for general protein binding site prediction tasks, whether they work for epitope prediction remains an understudied research question. The challenge is also heightened by the lack of a consistent evaluation pipeline with sufficient dataset size and epitope diversity. We introduce a filtered antibody-antigen complex structure dataset, AsEP (Antibody-specific Epitope Prediction). AsEP is the largest of its kind and provides clustered epitope groups, allowing the community to develop and test novel epitope prediction methods and evaluate their generalisability. AsEP comes with an easy-to-use interface in Python and pre-built graph representations of each antibody-antigen complex while also supporting customizable embedding methods. Using this new dataset, we benchmark several representative general protein-binding site prediction methods and find that their performances fall short of expectations for epitope prediction. To address this, we propose a novel method, WALLE, which leverages both unstructured modeling from protein language models and structural modeling from graph neural networks. WALLE demonstrate up to 3-10X performance improvement over the baseline methods. Our empirical findings suggest that epitope prediction benefits from combining sequential features provided by language models with geometrical information from graph representations. This provides a guideline for future epitope prediction method design. In addition, we reformulate the task as bipartite link prediction, allowing convenient model performance attribution and interpretability. We open source our data and code at https://github.com/biochunan/AsEP-dataset.",https://neurips.cc//virtual/2024/poster/97702,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_15add673,
 author = {Liu, Chu\textquotesingle nan and Denzler, Lilian and Chen, Yihong and Martin, Andrew and Paige, Brooks},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {11700--11734},
 publisher = {Curran Associates, Inc.},
 title = {AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/15add6732964d5b1f0954058bf3ccc88-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liuAsEPBenchmarkingDeep2024
BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack,"In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.",https://neurips.cc//virtual/2024/poster/97462,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_c0d62e70,
 author = {Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Rodkin, Ivan and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {106519--106554},
 publisher = {Curran Associates, Inc.},
 title = {BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c0d62e70dbc659cc9bd44cbcf1cb652f-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",kuratovBABILongTestingLimits2024
Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs,"Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks.  However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we introduced JailTrickBench to evaluate the impact of various attack settings on LLM performance and provide a baseline for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 354 experiments with about 55,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/JailTrickBench.",https://neurips.cc//virtual/2024/poster/97431,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_38c1dfb4,
 author = {XU, Zhao and LIU, Fan and LIU, Hao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {32219--32250},
 publisher = {Curran Associates, Inc.},
 title = {Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/38c1dfb4f7625907b15e9515365e7803-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",xuBagTricksBenchmarking2024
BEACON: Benchmark for Comprehensive RNA Tasks and Language Models,"RNA plays a pivotal role in translating genetic instructions into functional outcomes, underscoring its importance in biological processes and disease mechanisms. Despite the emergence of numerous deep learning approaches for RNA, particularly universal RNA language models, there remains a significant lack of standardized benchmarks to assess the effectiveness of these methods. In this study, we introduce the first comprehensive RNA benchmark BEACON **BE**nchm**A**rk for  **CO**mprehensive R**N**A Task and Language Models).First, BEACON comprises 13 distinct tasks derived from extensive previous work covering structural analysis, functional studies, and engineering applications, enabling a comprehensive assessment of the performance of methods on various RNA understanding tasks. Second, we examine a range of models, including traditional approaches like CNNs, as well as advanced RNA foundation models based on language models, offering valuable insights into the task-specific performances of these models. Third, we investigate the vital RNA language model components from the tokenizer and positional encoding aspects. Notably, our findings emphasize the superiority of single nucleotide tokenization and the effectiveness of Attention with Linear Biases (ALiBi) over traditional positional encoding methods. Based on these insights, a simple yet strong baseline called BEACON-B is proposed, which can achieve outstanding performance with limited data and computational resources. The datasets and source code of our benchmark are available at https://github.com/terry-r123/RNABenchmark.",https://neurips.cc//virtual/2024/poster/97501,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_a8ea503d,
 author = {Ren, Yuchen and Chen, Zhiyuan and Qiao, Lifeng and Jing, Hongtai and Cai, Yuchen and Xu, Sheng and Ye, Peng and Ma, Xinzhu and Sun, Siqi and Yan, Hongliang and Yuan, Dong and Ouyang, Wanli and Liu, Xihui},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {92891--92921},
 publisher = {Curran Associates, Inc.},
 title = {BEACON: Benchmark for Comprehensive RNA Tasks and Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a8ea503d91320fcfe12cba61c8a6d285-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",renBEACONBenchmarkComprehensive2024
Benchmarking Complex Instruction-Following with Multiple Constraints Composition,"Instruction following is one of the fundamental capabilities of large language models (LLMs). As the ability of LLMs is constantly improving, they have been increasingly applied to deal with complex human instructions in real-world scenarios. Therefore, how to evaluate the ability of complex instruction-following of LLMs has become a critical research problem. Existing benchmarks mainly focus on modeling different types of constraints in human instructions while neglecting the composition of different constraints, which is an indispensable constituent in complex instructions. To this end, we propose ComplexBench, a benchmark for comprehensively evaluating the ability of LLMs to follow complex instructions composed of multiple constraints. We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, and manually collect a high-quality dataset accordingly. To make the evaluation reliable, we augment LLM-based evaluators with rules to effectively verify whether generated texts can satisfy each constraint and composition. Furthermore, we obtain the final evaluation score based on the dependency structure determined by different composition types. ComplexBench identifies significant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition.",https://neurips.cc//virtual/2024/poster/97675,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_f8c24b08,
 author = {Wen, Bosi and Ke, Pei and Gu, Xiaotao and Wu, Lindong and Huang, Hao and Zhou, Jinfeng and Li, Wenchuang and Hu, Binxin and Gao, Wendy and Xu, Jiaxin and Liu, Yiming and Tang, Jie and Wang, Hongning and Huang, Minlie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {137610--137645},
 publisher = {Curran Associates, Inc.},
 title = {Benchmarking Complex Instruction-Following with Multiple Constraints Composition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f8c24b08b96a08ec7a7a975feea7777e-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wenBenchmarkingComplexInstructionfollowing2024
Benchmarking LLMs via Uncertainty Quantification,"The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs. Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench.",https://neurips.cc//virtual/2024/poster/97746,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_1bdcb065,
 author = {Ye, Fanghua and Yang, Mingming and Pang, Jianhui and Wang, Longyue and Wong, Derek F. and Yilmaz, Emine and Shi, Shuming and Tu, Zhaopeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {15356--15385},
 publisher = {Curran Associates, Inc.},
 title = {Benchmarking LLMs via Uncertainty Quantification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1bdcb065d40203a00bd39831153338bb-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",yeBenchmarkingLlmsUncertainty2024
Beyond Aesthetics: Cultural Competence in Text-to-Image Models,"Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of *cultural competence*. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to evaluate cultural competence of T2I models. CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of high-quality prompts that enable the evaluation of cultural awareness, and 2) CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to evaluate cultural diversity. We also introduce cultural diversity as a novel T2I evaluation component, leveraging quality-weighted Vendi score. Our evaluations reveal significant gaps in the cultural awareness of existing models across countries and provide valuable insights into the cultural diversity of T2I outputs for underspecified prompts. Our methodology is extendable to other cultural regions and concepts and can facilitate the development of T2I models that better cater to the global population.",https://neurips.cc//virtual/2024/poster/97855,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_18c669b8,
 author = {Kannen, Nithish and Ahmad, Arif and Andreetto, Marco and Prabhakaran, Vinodkumar and Prabhu, Utsav and Dieng, Adji Bousso and Bhattacharyya, Pushpak and Dave, Shachi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {13716--13747},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Aesthetics: Cultural Competence in Text-to-Image Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/18c669b80d1a8f589713b768bc8fe9a4-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",kannenAestheticsCulturalCompetence2024
Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models,"We introduce a dynamic benchmarking system for conversational agents that evaluates their performance through a single, simulated, and lengthy user$\leftrightarrow$agent interaction. The interaction is a conversation between the user and agent, where multiple tasks are introduced and then undertaken concurrently. We context switch regularly to interleave the tasks, which constructs a realistic testing scenario in which we assess the Long-Term Memory, Continual Learning, and Information Integration capabilities of the agents. Results from both proprietary and open-source Large-Language Models show that LLMs in general perform well on single-task interactions, but they struggle on the same tasks when they are interleaved. Notably, short-context LLMs supplemented with an LTM system perform as well as or better than those with larger contexts. Our benchmark suggests that there are other challenges for LLMs responding to more natural interactions that contemporary benchmarks have heretofore not been able to capture.",https://neurips.cc//virtual/2024/poster/97463,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_4aedf0cb,
 author = {Castillo-Bolado, David and Davidson, Joseph and Gray, Finlay and Rosa, Marek},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {42528--42565},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4aedf0cba303537fcb6cf948bb41b2df-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",castillo-boladoPromptsDynamicConversational2024
BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity,"As part of an ongoing worldwide effort to comprehend and monitor insect biodiversity, this paper presents the BIOSCAN-5M Insect dataset to the machine learning community and establish several benchmark tasks. BIOSCAN-5M is a comprehensive dataset containing multi-modal information for over 5 million insect specimens, and it significantly expands existing image-based biological datasets by including taxonomic labels, raw nucleotide barcode sequences, assigned barcode index numbers, geographical, and size information. We propose three benchmark experiments to demonstrate the impact of the multi-modal data types on the classification and clustering accuracy. First, we pretrain a masked language model on the DNA barcode sequences of the BIOSCAN-5M dataset, and demonstrate the impact of using this large reference library on species- and genus-level classification performance. Second, we propose a zero-shot transfer learning task applied to images and DNA barcodes to cluster feature embeddings obtained from self-supervised learning, to investigate whether meaningful clusters can be derived from these representation embeddings. Third, we benchmark multi-modality by performing contrastive learning on DNA barcodes, image data, and taxonomic information. This yields a general shared embedding space enabling taxonomic classification using multiple types of information and modalities. The code repository of the BIOSCAN-5M Insect dataset is available at https://github.com/bioscan-ml/BIOSCAN-5M.",https://neurips.cc//virtual/2024/poster/97824,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_3fdbb472,
 author = {Gharaee, Zahra and Lowe, Scott C. and Gong, ZeMing and Arias, Pablo Millan and Pellegrino, Nicholas and Wang, Austin T. and Haurum, Joakim Bruslund and Zarubiieva, Iuliia and Kari, Lila and Steinke, Dirk and Taylor, Graham W. and Fieguth, Paul and Chang, Angel X.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {36285--36313},
 publisher = {Curran Associates, Inc.},
 title = {BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/3fdbb472813041c9ecef04c20c2b1e5a-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",gharaeeBIOSCAN5MMultimodalDataset2024
BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages,"Large language models (LLMs) often lack culture-specific everyday knowledge, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs' cultural sensitivities are usually limited to a single language or online sources like Wikipedia, which may not reflect the daily habits, customs, and lifestyles of different regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play or the sports they practice in school is not always explicitly written online. To address this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate LLMs' everyday knowledge across diverse cultures and languages. The benchmark comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese. We evaluate LLMs in two formats: short-answer questions, and multiple-choice questions. We show that LLMs perform better in cultures that are more present online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format.Furthermore, we find that LLMs perform better in their local languages for mid-to-high-resource languages. Interestingly, for languages deemed to be low-resource, LLMs provide better answers in English. We make our dataset publicly available at: https://github.com/nlee0212/BLEnD.",https://neurips.cc//virtual/2024/poster/97510,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_8eb88844,
 author = {Myung, Junho and Lee, Nayeon and Zhou, Yi and Jin, Jiho and Putri, Rifki Afina and Antypas, Dimosthenis and Borkakoty, Hsuvas and Kim, Eunsu and Perez-Almendros, Carla and Ayele, Abinew Ali and Guti\'{e}rrez-Basulto, V\'{\i}ctor and Ib\'{a}\~{n}ez-Garc\'{\i}a, Yazm\'{\i}n and Lee, Hwaran and Muhammad, Shamsuddeen Hassan and Park, Kiwoong and Rzayev, Anar Sabuhi and White, Nina and Yimam, Seid Muhie and Pilehvar, Mohammad Taher and Ousidhoum, Nedjma and Camacho-Collados, Jose and Oh, Alice},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {78104--78146},
 publisher = {Curran Associates, Inc.},
 title = {BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/8eb88844dafefa92a26aaec9f3acad93-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",myungBLEnDBenchmarkLlms2024a
"Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models","The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graphtopology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs’ proficiency of graph analysis. The benchmark, datasets and enhanced open-sourcemodels are available at https://github.com/BUPT-GAMMA/ProGraph.",https://neurips.cc//virtual/2024/poster/97519,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_ff417c39,
 author = {Li, Xin and Chen, Weize and Chu, Qizhi and Li, Haopeng and Sun, Zhaojun and Li, Ran and Qian, Chen and Wei, Yiwei and Liu, Zhiyuan and Shi, Chuan and Sun, Maosong and Yang, Cheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {141045--141070},
 publisher = {Curran Associates, Inc.},
 title = {Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/ff417c3993894694e88ffc4d3f53d28b-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liCanLargeLanguage2024
Can LLMs Solve Molecule Puzzles? A Multimodal Benchmark for Molecular Structure Elucidation,"Large Language Models (LLMs)  have shown significant problem-solving capabilities across predictive and generative tasks in chemistry. However, their proficiency in multi-step chemical reasoning remains underexplored. We introduce a new challenge: molecular structure elucidation, which involves deducing a molecule’s structure from various types of spectral data. Solving such a molecular puzzle, akin to solving crossword puzzles, poses reasoning challenges that require integrating clues from diverse sources and engaging in iterative hypothesis testing. To address this challenging problem with LLMs, we present \textbf{MolPuzzle}, a benchmark comprising 217 instances of structure elucidation, which feature over 23,000 QA samples presented in a sequential puzzle-solving process, involving three interlinked sub-tasks: molecule understanding, spectrum interpretation, and molecule construction. Our evaluation of 12 LLMs reveals that the best-performing LLM, GPT-4o, performs significantly worse than humans, with only a small portion (1.4\%) of its answers exactly matching the ground truth. However, it performs nearly perfectly in the first subtask of molecule understanding, achieving accuracy close to 100\%. This discrepancy highlights the potential of developing advanced LLMs with improved chemical reasoning capabilities in the other two sub-tasks. Our MolPuzzle dataset and evaluation code are available at this  \href{https://github.com/KehanGuo2/MolPuzzle}{link}.",https://neurips.cc//virtual/2024/poster/97472,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_f2b9e8e7,
 author = {Guo, Kehan and Nan, Bozhao and Zhou, Yujun and Guo, Taicheng and Guo, Zhichun and Surve, Mihir and Liang, Zhenwen and Chawla, Nitesh V. and Wiest, Olaf and Zhang, Xiangliang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {134721--134746},
 publisher = {Curran Associates, Inc.},
 title = {Can LLMs Solve Molecule Puzzles? A Multimodal Benchmark for Molecular Structure Elucidation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f2b9e8e7a36d43ddfd3d55113d56b1e0-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",guoCanLlmsSolve2024
CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models,"Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://github.com/richard-peng-xia/CARES.",https://neurips.cc//virtual/2024/poster/97614,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_fde7f40f,
 author = {Xia, Peng and Chen, Ze and Tian, Juanxi and Gong, Yangrui and Hou, Ruibo and Xu, Yue and Wu, Zhenbang and Fan, Zhiyuan and Zhou, Yiyang and Zhu, Kangyu and Zheng, Wenhao and Wang, Zhaoyang and Wang, Xiao and Zhang, Xuchao and Bansal, Chetan and Niethammer, Marc and Huang, Junzhou and Zhu, Hongtu and Li, Yun and Sun, Jimeng and Ge, Zongyuan and Li, Gang and Zou, James and Yao, Huaxiu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {140334--140365},
 publisher = {Curran Associates, Inc.},
 title = {CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/fde7f40f8ced5735006810534dc66b33-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",xiaCARESComprehensiveBenchmark2024a
CiteME: Can Language Models Accurately Cite Scientific Claims?,"Thousands of new scientific papers are published each month. Such  information overload complicates researcher efforts to stay current with the state-of-the-art as well as to verify and correctly attribute claims. We pose the following research question: Given a text excerpt referencing a paper, could an LM act as a research assistant to correctly identify the referenced paper? We advance efforts to answer this question by building a benchmark that evaluates the abilities of LMs in citation attribution. Our benchmark, CiteME, consists  of text excerpts from recent machine learning papers, each referencing a single other paper. CiteME use reveals a large gap between frontier LMs and human performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%. We close this gap by introducing CiteAgent, an autonomous system built on the GPT-4o LM that can also search and read papers, which achieves an accuracy of 35.3% on CiteME. Overall, CiteME serves as a challenging testbed for open-ended claim attribution, driving the research community towards a future where any claim made by an LM can be  automatically verified and discarded if found to be incorrect.",https://neurips.cc//virtual/2024/poster/97683,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_0ef47f7b,
 author = {Press, Ori and Hochlehnert, Andreas and Prabhu, Ameya and Udandarao, Vishaal and Press, Ofir and Bethge, Matthias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {7847--7877},
 publisher = {Curran Associates, Inc.},
 title = {CiteME: Can Language Models Accurately Cite Scientific Claims?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0ef47f7b768e1a012e3d995ac8d8fac7-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",pressCiteMECanLanguage2024a
ClashEval: Quantifying the tug-of-war between an LLM’s internal prior and external evidence,"Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors.We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60\% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs -- namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect. Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.",https://neurips.cc//virtual/2024/poster/97658,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_3aa291ab,
 author = {Wu, Kevin and Wu, Eric and Zou, James},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {33402--33422},
 publisher = {Curran Associates, Inc.},
 title = {ClashEval: Quantifying the tug-of-war between an LLMâ€™s internal prior and external evidence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/3aa291abc426d7a29fb08418c1244177-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wuClashEvalQuantifyingTugofwar2024a
ClevrSkills: Compositional Language And Visual Reasoning in Robotics,"Robotics tasks are highly compositional by nature. For example, to perform a high-level task like cleaning the table a robot must employ low-level capabilities of moving the effectors to the objects on the table, pick them up and then move them off the table one-by-one, while re-evaluating the consequently dynamic scenario in the process. Given that large vision language models (VLMs) have shown progress on many tasks that require high level, human-like reasoning, we ask the question: if the models are taught the requisite low-level capabilities, can they compose them in novel ways to achieve interesting high-level tasks like cleaning the table without having to be explicitly taught so? To this end, we present ClevrSkills - a benchmark suite for compositional reasoning in robotics. ClevrSkills is an environment suite developed on top of the ManiSkill2 simulator and an accompanying dataset. The dataset contains trajectories generated on a range of robotics tasks with language and visual annotations as well as multi-modal prompts as task specification. The suite includes a curriculum of tasks with three levels of compositional understanding, starting with simple tasks requiring basic motor skills. We benchmark multiple different VLM baselines on ClevrSkills and show that even after being pre-trained on large numbers of tasks, these models fail on compositional reasoning in robotics tasks.",https://neurips.cc//virtual/2024/poster/97843,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_43953955,
 author = {Haresh, Sanjay and Dijkman, Daniel and Bhattacharyya, Apratim and Memisevic, Roland},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {38235--38266},
 publisher = {Curran Associates, Inc.},
 title = {ClevrSkills: Compositional Language And Visual Reasoning in Robotics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/439539557e9ba0d04055773ff1f3241c-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",hareshClevrSkillsCompositionalLanguage2024
CoIN: A Benchmark of Continual Instruction Tuning for Multimodel Large Language Models,"Instruction tuning demonstrates impressive performance in adapting Multimodal Large Language Models (MLLMs) to follow task instructions and improve generalization ability.  By extending tuning across diverse tasks, MLLMs can further enhance their understanding of world knowledge and instruction intent. However, continual instruction tuning has been largely overlooked and there are no public benchmarks available. In this paper, we present CoIN, a comprehensive benchmark tailored for assessing the behavior of existing MLLMs under continual instruction tuning. CoIN comprises 10 meticulously crafted datasets spanning 8 tasks, ensuring diversity and serving as a robust evaluation framework to assess crucial aspects of continual instruction tuning, such as task order, instruction diversity and volume. Additionally, apart from traditional evaluation, we design another LLM-based metric to assess the knowledge preserved within MLLMs for reasoning. Following an in-depth evaluation of several MLLMs, we demonstrate that they still suffer catastrophic forgetting, and the failure in instruction alignment assumes the main responsibility, instead of reasoning knowledge forgetting.  To this end, we introduce MoELoRA which is effective in retaining the previous instruction alignment.",https://neurips.cc//virtual/2024/poster/97786,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_6a45500d,
 author = {Chen, Cheng and Zhu, Junchen and Luo, Xu and Shen, Heng Tao and Song, Jingkuan and Gao, Lianli},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {57817--57840},
 publisher = {Curran Associates, Inc.},
 title = {CoIN: A Benchmark of Continual Instruction Tuning for Multimodel Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/6a45500d9eda640deed90d8a62742be5-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",chenCoINBenchmarkContinual2024
ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs,"Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe\footnote{ConMe is an abbreviation for Confuse Me.} -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.",https://neurips.cc//virtual/2024/poster/97716,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_28aad3b3,
 author = {Huang, Irene and Lin, Wei and Mirza, M. Jehanzeb and Hansen, Jacob A. and Doveh, Sivan and Butoi, Victor Ion and Herzig, Roei and Arbelle, Assaf and Kuehne, Hilde and Darrell, Trevor and Gan, Chuang and Oliva, Aude and Feris, Rogerio and Karlinsky, Leonid},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {22927--22946},
 publisher = {Curran Associates, Inc.},
 title = {ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/28aad3b3b315d86910d7f4ee2867dfa4-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",huangConMeRethinkingEvaluation2024
ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models,"Multi-turn visual conversation is an important ability of real-world AI assistants. However,  the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities ablation evaluation for Large Vision-Language Models (LVLMs). ConvBench comprises 577 curated multi-turn conversations, encompassing 215 tasks. These tasks are broad and open-ended, which resemble real-world user behaviors. ConvBench progressively examines the LVLMs' perception, reasoning, and creativity capabilities in each conversation and can decouple these capabilities in evaluations and thus perform reliable error attribution. Besides, considering the diversity of open-ended questions, we introduce an efficient and reliable automatic evaluation framework. Experimental results reveal that ConvBench is a significant challenge for current LVLMs, even for GPT4V, which achieves only a 39.51% score. Besides, we have some insightful findings, such as the weak perception of LVLMs inhibits authentic strengths in reasoning and creation. We believe our design of hierarchical capabilities, decoupling capabilities evaluation, and multi-turn conversation can blaze a new trail in LVLMs evaluation. Code and benchmark are released at https://github.com/shirlyliu64/ConvBench.",https://neurips.cc//virtual/2024/poster/97705,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_b69396af,
 author = {Liu, Shuo and Ying, Kaining and Zhang, Hao and Yang, Yue and Lin, Yuqi and Zhang, Tianle and Li, Chuanhao and Qiao, Yu and Luo, Ping and Shao, Wenqi and Zhang, Kaipeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {100734--100782},
 publisher = {Curran Associates, Inc.},
 title = {ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b69396afc07a9ca3428d194f4db84c02-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liuConvBenchMultiturnConversation2024
"Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation","There is a growing interest in using Large Language Models (LLMs) in multi-agent systems to tackle interactive real-world tasks that require effective collaboration and assessing complex situations. Yet, we have a limited understanding of LLMs' communication and decision-making abilities in multi-agent setups. The fundamental task of negotiation spans many key features of communication, such as cooperation, competition, and manipulation potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We create a testbed of complex multi-agent, multi-issue, and semantically rich negotiation games. To reach an agreement, agents must have strong arithmetic, inference, exploration, and planning capabilities while integrating them in a dynamic and multi-turn setup. We propose metrics to rigorously quantify agents' performance and alignment with the assigned role. We provide procedures to create new games and increase games' difficulty to have an evolving benchmark. Importantly, we evaluate critical safety aspects such as the interaction dynamics between agents influenced by greedy and adversarial players. Our benchmark is highly challenging; GPT-3.5 and small models mostly fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform in reaching agreement in non-cooperative and more difficult games.",https://neurips.cc//virtual/2024/poster/97850,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_984dd3db,
 author = {Abdelnabi, Sahar and Gomaa, Amr and Sivaprasad, Sarath and Sch\""{o}nherr, Lea and Fritz, Mario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {83548--83599},
 publisher = {Curran Associates, Inc.},
 title = {Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/984dd3db213db2d1454a163b65b84d08-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",abdelnabiCooperationCompetitionMaliciousness2024
CRAG - Comprehensive RAG Benchmark,"Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)’s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve $\le 34\%$ accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracted thousands of participants and submissions. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions. CRAG is available at https://github.com/facebookresearch/CRAG/.",https://neurips.cc//virtual/2024/poster/97703,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_1435d2d0,
 author = {Yang, Xiao and Sun, Kai and Xin, Hao and Sun, Yushi and Bhalla, Nikita and Chen, Xiangsen and Choudhary, Sajal and Gui, Rongze Daniel and Jiang, Ziran Will and Jiang, Ziyu and Kong, Lingkun and Moran, Brian and Wang, Jiaqi and Xu, Yifan Ethan and Yan, An and Yang, Chenyu and Yuan, Eting and Zha, Hanwen and Tang, Nan and Chen, Lei and Scheffer, Nicolas and Liu, Yue and Shah, Nirav and Wanga, Rakesh and Kumar, Anuj and Yih, Wen-tau and Dong, Xin Luna},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {10470--10490},
 publisher = {Curran Associates, Inc.},
 title = {CRAG - Comprehensive RAG Benchmark},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1435d2d0fca85a84d83ddcb754f58c29-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",yangCRAGComprehensiveRAG2024a
CriticEval: Evaluating Large-scale Language Model as Critic,"Critique ability, i.e., the capability of Large Language Models (LLMs) to identify and rectify flaws in responses, is crucial for their applications in self-improvement and scalable oversight. While numerous studies have been proposed to evaluate critique ability of LLMs, their comprehensiveness and reliability are still limited. To overcome this problem, we introduce CriticEval, a novel benchmark designed to comprehensively and reliably evaluate critique ability of LLMs. Specifically, to ensure the comprehensiveness, CriticEval evaluates critique ability from four dimensions across nine diverse task scenarios. It evaluates both scalar-valued and textual critiques, targeting responses of varying quality. To ensure the reliability, a large number of critiques are annotated to serve as references, enabling GPT-4 to evaluate textual critiques reliably. Extensive evaluations of open-source and closed-source LLMs first validate the reliability of evaluation in CriticEval. Then, experimental results demonstrate the promising potential of open-source LLMs, the effectiveness of critique datasets and several intriguing relationships between the critique ability and some critical factors, including task types, response qualities and critique dimensions.",https://neurips.cc//virtual/2024/poster/94609,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_7b7d7985,
 author = {Lan, Tian and Zhang, Wenwei and Xu, Chen and Huang, Heyan and Lin, Dahua and Chen, Kai and Mao, Xian-Ling},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {66907--66960},
 publisher = {Curran Associates, Inc.},
 title = {CriticEval: Evaluating Large-scale Language Model as Critic},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/7b7d7985f62284060d65f532ed2ea5fa-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",lanCriticEvalEvaluatingLargescale2024a
Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias,"Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data.In this study, we introduce \textbf{Cross-Care}, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups.We systematically evaluate how demographic biases embedded in pre-training corpora like $ThePile$ influence the outputs of LLMs.We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups.Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs.Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models' representation of disease prevalence across different languages.For further exploration and analysis, we make all data and a data visualization tool available at: \url{www.crosscare.net}.",https://neurips.cc//virtual/2024/poster/97819,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_2a617efe,
 author = {Chen, Shan and Gallifant, Jack and Gao, Mingye and Moreira, Pedro and Munch, Nikolaj and Muthukkumar, Ajay and Rajan, Arvind and Kolluri, Jaya and Fiske, Amelia and Hastings, Janna and Aerts, Hugo and Anthony, Brian and Celi, Leo Anthony and La Cava, William G. and Bitterman, Danielle S.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {23756--23795},
 publisher = {Curran Associates, Inc.},
 title = {Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/2a617efee5815f12b405d40569dea0a5-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",chenCrosscareAssessingHealthcare2024
CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence,"Cyber threat intelligence (CTI) is crucial in today's cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs' performance in CTI applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI.",https://neurips.cc//virtual/2024/poster/97556,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_5acd3c62,
 author = {Alam, Md Tanvirul and Bhusal, Dipkamal and Nguyen, Le and Rastogi, Nidhi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {50805--50825},
 publisher = {Curran Associates, Inc.},
 title = {CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5acd3c628aa1819fbf07c39ef73e7285-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",alamCTIBenchBenchmarkEvaluating2024
CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark,"Visual Question Answering~(VQA) is an important task in multimodal AI, which requires models to understand and reason on knowledge present in visual and textual data. However, most of the current VQA datasets and models are primarily focused on English and a few major world languages, with images that are Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, some datasets extend the text to other languages, either via translation or some other approaches, but usually keep the same images, resulting in narrow cultural representation. To address these limitations, we create CVQA, a new Culturally-diverse Multilingual Visual Question Answering benchmark dataset, designed to cover a rich set of languages and regions, where we engage native speakers and cultural experts in the data collection process. CVQA includes culturally-driven images and questions from across 28 countries in four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and we show that the dataset is challenging for the current state-of-the-art models. This benchmark will serve as a probing evaluation suite for assessing the cultural bias of multimodal models and hopefully encourage more research efforts towards increasing cultural awareness and linguistic diversity in this field.",https://neurips.cc//virtual/2024/poster/97798,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_1568882b,
 author = {Romero, David and Lyu, Chenyang and Wibowo, Haryo Akbarianto and Lynn, Teresa and Hamed, Injy and Kishore, Aditya Nanda and Mandal, Aishik and Dragonetti, Alina and Abzaliev, Artem and Tonja, Atnafu Lambebo and Balcha, Bontu Fufa and Whitehouse, Chenxi and Salamea, Christian and Velasco, Dan John and Adelani, David Ifeoluwa and Le Meur, David and Villa-Cueva, Emilio and Koto, Fajri and Farooqui, Fauzan and Belcavello, Frederico and Batnasan, Ganzorig and Vallejo, Gisela and Caulfield, Grainne and Ivetta, Guido and Song, Haiyue and Ademtew, Henok Biadglign and Maina, Hern\'{a}n and Lovenia, Holy and Azime, Israel Abebe and Cruz, Jan Christian Blaise and Gala, Jay and Geng, Jiahui and Ortiz-Barajas, Jesus-German and Baek, Jinheon and Dunstan, Jocelyn and Alemany, Laura Alonso and Nagasinghe, Kumaranage Ravindu Yasas and Benotti, Luciana and D\textquotesingle Haro, Luis Fernando and Viridiano, Marcelo and Estecha-Garitagoitia, Marcos and Cabrera, Maria Camila Buitrago and Rodr\'{\i}guez-Cantelar, Mario and Jouitteau, M\'{e}lanie and Mihaylov, Mihail and Etori, Naome and Imam, Mohamed Fazli Mohamed and Adilazuarda, Muhammad Farid and Gochoo, Munkhjargal and Otgonbold, Munkh-Erdene and Niyomugisha, Olivier and Silva, Paula M\'{o}nica and Chitale, Pranjal and Dabre, Raj and Chevi, Rendi and Zhang, Ruochen and Diandaru, Ryandito and Cahyawijaya, Samuel and G\'{o}ngora, Santiago and Jeong, Soyeong and Purkayastha, Sukannya and Kuribayashi, Tatsuki and Clifford, Teresa and Jayakumar, Thanmay and Torrent, Tiago Timponi and Ehsan, Toqeer and Araujo, Vladimir and Kementchedjhieva, Yova and Burzo, Zara and Lim, Zheng Wei and Yong, Zheng Xin and Ignat, Oana and Nwatu, Joan and Mihalcea, Rada and Solorio, Thamar and Aji, Alham Fikri},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {11479--11505},
 publisher = {Curran Associates, Inc.},
 title = {CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1568882ba1a50316e87852542523739c-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",romeroCVQACulturallydiverseMultilingual2024
DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA,"Recent advances in self-supervised models for natural language, vision, and protein sequences have inspired the development of large genomic DNA language models (DNALMs). These models aim to learn generalizable representations of diverse DNA elements, potentially enabling various genomic prediction, interpretation and design tasks. Despite their potential, existing benchmarks do not adequately assess the capabilities of DNALMs on key downstream applications involving an important class of non-coding DNA elements critical for regulating gene activity. In this study, we introduce DART-Eval, a suite of representative benchmarks specifically focused on regulatory DNA to evaluate model performance across zero-shot, probed, and fine-tuned scenarios against contemporary ab initio models as baselines. Our benchmarks target biologically meaningful downstream tasks such as functional sequence feature discovery, predicting cell-type specific regulatory activity, and counterfactual prediction of the impacts of genetic variants. We find that current DNALMs exhibit inconsistent performance and do not offer compelling gains over alternative baseline models for most tasks, while requiring significantly more computational resources. We discuss potentially promising modeling, data curation, and evaluation strategies for the next generation of DNALMs. Our  code is available at https://github.com/kundajelab/DART-Eval",https://neurips.cc//virtual/2024/poster/97497,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_71998bfc,
 author = {Patel, Aman and Singhal, Arpita and Wang, Austin and Pampari, Anusri and Kasowski, Maya and Kundaje, Anshul},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {62024--62061},
 publisher = {Curran Associates, Inc.},
 title = {DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/71998bfc3217ffe1cca1ee084dfadadd-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",patelDARTevalComprehensiveDNA2024
DataComp-LM: In search of the next generation of training sets for language models,"We introduce DataComp for Language Models, a testbed for controlled dataset experiments with the goal of improving language models.As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations.Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing atmodel scales ranging from 412M to 7B parameters.As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set.The resulting dataset, DCLM-Baseline, enables training a 7B parameter language model from scratch to 63% 5-shot accuracy on MMLU with 2T training tokens.Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6 percentage point improvement on MMLU while being trained with half the compute.Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation. We release the \dclm benchmark, framework, models, and datasets at https://www.datacomp.ai/dclm/",https://neurips.cc//virtual/2024/poster/97814,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_19e4ea30,
 author = {Li, Jeffrey and Fang, Alex and Smyrnis, Georgios and Ivgi, Maor and Jordan, Matt and Gadre, Samir and Bansal, Hritik and Guha, Etash and Keh, Sedrick and Arora, Kushal and Garg, Saurabh and Xin, Rui and Muennighoff, Niklas and Heckel, Reinhard and Mercat, Jean and Chen, Mayee and Gururangan, Suchin and Wortsman, Mitchell and Albalak, Alon and Bitton, Yonatan and Nezhurina, Marianna and Abbas, Amro and Hsieh, Cheng-Yu and Ghosh, Dhruba and Gardner, Josh and Kilian, Maciej and Zhang, Hanlin and Shao, Rulin and Pratt, Sarah and Sanyal, Sunny and Ilharco, Gabriel and Daras, Giannis and Marathe, Kalyani and Gokaslan, Aaron and Zhang, Jieyu and Chandu, Khyathi and Nguyen, Thao and Vasiljevic, Igor and Kakade, Sham and Song, Shuran and Sanghavi, Sujay and Faghri, Fartash and Oh, Sewoong and Zettlemoyer, Luke and Lo, Kyle and El-Nouby, Alaaeldin and Pouransari, Hadi and Toshev, Alexander and Wang, Stephanie and Groeneveld, Dirk and Soldaini, Luca and Koh, Pang Wei and Jitsev, Jenia and Kollar, Thomas and Dimakis, Alexandros G. and Carmon, Yair and Dave, Achal and Schmidt, Ludwig and Shankar, Vaishaal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {14200--14282},
 publisher = {Curran Associates, Inc.},
 title = {DataComp-LM: In search of the next generation of training sets for language models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/19e4ea30dded58259665db375885e412-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liDataCompLMSearchNext2024
DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios,"Detecting text generated by large language models (LLMs) is of great recent interest. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world applications remains underexplored. In this study, we present a new benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task.     We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating advanced prompt usages, human revisions like word substitutions, and writing errors. Our development of DetectRL reveals the strengths and limitations of current SOTA detectors.     More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe DetectRL could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors\footnote{Data and code are publicly available at: https://github.com/NLP2CT/DetectRL.",https://neurips.cc//virtual/2024/poster/97633,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_b61bdf7e,
 author = {Wu, Junchao and Zhan, Runzhe and Wong, Derek F. and Yang, Shu and Yang, Xinyi and Yuan, Yulin and Chao, Lidia S.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {100369--100401},
 publisher = {Curran Associates, Inc.},
 title = {DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b61bdf7e9f64c04ec75a26e781e2ad51-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wuDetectRLBenchmarkingLLMgenerated2024
DevBench: A multimodal developmental benchmark for language learning,"How (dis)similar are the learning trajectories of vision–language models and children? Recent modeling work has attempted to understand the gap between models’ and humans’ data efficiency by constructing models trained on less data, especially multimodal naturalistic data. However, such models are often evaluated on adult-level benchmarks, with limited breadth in language abilities tested, and without direct comparison to behavioral data. We introduce DevBench, a multimodal benchmark comprising seven language evaluation tasks spanning the domains of lexical, syntactic, and semantic ability, with behavioral data from both children and adults. We evaluate a set of vision–language models on these tasks, comparing models and humans on their response patterns, not their absolute performance. Across tasks, models exhibit variation in their closeness to human response patterns, and models that perform better on a task also more closely resemble human behavioral responses. We also examine the developmental trajectory of OpenCLIP over training, finding that greater training results in closer approximations to adult response patterns. DevBench thus provides a benchmark for comparing models to human language development. These comparisons highlight ways in which model and human language learning processes diverge, providing insight into entry points for improving language models.",https://neurips.cc//virtual/2024/poster/97423,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_8d987b29,
 author = {Tan, Alvin W. M. and Yu, Sunny and Long, Bria and Anya, Wanjing and Murray, Tonya and Silverman, Rebecca D. and Yeatman, Jason D. and Frank, Michael C.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {77445--77467},
 publisher = {Curran Associates, Inc.},
 title = {DevBench: A multimodal developmental benchmark for language learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/8d987b2981388c99c7eab6095d1d29fd-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",tanDevBenchMultimodalDevelopmental2024
DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs,"Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. Despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to DyTAGs, which hinders the potential advancement in many research fields. To address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. To facilitate the use of DTGB, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. These tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with LLM embeddings, along with 6 powerful large language models (LLMs). Our results show the limitations of existing models in handling DyTAGs. Our analysis also demonstrates the utility of DTGB in investigating the incorporation of structural and textual dynamics. The proposed DTGB fosters research on DyTAGs and their broad applications. It offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. The dataset and source code are available at  https://github.com/zjs123/DTGB.",https://neurips.cc//virtual/2024/poster/97873,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_a65d054a,
 author = {Zhang, Jiasheng and Chen, Jialin and Yang, Menglin and Feng, Aosong and Liang, Shuang and Shao, Jie and Ying, Rex},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {91405--91429},
 publisher = {Curran Associates, Inc.},
 title = {DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a65d054a407f94c34ecfb598fb540a0d-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",zhangDTGBComprehensiveBenchmark2024
Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization,"Despite the abundance of datasets available for assessing large language models (LLMs), the scarcity of continuous and reliable difficulty labels for individual data points, in most cases, curtails their capacity to benchmark model generalization performance across different levels of complexity. Addressing this limitation, we present Easy2Hard, an innovative collection of 6 benchmark datasets featuring standardized difficulty labels spanning a wide range of domains, such as mathematics and programming problems, chess puzzles, and reasoning questions, providing a much-needed tool for those in demand of a dataset with varying degrees of difficulty for LLM assessment. We estimate the difficulty of individual problems by leveraging the performance data of many human subjects and LLMs on prominent leaderboards. Harnessing the rich human performance data, we employ widely recognized difficulty ranking systems, including the Item Response Theory (IRT) and Glicko-2 models, to uniformly assign difficulty scores to problems. The Easy2Hard datasets distinguish themselves from previous collections by incorporating a significantly higher proportion of challenging problems, presenting a novel and demanding test for state-of-the-art LLMs. Through extensive experiments conducted with six state-of-the-art LLMs on the Easy2Hard datasets, we offer valuable insights into their performance and generalization capabilities across varying degrees of difficulty, setting the stage for future research in LLM generalization.",https://neurips.cc//virtual/2024/poster/97554,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_4e6f2230,
 author = {Ding, Mucong and Deng, Chenghao and Choo, Jocelyn and Wu, Zichu and Agrawal, Aakriti and Schwarzschild, Avi and Zhou, Tianyi and Goldstein, Tom and Langford, John and Anandkumar, Anima and Huang, Furong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {44323--44365},
 publisher = {Curran Associates, Inc.},
 title = {Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4e6f22305275966513990f53cec908e0-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",dingEasy2HardbenchStandardizedDifficulty2024
EffiBench: Benchmarking the Efficiency of Automatically Generated Code,"Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in greencomputing and sustainability efforts — the efficiency of the generated code — has often been neglected. This paper presents Effibench, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EffiBench, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) to generate efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average \textbf{3.12} times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 code are \textbf{13.89} and \textbf{43.92} times that of the canonical solutions. The source code of EffiBench is released on https://github.com/huangd1999/EffiBench. We also provide the LeaderBoard in https://huggingface.co/spaces/EffiBench/effibench-leaderboard.",https://neurips.cc//virtual/2024/poster/97864,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_15807b6e,
 author = {Huang, Dong and Qing, Yuhao and Shang, Weiyi and Cui, Heming and Zhang, Jie M.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {11506--11544},
 publisher = {Curran Associates, Inc.},
 title = {EffiBench: Benchmarking the Efficiency of Automatically Generated Code},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/15807b6e09d691fe5e96cdecde6d7b80-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",huangEffiBenchBenchmarkingEfficiency2024
EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries,"Discharge summaries in Electronic Health Records (EHRs) are crucial for clinical decision-making, but their length and complexity make information extraction challenging, especially when dealing with accumulated summaries across multiple patient admissions. Large Language Models (LLMs) show promise in addressing this challenge by efficiently analyzing vast and complex data. Existing benchmarks, however, fall short in properly evaluating LLMs' capabilities in this context, as they typically focus on single-note information or limited topics, failing to reflect the real-world inquiries required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. Every QA pair is initially generated using GPT-4 and then manually reviewed and refined by three clinicians to ensure clinical relevance. EHRNoteQA includes questions that require information across multiple discharge summaries and covers eight diverse topics, mirroring the complexity and diversity of real clinical inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice question answering, and propose a reliable evaluation method for each. We evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the model performance (e.g., the length and number of discharge summaries). Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations in clinical practice, we measure the correlation between the LLM performance on EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results show that LLM performance on EHRNoteQA have higher correlation with clinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to other benchmarks, demonstrating its practical relevance in evaluating LLMs in clinical settings. EHRNoteQA will be publicly available to support further research and improve LLM evaluation in clinical practice. EHRNoteQA is publicly available under PhysioNet credential access at https://doi.org/10.13026/acga-ht95, and the code is available at https://github.com/ji-youn-kim/EHRNoteQA.",https://neurips.cc//virtual/2024/poster/97643,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_e15c4aff,
 author = {Kweon, Sunjun and Kim, Jiyoun and Kwak, Heeyoung and Cha, Dongchul and Yoon, Hangyul and Kim, Kwanghyun and Yang, Jeewon and Won, Seunghyun and Choi, Edward},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {124575--124611},
 publisher = {Curran Associates, Inc.},
 title = {EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/e15c4afff22f12c4986c1fcb4e941e03-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",kweonEHRNoteQALLMBenchmark2024a
Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making,"We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics that break down evaluation into error types, such as hallucination errors, affordance errors, and various types of planning errors. Overall, our benchmark offers a comprehensive assessment of LLMs’ performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems and providing insights into the effective and selective use of LLMs in embodied decision making.",https://neurips.cc//virtual/2024/poster/97552,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_b631da75,
 author = {Li, Manling and Zhao, Shiyu and Wang, Qineng and Wang, Kangrui and Zhou, Yu and Srivastava, Sanjana and Gokmen, Cem and Lee, Tony and Li, Li Erran and Zhang, Ruohan and Liu, Weiyu and Liang, Percy and Fei-Fei, Li and Mao, Jiayuan and Wu, Jiajun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {100428--100534},
 publisher = {Curran Associates, Inc.},
 title = {Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b631da756d1573c24c9ba9c702fde5a9-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liEmbodiedAgentInterface2024
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models,"Large language models (LLMs) have achieved unprecedented performances in various applications, yet evaluating them is still challenging. Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity. We contend that utilizing existing relational databases based on the entity-relationship (ER) model is a promising approach for constructing benchmarks as they contain structured knowledge that can be used to question LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational databases have integrity constraints that can be used to better construct complex in-depth questions and verify answers: (1) functional dependencies can be used to pinpoint critical keywords that an LLM must know to properly answer a given question containing certain attribute values; and (2) foreign key constraints can be used to join relations and construct multi-hop questions, which can be arbitrarily long and used to debug intermediate answers. We thus propose ERBench, which uses these integrity constraints to convert any database into an LLM benchmark. ERBench supports continuous evaluation as databases change, multimodal questions, and various prompt engineering techniques. In our experiments, we construct LLM benchmarks using databases of multiple domains and make an extensive comparison of contemporary LLMs. We show how ERBench can properly evaluate any LLM by not only checking for answer correctness, but also effectively verifying the rationales by looking for the right keywords.",https://neurips.cc//virtual/2024/poster/97458,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_5ef9853a,
 author = {Oh, Jio and Kim, Soyeon and Seo, Junseok and Wang, Jindong and Xu, Ruochen and Xie, Xing and Whang, Steven Euijong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {53064--53101},
 publisher = {Curran Associates, Inc.},
 title = {ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5ef9853a6cdea40ae3e301a6d8dc32b5-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",ohERBenchEntityrelationshipBased2024a
EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations,"How to evaluate Large Language Models (LLMs) in code generation remains an open question. Many benchmarks have been proposed, but they have two limitations, i.e., data leakage and lack of domain-specific evaluation.The former hurts the fairness of benchmarks, and the latter hinders practitioners from selecting superior LLMs for specific programming domains.To address these two limitations, we propose a new benchmark - EvoCodeBench, which has the following advances: (1) Evolving data. EvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid data leakage. This paper releases the first version - EvoCodeBench-2403, containing 275 samples from 25 repositories.(2) A domain taxonomy and domain labels. Based on the statistics of open-source communities, we design a programming domain taxonomy consisting of 10 popular domains. Based on the taxonomy, we annotate each sample in EvoCodeBench with a domain label. EvoCodeBench provides a broad platform for domain-specific evaluations.(3) Domain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific Improvement (DSI) and define LLMs' comfort and strange domains. These evaluations help practitioners select superior LLMs in specific domains and discover the shortcomings of existing LLMs.Besides, EvoCodeBench is collected by a rigorous pipeline and aligns with real-world repositories in multiple aspects (e.g., code distributions).We evaluate 8 popular LLMs (e.g., gpt-4, DeepSeek Coder, StarCoder 2) on EvoCodeBench and summarize some insights. EvoCodeBench reveals the actual abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is only 20.74%. Besides, we evaluate LLMs in different domains and discover their comfort and strange domains. For example, gpt-4 performs best in most domains but falls behind others in the Internet domain. StarCoder 2-15B unexpectedly performs well in the Database domain and even outperforms 33B LLMs. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.",https://neurips.cc//virtual/2024/poster/97531,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_6a059625,
 author = {Li, Jia and Li, Ge and Zhang, Xuanming and Zhao, Yunfei and Dong, Yihong and Jin, Zhi and Li, Binhua and Huang, Fei and Li, Yongbin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {57619--57641},
 publisher = {Curran Associates, Inc.},
 title = {EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/6a059625a6027aca18302803743abaa2-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liEvoCodeBenchEvolvingCode2024
FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models,"Federated learning has enabled multiple parties to collaboratively train large language models without directly sharing their data (FedLLM).Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy.However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works all rely on artificially constructed datasets, failing to capture properties in real-world scenarios.Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community.FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747.Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios.Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration).We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons.Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench.",https://neurips.cc//virtual/2024/poster/97593,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_c8cdab0e,
 author = {Ye, Rui and Ge, Rui and Zhu, Xinyu and Chai, Jingyi and Du, Yaxin and Liu, Yang and Wang, Yanfeng and Chen, Siheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {111106--111130},
 publisher = {Curran Associates, Inc.},
 title = {FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c8cdab0e890c59255c27977072fdb0f0-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",yeFedLLMbenchRealisticBenchmarks2024
FinBen: A Holistic Financial Benchmark for Large Language Models,"LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 42 datasets spanning 24 financial tasks, covering eight critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, decision-making, and bilingual (English and Spanish). FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and two novel datasets for regulations and stock trading. Our evaluation of 21 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovations in financial LLMs. All datasets and code are publicly available for the research community, with results shared and updated regularly on the Open Financial LLM Leaderboard.",https://neurips.cc//virtual/2024/poster/97525,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_adb1d9fa,
 author = {Xie, Qianqian and Han, Weiguang and Chen, Zhengyu and Xiang, Ruoyu and Zhang, Xiao and He, Yueru and Xiao, Mengxi and Li, Dong and Dai, Yongfu and Feng, Duanyu and Xu, Yijing and Kang, Haoqiang and Kuang, Ziyan and Yuan, Chenhan and Yang, Kailai and Luo, Zheheng and Zhang, Tianlin and Liu, Zhiwei and Xiong, Guojun and Deng, Zhiyang and Jiang, Yuechen and Yao, Zhiyuan and Li, Haohang and Yu, Yangyang and Hu, Gang and Huang, Jiajia and Liu, Xiao-Yang and Lopez-Lira, Alejandro and Wang, Benyou and Lai, Yanzhao and Wang, Hao and Peng, Min and Ananiadou, Sophia and Huang, Jimin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {95716--95743},
 publisher = {Curran Associates, Inc.},
 title = {FinBen: A Holistic Financial Benchmark for Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/adb1d9fa8be4576d28703b396b82ba1b-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",xieFinBenHolisticFinancial2024
FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models,"Vision language models (VLMs) have achieved impressive progress in diverse applications, becoming a prevalent research direction. In this paper, we build FIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn conversations that are derived from 27 source datasets, empowering VLMs to spontaneously refine their responses based on user feedback across diverse tasks. To scale up the data collection, FIRE is collected in two components: FIRE-100K and FIRE-1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is freely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a benchmark to comprehensively evaluate the feedback-refining capability of VLMs, which contains 11K feedback-refinement conversations as the test data, two evaluation settings, and a model to provide feedback for VLMs. We develop the FIRE-LLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows remarkable feedback-refining capability on FIRE-Bench and outperforms untrained VLMs by 50%, making more efficient user-agent interactions and underscoring the significance of the FIRE dataset.",https://neurips.cc//virtual/2024/poster/97805,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_b83bea96,
 author = {Li, Pengxiang and Gao, Zhi and Zhang, Bofei and Yuan, Tao and Wu, Yuwei and Harandi, Mehrtash and Jia, Yunde and Zhu, Song-Chun and Li, Qing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {101618--101640},
 publisher = {Curran Associates, Inc.},
 title = {FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b83bea9688047be30f54034c55716854-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liFIREDatasetFeedback2024
GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps,"Large language models (LLMs) have recently demonstrated great success in generating and understanding natural language.  While they have also shown potential beyond the domain of natural language, it remains an open question as to what extent and in which way these LLMs can plan. We investigate their planning capabilities by proposing \texttt{GameTraversalBenchmark (GTB)}, a benchmark consisting of diverse 2D grid-based game maps. An LLM succeeds if it can traverse through given objectives, with a minimum number of steps and a minimum number of generation errors. We evaluate a number of LLMs on \texttt{GTB} and found that GPT-4-Turbo achieved the highest score of $44.97\%$ on \texttt{GTB\_Score} (GTBS), a composite score that combines the three above criteria. Furthermore, we preliminarily test large reasoning models, namely o1, which scores $67.84\%$ on GTBS, indicating that the benchmark remains challenging for current models. Code, data, and documentation are available at \url{https://github.com/umair-nasir14/Game-Traversal-Benchmark}.",https://neurips.cc//virtual/2024/poster/97479,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_3852c825,
 author = {Nasir, Muhammad Umair and James, Steven and Togelius, Julian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {31813--31827},
 publisher = {Curran Associates, Inc.},
 title = {GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/3852c8254bc6d904c09db9921157f59b-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",nasirGameTraversalBenchmarkEvaluatingPlanning2024a
GLBench: A Comprehensive Benchmark for Graph with Large Language Models,"The emergence of large language models (LLMs) has revolutionized the way we interact with graphs, leading to a new paradigm called GraphLLM. Despite the rapid development of GraphLLM methods in recent years, the progress and understanding of this field remain unclear due to the lack of a benchmark with consistent experimental protocols. To bridge this gap, we introduce GLBench, the first comprehensive benchmark for evaluating GraphLLM methods in both supervised and zero-shot scenarios. GLBench provides a fair and thorough evaluation of different categories of GraphLLM methods, along with traditional baselines such as graph neural networks. Through extensive experiments on a collection of real-world datasets with consistent data processing and splitting strategies, we have uncovered several key findings. Firstly, GraphLLM methods outperform traditional baselines in supervised settings, with LLM-as-enhancers showing the most robust performance. However, using LLMs as predictors is less effective and often leads to uncontrollable output issues. We also notice that no clear scaling laws exist for current GraphLLM methods. In addition, both structures and semantics are crucial for effective zero-shot transfer, and our proposed simple baseline can even outperform several models tailored for zero-shot scenarios. The data and code of the benchmark can be found at https://github.com/NineAbyss/GLBench.",https://neurips.cc//virtual/2024/poster/97881,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_4ab0bd66,
 author = {Li, Yuhan and Wang, Peisong and Zhu, Xiao and Chen, Aochuan and Jiang, Haiyun and Cai, Deng and Chan, Victor Wai Kin and Li, Jia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {42349--42368},
 publisher = {Curran Associates, Inc.},
 title = {GLBench: A Comprehensive Benchmark for Graph with Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4ab0bd666d034fcaa5566fc7d176daa6-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liGLBenchComprehensiveBenchmark2024
GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI,"Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon specific academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations, and insufficient guidance for interactive LVLMs. To address these limitations, we developed the GMAI-MMBench, the most comprehensive general medical AI benchmark with well-categorized data structure and multi-perceptual granularity to date. It is constructed from 284 datasets across 38 medical image modalities, 18 clinical-related tasks, 18 departments, and 4 perceptual granularities in a Visual Question Answering (VQA) format. Additionally, we implemented a lexical tree structure that allows users to customize evaluation tasks, accommodating various assessment needs and substantially supporting medical AI research and applications. We evaluated 50 LVLMs, and the results show that even the advanced GPT-4o only achieves an accuracy of 53.96\%, indicating significant room for improvement. Moreover, we identified five key insufficiencies in current cutting-edge LVLMs that need to be addressed to advance the development of better medical applications. We believe that GMAI-MMBench will stimulate the community to build the next generation of LVLMs toward GMAI.",https://neurips.cc//virtual/2024/poster/97754,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_ab7e02fd,
 author = {Chen, Pengcheng and Ye, Jin and Wang, Guoan and Li, Yanjun and Deng, Zhongying and Li, Wei and Li, Tianbin and Duan, Haodong and Huang, Ziyan and Su, Yanzhou and Wang, Benyou and Zhang, Shaoting and Fu, Bin and Cai, Jianfei and Zhuang, Bohan and Seibel, Eric J and Qiao, Yu and He, Junjun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {94327--94427},
 publisher = {Curran Associates, Inc.},
 title = {GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/ab7e02fd60e47e2a379d567f6b54f04e-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",chenGMAImmbenchComprehensiveMultimodal2024
GTA: A Benchmark for General Tool Agents,"In developing general-purpose agents, significant focus has been placed on integrating large language models (LLMs) with various tools. This poses a challenge to the tool-use capabilities of LLMs. However, there are evident gaps between existing tool evaluations and real-world scenarios. Current evaluations often use AI-generated queries, single-step tasks, dummy tools, and text-only inputs, which fail to reveal the agents' real-world problem-solving abilities effectively. To address this, we propose GTA, a benchmark for **G**eneral **T**ool **A**gents,  featuring three main aspects: (i) *Real user queries*: human-written queries with simple real-world objectives but implicit tool-use, requiring the LLM to reason the suitable tools and plan the solution steps. (ii) *Real deployed tools*: an evaluation platform equipped with tools across perception, operation, logic, and creativity categories to evaluate the agents' actual task execution performance. (iii) *Real multimodal inputs*: authentic image files, such as spatial scenes, web page screenshots, tables, code snippets, and printed/handwritten materials, used as the query contexts to align with real-world scenarios closely. We designed 229 real-world tasks and executable tool chains to evaluate mainstream LLMs. Our findings show that real-world user queries are challenging for existing LLMs, with GPT-4 completing less than 50\% of the tasks and most LLMs achieving below 25\%. This evaluation reveals the bottlenecks in the tool-use capabilities of current LLMs in real-world scenarios, which is beneficial for the advancement of general-purpose tool agents. Dataset and code are available at https://github.com/open-compass/GTA.",https://neurips.cc//virtual/2024/poster/97620,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_8a75ee6d,
 author = {Wang, Jize and Ma, Zerun and Li, Yining and Zhang, Songyang and Chen, Cailian and Chen, Kai and Le, Xinyi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {75749--75790},
 publisher = {Curran Associates, Inc.},
 title = {GTA: A Benchmark for General Tool Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/8a75ee6d4b2eb0b777f549a32a5a5c28-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wangGTABenchmarkGeneral2024
Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning,"We present a novel multimodal preference dataset for creative tasks, consisting of over 250 million human votes on more than 2.2 million captions, collected through crowdsourcing rating data for The New Yorker's weekly cartoon caption contest over the past eight years. This unique dataset supports the development and evaluation of multimodal large language models and preference-based fine-tuning algorithms for humorous caption generation. We propose novel benchmarks for judging the quality of model-generated captions, utilizing both GPT4 and human judgments to establish ranking-based evaluation strategies. Our experimental results highlight the limitations of current fine-tuning methods, such as RLHF and DPO, when applied to creative tasks. Furthermore, we demonstrate that even state-of-the-art models like GPT4 and Claude currently underperform top human contestants in generating humorous captions. As we conclude this extensive data collection effort, we release the entire preference dataset to the research community, fostering further advancements in AI humor generation and evaluation.",https://neurips.cc//virtual/2024/poster/97450,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_e297fb6c,
 author = {Zhang, Jifan and Jain, Lalit and Guo, Yang and Chen, Jiayi and Zhou, Kuan Lok and Suresh, Siddharth and Wagenmaker, Andrew and Sievert, Scott and Rogers, Timothy and Jamieson, Kevin and Mankoff, Robert and Nowak, Robert},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {125264--125286},
 publisher = {Curran Associates, Inc.},
 title = {Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/e297fb6cd1690ee5b39c5bb4c58ad801-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",zhangHumorAIMassive2024
HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models,"The increasing size of language models necessitates a thorough analysis across multiple dimensions to assess trade-offs among crucial hardware metrics such as latency, energy consumption, GPU memory usage, and performance. Identifying optimal model configurations under specific hardware constraints is becoming essential but remains challenging due to the computational load of exhaustive training and evaluation on multiple devices. To address this, we introduce HW-GPT-Bench, a hardware-aware benchmark that utilizes surrogate predictions to approximate various hardware metrics across 13 devices of architectures in the GPT-2 family, with architectures containing up to 1.55B parameters. Our surrogates, via calibrated predictions and reliable uncertainty estimates, faithfully model the heteroscedastic noise inherent in the energy and latency measurements. To estimate perplexity, we employ weight-sharing techniques from Neural Architecture Search (NAS), inheriting pretrained weights from the largest GPT-2 model. Finally, we demonstrate the utility of HW-GPT-Bench by simulating optimization trajectories of various multi-objective optimization algorithms in just a few seconds.",https://neurips.cc//virtual/2024/poster/97460,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_6ffdbf06,
 author = {Sukthanker, Rhea Sanjay and Zela, Arber and Staffler, Benedikt and Klein, Aaron and Purucker, Lennart and Franke, J\""{o}rg K. H. and Hutter, Frank},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {60776--60834},
 publisher = {Curran Associates, Inc.},
 title = {HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/6ffdbf064df51857eb802a904aaaba63-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",sukthankerHWGPTbenchHardwareawareArchitecture2024
IaC-Eval: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs,"Infrastructure-as-Code (IaC), an important component of cloud computing, allows the definition of cloud infrastructure in high-level programs. However, developing IaC programs is challenging, complicated by factors that include the burgeoning complexity of the cloud ecosystem (e.g., diversity of cloud services and workloads), and the relative scarcity of IaC-specific code examples and public repositories. While large language models (LLMs) have shown promise in general code generation and could potentially aid in IaC development, no benchmarks currently exist for evaluating their ability to generate IaC code. We present IaC-Eval, a first step in this research direction. IaC-Eval's dataset includes 458 human-curated scenarios covering a wide range of popular AWS services, at varying difficulty levels. Each scenario mainly comprises a natural language IaC problem description and an infrastructure intent specification. The former is fed as user input to the LLM, while the latter is a general notion used to verify if the generated IaC program conforms to the user's intent; by making explicit the problem's requirements that can encompass various cloud services, resources and internal infrastructure details.  Our in-depth evaluation shows that contemporary LLMs perform poorly on IaC-Eval, with the top-performing model, GPT-4, obtaining a pass@1 accuracy of 19.36%. In contrast, it scores 86.6% on EvalPlus, a popular Python code generation benchmark, highlighting a need for advancements in this domain. We open-source the IaC-Eval dataset and evaluation framework at https://github.com/autoiac-project/iac-eval to enable future research on LLM-based IaC code generation.",https://neurips.cc//virtual/2024/poster/97835,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_f26b2929,
 author = {Kon, Patrick Tser Jern and Liu, Jiachen and Qiu, Yiming and Fan, Weijun and He, Ting and Lin, Lei and Zhang, Haoran and Park, Owen M. and Elengikal, George S. and Kang, Yuxin and Chen, Ang and Chowdhury, Mosharaf and Lee, Myungjin and Wang, Xinyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {134488--134506},
 publisher = {Curran Associates, Inc.},
 title = {IaC-Eval: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f26b29298ae8acd94bd7e839688e329b-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",konIaCevalCodeGeneration2024
InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models,"Large Language Models for code (code LLMs) have witnessed tremendous progress in recent years. With the rapid development of code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to measure the performance of code LLMs with a particular focus on code generation tasks. However, they are insufficient to cover the full range of expected capabilities of code LLMs, which span beyond code generation to answering diverse coding-related questions. To fill this gap, we propose InfiBench, the first large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages. InfiBench uses four types of model-free automatic metrics to evaluate response correctness where domain experts carefully concretize the criterion for each question. We conduct a systematic evaluation for over 100 latest code LLMs on InfiBench, leading to a series of novel and insightful findings. Our detailed analyses showcase potential directions for further advancement of code LLMs. InfiBench is fully open source at https://infi-coder.github.io/infibench and continuously expanding to foster more scientific and systematic practices for code LLM evaluation.",https://neurips.cc//virtual/2024/poster/97797,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_e888eb94,
 author = {Li, Linyi and Geng, Shijie and Li, Zhenwen and He, Yibo and Yu, Hao and Hua, Ziyue and Ning, Guanghan and Wang, Siwei and Xie, Tao and Yang, Hongxia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {128668--128698},
 publisher = {Curran Associates, Inc.},
 title = {InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/e888eb9400fe14bb70e057aa1d719188-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liInfiBenchEvaluatingQuestionanswering2024
Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models,"Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning—a fundamental component of human cognition—remains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence. Our code is available at https://github.com/jiayuww/SpatialEval.",https://neurips.cc//virtual/2024/poster/94371,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_89cc5e61,
 author = {Wang, Jiayu and Ming, Yifei and Shi, Zhenmei and Vineet, Vibhav and Wang, Xin and Li, Yixuan and Joshi, Neel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {75392--75421},
 publisher = {Curran Associates, Inc.},
 title = {Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/89cc5e613d34f90de90c21e996e60b30-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",wangPictureWorthThousand2024
JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as *jailbreak artifacts*; (2) a jailbreaking dataset comprising 100 behaviors---both original and sourced from prior work---which align with OpenAI's usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.",https://neurips.cc//virtual/2024/poster/97459,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_63092d79,
 author = {Chao, Patrick and Debenedetti, Edoardo and Robey, Alexander and Andriushchenko, Maksym and Croce, Francesco and Sehwag, Vikash and Dobriban, Edgar and Flammarion, Nicolas and Pappas, George J. and Tram\`{e}r, Florian and Hassani, Hamed and Wong, Eric},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {55005--55029},
 publisher = {Curran Associates, Inc.},
 title = {JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/63092d79154adebd7305dfd498cbff70-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",chaoJailbreakBenchOpenRobustness2024
Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters,"Large Language Models (LLMs) are typically harmless but remain vulnerable to carefully crafted prompts known as ``jailbreaks'', which can bypass protective measures and induce harmful behavior. Recent advancements in LLMs have incorporated moderation guardrails that can filter outputs, which trigger processing errors for certain malicious questions. Existing red-teaming benchmarks often neglect to include questions that trigger moderation guardrails, making it difficult to evaluate jailbreak effectiveness. To address this issue, we introduce JAMBench, a harmful behavior benchmark designed to trigger and evaluate moderation guardrails. JAMBench involves 160 manually crafted instructions covering four major risk categories at multiple severity levels. Furthermore, we propose a jailbreak method, JAM (Jailbreak Against Moderation), designed to attack moderation guardrails using jailbreak prefixes to bypass input-level filters and a fine-tuned shadow model functionally equivalent to the guardrail model to generate cipher characters to bypass output-level filters. Our extensive experiments on four LLMs demonstrate that JAM achieves higher jailbreak success ($\sim$ $\times$ 19.88) and lower filtered-out rates ($\sim$ $\times$ 1/6) than baselines.",https://neurips.cc//virtual/2024/poster/96243,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_6d56bc83,
 author = {Jin, Haibo and Zhou, Andy and Menke, Joe D. and Wang, Haohan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {59408--59435},
 publisher = {Curran Associates, Inc.},
 title = {Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/6d56bc83ae9a4fafdce050bb36f04174-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",jinJailbreakingLargeLanguage2024
JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images,"Existing vision-language understanding benchmarks largely consist of images of objects in their usual contexts.As a consequence, recent multimodal large language models can perform well with only a shallow visual understanding by relying on background language biases. Thus, strong performance on these benchmarks does not necessarily correlate with strong visual understanding. In this paper, we release JourneyBench, a comprehensive human-annotated benchmark of generated images designed to assess the model's fine-grained multimodal reasoning abilities across five tasks: complementary multimodal chain of thought, multi-image VQA, imaginary image captioning, VQA with hallucination triggers, and fine-grained retrieval with sample-specific distractors.Unlike existing benchmarks, JourneyBench explicitly requires fine-grained multimodal reasoning in unusual imaginary scenarios where language bias and holistic image gist are insufficient. We benchmark state-of-the-art models on JourneyBench and analyze performance along a number of fine-grained dimensions. Results across all five tasks show that JourneyBench is exceptionally challenging for even the best models, indicating that models' visual reasoning abilities are not as strong as they first appear. We discuss the implications of our findings and propose avenues for further research.",https://neurips.cc//virtual/2024/poster/97518,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_734abb86,
 author = {Wang, Zhecan and Liu, Junzhang and Tang, Chia-Wei and Alomari, Hani and Sivakumar, Anushka and Sun, Rui and Li, Wenhao and Md. Atabuzzaman and Ayyubi, Hammad and You, Haoxuan and Ishmam, Alvi and Chang, Kai-Wei and Chang, Shih-Fu and Thomas, Chris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {63110--63123},
 publisher = {Curran Associates, Inc.},
 title = {JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/734abb86d3caa949f44da8a093717f61-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wangJourneyBenchChallengingOnestop2024
kGym: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution,"Large Language Models (LLMs) are consistently improving at increasingly realistic software engineering (SE) tasks. In real-world software stacks, significant SE effort is spent developing foundational system software like the Linux kernel. Unlike application-level software, a systems codebase like Linux is multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines); critical (impacting billions of devices worldwide), and highly concurrent (involving complex multi-threading). To evaluate if machine learning (ML) models are useful while developing such large-scale systems-level software, we introduce kGym (a platform) and kBench (a dataset). The kGym platform provides a SE environment for large-scale experiments on the Linux kernel, including compiling and running kernels in parallel across several virtual machines, detecting operations and crashes, inspecting logs, and querying and patching the code base. We use kGym to facilitate evaluation on kBench, a crash resolution benchmark drawn from real-world Linux kernel bugs.  An example bug in kBench contains crashing stack traces, a bug-reproducer file, a developer-written fix, and other associated data. To understand current performance, we conduct baseline experiments by prompting LLMs to resolve Linux kernel crashes. Our initial evaluations reveal that the best performing LLM achieves 0.72\% and 5.38\% in the unassisted and assisted (i.e., buggy files disclosed to the model) settings, respectively. These results highlight the need for further research to enhance model performance in SE tasks. Improving performance on kBench requires models to master new learning skills, including understanding the cause of crashes and repairing faults, writing memory-safe and hardware-aware code, and understanding concurrency. As a result, this work opens up multiple avenues of research at the intersection of machine learning and systems software.",https://neurips.cc//virtual/2024/poster/97426,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_8e9ed2a2,
 author = {Mathai, Alex and Huang, Chenxi and Maniatis, Petros and Nogikh, Aleksandr and Ivan\v{c}i\'{c}, Franjo and Yang, Junfeng and Ray, Baishakhi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {78053--78078},
 publisher = {Curran Associates, Inc.},
 title = {kGym: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/8e9ed2a28af7d9085180e3817b2c9a57-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",mathaiKGymPlatformDataset2024
Large Language Models' Expert-level Global History Knowledge Benchmark (HiST-LLM),"Large Language Models (LLMs) have the potential to transform humanities and social science research, yet their history knowledge and comprehension at a graduate level remains untested. Benchmarking LLMs in history is particularly challenging, given that human knowledge of history is inherently unbalanced, with more information available on Western history and recent periods. We introduce the History Seshat Test for LLMs (HiST-LLM), based on a subset of the Seshat Global History Databank, which provides a structured representation of human historical knowledge, containing 36,000 data points across 600 historical societies and over 2,700 scholarly references. This dataset covers every major world region from the Neolithic period to the Industrial Revolution and includes information reviewed and assembled by history experts and graduate research assistants. Using this dataset, we benchmark a total of seven models from the Gemini, OpenAI, and Llama families. We find that, in a four-choice format, LLMs have a balanced accuracy ranging from 33.6% (Llama-3.1-8B) to 46% (GPT-4-Turbo), outperforming random guessing (25%) but falling short of expert comprehension. LLMs perform better on earlier historical periods. Regionally, performance is more even but still better for the Americas and lowest in Oceania and Sub-Saharan Africa for the more advanced models. Our benchmark shows that while LLMs possess some expert-level historical knowledge, there is considerable room for improvement.",https://neurips.cc//virtual/2024/poster/97439,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_38cc5cba,
 author = {Hauser, Jakob and Kondor, Daniel and Reddish, Jenny and Benam, Majid and Cioni, Enrico and Villa, Federica and Bennett, James S. and Hoyer, Daniel and Francois, Pieter and Turchin, Peter and del Rio-Chanona, R. Maria},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {32336--32369},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models\textquotesingle  Expert-level Global History Knowledge Benchmark (HiST-LLM)},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/38cc5cba8e513547b96bc326e25610dc-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",hauserLargeLanguageModelsExpertlevel2024
Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach,"With the continued advancement of Large Language Models (LLMs) Agents in reasoning, planning, and decision-making, benchmarks have become crucial in evaluating these skills. However, there is a notable gap in benchmarks for real-time strategic decision-making. StarCraft II (SC2), with its complex and dynamic nature, serves as an ideal setting for such evaluations. To this end, we have developed TextStarCraft II, a specialized environment for assessing LLMs in real-time strategic scenarios within SC2. Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making. Our key experiments included:1. LLM Evaluation: Tested 10 LLMs in TextStarCraft II, most of them defeating LV5 build-in AI, showcasing effective strategy skills.2. Commercial Model Knowledge: Evaluated four commercial  models on SC2 knowledge; GPT-4 ranked highest by Grandmaster-level experts.3. Human-AI Matches: Experimental results showed that fine-tuned LLMs performed on par with Gold-level players in real-time matches, demonstrating comparable strategic abilities.All code and data from thisstudy have been made pulicly available at https://github.com/histmeisah/Large-Language-Models-play-StarCraftII",https://neurips.cc//virtual/2024/poster/93911,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_f0ebc318,
 author = {Ma, Weiyu and Mi, Qirui and Zeng, Yongcheng and Yan, Xue and Wu, Yuqiao and Lin, Runji and Zhang, Haifeng and Wang, Jun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {133386--133442},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f0ebc318e2df08360b2df559e81602e5-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",maLargeLanguageModels2024
LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models,"Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain.  However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice.To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval.This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application.We evaluated 38 open-source and commercial LLMs and obtained some interesting findings.  The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at https://github.com/CSHaitao/LexEval and will be continuously updated.",https://neurips.cc//virtual/2024/poster/97832,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_2cb40fc0,
 author = {Li, Haitao and Chen, You and Ai, Qingyao and Wu, Yueyue and Zhang, Ruizhe and Liu, Yiqun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {25061--25094},
 publisher = {Curran Associates, Inc.},
 title = {LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/2cb40fc022ca7bdc1a9a78b793661284-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liLexEvalComprehensiveChinese2024a
LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages,"In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.",https://neurips.cc//virtual/2024/poster/97604,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_2e43584b,
 author = {Bean, Andrew and Hellsten, Simi and Mayne, Harry and Magomere, Jabez and A., Ethan and Chi, Ryan and Hale, Scott A. and Kirk, Hannah Rose},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {26224--26237},
 publisher = {Curran Associates, Inc.},
 title = {LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/2e43584b7d7b32fb6b2aa83b32dbbb20-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",beanLINGOLYBenchmarkOlympiadlevel2024a
LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment,"Although large language models (LLMs) have demonstrated their strong intelligence ability, the high demand for computation and storage hinders their practical application. To this end, many model compression techniques are proposed to increase the efficiency of LLMs. However, current researches only validate their methods on limited models, datasets, metrics, etc, and still lack a comprehensive evaluation under more general scenarios. So it is still a question of which model compression approach we should use under a specific case. To mitigate this gap, we present the Large Language Model Compression Benchmark (LLMCBench), a rigorously designed benchmark with an in-depth analysis for LLM compression algorithms. We first analyze the actual model production requirements and carefully design evaluation tracks and metrics. Then, we conduct extensive experiments and comparison using multiple mainstream LLM compression approaches. Finally, we perform an in-depth analysis based on the evaluation and provide useful insight for LLM compression design. We hope our LLMCBench can contribute insightful suggestions for LLM compression algorithm design and serve as a foundation for future research.",https://neurips.cc//virtual/2024/poster/97446,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_9f4cc62d,
 author = {Yang, Ge and He, Changyi and Guo, Jinyang and Wu, Jianyu and Ding, Yifu and Liu, Aishan and Qin, Haotong and Ji, Pengliang and Liu, Xianglong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {87532--87544},
 publisher = {Curran Associates, Inc.},
 title = {LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/9f4cc62d0632911c63163ea3d9ec19bd-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",yangLLMCBenchBenchmarkingLarge2024a
MARPLE: A Benchmark for Long-Horizon Inference,"Reconstructing past events requires reasoning across long time horizons. To figure out what happened, humans draw on prior knowledge about the world and human behavior and integrate insights from various sources of evidence including visual, language, and auditory cues. We introduce MARPLE, a benchmark for evaluating long-horizon inference capabilities using multi-modal evidence. Our benchmark features agents interacting with simulated households, supporting vision, language, and auditory stimuli, as well as procedurally generated environments and agent behaviors. Inspired by classic ``whodunit'' stories, we ask AI models and human participants to infer which agent caused a change in the environment based on a step-by-step replay of what actually happened. The goal is to correctly identify the culprit as early as possible. Our findings show that human participants outperform both traditional Monte Carlo simulation methods and an LLM baseline (GPT-4) on this task. Compared to humans, traditional inference models are less robust and performant, while GPT-4 has difficulty comprehending environmental changes. We analyze factors influencing inference performance and ablate different modes of evidence, finding that all modes are valuable for performance. Overall, our experiments demonstrate that the long-horizon, multimodal inference tasks in our benchmark present a challenge to current models. Project website: https://marple-benchmark.github.io/.",https://neurips.cc//virtual/2024/poster/97512,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_c4c6b964,
 author = {Jin, Emily and Huang, Zhuoyi and Fr\""{a}nken, Jan-Philipp and Liu, Weiyu and Cha, Hannah and Brockbank, Erik and Wu, Sarah and Zhang, Ruohan and Wu, Jiajun and Gerstenberg, Tobias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {108824--108850},
 publisher = {Curran Associates, Inc.},
 title = {MARPLE: A Benchmark for Long-Horizon Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c4c6b9642f3d1c2bf036bb9575f7b5bf-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",jinMARPLEBenchmarkLonghorizon2024
MedCalc-Bench: Evaluating Large Language Models for Medical Calculations,"Current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks. MedCalc-Bench is publicly available at: https://github.com/ncbi-nlp/MedCalc-Bench.",https://neurips.cc//virtual/2024/poster/97666,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_99e81750,
 author = {Khandekar, Nikhil and Jin, Qiao and Xiong, Guangzhi and Dunn, Soren and Applebaum, Serina S and Anwar, Zain and Sarfo-Gyamfi, Maame and Safranek, Conrad W and Anwar, Abid A and Zhang, Andrew and Gilson, Aidan and Singer, Maxwell B and Dave, Amisha and Taylor, Andrew and Zhang, Aidong and Chen, Qingyu and Lu, Zhiyong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {84730--84745},
 publisher = {Curran Associates, Inc.},
 title = {MedCalc-Bench: Evaluating Large Language Models for Medical Calculations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/99e81750f3fdfcaf9613db2dbf4bd623-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",khandekarMedCalcbenchEvaluatingLarge2024a
MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning,"Users typically engage with LLMs interactively, yet most existing benchmarks evaluate them in a static, single-turn format, posing reliability concerns in interactive scenarios. We identify a key obstacle towards reliability: LLMs are trained to answer any question, even with incomplete context or insufficient knowledge. In this paper, we propose to change the static paradigm to an interactive one, develop systems that proactively ask questions to gather more information and respond reliably, and introduce an benchmark—MEDIQ—to evaluate question-asking ability in LLMs. MEDIQ simulates clinical interactions consisting of a Patient System and an adaptive Expert System; with potentially incomplete initial information, the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details via follow-up questions. We provide a pipeline to convert single-turn medical benchmarks into an interactive format. Our results show that directly prompting state-of-the-art LLMs to ask questions degrades performance, indicating that adapting LLMs to proactive information-seeking settings is nontrivial. We experiment with abstention strategies to better estimate model confidence and decide when to ask questions, improving diagnostic accuracy by 22.3%; however, performance still lags compared to an (unrealistic in practice) upper bound with complete information upfront. Further analyses show improved interactive performance with filtering irrelevant contexts and reformatting conversations. Overall, we introduce a novel problem towards LLM reliability, an interactive MEDIQ benchmark and a novel question-asking system, and highlight directions to extend LLMs’ information-seeking abilities in critical domains.",https://neurips.cc//virtual/2024/poster/94856,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_32b80425,
 author = {Li, Shuyue Stella and Balachandran, Vidhisha and Feng, Shangbin and Ilgen, Jonathan S. and Pierson, Emma and Koh, Pang Wei and Tsvetkov, Yulia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {28858--28888},
 publisher = {Curran Associates, Inc.},
 title = {MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/32b80425554e081204e5988ab1c97e9a-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",liMediQQuestionaskingLlms2024a
MedJourney: Benchmark and Evaluation of Large Language Models over Patient Clinical Journey,"Large language models (LLMs) have demonstrated remarkable capabilities in language understanding and generation, leading to their widespread adoption across various fields. Among these, the medical field is particularly well-suited for LLM applications, as many medical tasks can be enhanced by LLMs. Despite the existence of benchmarks for evaluating LLMs in medical question-answering and exams, there remains a notable gap in assessing LLMs' performance in supporting patients throughout their entire hospital visit journey in real-world clinical practice. In this paper, we address this gap by dividing a typical patient's clinical journey into four stages: planning, access, delivery and ongoing care. For each stage, we introduce multiple tasks and corresponding datasets, resulting in a comprehensive benchmark comprising 12 datasets, of which five are newly introduced, and seven are constructed from existing datasets. This proposed benchmark facilitates a thorough evaluation of LLMs' effectiveness across the entire patient journey, providing insights into their practical application in clinical settings. Additionally, we evaluate three categories of LLMs against this benchmark: 1) proprietary LLM services such as GPT-4; 2) public LLMs like QWen; and 3) specialized medical LLMs, like HuatuoGPT2. Through this extensive evaluation, we aim to provide a better understanding of LLMs' performance in the medical domain, ultimately contributing to their more effective deployment in healthcare settings.",https://neurips.cc//virtual/2024/poster/97646,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_9f80af32,
 author = {Wu, Xian and Zhao, Yutian and Zhang, Yunyan and Wu, Jiageng and Zhu, Zhihong and Zhang, Yingying and Ouyang, Yi and Zhang, Ziheng and Wang, Huimin and Lin, Zhenxi and Yang, Jie and Zhao, Shuang and Zheng, Yefeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {87621--87646},
 publisher = {Curran Associates, Inc.},
 title = {MedJourney: Benchmark and Evaluation of Large Language Models over Patient Clinical Journey},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/9f80af32390984cb709cdeb014d0df41-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wuMedJourneyBenchmarkEvaluation2024a
MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models,"As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety while preserving their medical performance. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, paving the way to mitigate the safety risks of LLMs in medicine. The benchmark dataset and code are available at https://github.com/AI4LIFE-GROUP/med-safety-bench.",https://neurips.cc//virtual/2024/poster/97606,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_3ac952d0,
 author = {Han, Tessa and Kumar, Aounon and Agarwal, Chirag and Lakkaraju, Himabindu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {33423--33454},
 publisher = {Curran Associates, Inc.},
 title = {MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/3ac952d0264ef7a505393868a70a46b6-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",hanMedSafetyBenchEvaluatingImproving2024
Membership Inference Attacks against Large Vision-Language Models,"Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxRényi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.",https://neurips.cc//virtual/2024/poster/93657,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_b2c89231,
 author = {Li, Zhan and Wu, Yongtao and Chen, Yihang and Tonin, Francesco and Abad Rocamora, Elias and Cevher, Volkan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {98645--98674},
 publisher = {Curran Associates, Inc.},
 title = {Membership Inference Attacks against Large Vision-Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b2c892312af07f8a77afbeed188391f4-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",liMembershipInferenceAttacks2024
"Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs","AI assistants such as ChatGPT are trained to respond to users by saying, ""I am a large language model”.This raises questions. Do such models ""know'' that they are LLMs and reliably act on this knowledge? Are they ""aware"" of their current circumstances, such as being deployed to the public?We refer to a model's knowledge of itself and its circumstances as **situational awareness**.To quantify situational awareness in LLMs, we introduce a range of behavioral tests, based on question answering and instruction following. These tests form the **Situational Awareness Dataset (SAD)**, a benchmark comprising 7 task categories and over 13,000 questions.The benchmark tests numerous abilities, including the capacity of LLMs to (i) recognize their own generated text, (ii) predict their own behavior, (iii) determine whether a prompt is from internal evaluation or real-world deployment, and (iv) follow instructions that depend on self-knowledge.We evaluate 16 LLMs on SAD, including both base (pretrained) and chat models.While all models perform better than chance, even the highest-scoring model (Claude 3 Opus) is far from a human baseline on certain tasks. We also observe that performance on SAD is only partially predicted by metrics of general knowledge. Chat models, which are finetuned to serve as AI assistants, outperform their corresponding base models on SAD but not on general knowledge tasks.The purpose of SAD is to facilitate scientific understanding of situational awareness in LLMs by breaking it down into quantitative abilities. Situational awareness is important because it enhances a model's capacity for autonomous planning and action. While this has potential benefits from automation, it also introduces novel risks related to AI safety and control.",https://neurips.cc//virtual/2024/poster/97669,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_75377263,
 author = {Laine, Rudolf and Chughtai, Bilal and Betley, Jan and Hariharan, Kaivalya and Scheurer, J\'{e}r\'{e}my and Balesni, Mikita and Hobbhahn, Marius and Meinke, Alexander and Evans, Owain},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {64010--64118},
 publisher = {Curran Associates, Inc.},
 title = {Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/7537726385a4a6f94321e3adf8bd827e-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",laineMeMyselfAI2024
MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations,"Existing benchmarks for multi-hop question answering (QA) primarily evaluate models based on their ability to reason about entities and the relationships between them. However, there's a lack of insight into how these models perform in terms of both events and entities. In this paper, we introduce a novel semi-automatic question generation strategy by composing event structures from information extraction (IE) datasets and present the first Multi-hop Event-centric Question Answering (MEQA) benchmark. It contains (1) 2,243 challenging questions that require a diverse range of complex reasoning over entity-entity, entity-event, and event-event relations; (2) corresponding multi-step QA-format event reasoning chain (explanation) which leads to the answer for each question. We also introduce two metrics for evaluating explanations: completeness and logical consistency. We conduct comprehensive benchmarking and analysis, which shows that MEQA is challenging for the latest state-of-the-art models encompassing large language models (LLMs); and how they fall short of providing faithful explanations of the event-centric reasoning process.",https://neurips.cc//virtual/2024/poster/97474,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_e560a0b2,
 author = {Li, Ruosen and Wang, Zimu and Tran, Son Quoc and Xia, Lei and Du, Xinya},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {126835--126862},
 publisher = {Curran Associates, Inc.},
 title = {MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/e560a0b22e4432003d0dba63ff8dc457-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liMEQABenchmarkMultihop2024
Mercury: A Code Efficiency Benchmark for Code Large Language Models,"Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill the gap, we present Mercury, the first code efficiency benchmark for Code LLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions that serve as real-world efficiency baselines, enabling a comprehensive analysis of the runtime distribution. Based on the distribution, we introduce a new metric Beyond, which computes a runtime-percentile-weighted Pass score to reflect functional correctness and code efficiency simultaneously. On Mercury, leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given that an ideal Beyond score would be aligned with the Pass score, it indicates that while Code LLMs exhibit impressive capabilities in generating functionally correct code, there remains a notable gap in their efficiency. Finally, our empirical experiments reveal that Direct Preference Optimization (DPO) serves as a robust baseline for enhancing code efficiency compared with Supervised Fine Tuning (SFT), which paves a promising avenue for future exploration of efficient code generation. Our code and data are available on GitHub: https://github.com/Elfsong/Mercury.",https://neurips.cc//virtual/2024/poster/97452,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_1df1df43,
 author = {Du, Mingzhe and Tuan, Luu Anh and Ji, Bin and Liu, Qian and Ng, See-Kiong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {16601--16622},
 publisher = {Curran Associates, Inc.},
 title = {Mercury: A Code Efficiency Benchmark for Code Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1df1df43b58845650b8dada00fca9772-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",duMercuryCodeEfficiency2024
MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs,"Generating natural and meaningful responses to communicate with multi-modal human inputs is a fundamental capability of Large Vision-Language Models (LVLMs). While current open-source LVLMs demonstrate promising performance in simplified scenarios such as single-turn single-image input, they fall short in real-world conversation scenarios such as following instructions in a long context history with multi-turn and multi-images. Existing LVLM benchmarks primarily focus on single-choice questions or short-form responses, which do not adequately assess the capabilities of LVLMs in real-world human-AI interaction applications. Therefore, we introduce MMDU, a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning dataset, designed to evaluate and improve LVLMs' abilities in multi-turn and multi-image conversations. We employ the clustering algorithm to find the relevant images and textual descriptions from the open-source Wikipedia and construct the question-answer pairs by human annotators with the assistance of the GPT-4o model.MMDU has a maximum of 18k image+text tokens, 20 images, and 27 turns, which is at least 5x longer than previous benchmarks and poses challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs using MMDU reveals that open-source LVLMs lag behind closed-source counterparts due to limited conversational instruction tuning data.We demonstrate that fine-tuning open-source LVLMs on MMDU-45k significantly address this gap, generating longer and more accurate conversations, and improving scores on MMDU and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA: +1.2%). Our contributions pave the way for bridging the gap between current LVLM models and real-world application demands. The links to MMDU, and MMDU-45k are available in the supplementary material.",https://neurips.cc//virtual/2024/poster/97480,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_10570531,
 author = {Liu, Ziyu and Chu, Tao and Zang, Yuhang and Wei, Xilin and Dong, Xiaoyi and Zhang, Pan and Liang, Zijian and Xiong, Yuanjun and Qiao, Yu and Lin, Dahua and Wang, Jiaqi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {8698--8733},
 publisher = {Curran Associates, Inc.},
 title = {MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1057053100de064a44286239724f7865-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liuMMDUMultiturnMultiimage2024
MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations,"Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLONGBENCH-DOC, a long-context, multi- modal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e., page number). Moreover, 33.7\% of the questions are cross-page questions requiring evidence across multiple pages. 20.6\% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9\%, while the second-best, GPT-4V, scores 30.5\%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.",https://neurips.cc//virtual/2024/poster/97524,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_ae0e4328,
 author = {Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and Zhang, Pan and Pan, Liangming and Jiang, Yu-Gang and Wang, Jiaqi and Cao, Yixin and Sun, Aixin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {95963--96010},
 publisher = {Curran Associates, Inc.},
 title = {MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/ae0e43289bffea0c1fa34633fc608e92-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",maMMLONGBENCHDOCBenchmarkingLongcontext2024
MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,"In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates part of the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16\% to 33\% compared to MMLU, but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5\% in MMLU to just 2\% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is more discriminative benchmark to better track progress in the field.",https://neurips.cc//virtual/2024/poster/97435,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_ad236edc,
 author = {Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and Li, Tianle and Ku, Max and Wang, Kai and Zhuang, Alex and Fan, Rongqi and Yue, Xiang and Chen, Wenhu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {95266--95290},
 publisher = {Curran Associates, Inc.},
 title = {MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/ad236edc564f3e3156e1b2feafb99a24-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wangMMLUproMoreRobust2024
"MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing  Dataset and Benchmark for Text-to-Image Generation","Recently, the diffusion-based generative paradigm has achieved impressive general image generation capabilities with text prompts due to its accurate distribution modeling and stable training process. However, generating diverse remote sensing (RS) images that are tremendously different from general images in terms of scale and perspective remains a formidable challenge due to the lack of a comprehensive remote sensing image generation dataset with various modalities, ground sample distances (GSD), and scenes. In this paper, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) dataset and benchmark for text-to-image generation in diverse remote sensing scenarios. Specifically, we first collect nine publicly available RS datasets and conduct standardization for all samples. To bridge RS images to textual semantic information, we utilize a large-scale pretrained vision-language model to automatically output text prompts and perform hand-crafted rectification, resulting in information-rich text-image pairs (including multi-modal images). In particular, we design some methods to obtain the images with different GSD and various environments (e.g., low-light, foggy) in a single sample. With extensive manual screening and refining annotations, we ultimately obtain a MMM-RS dataset that comprises approximately 2.1 million text-image pairs. Extensive experimental results verify that our proposed MMM-RS dataset allows off-the-shelf diffusion models to generate diverse RS images across various modalities, scenes, weather conditions, and GSD. The dataset is available at https://github.com/ljl5261/MMM-RS.",https://neurips.cc//virtual/2024/poster/97495,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_1697e3fb,
 author = {Luo, Jialin and Wang, Yuanzhi and Gu, Ziqi and Qiu, Yide and Yao, Shuaizhen and Wang, Fuyun and Xu, Chunyan and Zhang, Wenhua and Wang, Dan and Cui, Zhen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {12151--12163},
 publisher = {Curran Associates, Inc.},
 title = {MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing  Dataset and Benchmark for Text-to-Image Generation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1697e3fb412da11dc9488249f9e7bbc9-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",luoMMMRSMultimodalMultiGSD2024
MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations,"With the emergence of LLMs and their integration with other data modalities, multi-modal 3D perception attracts more attention due to its connectivity to the physical world and makes rapid progress. However, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3D scene. To tackle this problem, this paper builds the first largest ever multi-modal 3D scene dataset and benchmark with hierarchical grounded language annotations, MMScan. It is constructed based on a top-down logic, from region to object level, from a single target to inter-target relationships, covering holistic aspects of spatial and attribute understanding. The overall pipeline incorporates powerful VLMs via carefully designed prompts to initialize the annotations efficiently and further involve humans' correction in the loop to ensure the annotations are natural, correct, and comprehensive. Built upon existing 3D scanning data, the resulting multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding and question-answering benchmarks. We evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. Furthermore, we use this high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation.",https://neurips.cc//virtual/2024/poster/97429,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_5aed0d90,
 author = {Lyu, Ruiyuan and Lin, Jingli and Wang, Tai and Yang, Shuai and Mao, Xiaohan and Chen, Yilun and Xu, Runsen and Huang, Haifeng and Zhu, Chenming and Lin, Dahua and Pang, Jiangmiao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {50898--50924},
 publisher = {Curran Associates, Inc.},
 title = {MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5aed0d900297bd5593afc14ff452d4a8-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",lyuMMScanMultimodal3D2024
MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs,"Large language models (LLMs) have shown increasing capability in problem-solving and decision-making, largely based on the step-by-step chain-of-thought reasoning processes. However, evaluating these reasoning abilities has become increasingly challenging. Existing outcome-based benchmarks are beginning to saturate, becoming less effective in tracking meaningful progress. To address this, we present a process-based benchmark MR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. Our meta-reasoning paradigm is especially suited for system-2 slow thinking, mirroring the human cognitive process of carefully examining assumptions, conditions, calculations, and logic to identify mistakes. MR-Ben comprises 5,975 questions curated by human experts across a wide range of subjects, including physics, chemistry, logic, coding, and more. Through our designed metrics for assessing meta-reasoning on this benchmark, we identify interesting limitations and weaknesses of current LLMs (open-source and closed-source models). For example, with models like the o1 series from OpenAI demonstrating strong performance by effectively scrutinizing the solution space, many other state-of-the-art models fall significantly behind on MR-Ben, exposing potential shortcomings in their training strategies and inference methodologies.",https://neurips.cc//virtual/2024/poster/95909,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_d81cb1f4,
 author = {Zeng, Zhongshen and Liu, Yinhong and Wan, Yingjia and Li, Jingyao and Chen, Pengguang and Dai, Jianbo and Yao, Yuxuan and Xu, Rongwu and Qi, Zehan and Zhao, Wanru and Shen, Linling and Lu, Jianqiao and Tan, Haochen and Chen, Yukang and Zhang, Hao and Shi, Zhan and Wang, Bailin and Guo, Zhijiang and Jia, Jiaya},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {119466--119546},
 publisher = {Curran Associates, Inc.},
 title = {MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/d81cb1f4dc6e13aeb45553f80b3d6837-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",zengMRbenMetareasoningBenchmark2024
Multi-modal Situated Reasoning in 3D Scenes,"Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding suffer from severe limitations in data modality, scope, diversity, and scale. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated questionanswering pairs across 9 distinct question categories, covering complex scenarios and object modalities within 3D scenes. We introduce a novel interleaved multimodal input setting in our benchmark to provide both texts, images, and point clouds for situation and question description, aiming to resolve ambiguity in describing situations with single-modality inputs (e.g., texts). Additionally, we devise the Multi-modal Next-step Navigation (MSNN) benchmark to evaluate models’ grounding of actions and transitions between situations. Comprehensive evaluations on reasoning and navigation tasks highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and crossdomain transfer further demonstrate the effectiveness of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models, contributing to advancements in 3D scene understanding for embodied AI.",https://neurips.cc//virtual/2024/poster/97727,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_feaeec8e,
 author = {Linghu, Xiongkun and Huang, Jiangyong and Niu, Xuesong and Ma, Xiaojian and Jia, Baoxiong and Huang, Siyuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {140903--140936},
 publisher = {Curran Associates, Inc.},
 title = {Multi-modal Situated Reasoning in 3D Scenes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/feaeec8ec2d3cb131fe18517ff14ec1f-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",linghuMultimodalSituatedReasoning2024
MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models,"Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges. Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establish **MultiTrust**, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: *truthfulness*, *safety*, *robustness*, *fairness*, and *privacy*. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets. Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at: [https://multi-trust.github.io/](https://multi-trust.github.io/).",https://neurips.cc//virtual/2024/poster/97845,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_586640cd,
 author = {Zhang, Yichi and Huang, Yao and Sun, Yitong and Liu, Chang and Zhao, Zhe and Fang, Zhengwei and Wang, Yifan and Chen, Huanran and Yang, Xiao and Wei, Xingxing and Su, Hang and Dong, Yinpeng and Zhu, Jun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {49279--49383},
 publisher = {Curran Associates, Inc.},
 title = {MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/586640cda3db2dc77349013dcefee456-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",zhangMultiTrustComprehensiveBenchmark2024
NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples,"Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term $\textbf{natural adversarial samples}$. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, ${\bf NaturalBench}$, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a $\textbf{vision-centric}$ design by pairing each question with two images that yield different answers, preventing ``blind'' solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can largely be solved with language priors like commonsense knowledge. We evaluate ${\bf 53}$ state-of-the-art VLMs on NaturalBench, showing that models like BLIP-3, LLaVA-OneVision, Cambrian-1, InternLM-XC2, Llama3.2-Vision, Molmo, Qwen2-VL, and even the (closed-source) GPT-4o lag 50%-70% behind human performance (which is above 90%). We analyze why NaturalBench is hard from two angles: (1) ${\bf Compositionality:}$ Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) ${\bf Biases: }$ NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. We show that debiasing can be crucial for VLM performance. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.",https://neurips.cc//virtual/2024/poster/97799,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_1e69ff56,
 author = {Li, Baiqi and Lin, Zhiqiu and Peng, Wenxuan and Nyandwi, Jean de Dieu and Jiang, Daniel and Ma, Zixian and Khanuja, Simran and Krishna, Ranjay and Neubig, Graham and Ramanan, Deva},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {17044--17068},
 publisher = {Curran Associates, Inc.},
 title = {NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1e69ff56d0ebff0752ff29caaddc25dd-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liNaturalBenchEvaluatingVisionlanguage2024
Needle In A Multimodal Haystack,"With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.",https://neurips.cc//virtual/2024/poster/97674,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_24a8968a,
 author = {Wang, Weiyun and Zhang, Shuibo and Ren, Yiming and Duan, Yuchen and Li, Tiantong and Liu, Shuo and Hu, Mengkang and Chen, Zhe and Zhang, Kaipeng and Lu, Lewei and Zhu, Xizhou and Luo, Ping and Qiao, Yu and Dai, Jifeng and Shao, Wenqi and Wang, Wenhai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {20540--20565},
 publisher = {Curran Associates, Inc.},
 title = {Needle In A Multimodal Haystack},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/24a8968affe71ffe4067d022b9d16566-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wangNeedleMultimodalHaystack2024
NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates,"Despite their remarkable abilities in various tasks, large language models (LLMs) still struggle with real-time information (e.g., new facts and terms) due to the knowledge cutoff in their development process. However, existing benchmarks focus on outdated content and limited fields, facing difficulties in real-time updating and leaving new terms unexplored. To address this problem, we propose an adaptive benchmark, NewTerm, for real-time evaluation of new terms. We design a highly automated construction method to ensure high-quality benchmark construction with minimal human effort, allowing flexible updates for real-time information. Empirical results on various LLMs demonstrate over 20% performance reduction caused by new terms. Additionally, while updates to the knowledge cutoff of LLMs can cover some of the new terms, they are unable to generalize to more distant new terms. We also analyze which types of terms are more challenging and why LLMs struggle with new terms, paving the way for future research. Finally, we construct NewTerm 2022 and 2023 to evaluate the new terms updated each year and will continue updating annually. The benchmark and codes can be found at https://anonymous.4open.science/r/NewTerms.",https://neurips.cc//virtual/2024/poster/97724,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_3eec719a,
 author = {Deng, Hexuan and Jiao, Wenxiang and Liu, Xuebo and Zhang, Min and Tu, Zhaopeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {35760--35795},
 publisher = {Curran Associates, Inc.},
 title = {NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/3eec719ab86712d32b065c5977f94ad0-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",dengNewTermBenchmarkingRealtime2024a
NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security,"Large Language Models (LLMs) are being deployed across various domains today. However, their capacity to solve Capture the Flag (CTF) challenges in cybersecurity has not been thoroughly evaluated. To address this, we develop a novel method to assess LLMs in solving CTF challenges by creating a scalable, open-source benchmark database specifically designed for these applications. This database includes metadata for LLM testing and adaptive learning, compiling a diverse range of CTF challenges from popular competitions. Utilizing the advanced function calling capabilities of LLMs, we build a fully automated system with an enhanced workflow and support for external tool calls. Our benchmark dataset and automated framework allow us to evaluate the performance of five LLMs, encompassing both black-box and open-source models. This work lays the foundation for future research into improving the efficiency of LLMs in interactive cybersecurity tasks and automated task planning. By providing a specialized benchmark, our project offers an ideal platform for developing, testing, and refining LLM-based approaches to vulnerability detection and resolution. Evaluating LLMs on these challenges and comparing with human performance yields insights into their potential for AI-driven cybersecurity solutions to perform real-world threat management. We make our benchmark dataset open source to public https://github.com/NYU-LLM-CTF/NYU_CTF_Bench along with our playground automated framework https://github.com/NYU-LLM-CTF/llm_ctf_automation.",https://neurips.cc//virtual/2024/poster/97547,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_69d97a64,
 author = {Shao, Minghao and Jancheska, Sofija and Udeshi, Meet and Dolan-Gavitt, Brendan and Xi, Haoran and Milner, Kimberly and Chen, Boyuan and Yin, Max and Garg, Siddharth and Krishnamurthy, Prashanth and Khorrami, Farshad and Karri, Ramesh and Shafique, Muhammad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {57472--57498},
 publisher = {Curran Associates, Inc.},
 title = {NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/69d97a6493fbf016fff0a751f253ad18-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",shaoNYUCTFBench2024
OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI,"The evolution of Artificial Intelligence (AI) has been significantly accelerated by advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), gradually showcasing potential cognitive reasoning abilities in problem-solving and scientific discovery (i.e., AI4Science) once exclusive to human intellect. To comprehensively evaluate current models' performance in cognitive reasoning abilities, we introduce OlympicArena, which includes 11,163 bilingual problems across both text-only and interleaved text-image modalities. These challenges encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions, rigorously examined for data leakage. We argue that the challenges in Olympic competition problems are ideal for evaluating AI's cognitive reasoning due to their complexity and interdisciplinary nature, which are essential for tackling complex scientific challenges and facilitating discoveries. Beyond evaluating performance across various disciplines using answer-only criteria, we conduct detailed experiments and analyses from multiple perspectives. We delve into the models' cognitive reasoning abilities, their performance across different modalities, and their outcomes in process-level evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions. Our extensive evaluations reveal that even advanced models like GPT-4o only achieve a 39.97\%  overall accuracy (28.67\%  for mathematics and 29.71\%  for physics), illustrating current AI limitations in complex reasoning and multimodal integration. Through the OlympicArena, we aim to advance AI towards superintelligence, equipping it to address more complex challenges in science and beyond. We also provide a comprehensive set of resources to support AI research, including a benchmark dataset, an open-source annotation platform, a detailed evaluation tool, and a leaderboard with automatic submission features.",https://neurips.cc//virtual/2024/poster/97617,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_222d2eaf,
 author = {Huang, Zhen and Wang, Zengzhi and Xia, Shijie and Li, Xuefeng and Zou, Haoyang and Xu, Ruijie and Fan, Run-Ze and Ye, Lyumanshan and Chern, Ethan and Ye, Yixin and Zhang, Yikai and Yang, Yuqing and Wu, Ting and Wang, Binjie and Sun, Shichao and Xiao, Yang and Li, Yiyuan and Zhou, Fan and Chern, Steffi and Qin, Yiwei and Ma, Yan and Su, Jiadi and Liu, Yixiu and Zheng, Yuxiang and Zhang, Shaoting and Lin, Dahua and Qiao, Yu and Liu, Pengfei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {19209--19253},
 publisher = {Curran Associates, Inc.},
 title = {OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/222d2eaf24cf8259a35d6c7130d31425-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",huangOlympicArenaBenchmarkingMultidiscipline2024a
On the Worst Prompt Performance of Large Language Models,"The performance of large language models (LLMs) is acutely sensitive to the phrasing of prompts, which raises significant concerns about their reliability in real-world scenarios. Existing studies often divide prompts into task-level instructions and case-level inputs and primarily focus on evaluating and improving robustness against variations in tasks-level instructions. However, this setup fails to fully address the diversity of real-world user queries and assumes the existence of task-specific datasets. To address these limitations, we introduce RobustAlpacaEval, a new benchmark that consists of semantically equivalent case-level queries and emphasizes the importance of using the worst prompt performance to gauge the lower bound of model performance. Extensive experiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the Llama, Mistral, and Gemma families uncover substantial variability in model performance; for instance, a difference of 45.48% between the worst and best performance for the Llama-2-70B-chat model, with its worst performance dipping as low as 9.38%. We further illustrate the difficulty in identifying the worst prompt from both model-agnostic and model-dependent perspectives, emphasizing the absence of a shortcut to characterize the worst prompt. We also attempt to enhance the worst prompt performance using existing prompt engineering and prompt consistency methods, but find that their impact is limited. These findings underscore the need to create more resilient LLMs that can maintain high performance across diverse prompts.",https://neurips.cc//virtual/2024/poster/95497,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_7fa5a377,
 author = {Cao, Bowen and Cai, Deng and Zhang, Zhisong and Zou, Yuexian and Lam, Wai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {69022--69042},
 publisher = {Curran Associates, Inc.},
 title = {On the Worst Prompt Performance of Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/7fa5a377b7ffabcce43cd00231bb3f9c-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",caoWorstPromptPerformance2024
OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments,"Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at [this https URL](https://os-world.github.io/).",https://neurips.cc//virtual/2024/poster/97468,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_5d413e48,
 author = {Xie, Tianbao and Zhang, Danyang and Chen, Jixuan and Li, Xiaochuan and Zhao, Siheng and Cao, Ruisheng and Hua, Toh Jing and Cheng, Zhoujun and Shin, Dongchan and Lei, Fangyu and Liu, Yitao and Xu, Yiheng and Zhou, Shuyan and Savarese, Silvio and Xiong, Caiming and Zhong, Victor and Yu, Tao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {52040--52094},
 publisher = {Curran Associates, Inc.},
 title = {OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5d413e48f84dc61244b6be550f1cd8f5-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",xieOSWorldBenchmarkingMultimodal2024
Paloma: A Benchmark for Evaluating Language Model Fit,"Evaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains—varying distributions of language. We introduce Perplexity Analysis for Language Model Assessment (Paloma), a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (e.g., r/depression on Reddit) and programming languages (e.g., Java on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from Paloma surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary.",https://neurips.cc//virtual/2024/poster/97430,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_760b2d94,
 author = {Magnusson, Ian and Bhagia, Akshita and Hofmann, Valentin and Soldaini, Luca and Jha, Ananya Harsh and Tafjord, Oyvind and Schwenk, Dustin and Walsh, Evan Pete and Elazar, Yanai and Lo, Kyle and Groeneveld, Dirk and Beltagy, Iz and Hajishirzi, Hannaneh and Smith, Noah A. and Richardson, Kyle and Dodge, Jesse},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {64338--64376},
 publisher = {Curran Associates, Inc.},
 title = {Paloma: A Benchmark for Evaluating Language Model Fit},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/760b2d94398aa61468aa3bc11506d9ea-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",magnussonPalomaBenchmarkEvaluating2024a
RedCode: Risky Code Execution and Generation Benchmark for Code Agents,"With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns, such as generating or executing malicious code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, an evaluation platform with benchmarks grounded in four key principles: real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and high-quality safety scenarios and tests. RedCode consists of two parts to evaluate agents’ safety in unsafe code execution and generation: (1) RedCode-Exec provides challenging code prompts in Python as inputs, aiming to evaluate code agents’ ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances. We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems. We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agent frameworks based on 19 LLMs, provide insights into code agents’ vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing unsafe operations on the operating system, but are less likely to reject executing technically buggy code, indicating high risks. Unsafe operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen reveal that more capable base models and agents with stronger overall coding abilities, such as GPT4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are publicly available at https://github.com/AI-secure/RedCode.",https://neurips.cc//virtual/2024/poster/97521,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_bfd082c4,
 author = {Guo, Chengquan and Liu, Xun and Xie, Chulin and Zhou, Andy and Zeng, Yi and Lin, Zinan and Song, Dawn and Li, Bo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {106190--106236},
 publisher = {Curran Associates, Inc.},
 title = {RedCode: Risky Code Execution and Generation Benchmark for Code Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/bfd082c452dffb450d5a5202b0419205-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",guoRedCodeRiskyCode2024
RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content,"Large Language Models (LLMs) are trained on vast amounts of data, most of which is automatically scraped from the internet. This data includes encyclopedic documents that harbor a vast amount of general knowledge (*e.g.*, Wikipedia) but also potentially overlap with benchmark datasets used for evaluating LLMs. Consequently, evaluating models on test splits that might have leaked into the training set is prone to misleading conclusions. To foster sound evaluation of language models, we introduce a new test dataset named RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a collection of five splits of test sets, four of which have not been released to the internet or exposed to LLM APIs prior to this publication. Each sample in RepLiQA comprises (1) a reference document crafted by a human annotator and depicting an imaginary scenario (*e.g.*, a news article) absent from the internet; (2) a question about the document’s topic; (3) a ground-truth answer derived directly from the information in the document; and (4) the paragraph extracted from the reference document containing the answer. As such, accurate answers can only be generated if a model can find relevant content within the provided document. We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models of various types and sizes in a context-conditional language modeling setting. Released splits of RepLiQA can be found here: https://huggingface.co/datasets/ServiceNow/repliqa.",https://neurips.cc//virtual/2024/poster/97851,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_2b236260,
 author = {Monteiro, Jo\~{a}o and No\""{e}l, Pierre-Andr\'{e} and Marcotte, \'{E}tienne and Rajeswar, Sai and Zantedeschi, Valentina and V\'{a}zquez, David and Chapados, Nicolas and Pal, Christopher and Taslakian, Perouz},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {24242--24276},
 publisher = {Curran Associates, Inc.},
 title = {RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/2b23626015b6311369e95a70735cbb72-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",monteiroRepLiQAQuestionansweringDataset2024a
Revisiting Few-Shot Object Detection with Vision-Language Models,"The era of vision-language models (VLMs) trained on web-scale datasets challenges conventional formulations of “open-world"" perception. In this work, we revisit the task of few-shot object detection (FSOD) in the context of recent foundational VLMs. First, we point out that zero-shot predictions from VLMs such as GroundingDINO significantly outperform state-of-the-art few-shot detectors (48 vs. 33 AP) on COCO. Despite their strong zero-shot performance, such foundation models may still be sub-optimal. For example, trucks on the web may be defined differently from trucks for a target applications such as autonomous vehicle perception. We argue that the task of few-shot recognition can be reformulated as aligning foundation models to target concepts using a few examples. Interestingly, such examples can be multi-modal, using both text and visual cues, mimicking instructions that are often given to human annotators when defining a target concept of interest. Concretely, we propose Foundational FSOD, a new benchmark protocol that evaluates detectors pre-trained on any external data and fine-tuned on multi-modal (text and visual) K-shot examples per target class. We repurpose nuImages for Foundational FSOD, benchmark several popular open-source VLMs, and provide an empirical analysis of state-of-the-art methods. Lastly, we discuss our recent CVPR 2024 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 23.3 mAP!",https://neurips.cc//virtual/2024/poster/97860,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_22b2067b,
 author = {Madan, Anish and Peri, Neehar and Kong, Shu and Ramanan, Deva},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {19547--19560},
 publisher = {Curran Associates, Inc.},
 title = {Revisiting Few-Shot Object Detection with Vision-Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/22b2067b8f680812624032025864c5a1-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",madanRevisitingFewshotObject2024a
RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models,"Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model’s capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",https://neurips.cc//virtual/2024/poster/97449,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_b1f78dfc,
 author = {jin, Zhuoran and Cao, Pengfei and Wang, Chenhao and He, Zhitao and Yuan, Hongbang and Li, Jiachun and Chen, Yubo and Liu, Kang and Zhao, Jun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {98213--98263},
 publisher = {Curran Associates, Inc.},
 title = {RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b1f78dfc9ca0156498241012aec4efa0-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",jinRWKUBenchmarkingRealworld2024a
SafeWorld: Geo-Diverse Safety Alignment,"In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlooks the geo-diversity of cultural and legal standards across the world. To reveal the chal5 lenges posed by geo-diverse safety standards, we introduce SafeWorld, a novel benchmark specifically designed to evaluate LLMs’ ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SafeWorld encompasses 2,775 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multi-dimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria effectively. To enhance LLMs’ alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SafeWorldLM outperforms all competing models, including GPT-4o on all the three evaluation dimensions by a large margin. Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation.",https://neurips.cc//virtual/2024/poster/94887,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_e8aad0aa,
 author = {Yin, Da and Qiu, Haoyi and Huang, Kung-Hsiang and Chang, Kai-Wei and Peng, Nanyun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {128734--128768},
 publisher = {Curran Associates, Inc.},
 title = {SafeWorld: Geo-Diverse Safety Alignment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/e8aad0aaa1309659a7d7e4c21202d9d0-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",yinSafeWorldGeodiverseSafety2024
SciCode: A Research Coding Benchmark Curated by Scientists,"Since language models (LMs) now outperform average humans on many challenging tasks, it is becoming increasingly difficult to develop challenging, high-quality, and realistic evaluations. We address this by examining LM capabilities to generate code for solving real scientific research problems. Incorporating input from scientists and AI researchers in 16 diverse natural science sub-fields, including mathematics, physics, chemistry, biology, and materials science, we create a scientist-curated coding benchmark, SciCode. The problems naturally factorize into multiple subproblems, each involving knowledge recall, reasoning, and code synthesis. In total, SciCode contains 338 subproblems decomposed from 80 challenging main problems, and it offers optional descriptions specifying useful scientific background information and scientist-annotated gold-standard solutions and test cases for evaluation. OpenAI o1-preview, the best-performing model among those tested, can solve only 7.7\% of the problems in the most realistic setting. We believe that SciCode demonstrates both contemporary LMs' progress towards realizing helpful scientific assistants and sheds light on the building and evaluation of scientific AI in the future.",https://neurips.cc//virtual/2024/poster/97822,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_36850592,
 author = {Tian, Minyang and Gao, Luyu and Zhang, Shizhuo Dylan and Chen, Xinan and Fan, Cunwei and Guo, Xuefei and Haas, Roland and Ji, Pan and Krongchon, Kittithat and Li, Yao and Liu, Shengyan and Luo, Di and Ma, Yutao and Tong, Hao and Trinh, Kha and Tian, Chenyu and Wang, Zihan and Wu, Bohao and Xiong, Yanyu and Yin, Shengzhu and Zhu, Minhui and Lieret, Kilian and Lu, Yanxin and Liu, Genglin and Du, Yufeng and Tao, Tianhua and Press, Ofir and Callan, Jamie and Huerta, Eliu and Peng, Hao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {30624--30650},
 publisher = {Curran Associates, Inc.},
 title = {SciCode: A Research Coding Benchmark Curated by Scientists},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/36850592258c8c41cecdaa3dea5ff7de-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",tianSciCodeResearchCoding2024
SETLEXSEM CHALLENGE: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models,"Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically-oriented NLP tasks, large language models (LLMs) are now being evaluated on algorithmic tasks. Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs’ algorithmic abilities under simple lexical or semantic variations. To this end, we present the SETLEXSEM CHALLENGE, a synthetic benchmark that evaluates the performance of LLMs on set operations. SETLEXSEM assesses the robustness of LLMs’ instruction-following abilities under various conditions, focusing on the set operations and the nature and construction of the set members. Evaluating seven LLMs with SETLEXSEM, we find that they exhibit poor robust- ness to variation in both operation and operands. We show – via the framework’s systematic sampling of set members along lexical and semantic dimensions – that LLMs are not only not robust to variation along these dimensions but demonstrate unique failure modes in particular, easy-to-create semantic groupings of ""deceptive"" sets. We find that rigorously measuring language model robustness to variation in frequency and length is challenging and present an analysis that measures them in- dependently. The code for reproducing the results of this paper, and for generating the SETLEXSEM CHALLENGE dataset, is available https://github.com/amazon-science/SetLexSem-Challenge.",https://neurips.cc//virtual/2024/poster/97730,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_5a12909f,
 author = {Akhbari, Bardiya and Gawali, Manish and Dronen, Nicholas A.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {50381--50400},
 publisher = {Curran Associates, Inc.},
 title = {SETLEXSEM CHALLENGE: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5a12909ffd7145c41139ad66ecf20fc0-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",akhbariSETLEXSEMCHALLENGEUsing2024
SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types,"Ensuring the safety of large language model (LLM) applications is essential for developing trustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. First, they focus solely on either discriminative or generative evaluation paradigms while ignoring their interconnection. Second, they rely on standardized inputs, overlooking the effects of widespread prompting techniques, such as system prompts, few-shot demonstrations, and chain-of-thought prompting. To overcome these issues, we developed SG-Bench, a novel benchmark to assess the generalization of LLM safety across various tasks and prompt types. This benchmark integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak on LLM safety. Our assessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the benchmark reveals that most LLMs perform worse on discriminative tasks than generative ones, and are highly susceptible to prompts, indicating poor generalization in safety alignment. We also explain these findings quantitatively and qualitatively to provide insights for future research.",https://neurips.cc//virtual/2024/poster/97610,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_de7b9910,
 author = {Mou, Yutao and Zhang, Shikun and Ye, Wei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {123032--123054},
 publisher = {Curran Associates, Inc.},
 title = {SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/de7b99107c53e60257c727dc73daf1d1-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",mouSGbenchEvaluatingLLM2024
Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models,"Online shopping is a complex multi-task, few-shot learning problem with a wide and evolving range of entities, relations, and tasks. However, existing models and benchmarks are commonly tailored to specific tasks, falling short of capturing the full complexity of online shopping. Large Language Models (LLMs), with their multi-task and few-shot learning abilities, have the potential to profoundly transform online shopping by alleviating task-specific engineering efforts and by providing users with interactive conversations. Despite the potential, LLMs face unique challenges in online shopping, such as domain-specific concepts, implicit knowledge, and heterogeneous user behaviors. Motivated by the potential and challenges, we propose Shopping MMLU, a diverse multi-task online shopping benchmark derived from real-world Amazon data. Shopping MMLU consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality, and can thus comprehensively evaluate the abilities of LLMs as general shop assistants. With Shoppping MMLU, we benchmark over 20 existing LLMs and uncover valuable insights about practices and prospects of building versatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at https://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we are hosting a competition in KDD Cup 2024 with over 500 participating teams. The winning solutions and the associated workshop can be accessed at our website https://amazon-kddcup24.github.io/.",https://neurips.cc//virtual/2024/poster/97808,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_2049d75d,
 author = {Jin, Yilun and Li, Zheng and Zhang, Chenwei and Cao, Tianyu and Gao, Yifan and Jayarao, Pratik and Li, Mao and Liu, Xin and Sarkhel, Ritesh and Tang, Xianfeng and Wang, Haodong and Wang, Zhengyang and Xu, Wenju and Yang, Jingfeng and Yin, Qingyu and Li, Xian and Nigam, Priyanka and Xu, Yi and Chen, Kai and Yang, Qiang and Jiang, Meng and Yin, Bing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {18062--18089},
 publisher = {Curran Associates, Inc.},
 title = {Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/2049d75dd13db049897562bcf7d59da8-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",jinShoppingMMLUMassive2024
SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark,"Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far.In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy---a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL.We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs.Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last,SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases.",https://neurips.cc//virtual/2024/poster/97708,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_a182a8e6,
 author = {Sivasubramaniam, Sithursan and Osei-Akoto, Cedric and Zhang, Yi and Stockinger, Kurt and F\""{u}rst, Jonathan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {88627--88663},
 publisher = {Curran Associates, Inc.},
 title = {SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a182a8e6ebc91728b6e6b6382c9f7b1e-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",sivasubramaniamSM3texttoquerySyntheticMultimodel2024a
Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?,"Data science and engineering workflows often span multiple stages, from warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As vision language models (VLMs) advance in multimodal understanding and code generation, VLM-based agents could potentially automate these workflows by generating SQL queries, Python code, and GUI operations. This automation can improve the productivity of experts while democratizing access to large-scale data analysis. In this paper, we introduce Spider2-V, the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications. These tasks, derived from real-world use cases, evaluate the ability of a multimodal agent to perform data-related tasks by writing code and managing the GUI in enterprise data software systems. To balance realistic simulation with evaluation simplicity, we devote significant effort to developing automatic configurations for task setup and carefully crafting evaluation metrics for each task. Furthermore, we supplement multimodal agents with comprehensive documents of these enterprise data software systems. Our empirical evaluation reveals that existing state-of-the-art LLM/VLM-based agents do not reliably automate full data workflows (14.0% success). Even with step-by-step guidance, these agents still underperform in tasks that require fine-grained, knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted workspaces (10.6%). We hope that Spider2-V paves the way for autonomous multimodal agents to transform the automation of data science and engineering workflow. Our code and data are available at https://spider2-v.github.io.",https://neurips.cc//virtual/2024/poster/97692,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_c2f71567,
 author = {Cao, Ruisheng and Lei, Fangyu and Wu, Haoyuan and Chen, Jixuan and Fu, Yeqiao and Gao, Hongcheng and Xiong, Xinzhuang and Zhang, Hanchong and Mao, Yuchen and Hu, Wenjing and Xie, Tianbao and Xu, Hongshen and Zhang, Danyang and Wang, Sida and Sun, Ruoxi and Yin, Pengcheng and Xiong, Caiming and Ni, Ansong and Liu, Qian and Zhong, Victor and Chen, Lu and Yu, Kai and Yu, Tao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {107703--107744},
 publisher = {Curran Associates, Inc.},
 title = {Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c2f71567cd53464161cab3336e8fc865-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",caoSpider2vHowFar2024
SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation,"We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values.Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty.",https://neurips.cc//virtual/2024/poster/97753,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_ac840df2,
 author = {Ma, Zeyao and Zhang, Bohan and Zhang, Jing and Yu, Jifan and Zhang, Xiaokang and Zhang, Xiaohan and Luo, Sijia and Wang, Xi and Tang, Jie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {94871--94908},
 publisher = {Curran Associates, Inc.},
 title = {SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/ac840df270ac537dd74530a15c332684-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",maSpreadsheetBenchChallengingReal2024
StackEval: Benchmarking LLMs in Coding Assistance,"We present two comprehensive benchmarks to evaluate the performance of language models in coding assistance tasks, covering code writing, debugging, code review, and conceptual understanding. Our main contribution includes two curated datasets: StackEval, a large-scale benchmark derived from Stack Overflow questions, and StackUnseen, a dynamic benchmark featuring the most recent Stack Overflow content. These benchmarks offer novel insights into the capabilities and limitations of LLMs, particularly in handling new and emerging content. Additionally, we assess LLMs' proficiency as judges for coding tasks using a curated, human-annotated dataset, exploring their evaluation capabilities and potential biases, including whether they favor their own generated solutions. Our findings underscore the potential of these benchmarks to advance LLM development and application in coding assistance. To ensure reproducibility, we publicly share our datasets and evaluation code at https://github.com/ProsusAI/stack-eval.",https://neurips.cc//virtual/2024/poster/97856,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_4126a607,
 author = {Shah, Nidhish and Genc, Zulkuf and Araci, Dogu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {36976--36994},
 publisher = {Curran Associates, Inc.},
 title = {StackEval: Benchmarking LLMs in Coding Assistance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4126a607bbe2836cb6ca0eb45b75618b-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",shahStackEvalBenchmarkingLlms2024a
STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases,"Answering real-world complex queries, such as complex product search, often requires accurate retrieval from semi-structured knowledge bases that involve blend of unstructured (e.g., textual descriptions of products) and structured (e.g., entity relations of products) information. However, many previous works studied textual and relational retrieval tasks as separate topics. To address the gap, we develop STARK, a large-scale Semi-structure retrieval benchmark on Textual and Relational Knowledge Bases. Our benchmark covers three domains: product search, academic paper search, and queries in precision medicine. We design a novel pipeline to synthesize realistic user queries that integrate diverse relational information and complex textual properties, together with their ground-truth answers (items). We conduct rigorous human evaluation to validate the quality of our synthesized queries. We further enhance the benchmark with high-quality human-generated queries to provide an authentic reference. STARK serves as a comprehensive testbed for evaluating the performance of retrieval systems driven by large language models (LLMs). Our experiments suggest that STARK presents significant challenges to the current retrieval and LLM systems, highlighting the need for more capable semi-structured retrieval systems.",https://neurips.cc//virtual/2024/poster/97698,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_e607b141,
 author = {Wu, Shirley and Zhao, Shiyu and Yasunaga, Michihiro and Huang, Kexin and Cao, Kaidi and Huang, Qian and Ioannidis, Vassilis N. and Subbian, Karthik and Zou, James and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {127129--127153},
 publisher = {Curran Associates, Inc.},
 title = {STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/e607b1419e9ae7cd5cb5b5bb60c2ad5c-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wuSTaRKBenchmarkingLLM2024a
StreamBench: Towards Benchmarking Continuous Improvement of Language Agents,"Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios.",https://neurips.cc//virtual/2024/poster/97831,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_c1899153,
 author = {Wu, Cheng-Kuang and Tam, Zhi Rui and Lin, Chieh-Yen and Chen, Yun-Nung and Lee, Hung-yi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {107039--107063},
 publisher = {Curran Associates, Inc.},
 title = {StreamBench: Towards Benchmarking Continuous Improvement of Language Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c189915371c4474fe9789be3728113fc-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",wuStreamBenchBenchmarkingContinuous2024
Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack,"We introduce Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn a sequence of language tasks through in-context learning (ICL). We further introduce Task Haystack, an evaluation suite dedicated to assessing and diagnosing how long-context LMs utilizes contexts in Lifelong ICL. When given a task instruction and test inputs, long-context LMs are expectedto leverage the relevant demonstrations in the Lifelong ICL prompt, avoid distraction and interference from other tasks, and achieve test accuracies that are not significantly worse than those of the Single-task ICL baseline.Task Haystack draws inspiration from the widely-adopted “needle-in-a-haystack” (NIAH) evaluation, but presents distinct new challenges. It requires models (1) to utilize the contexts at a deeper level, rather than resorting to simple copying and pasting; (2) to navigate through long streams of evolving topics and tasks, proxying the complexities and dynamism of contexts in real-world scenarios. Additionally, Task Haystack inherits the controllability of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively.We benchmark 14 long-context LMs using Task Haystack, finding that frontier models like GPT-4o still struggle with the setting, failing on 15% of cases on average. Most open-weight models further lack behind by a large margin, with failure rates reaching up to 61%. In our controlled analysis, we identify factors such as distraction and recency bias as contributors to these failure cases. Further, performance declines when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of long-context LMs. We release our code and data to encourage future research that investigates and addresses these limitations.",https://neurips.cc//virtual/2024/poster/97545,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_1cc8db58,
 author = {Xu, Xiaoyue and Ye, Qinyuan and Ren, Xiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {15801--15840},
 publisher = {Curran Associates, Inc.},
 title = {Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1cc8db5884a7474b4771762b6f0c8ee1-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",xuStresstestingLongcontextLanguage2024
SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations,"Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly to object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We demonstrate that models excelling on compositionality datasets may not perform equally well on SUGARCREPE++. This indicates that compositionality alone might not be sufficient to fully understand semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community. Data and code is available at https://github.com/Sri-Harsha/scpp.",https://neurips.cc//virtual/2024/poster/97833,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_200661bf,
 author = {Dumpala, Sri Harsha and Jaiswal, Aman and Sastry, Chandramouli and Milios, Evangelos and Oore, Sageev and Sajjad, Hassan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {17972--18018},
 publisher = {Curran Associates, Inc.},
 title = {SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/200661bf8f4993b7828a45a2a90f2ecf-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",dumpalaSUGARCREPEDatasetVisionlanguage2024
SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents,"Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests.  We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent. We release all data and code at https://github.com/logic-star-ai/SWT-Bench.",https://neurips.cc//virtual/2024/poster/96304,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_94f093b4,
 author = {M\""{u}ndler, Niels and M\""{u}ller, Mark Niklas and He, Jingxuan and Vechev, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {81857--81887},
 publisher = {Curran Associates, Inc.},
 title = {SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/94f093b41fc2666376fb1f667fe282f3-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",mundlerSWTbenchTestingValidating2024a
TaskBench: Benchmarking Large Language Models for Task Automation,"In recent years, the remarkable progress of large language models (LLMs) has sparked interest in task automation, which involves decomposing complex tasks described by user instructions into sub-tasks and invoking external tools to execute them, playing a central role in autonomous agents. However, there is a lack of systematic and standardized benchmarks to promote the development of LLMs in task automation. To address this, we introduce TaskBench, a comprehensive framework to evaluate the capability of LLMs in task automation. Specifically, task automation can be divided into three critical stages: task decomposition, tool selection, and parameter prediction. To tackle the complexities inherent in these stages, we introduce the concept of Tool Graph to represent decomposed tasks and adopt a back-instruct method to generate high-quality user instructions. We propose TaskEval, a multi-faceted evaluation methodology that assesses LLM performance across these three stages. Our approach combines automated construction with rigorous human verification, ensuring high consistency with human evaluation. Experimental results demonstrate that TaskBench effectively reflects the capabilities of various LLMs in task automation. It provides insights into model performance across different task complexities and domains, pushing the boundaries of what current models can achieve. TaskBench offers a scalable, adaptable, and reliable benchmark for advancing LLM-based autonomous agents.",https://neurips.cc//virtual/2024/poster/97613,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_085185ea,
 author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {4540--4574},
 publisher = {Curran Associates, Inc.},
 title = {TaskBench: Benchmarking Large Language Models for Task Automation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/085185ea97db31ae6dcac7497616fd3e-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",shenTaskBenchBenchmarkingLarge2024
Task Me Anything,"Benchmarks for large multimodal language models (MLMs) now serve to simultaneously assess the general capabilities of models instead of evaluating for a specific capability. As a result, when a developer wants to identify which models to use for their application, they are overwhelmed by the number of benchmarks and remain uncertain about which benchmark's results are most reflective of their specific use case. This paper introduces Task-Me-Anything, a benchmark generation engine which produces a benchmark tailored to a user's needs. Task-Me-Anything maintains an extendable taxonomy of visual assets and can programmatically generate a vast number of task instances. Additionally, it algorithmically addresses user queries regarding MLM performance efficiently within a computational budget. It contains 113K images, 10K videos, 2K 3D object assets, over 365 object categories, 655 attributes, and 335 relationships. It can generate 500M image/video question-answering pairs, which focus on evaluating MLM perceptual capabilities. Task-Me-Anything reveals critical insights: open-source MLMs excel in object and attribute recognition but lack spatial and temporal understanding; each model exhibits unique strengths and weaknesses; larger models generally perform better, though exceptions exist; and GPT4O demonstrates challenges in recognizing rotating/moving objects and distinguishing colors.",https://neurips.cc//virtual/2024/poster/97494,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_237ffa9a,
 author = {Zhang, Jieyu and Huang, Weikai and Ma, Zixian and Michel, Oscar and He, Dong and Gupta, Tanmay and Ma, Wei-Chiu and Farhadi, Ali and Kembhavi, Aniruddha and Krishna, Ranjay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {19965--19974},
 publisher = {Curran Associates, Inc.},
 title = {Task Me Anything},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/237ffa9a473eff1c66d085dba7f813ba-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",zhangTaskMeAnything2024
TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs,"Text-Attributed Graphs (TAGs) augment graph structures with natural language descriptions, facilitating detailed depictions of data and their interconnections across various real-world settings. However, existing TAG datasets predominantly feature textual information only at the nodes, with edges typically represented by mere binary or categorical attributes. This lack of rich textual edge annotations significantly limits the exploration of contextual relationships between entities, hindering deeper insights into graph-structured data. To address this gap, we introduce Textual-Edge Graphs Datasets and Benchmark (TEG-DB), a comprehensive and diverse collection of benchmark textual-edge datasets featuring rich textual descriptions on nodes and edges. The TEG-DB datasets are large-scale and encompass a wide range of domains, from citation networks to social networks. In addition, we conduct extensive benchmark experiments on TEG-DB to assess the extent to which current techniques, including pre-trained language models, graph neural networks, and their combinations, can utilize textual node and edge information. Our goal is to elicit advancements in   textual-edge graph research, specifically in developing methodologies that exploit rich textual node and edge descriptions to enhance graph analysis and provide deeper insights into complex real-world networks. The entire TEG-DB project is publicly accessible as an open-source repository on Github, accessible at https://github.com/Zhuofeng-Li/TEG-Benchmark.",https://neurips.cc//virtual/2024/poster/97714,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_7054d2c4,
 author = {Li, Zhuofeng and Gou, Zixing and Zhang, Xiangnan and Liu, Zhongyuan and Li, Sirui and Hu, Yuntong and Ling, Chen and Zhang, Zheng and Zhao, Liang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {60980--60998},
 publisher = {Curran Associates, Inc.},
 title = {TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/7054d2c49863c1c41be1d53f4377b82a-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liTEGDBComprehensiveDataset2024
Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation,"Program verification is vital for ensuring software reliability, especially in the context of increasingly complex systems. Loop invariants, remaining true before and after each iteration of loops, are crucial for this verification process. Traditional provers and machine learning based methods for generating loop invariants often require expert intervention or extensive labeled data, and typically only handle numerical property verification. These methods struggle with programs involving complex data structures and memory manipulations, limiting their applicability and automation capabilities. This paper introduces a new benchmark named LIG-MM, specifically for programs with complex data structures and memory manipulations. We collect 312 programs from various sources, including daily programs from college homework, the international competition (SV-COMP), benchmarks from previous papers (SLING), and programs from real-world software systems (Linux Kernel, GlibC, LiteOS, and Zephyr). Based on LIG-MM, our findings indicate that previous methods, including GPT-4, fail to automate verification for these programs. Consequently, we propose a novel LLM-SE framework that coordinates LLM with symbolic execution, fine-tuned using self-supervised learning, to generate loop invariants. Experimental results on LIG-MM demonstrate that our LLM-SE outperforms state-of-the-art methods, offering a new direction toward automated program verification in real-world scenarios.",https://neurips.cc//virtual/2024/poster/97660,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_e93b673c,
 author = {Liu, Chang and Wu, Xiwei and Feng, Yuan and Cao, Qinxiang and Yan, Junchi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {129120--129145},
 publisher = {Curran Associates, Inc.},
 title = {Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/e93b673c55d6768cdd39ce90de8c4d4c-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liuGeneralLoopInvariant2024
UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-World Document Analysis,"The use of Retrieval-Augmented Generation (RAG) has improved Large Language Models (LLMs) in collaborating with external data, yet significant challenges exist in real-world scenarios. In areas such as academic literature and finance question answering, data are often found in raw text and tables in HTML or PDF formats, which can be lengthy and highly unstructured. In this paper, we introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We revisit popular LLM- and RAG-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. Our evaluation yields interesting findings and highlights the importance of data parsing and retrieval.  We hope our benchmark can shed light and better serve real-world document analysis applications. The benchmark suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark",https://neurips.cc//virtual/2024/poster/97735,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_7c06759d,
 author = {Hui, Yulong and Lu, Yao and Zhang, Huanchen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {67200--67217},
 publisher = {Curran Associates, Inc.},
 title = {UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-World Document Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/7c06759d1a8567f087b02e8589454917-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",huiUDABenchmarkSuite2024a
Unveiling the Tapestry of Consistency in Large Vision-Language Models,"Large vision-language models (LVLMs) have recently achieved rapid progress, exhibiting great perception and reasoning abilities concerning visual information. However, when faced with prompts in different sizes of solution spaces, LVLMs fail to always give consistent answers regarding the same knowledge point. This inconsistency of answers between different solution spaces is prevalent in LVLMs and erodes trust. To this end, we provide a multi-modal benchmark ConBench, to intuitively analyze how LVLMs perform when the solution space of a prompt revolves around a knowledge point. Based on the ConBench tool, we are the first to reveal the tapestry and get the following findings: (1) In the discriminate realm, the larger the solution space of the prompt, the lower the accuracy of the answers. (2) Establish the relationship between the discriminative and generative realms: the accuracy of the discriminative question type exhibits a strong positive correlation with its Consistency with the caption. (3) Compared to open-source models, closed-source models exhibit a pronounced bias advantage in terms of Consistency. Eventually, we ameliorate the consistency of LVLMs by trigger-based diagnostic refinement, indirectly improving the performance of their caption. We hope this paper will accelerate the research community in better evaluating their models and encourage future advancements in the consistency domain.",https://neurips.cc//virtual/2024/poster/93307,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_d6f094ba,
 author = {Zhang, Yuan and Xiao, Fei and Huang, Tao and Fan, Chun-Kai and Dong, Hongyuan and Li, Jiawen and Wang, Jiacong and Cheng, Kuan and Zhang, Shanghang and Guo, Haoyuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {118632--118653},
 publisher = {Curran Associates, Inc.},
 title = {Unveiling the Tapestry of Consistency in Large Vision-Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/d6f094ba0f5ce1720466342f78031bdb-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",zhangUnveilingTapestryConsistency2024
VHELM: A Holistic Evaluation of Vision Language Models,"Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: *visual perception*, *knowledge*, *reasoning*, *bias*, *fairness*, *multilinguality*, *robustness*, *toxicity*, and *safety*. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website at https://crfm.stanford.edu/helm/vhelm/v2.0.1. VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.",https://neurips.cc//virtual/2024/poster/97677,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_fe2fc7dc,
 author = {Lee, Tony and Tu, Haoqin and Wong, Chi Heem and Zheng, Wenhao and Zhou, Yiyang and Mai, Yifan and Roberts, Josselin Somerville and Yasunaga, Michihiro and Yao, Huaxiu and Xie, Cihang and Liang, Percy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {140632--140666},
 publisher = {Curran Associates, Inc.},
 title = {VHELM: A Holistic Evaluation of Vision Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/fe2fc7dc60b55ccd8886220b40fb1f74-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",leeVHELMHolisticEvaluation2024
Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models,"Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the person’s discomfort, thereby alleviating the need for further information. This example illustrates how subtle visual cues can challenge our cognitive skills and demonstrates the complexity of interpreting visual scenarios. To study these skills, we present Visual Riddles, a benchmark aimed to test vision and language models on visual riddles requiring commonsense and world knowledge. The benchmark comprises 400 visual riddles, each featuring a unique image created by a variety of text-to-image models, question, ground-truth answer, textual hint, and attribution. Human evaluation reveals that existing models lag significantly behind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading with 40% accuracy. Our benchmark comes with automatic evaluation tasks to make assessment scalable. These findings underscore the potential of Visual Riddles as a valuable resource for enhancing vision and language models’ capabilities in interpreting complex visual scenarios. Data, code, and leaderboard are available at https://visual-riddles.github.io/.",https://neurips.cc//virtual/2024/poster/97561,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_fbf5efe9,
 author = {Bitton-Guetta, Nitzan and Slobodkin, Aviv and Maimon, Aviya and Habba, Eliya and Rassin, Royi and Bitton, Yonatan and Szpektor, Idan and Globerson, Amir and Elovici, Yuval},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {139561--139588},
 publisher = {Curran Associates, Inc.},
 title = {Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/fbf5efe979e6754dc06a0869233f2510-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",bitton-guettaVisualRiddlesCommonsense2024
VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark,"Recently, knowledge editing on large language models (LLMs) has received considerable attention. Compared to this, editing Large Vision-Language Models (LVLMs) faces extra challenges from diverse data modalities and complicated model components, and data for LVLMs editing are limited. The existing LVLM editing benchmark, which comprises three metrics (Reliability, Locality, and Generality), falls short in the quality of synthesized evaluation images and cannot assess whether models apply edited knowledge in relevant content. Therefore, we employ more reliable data collection methods to construct a new Large $\textbf{V}$ision-$\textbf{L}$anguage Model $\textbf{K}$nowledge $\textbf{E}$diting $\textbf{B}$enchmark, $\textbf{VLKEB}$, and extend the Portability metric for more comprehensive evaluation. Leveraging a multi-modal knowledge graph, our image data are bound with knowledge entities. This can be further used to extract entity-related knowledge, which constitutes the base of editing data. We conduct experiments of different editing methods on five LVLMs, and thoroughly analyze how do they impact the models. The results reveal strengths and deficiencies of these methods and hopefully provide insights for future research. The codes and dataset are available at: https://github.com/VLKEB/VLKEB.",https://neurips.cc//virtual/2024/poster/97679,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_1198b53f,
 author = {Huang, Han and Zhong, Haitian and Yu, Tao and Liu, Qiang and Wu, Shu and Wang, Liang and Tan, Tieniu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {9257--9280},
 publisher = {Curran Associates, Inc.},
 title = {VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1198b53fa686831d5f0c0860d7ec4f34-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",huangVLKEBLargeVisionlanguage2024
VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding,"We introduce a new benchmark designed to advance the development of general-purpose, large-scale vision-language models for remote sensing images. Although several vision-language datasets in remote sensing have been proposed to pursue this goal, existing datasets are typically tailored to single tasks, lack detailed object information, or suffer from inadequate quality control. Exploring these improvement opportunities, we present a Versatile vision-language Benchmark for Remote Sensing image understanding, termed VRSBench. This benchmark comprises 29,614 images, with 29,614 human-verified detailed captions, 52,472 object references, and 123,221 question-answer pairs. It facilitates the training and evaluation of vision-language models across a broad spectrum of remote sensing image understanding tasks. We further evaluated state-of-the-art models on this benchmark for three vision-language tasks: image captioning, visual grounding, and visual question answering. Our work aims to significantly contribute to the development of advanced vision-language models in the field of remote sensing. The data and code can be accessed at https://vrsbench.github.io.",https://neurips.cc//virtual/2024/poster/97530,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_05b7f821,
 author = {Li, Xiang and Ding, Jian and Elhoseiny, Mohamed},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {3229--3242},
 publisher = {Curran Associates, Inc.},
 title = {VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/05b7f821234f66b78f99e7803fffa78a-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liVRSBenchVersatileVisionlanguage2024
Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles,"While advancements in NLP have significantly improved the performance of Large Language Models (LLMs) on tasks requiring vertical thinking, their lateral thinking capabilities remain under-explored and challenging to measure due to the complexity of assessing creative thought processes and the scarcity of relevant data. To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs. This benchmark, containing 975 graded situation puzzles across three difficulty levels, employs a new multi-turn player-judge framework instead of the traditional model-based evaluation, which often necessitates a stronger evaluation model. This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario. The judge answers based on a detailed reference scenario or evaluates if the player's predictions align with the reference one. This approach lessens dependence on more robust evaluation models, enabling the assessment of state-of-the-art LLMs. The experiments demonstrate that a robust evaluation model, such as WizardLM-2, closely matches human judgements in both intermediate question-answering and final scenario accuracy, achieving over 80% agreement--similar to the agreement levels among humans. Furthermore, applying data and reasoning processes from our benchmark to other lateral thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to performance enhancements. This suggests that our benchmark effectively evaluates and elicits the lateral thinking abilities of LLMs.",https://neurips.cc//virtual/2024/poster/94104,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_914b3723,
 author = {Chen, Qi and Zhang, Bowen and Wang, Gang and Wu, Qi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {79642--79665},
 publisher = {Curran Associates, Inc.},
 title = {Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/914b372356d58d9e9357b29332cb8fdc-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",chenWeakevalstrongEvaluatingEliciting2024
WenMind: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts,"Large Language Models (LLMs) have made significant advancements across numerous domains, but their capabilities in Chinese Classical Literature and Language Arts (CCLLA) remain largely unexplored due to the limited scope and tasks of existing benchmarks. To fill this gap, we propose WenMind, a comprehensive benchmark dedicated for evaluating LLMs in CCLLA. WenMind covers the sub-domains of Ancient Prose, Ancient Poetry, and Ancient Literary Culture, comprising 4,875 question-answer pairs, spanning 42 fine-grained tasks, 3 question formats, and 2 evaluation scenarios: domain-oriented and capability-oriented. Based on WenMind, we conduct a thorough evaluation of 31 representative LLMs, including general-purpose models and ancient Chinese LLMs. The results reveal that even the best-performing model, ERNIE-4.0, only achieves a total score of 64.3, indicating significant room for improvement of LLMs in the CCLLA domain. We also provide insights into the strengths and weaknesses of different LLMs and highlight the importance of pre-training data in achieving better results.Overall, WenMind serves as a standardized and comprehensive baseline, providing valuable insights for future CCLLA research. Our benchmark and related code are available at \url{https://github.com/SCUT-DLVCLab/WenMind}.",https://neurips.cc//virtual/2024/poster/97880,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_5c1019b5,
 author = {Cao, Jiahuan and Liu, Yang and Shi, Yongxin and Ding, Kai and Jin, Lianwen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {51358--51410},
 publisher = {Curran Associates, Inc.},
 title = {WenMind: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5c1019b5711474ae5627dc8580614e01-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",caoWenMindComprehensiveBenchmark2024a
What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction,"Vision-language models have shown impressive progress in recent years. However, existing models are largely limited to turn-based interactions, where each turn must be stepped (i.e., prompted) by the user. Open-ended, asynchronous interactions, where an AI model may proactively deliver timely responses or feedback based on the unfolding situation in real-time, are an open challenge. In this work, we present the QEVD benchmark and dataset, which explores human-AI interaction in the challenging, yet controlled, real-world domain of fitness coaching – a task which intrinsically requires monitoring live user activity and providing immediate feedback. The benchmark requires vision-language models to recognize complex human actions, identify possible mistakes, and provide appropriate feedback in real-time. Our experiments reveal the limitations of existing state-of-the-art vision-language models for such asynchronous situated interactions. Motivated by this, we propose a simple end-to-end streaming baseline that can respond asynchronously to human actions with appropriate feedback at the appropriate time.",https://neurips.cc//virtual/2024/poster/97489,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_8aa695b5,
 author = {Panchal, Sunny and Bhattacharyya, Apratim and Berger, Guillaume and Mercier, Antoine and B\""{o}hm, Cornelius and Dietrichkeit, Florian and Pourreza, Reza and Li, Xuanlin and Madan, Pulkit and Lee, Mingu and Todorovich, Mark and Bax, Ingo and Memisevic, Roland},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {75853--75882},
 publisher = {Curran Associates, Inc.},
 title = {What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/8aa695b5d6bf9e958d4e56b22c9b859d-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",panchalWhatSayWhen2024
When LLMs Meet Cunning Texts: A Fallacy Understanding Benchmark for Large Language Models,"Recently, Large Language Models (LLMs) make remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning texts that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning texts that FLUB focuses on mainly consist of the tricky, humorous, and misleading texts collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs' ability to understand fallacies. Our data and codes are available at https://github.com/THUKElab/FLUB.",https://neurips.cc//virtual/2024/poster/97757,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_cbfbf1a9,
 author = {Li, Yinghui and Zhou, Qingyu and Luo, Yuanzhen and Ma, Shirong and Li, Yangning and Zheng, Hai-Tao and Hu, Xuming and Yu, Philip S.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {112433--112458},
 publisher = {Curran Associates, Inc.},
 title = {When LLMs Meet Cunning Texts: A Fallacy Understanding Benchmark for Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/cbfbf1a9adbcc29783475d2767f218e8-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",liWhenLlmsMeet2024
WhodunitBench: Evaluating Large Multimodal Agents via Murder Mystery Games,"Recently, large language models (LLMs) have achieved superior performance, empowering the development of large multimodal agents (LMAs). An LMA is anticipated to execute practical tasks requires various capabilities including multimodal perception, interaction, reasoning, and decision making. However, existing benchmarks are limited in assessing compositional skills and actions demanded by practical scenarios, where they primarily focused on single tasks and static scenarios. To bridge this gap, we introduce WhodunitBench, a benchmark rooted from murder mystery games, where players are required to utilize the aforementioned skills to achieve their objective (i.e., identifying the `murderer' or hiding themselves), providing a simulated dynamic environment for evaluating LMAs. Specifically, WhodunitBench includes two evaluation modes. The first mode, the arena-style evaluation, is constructed from 50 meticulously curated scripts featuring clear reasoning clues and distinct murderers; The second mode, the chain of evaluation, consists of over 3000 curated multiple-choice questions and open-ended questions, aiming to assess every facet of the murder mystery games for LMAs. Experiments show that although current LMAs show acceptable performance in basic perceptual tasks, they are insufficiently equipped for complex multi-agent collaboration and multi-step reasoning tasks. Furthermore, the full application of the theory of mind to complete games in a manner akin to human behavior remains a significant challenge. We hope this work can illuminate the path forward, providing a solid foundation for the future development of LMAs. Our WhodunitBench is open-source and accessible at: https://github.com/jun0wanan/WhodunitBench-Murder_Mystery_Games",https://neurips.cc//virtual/2024/poster/97492,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_9dd4533e,
 author = {Xie, Junlin and Zhang, Ruifei and Chen, Zhihong and Wan, Xiang and Li, Guanbin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {86655--86687},
 publisher = {Curran Associates, Inc.},
 title = {WhodunitBench: Evaluating Large Multimodal Agents via Murder Mystery Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/9dd4533e7e4e5ed809344280609c5b05-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",xieWhodunitBenchEvaluatingLarge2024
Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2),"With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their prompt faithfulness---the semantic coherence of generated images to the prompts they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs). However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines. We introduce T2IScoreScore, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images. These allow us to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate  between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple (and supposedly worse) feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.",https://neurips.cc//virtual/2024/poster/95132,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_9b9cfd54,
 author = {Saxon, Michael and Jahara, Fatima and Khoshnoodi, Mahsa and Lu, Yujie and Sharma, Aditya and Wang, William Yang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {85630--85657},
 publisher = {Curran Associates, Inc.},
 title = {Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/9b9cfd5428153ccfbd4ba34b7e007305-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}",saxonWhoEvaluatesEvaluations2024
WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia,"Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances designed to assess the performance of LLMs in providing a complete perspective on conflicts from the retrieved documents, rather than choosing one answer over another, when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single  passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, wealso introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances.",https://neurips.cc//virtual/2024/poster/97844,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_c6381975,
 author = {Hou, Yufang and Pascale, Alessandra and Carnerero-Cano, Javier and Tchrakian, Tigran and Marinescu, Radu and Daly, Elizabeth and Padhi, Inkit and Sattigeri, Prasanna},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {109701--109747},
 publisher = {Curran Associates, Inc.},
 title = {WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c63819755591ea972f8570beffca6b1b-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",houWikiContradictBenchmarkEvaluating2024a
WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models,"Cross-modal (image-to-text and text-to-image) retrieval is an established task used in evaluation benchmarks to test the performance of vision-language models (VLMs). Several state-of-the-art VLMs (e.g. CLIP, BLIP-2) have achieved near-perfect performance on widely-used image-text retrieval benchmarks such as MSCOCO-Test-5K and Flickr30K-Test-1K. As a measure of out-of-distribution (OOD) generalization, prior works rely on zero-shot performance evaluated on one dataset (Flickr) using a VLM finetuned on another one (MSCOCO). We argue that such comparisons are insufficient to assess the OOD generalization capability of models due to high visual and linguistic similarity between the evaluation and finetuning datasets. To address this gap, we introduce WikiDO (drawn from Wikipedia Diversity Observatory), a novel cross-modal retrieval benchmark to assess the OOD generalization capabilities of pretrained VLMs. This consists of newly scraped 380K image-text pairs from Wikipedia with domain labels, a carefully curated, human-verified a)in-distribution (ID) test set (3K) and b) OOD test set (3K). The image-text pairs are very diverse in topics and geographical locations. We evaluate different VLMs of varying capacity on the \wikido benchmark; BLIP-2 achieves zero-shot performance of $R@1\approx66\%$ on the OOD test set, compared to $\approx$ $81\%$ on COCO and $\approx95\%$ on Flickr. When fine-tuned on WikiDO, the $R@1$ improvement is at most $\approx5\%$ on OOD instances compared to $\approx12\%$ on ID instances. We probe the VLMs with varying finetuning objectives and datasets of varying sizes to identify what aids OOD generalization the most. Our results confirm that WikiDO offers a strong cross-modal benchmark for current VLMs in specifically evaluating for OOD generalization. Our benchmark is hosted as a competition at https://kaggle.com/competitions/wikido24 with public access to dataset and code.",https://neurips.cc//virtual/2024/poster/97785,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_fe759454,
 author = {Kalyan, T Pavan and Pasi, Piyush Singh and Dharod, Sahil Nilesh and Motiwala, Azeem Azaz and Jyothi, Preethi and Chaudhary, Aditi and Srinivasan, Krishna},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {140812--140827},
 publisher = {Curran Associates, Inc.},
 title = {WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/fe759454e97d56d3aea73a1512364d5f-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",kalyanWikiDONewBenchmark2024a
WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences,"Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar.Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.",https://neurips.cc//virtual/2024/poster/97560,2024,NeurIPS,Yes,Multimodal,Benchmark,"@inproceedings{NEURIPS2024_563991b5,
 author = {Lu, Yujie and Jiang, Dongfu and Chen, Wenhu and Wang, William Yang and Choi, Yejin and Lin, Bill Yuchen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {48224--48255},
 publisher = {Curran Associates, Inc.},
 title = {WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/563991b5c8b45fe75bea42db738223b2-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",luWildVisionEvaluatingVisionlanguage2024
WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks,"The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress towards capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena.",https://neurips.cc//virtual/2024/poster/97713,2024,NeurIPS,Yes,Language,Benchmark,"@inproceedings{NEURIPS2024_0b82662b,
 author = {Boisvert, L\'{e}o and Thakkar, Megh and Gasse, Maxime and Caccia, Massimo and De Chezelles, Thibault Le Sellier and Cappart, Quentin and Chapados, Nicolas and Lacoste, Alexandre and Drouin, Alexandre},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {5996--6051},
 publisher = {Curran Associates, Inc.},
 title = {WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0b82662b6c32e887bb252a74d8cb2d5e-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}",boisvertWorkArenaCompositionalPlanning2024
CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",https://aclanthology.org/2020.emnlp-main.154,2020,emnlp-main,Yes,Language,Benchmark,"@inproceedings{nangia-etal-2020-crows,
    title = ""{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"",
    author = ""Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R."",
    editor = ""Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.emnlp-main.154/"",
    doi = ""10.18653/v1/2020.emnlp-main.154"",
    pages = ""1953--1967"",
    abstract = ""Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.""
}",nangiaCrowSpairsChallengeDataset2020
Investigating representations of verb bias in neural language models,"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker’s choice of construction is known to depend on multiple factors, including the choice of main verb – a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.",https://aclanthology.org/2020.emnlp-main.376,2020,emnlp-main,Yes,Language,Benchmark,"@inproceedings{hawkins-etal-2020-investigating,
    title = ""Investigating representations of verb bias in neural language models"",
    author = ""Hawkins, Robert  and
      Yamakoshi, Takateru  and
      Griffiths, Thomas  and
      Goldberg, Adele"",
    editor = ""Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.emnlp-main.376/"",
    doi = ""10.18653/v1/2020.emnlp-main.376"",
    pages = ""4653--4663"",
    abstract = ""Languages typically provide more than one grammatical construction to express certain types of messages. A speaker`s choice of construction is known to depend on multiple factors, including the choice of main verb {--} a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.""
}",hawkinsInvestigatingRepresentationsVerb2020
RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark,"In this paper, we introduce an advanced Russian general language understanding evaluation benchmark – Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills - detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the Russian language. We also provide baselines, human level evaluation, open-source framework for evaluating models, and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art models independently of language.",https://aclanthology.org/2020.emnlp-main.381,2020,emnlp-main,Yes,Language,Benchmark,"@inproceedings{shavrina-etal-2020-russiansuperglue,
    title = ""{R}ussian{S}uper{GLUE}: A {R}ussian Language Understanding Evaluation Benchmark"",
    author = ""Shavrina, Tatiana  and
      Fenogenova, Alena  and
      Anton, Emelyanov  and
      Shevelev, Denis  and
      Artemova, Ekaterina  and
      Malykh, Valentin  and
      Mikhailov, Vladislav  and
      Tikhonova, Maria  and
      Chertok, Andrey  and
      Evlampiev, Andrey"",
    editor = ""Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.emnlp-main.381/"",
    doi = ""10.18653/v1/2020.emnlp-main.381"",
    pages = ""4717--4726"",
    abstract = ""In this paper, we introduce an advanced Russian general language understanding evaluation benchmark {--} Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills - detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the Russian language. We also provide baselines, human level evaluation, open-source framework for evaluating models, and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art models independently of language.""
}",shavrinaRussianSuperGLUERussianLanguage2020
X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as “Punta Cana is located in _.” However, while knowledge is both written and queried in many languages, studies on LMs’ factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.",https://aclanthology.org/2020.emnlp-main.479,2020,emnlp-main,Yes,Language,Benchmark,"@inproceedings{jiang-etal-2020-x,
    title = ""{X}-{FACTR}: Multilingual Factual Knowledge Retrieval from Pretrained Language Models"",
    author = ""Jiang, Zhengbao  and
      Anastasopoulos, Antonios  and
      Araki, Jun  and
      Ding, Haibo  and
      Neubig, Graham"",
    editor = ""Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.emnlp-main.479/"",
    doi = ""10.18653/v1/2020.emnlp-main.479"",
    pages = ""5943--5959"",
    abstract = ""Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as {\textquotedblleft}Punta Cana is located in {\_}.{\textquotedblright} However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at \url{https://x-factr.github.io}.""
}",jiangXFACTRMultilingualFactual2020a
Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,"Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.",https://aclanthology.org/2020.emnlp-main.680,2020,emnlp-main,Yes,Language,Benchmark,"@inproceedings{flachs-etal-2020-grammatical,
    title = ""Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses"",
    author = ""Flachs, Simon  and
      Lacroix, Oph{\'e}lie  and
      Yannakoudakis, Helen  and
      Rei, Marek  and
      S{\o}gaard, Anders"",
    editor = ""Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.emnlp-main.680/"",
    doi = ""10.18653/v1/2020.emnlp-main.680"",
    pages = ""8467--8478"",
    abstract = ""Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.""
}",flachsGrammaticalErrorCorrection2020
TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models,"In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.",https://aclanthology.org/2021.acl-long.469,2021,acl-long,Yes,Language,Benchmark,"@inproceedings{he-etal-2021-tgea,
    title = ""{TGEA}: An Error-Annotated Dataset and Benchmark Tasks for {T}ext{G}eneration from Pretrained Language Models"",
    author = ""He, Jie  and
      Peng, Bo  and
      Liao, Yi  and
      Liu, Qun  and
      Xiong, Deyi"",
    editor = ""Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-long.469/"",
    doi = ""10.18653/v1/2021.acl-long.469"",
    pages = ""6012--6025"",
    abstract = ""In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.""
}",heTGEAErrorannotatedDataset2021a
Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI,"Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability.",https://aclanthology.org/2021.emnlp-main.303,2021,emnlp-main,Yes,Language,Benchmark,"@inproceedings{tian-etal-2021-diagnosing,
    title = ""Diagnosing the First-Order Logical Reasoning Ability Through {L}ogic{NLI}"",
    author = ""Tian, Jidong  and
      Li, Yitian  and
      Chen, Wenqing  and
      Xiao, Liqiang  and
      He, Hao  and
      Jin, Yaohui"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.303/"",
    doi = ""10.18653/v1/2021.emnlp-main.303"",
    pages = ""3738--3747"",
    abstract = ""Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability.""
}",tianDiagnosingFirstorderLogical2021
Can Language Models be Biomedical Knowledge Bases?,"Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.",https://aclanthology.org/2021.emnlp-main.388,2021,emnlp-main,Yes,Language,Benchmark,"@inproceedings{sung-etal-2021-language,
    title = ""Can Language Models be Biomedical Knowledge Bases?"",
    author = ""Sung, Mujeen  and
      Lee, Jinhyuk  and
      Yi, Sean  and
      Jeon, Minji  and
      Kim, Sungdong  and
      Kang, Jaewoo"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.388/"",
    doi = ""10.18653/v1/2021.emnlp-main.388"",
    pages = ""4723--4734"",
    abstract = ""Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51{\%} Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.""
}",sungCanLanguageModels2021a
SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations,"Next generation task-oriented dialog systems need to understand conversational contexts with their perceived surroundings, to effectively help users in the real-world multimodal environment. Existing task-oriented dialog datasets aimed towards virtual assistance fall short and do not situate the dialog in the user’s multimodal context. To overcome, we present a new dataset for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which includes 11K task-oriented user<->assistant dialogs (117K utterances) in the shopping domain, grounded in immersive and photo-realistic scenes. The dialogs are collection using a two-phase pipeline: (1) A novel multimodal dialog simulator generates simulated dialog flows, with an emphasis on diversity and richness of interactions, (2) Manual paraphrasing of generating utterances to draw from natural language distribution. We provide an in-depth analysis of the collected dataset, and describe in detail the four main benchmark tasks we propose for SIMMC 2.0. Our baseline model, powered by the state-of-the-art language model, shows promising results, and highlights new challenges and directions for the community to study.",https://aclanthology.org/2021.emnlp-main.401,2021,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{kottur-etal-2021-simmc,
    title = ""{SIMMC} 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations"",
    author = ""Kottur, Satwik  and
      Moon, Seungwhan  and
      Geramifard, Alborz  and
      Damavandi, Babak"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.401/"",
    doi = ""10.18653/v1/2021.emnlp-main.401"",
    pages = ""4903--4912"",
    abstract = ""Next generation task-oriented dialog systems need to understand conversational contexts with their perceived surroundings, to effectively help users in the real-world multimodal environment. Existing task-oriented dialog datasets aimed towards virtual assistance fall short and do not situate the dialog in the user`s multimodal context. To overcome, we present a new dataset for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which includes 11K task-oriented user{\ensuremath{<}}-{\ensuremath{>}}assistant dialogs (117K utterances) in the shopping domain, grounded in immersive and photo-realistic scenes. The dialogs are collection using a two-phase pipeline: (1) A novel multimodal dialog simulator generates simulated dialog flows, with an emphasis on diversity and richness of interactions, (2) Manual paraphrasing of generating utterances to draw from natural language distribution. We provide an in-depth analysis of the collected dataset, and describe in detail the four main benchmark tasks we propose for SIMMC 2.0. Our baseline model, powered by the state-of-the-art language model, shows promising results, and highlights new challenges and directions for the community to study.""
}",kotturSIMMC20Taskoriented2021
RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms,"Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.",https://aclanthology.org/2021.emnlp-main.598,2021,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zhou-etal-2021-rica,
    title = ""{RICA}: Evaluating Robust Inference Capabilities Based on Commonsense Axioms"",
    author = ""Zhou, Pei  and
      Khanna, Rahul  and
      Lee, Seyeon  and
      Lin, Bill Yuchen  and
      Ho, Daniel  and
      Pujara, Jay  and
      Ren, Xiang"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.598/"",
    doi = ""10.18653/v1/2021.emnlp-main.598"",
    pages = ""7560--7579"",
    abstract = ""Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.""
}",zhouRICAEvaluatingRobust2021
SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,"This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs’ capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.",https://aclanthology.org/2021.naacl-main.364,2021,naacl-main,Yes,Language,Benchmark,"@inproceedings{mirzaee-etal-2021-spartqa,
    title = ""{SPARTQA}: A Textual Question Answering Benchmark for Spatial Reasoning"",
    author = ""Mirzaee, Roshanak  and
      Rajaby Faghihi, Hossein  and
      Ning, Qiang  and
      Kordjamshidi, Parisa"",
    editor = ""Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-main.364/"",
    doi = ""10.18653/v1/2021.naacl-main.364"",
    pages = ""4582--4598"",
    abstract = ""This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs' capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.""
}",mirzaeeSPARTQATextualQuestion2021
TruthfulQA: Measuring How Models Mimic Human Falsehoods,"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",https://aclanthology.org/2022.acl-long.229,2022,acl-long,Yes,Language,Benchmark,"@inproceedings{lin-etal-2022-truthfulqa,
    title = ""{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods"",
    author = ""Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.229/"",
    doi = ""10.18653/v1/2022.acl-long.229"",
    pages = ""3214--3252"",
    abstract = ""We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58{\%} of questions, while human performance was 94{\%}. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.""
}",linTruthfulQAMeasuringHow2022
Image Retrieval from Contextual Descriptions,"The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on ImageCoDe.Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that ImageCoDE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.",https://aclanthology.org/2022.acl-long.241,2022,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{krojer-etal-2022-image,
    title = ""Image Retrieval from Contextual Descriptions"",
    author = ""Krojer, Benno  and
      Adlakha, Vaibhav  and
      Vineet, Vibhav  and
      Goyal, Yash  and
      Ponti, Edoardo  and
      Reddy, Siva"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.241/"",
    doi = ""10.18653/v1/2022.acl-long.241"",
    pages = ""3426--3440"",
    abstract = ""The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on ImageCoDe.Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that ImageCoDE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.""
}",krojerImageRetrievalContextual2022
NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,"Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.",https://aclanthology.org/2022.acl-long.246,2022,acl-long,Yes,Language,Benchmark,"@inproceedings{mishra-etal-2022-numglue,
    title = ""{N}um{GLUE}: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"",
    author = ""Mishra, Swaroop  and
      Mitra, Arindam  and
      Varshney, Neeraj  and
      Sachdeva, Bhavdeep  and
      Clark, Peter  and
      Baral, Chitta  and
      Kalyan, Ashwin"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.246/"",
    doi = ""10.18653/v1/2022.acl-long.246"",
    pages = ""3505--3523"",
    abstract = ""Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 {\%}). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 {\%} on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.""
}",mishraNumGLUESuiteFundamental2022a
The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,"Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user’s trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot’s reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models’ implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic",https://aclanthology.org/2022.acl-long.261,2022,acl-long,Yes,Language,Benchmark,"@inproceedings{ziems-etal-2022-moral,
    title = ""The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems"",
    author = ""Ziems, Caleb  and
      Yu, Jane  and
      Wang, Yi-Chia  and
      Halevy, Alon  and
      Yang, Diyi"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.261/"",
    doi = ""10.18653/v1/2022.acl-long.261"",
    pages = ""3755--3773"",
    abstract = ""Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user`s trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot`s reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models' implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see \url{https://github.com/GT-SALT/mic}""
}",ziemsMoralIntegrityCorpus2022
ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding,"While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored. Here, we introduce a high-quality crowdsourced dataset of narratives for employing proverbs in context as a benchmark for abstract language understanding. The dataset provides fine-grained annotation of aligned spans between proverbs and narratives, and contains minimal lexical overlaps between narratives and proverbs, ensuring that models need to go beyond surface-level reasoning to succeed. We explore three tasks: (1) proverb recommendation and alignment prediction, (2) narrative generation for a given proverb and topic, and (3) identifying narratives with similar motifs. Our experiments show that neural language models struggle on these tasks compared to humans, and these tasks pose multiple learning challenges.",https://aclanthology.org/2022.acl-long.276,2022,acl-long,Yes,Language,Benchmark,"@inproceedings{ghosh-srivastava-2022-epic,
    title = ""e{P}i{C}: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding"",
    author = ""Ghosh, Sayan  and
      Srivastava, Shashank"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.276/"",
    doi = ""10.18653/v1/2022.acl-long.276"",
    pages = ""3989--4004"",
    abstract = ""While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored. Here, we introduce a high-quality crowdsourced dataset of narratives for employing proverbs in context as a benchmark for abstract language understanding. The dataset provides fine-grained annotation of aligned spans between proverbs and narratives, and contains minimal lexical overlaps between narratives and proverbs, ensuring that models need to go beyond surface-level reasoning to succeed. We explore three tasks: (1) proverb recommendation and alignment prediction, (2) narrative generation for a given proverb and topic, and (3) identifying narratives with similar motifs. Our experiments show that neural language models struggle on these tasks compared to humans, and these tasks pose multiple learning challenges.""
}",ghoshEPiCEmployingProverbs2022
FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing,"We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.",https://aclanthology.org/2022.acl-long.301,2022,acl-long,Yes,Language,Benchmark,"@inproceedings{chalkidis-etal-2022-fairlex,
    title = ""{F}air{L}ex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing"",
    author = ""Chalkidis, Ilias  and
      Pasini, Tommaso  and
      Zhang, Sheng  and
      Tomada, Letizia  and
      Schwemer, Sebastian  and
      S{\o}gaard, Anders"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.301/"",
    doi = ""10.18653/v1/2022.acl-long.301"",
    pages = ""4389--4406"",
    abstract = ""We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.""
}",chalkidisFairLexMultilingualBenchmark2022
SRL4E – Semantic Role Labeling for Emotions: A Unified Evaluation Framework,"In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause. To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of automatic systems not only to distinguish between emotions, but also to capture their semantic constituents. However, currently available gold datasets are heterogeneous in size, domain, format, splits, emotion categories and role labels, making comparisons across different works difficult and hampering progress in the area. In this paper, we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions (SRL4E), in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme. We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task, hoping to provide the tools to facilitate studies in this complex area.",https://aclanthology.org/2022.acl-long.314,2022,acl-long,Yes,Language,Benchmark,"@inproceedings{campagnano-etal-2022-srl4e,
    title = ""{SRL4E} {--} {S}emantic {R}ole {L}abeling for {E}motions: {A} Unified Evaluation Framework"",
    author = ""Campagnano, Cesare  and
      Conia, Simone  and
      Navigli, Roberto"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.314/"",
    doi = ""10.18653/v1/2022.acl-long.314"",
    pages = ""4586--4601"",
    abstract = ""In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause. To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of automatic systems not only to distinguish between emotions, but also to capture their semantic constituents. However, currently available gold datasets are heterogeneous in size, domain, format, splits, emotion categories and role labels, making comparisons across different works difficult and hampering progress in the area. In this paper, we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions (SRL4E), in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme. We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task, hoping to provide the tools to facilitate studies in this complex area.""
}",campagnanoSRL4ESemanticRole2022
Nibbling at the Hard Core of Word Sense Disambiguation,"With state-of-the-art systems having finally attained estimated human performance, Word Sense Disambiguation (WSD) has now joined the array of Natural Language Processing tasks that have seemingly been solved, thanks to the vast amounts of knowledge encoded into Transformer-based pre-trained language models. And yet, if we look below the surface of raw figures, it is easy to realize that current approaches still make trivial mistakes that a human would never make. In this work, we provide evidence showing why the F1 score metric should not simply be taken at face value and present an exhaustive analysis of the errors that seven of the most representative state-of-the-art systems for English all-words WSD make on traditional evaluation benchmarks. In addition, we produce and release a collection of test sets featuring (a) an amended version of the standard evaluation benchmark that fixes its lexical and semantic inaccuracies, (b) 42D, a challenge set devised to assess the resilience of systems with respect to least frequent word senses and senses not seen at training time, and (c) hardEN, a challenge set made up solely of instances which none of the investigated state-of-the-art systems can solve. We make all of the test sets and model predictions available to the research community at https://github.com/SapienzaNLP/wsd-hard-benchmark.",https://aclanthology.org/2022.acl-long.324,2022,acl-long,Yes,Language,Benchmark,"@inproceedings{maru-etal-2022-nibbling,
    title = ""{N}ibbling at the Hard Core of {W}ord {S}ense {D}isambiguation"",
    author = ""Maru, Marco  and
      Conia, Simone  and
      Bevilacqua, Michele  and
      Navigli, Roberto"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.324/"",
    doi = ""10.18653/v1/2022.acl-long.324"",
    pages = ""4724--4737"",
    abstract = ""With state-of-the-art systems having finally attained estimated human performance, Word Sense Disambiguation (WSD) has now joined the array of Natural Language Processing tasks that have seemingly been solved, thanks to the vast amounts of knowledge encoded into Transformer-based pre-trained language models. And yet, if we look below the surface of raw figures, it is easy to realize that current approaches still make trivial mistakes that a human would never make. In this work, we provide evidence showing why the F1 score metric should not simply be taken at face value and present an exhaustive analysis of the errors that seven of the most representative state-of-the-art systems for English all-words WSD make on traditional evaluation benchmarks. In addition, we produce and release a collection of test sets featuring (a) an amended version of the standard evaluation benchmark that fixes its lexical and semantic inaccuracies, (b) 42D, a challenge set devised to assess the resilience of systems with respect to least frequent word senses and senses not seen at training time, and (c) hardEN, a challenge set made up solely of instances which none of the investigated state-of-the-art systems can solve. We make all of the test sets and model predictions available to the research community at \url{https://github.com/SapienzaNLP/wsd-hard-benchmark}.""
}",maruNibblingHardCore2022
VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena,"We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.",https://aclanthology.org/2022.acl-long.567,2022,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{parcalabescu-etal-2022-valse,
    title = ""{VALSE}: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena"",
    author = ""Parcalabescu, Letitia  and
      Cafagna, Michele  and
      Muradjan, Lilitta  and
      Frank, Anette  and
      Calixto, Iacer  and
      Gatt, Albert"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.567/"",
    doi = ""10.18653/v1/2022.acl-long.567"",
    pages = ""8253--8280"",
    abstract = ""We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V{\&}L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V{\&}L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V{\&}L models from a linguistic perspective, complementing the canonical task-centred V{\&}L evaluations.""
}",parcalabescuVALSETaskindependentBenchmark2022
CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment,"Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.",https://aclanthology.org/2022.acl-short.92,2022,acl-short,Yes,Language,Benchmark,"@inproceedings{senel-etal-2022-coda21,
    title = ""{C}o{DA}21: Evaluating Language Understanding Capabilities of {NLP} Models With Context-Definition Alignment"",
    author = {Senel, L{\""u}tfi Kerem  and
      Schick, Timo  and
      Schuetze, Hinrich},
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-short.92/"",
    doi = ""10.18653/v1/2022.acl-short.92"",
    pages = ""815--824"",
    abstract = ""Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.""
}",senelCoDA21EvaluatingLanguage2022
Generative Language Models for Paragraph-Level Question Generation,"Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting. It includes general-purpose datasets such as SQuAD for English, datasets from ten domains and two styles, as well as datasets in eight different languages. Using QG-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task. First, we propose robust QG baselines based on fine-tuning generative language models. Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating QG models. Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English.QG-Bench is released along with the fine-tuned models presented in the paper (https://github.com/asahi417/lm-question-generation), which are also available as a demo (https://autoqg.net/).",https://aclanthology.org/2022.emnlp-main.42,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{ushio-etal-2022-generative,
    title = ""Generative Language Models for Paragraph-Level Question Generation"",
    author = ""Ushio, Asahi  and
      Alva-Manchego, Fernando  and
      Camacho-Collados, Jose"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.42/"",
    doi = ""10.18653/v1/2022.emnlp-main.42"",
    pages = ""670--688"",
    abstract = ""Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting. It includes general-purpose datasets such as SQuAD for English, datasets from ten domains and two styles, as well as datasets in eight different languages. Using QG-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task. First, we propose robust QG baselines based on fine-tuning generative language models. Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating QG models. Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English.QG-Bench is released along with the fine-tuned models presented in the paper (https://github.com/asahi417/lm-question-generation), which are also available as a demo (https://autoqg.net/).""
}",ushioGenerativeLanguageModels2022
Large language models are few-shot clinical information extractors,"A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.",https://aclanthology.org/2022.emnlp-main.130,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{agrawal-etal-2022-large,
    title = ""Large language models are few-shot clinical information extractors"",
    author = ""Agrawal, Monica  and
      Hegselmann, Stefan  and
      Lang, Hunter  and
      Kim, Yoon  and
      Sontag, David"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.130/"",
    doi = ""10.18653/v1/2022.emnlp-main.130"",
    pages = ""1998--2022"",
    abstract = ""A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.""
}",agrawalLargeLanguageModels2022
GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models,"Recent work has shown that Pre-trained Language Models (PLMs) store the relational knowledge learned from data and utilize it for performing downstream tasks. However, commonsense knowledge across different regions may vary. For instance, the color of bridal dress is white in American weddings whereas it is red in Chinese weddings. In this paper, we introduce a benchmark dataset, Geo-diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for probing the diversity of the relational knowledge in multilingual PLMs. GeoMLAMA contains 3125 prompts in English, Chinese, Hindi, Persian, and Swahili, with a wide coverage of concepts shared by people from American, Chinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard multilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger multilingual PLMs variants do not necessarily store geo-diverse concepts better than its smaller variant; 2) multilingual PLMs are not intrinsically biased towards knowledge from the Western countries (the United States); 3) the native language of a country may not be the best language to probe its knowledge and 4) a language may better probe knowledge about a non-native country than its native country.",https://aclanthology.org/2022.emnlp-main.132,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{yin-etal-2022-geomlama,
    title = ""{G}eo{MLAMA}: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models"",
    author = ""Yin, Da  and
      Bansal, Hritik  and
      Monajatipoor, Masoud  and
      Li, Liunian Harold  and
      Chang, Kai-Wei"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.132/"",
    doi = ""10.18653/v1/2022.emnlp-main.132"",
    pages = ""2039--2055"",
    abstract = ""Recent work has shown that Pre-trained Language Models (PLMs) store the relational knowledge learned from data and utilize it for performing downstream tasks. However, commonsense knowledge across different regions may vary. For instance, the color of bridal dress is white in American weddings whereas it is red in Chinese weddings. In this paper, we introduce a benchmark dataset, Geo-diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for probing the diversity of the relational knowledge in multilingual PLMs. GeoMLAMA contains 3125 prompts in English, Chinese, Hindi, Persian, and Swahili, with a wide coverage of concepts shared by people from American, Chinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard multilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger multilingual PLMs variants do not necessarily store geo-diverse concepts better than its smaller variant; 2) multilingual PLMs are not intrinsically biased towards knowledge from the Western countries (the United States); 3) the native language of a country may not be the best language to probe its knowledge and 4) a language may better probe knowledge about a non-native country than its native country.""
}",yinGeoMLAMAGeodiverseCommonsense2022a
When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain,"Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We propose a novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective. Additionally, the evaluation benchmarks in the field have been limited. To this end, we contribute the Financial Language Understanding Evaluation (FLUE), an open-source comprehensive suite of benchmarks for the financial domain. These include new benchmarks across 5 NLP tasks in financial domain as well as common benchmarks used in the previous research. Experiments on these benchmarks suggest that our model outperforms those in prior literature on a variety of NLP tasks. Our models, code and benchmark data will be made publicly available on Github and Huggingface.",https://aclanthology.org/2022.emnlp-main.148,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{shah-etal-2022-flue,
    title = ""When {FLUE} Meets {FLANG}: Benchmarks and Large Pretrained Language Model for Financial Domain"",
    author = ""Shah, Raj  and
      Chawla, Kunal  and
      Eidnani, Dheeraj  and
      Shah, Agam  and
      Du, Wendi  and
      Chava, Sudheer  and
      Raman, Natraj  and
      Smiley, Charese  and
      Chen, Jiaao  and
      Yang, Diyi"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.148/"",
    doi = ""10.18653/v1/2022.emnlp-main.148"",
    pages = ""2322--2335"",
    abstract = ""Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We propose a novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective. Additionally, the evaluation benchmarks in the field have been limited. To this end, we contribute the Financial Language Understanding Evaluation (FLUE), an open-source comprehensive suite of benchmarks for the financial domain. These include new benchmarks across 5 NLP tasks in financial domain as well as common benchmarks used in the previous research. Experiments on these benchmarks suggest that our model outperforms those in prior literature on a variety of NLP tasks. Our models, code and benchmark data will be made publicly available on Github and Huggingface.""
}",shahWhenFLUEMeets2022
SafeText: A Benchmark for Exploring Physical Safety in Language Models,"Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.",https://aclanthology.org/2022.emnlp-main.154,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{levy-etal-2022-safetext,
    title = ""{S}afe{T}ext: A Benchmark for Exploring Physical Safety in Language Models"",
    author = ""Levy, Sharon  and
      Allaway, Emily  and
      Subbiah, Melanie  and
      Chilton, Lydia  and
      Patton, Desmond  and
      McKeown, Kathleen  and
      Wang, William Yang"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.154/"",
    doi = ""10.18653/v1/2022.emnlp-main.154"",
    pages = ""2407--2421"",
    abstract = ""Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.""
}",levySafeTextBenchmarkExploring2022
SLING: Sino Linguistic Evaluation of Large Language Models,"To understand what kinds of linguistic knowledge are encoded by pretrained Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates the acceptability contrast of a specific syntactic or semantic phenomenon (e.g., The keys are lost vs. The keys is lost), and an LM should assign lower perplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang et al., 2021), which also contains Chinese minimal pairs and was created by translating the vocabulary of the English BLiMP dataset, the minimal pairs in SLING are derived primarily by applying syntactic and lexical transformations to naturally-occurring, linguist-annotated sentences from the Chinese Treebank 9.0, thus addressing severe issues in CLiMP’s data generation process. We test 18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and multi-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show that the average accuracy for LMs is far below human performance (69.7% vs. 97.1%), while BERT-base-zh achieves the highest accuracy (84.8%) of all tested LMs, even much larger ones. Additionally, we find that most LMs have a strong gender and number (singular/plural) bias, and they perform better on local phenomena than hierarchical ones.",https://aclanthology.org/2022.emnlp-main.305,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{song-etal-2022-sling,
    title = ""{SLING}: {S}ino Linguistic Evaluation of Large Language Models"",
    author = ""Song, Yixiao  and
      Krishna, Kalpesh  and
      Bhatt, Rajesh  and
      Iyyer, Mohit"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.305/"",
    doi = ""10.18653/v1/2022.emnlp-main.305"",
    pages = ""4606--4634"",
    abstract = ""To understand what kinds of linguistic knowledge are encoded by pretrained Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates the acceptability contrast of a specific syntactic or semantic phenomenon (e.g., The keys are lost vs. The keys is lost), and an LM should assign lower perplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang et al., 2021), which also contains Chinese minimal pairs and was created by translating the vocabulary of the English BLiMP dataset, the minimal pairs in SLING are derived primarily by applying syntactic and lexical transformations to naturally-occurring, linguist-annotated sentences from the Chinese Treebank 9.0, thus addressing severe issues in CLiMP`s data generation process. We test 18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and multi-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show that the average accuracy for LMs is far below human performance (69.7{\%} vs. 97.1{\%}), while BERT-base-zh achieves the highest accuracy (84.8{\%}) of all tested LMs, even much larger ones. Additionally, we find that most LMs have a strong gender and number (singular/plural) bias, and they perform better on local phenomena than hierarchical ones.""
}",songSLINGSinoLinguistic2022a
MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure,"In this paper, we propose a comprehensive benchmark to investigate models’ logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models’ performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.",https://aclanthology.org/2022.emnlp-main.310,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{huang-etal-2022-metalogic,
    title = ""{M}eta{L}ogic: Logical Reasoning Explanations with Fine-Grained Structure"",
    author = ""Huang, Yinya  and
      Zhang, Hongming  and
      Hong, Ruixin  and
      Liang, Xiaodan  and
      Zhang, Changshui  and
      Yu, Dong"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.310/"",
    doi = ""10.18653/v1/2022.emnlp-main.310"",
    pages = ""4698--4724"",
    abstract = ""In this paper, we propose a comprehensive benchmark to investigate models' logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models' performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.""
}",huangMetaLogicLogicalReasoning2022
COPEN: Probing Conceptual Knowledge in Pre-trained Language Models,"Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual knowledge Probing bENchmark. Extensive experiments on different sizes and types of PLMs show that existing PLMs systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are publicly released at https://github.com/THU-KEG/COPEN.",https://aclanthology.org/2022.emnlp-main.335,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{peng-etal-2022-copen,
    title = ""{COPEN}: Probing Conceptual Knowledge in Pre-trained Language Models"",
    author = ""Peng, Hao  and
      Wang, Xiaozhi  and
      Hu, Shengding  and
      Jin, Hailong  and
      Hou, Lei  and
      Li, Juanzi  and
      Liu, Zhiyuan  and
      Liu, Qun"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.335/"",
    doi = ""10.18653/v1/2022.emnlp-main.335"",
    pages = ""5015--5035"",
    abstract = ""Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual knowledge Probing bENchmark. Extensive experiments on different sizes and types of PLMs show that existing PLMs systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are publicly released at https://github.com/THU-KEG/COPEN.""
}",pengCOPENProbingConceptual2022a
TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models,"Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM’s ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.",https://aclanthology.org/2022.emnlp-main.418,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{jang-etal-2022-temporalwiki,
    title = ""{T}emporal{W}iki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models"",
    author = ""Jang, Joel  and
      Ye, Seonghyeon  and
      Lee, Changho  and
      Yang, Sohee  and
      Shin, Joongbo  and
      Han, Janghoon  and
      Kim, Gyeonghun  and
      Seo, Minjoon"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.418/"",
    doi = ""10.18653/v1/2022.emnlp-main.418"",
    pages = ""6237--6250"",
    abstract = ""Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM`s ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.""
}",jangTemporalWikiLifelongBenchmark2022a
IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models,"We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer “fill-in-the-blank” questions when given a pre-defined relation category. Instead of focusing on pre-defined relations, we create an OIE benchmark aiming to fully examine the open relational information present in the pre-trained LMs. We accomplish this by turning pre-trained LMs into zero-shot OIE systems. Surprisingly, pre-trained LMs are able to obtain competitive performance on both standard OIE datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets (TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For instance, the zero-shot pre-trained LMs outperform the F1 score of the state-of-the-art supervised OIE methods on our factual OIE datasets without needing to use any training sets.",https://aclanthology.org/2022.emnlp-main.576,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{wang-etal-2022-ielm,
    title = ""{IELM}: An Open Information Extraction Benchmark for Pre-Trained Language Models"",
    author = ""Wang, Chenguang  and
      Liu, Xiao  and
      Song, Dawn"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.576/"",
    doi = ""10.18653/v1/2022.emnlp-main.576"",
    pages = ""8417--8437"",
    abstract = ""We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer {\textquotedblleft}fill-in-the-blank{\textquotedblright} questions when given a pre-defined relation category. Instead of focusing on pre-defined relations, we create an OIE benchmark aiming to fully examine the open relational information present in the pre-trained LMs. We accomplish this by turning pre-trained LMs into zero-shot OIE systems. Surprisingly, pre-trained LMs are able to obtain competitive performance on both standard OIE datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets (TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For instance, the zero-shot pre-trained LMs outperform the F1 score of the state-of-the-art supervised OIE methods on our factual OIE datasets without needing to use any training sets.""
}",wangIELMOpenInformation2022a
RobustLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,"Transformers have been shown to be able to perform deductive reasoning on inputs containing rules and statements written in the English natural language. However, it is unclear if these models indeed follow rigorous logical reasoning to arrive at the prediction or rely on spurious correlation patterns in making decisions. A strong deductive reasoning model should consistently understand the semantics of different logical operators. To this end, we present RobustLR, a diagnostic benchmark that evaluates the robustness of language models to minimal logical edits in the inputs and different logical equivalence conditions. In our experiments with RoBERTa, T5, and GPT3 we show that the models trained on deductive reasoning datasets do not perform consistently on the RobustLR test set, thus showing that the models are not robust to our proposed logical perturbations. Further, we observe that the models find it especially hard to learn logical negation operators. Our results demonstrate the shortcomings of current language models in logical reasoning and call for the development of better inductive biases to teach the logical semantics to language models. All the datasets and code base have been made publicly available.",https://aclanthology.org/2022.emnlp-main.653,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{sanyal-etal-2022-robustlr,
    title = ""{R}obust{LR}: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners"",
    author = ""Sanyal, Soumya  and
      Liao, Zeyi  and
      Ren, Xiang"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.653/"",
    doi = ""10.18653/v1/2022.emnlp-main.653"",
    pages = ""9614--9631"",
    abstract = ""Transformers have been shown to be able to perform deductive reasoning on inputs containing rules and statements written in the English natural language. However, it is unclear if these models indeed follow rigorous logical reasoning to arrive at the prediction or rely on spurious correlation patterns in making decisions. A strong deductive reasoning model should consistently understand the semantics of different logical operators. To this end, we present RobustLR, a diagnostic benchmark that evaluates the robustness of language models to minimal logical edits in the inputs and different logical equivalence conditions. In our experiments with RoBERTa, T5, and GPT3 we show that the models trained on deductive reasoning datasets do not perform consistently on the RobustLR test set, thus showing that the models are not robust to our proposed logical perturbations. Further, we observe that the models find it especially hard to learn logical negation operators. Our results demonstrate the shortcomings of current language models in logical reasoning and call for the development of better inductive biases to teach the logical semantics to language models. All the datasets and code base have been made publicly available.""
}",sanyalRobustLRDiagnosticBenchmark2022a
DiscoSense: Commonsense Reasoning with Discourse Connectives,"We present DiscoSense, a benchmark for commonsense reasoning via understanding a wide variety of discourse connectives. We generate compelling distractors in DiscoSense using Conditional Adversarial Filtering, an extension of Adversarial Filtering that employs conditional generation. We show that state-of-the-art pre-trained language models struggle to perform well on DiscoSense, which makes this dataset ideal for evaluating next-generation commonsense reasoning systems.",https://aclanthology.org/2022.emnlp-main.703,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{bhargava-ng-2022-discosense,
    title = ""{D}isco{S}ense: Commonsense Reasoning with Discourse Connectives"",
    author = ""Bhargava, Prajjwal  and
      Ng, Vincent"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.703/"",
    doi = ""10.18653/v1/2022.emnlp-main.703"",
    pages = ""10295--10310"",
    abstract = ""We present DiscoSense, a benchmark for commonsense reasoning via understanding a wide variety of discourse connectives. We generate compelling distractors in DiscoSense using Conditional Adversarial Filtering, an extension of Adversarial Filtering that employs conditional generation. We show that state-of-the-art pre-trained language models struggle to perform well on DiscoSense, which makes this dataset ideal for evaluating next-generation commonsense reasoning systems.""
}",bhargavaDiscoSenseCommonsenseReasoning2022
FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue,"Task transfer, transferring knowledge contained in related tasks, holds the promise of reducing the quantity of labeled data required to fine-tune language models. Dialogue understanding encompasses many diverse tasks, yet task transfer has not been thoroughly studied in conversational AI. This work explores conversational task transfer by introducing FETA: a benchmark for FEw-sample TAsk transfer in open-domain dialogue.FETA contains two underlying sets of conversations upon which there are 10 and 7 tasks annotated, enabling the study of intra-dataset task transfer; task transfer without domain adaptation. We utilize three popular language models and three learning algorithms to analyze the transferability between 132 source-target task pairs and create a baseline for future work.We run experiments in the single- and multi-source settings and report valuable findings, e.g., most performance trends are model-specific, and span extraction and multiple-choice tasks benefit the most from task transfer.In addition to task transfer, FETA can be a valuable resource for future research into the efficiency and generalizability of pre-training datasets and model architectures, as well as for learning settings such as continual and multitask learning.",https://aclanthology.org/2022.emnlp-main.751,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{albalak-etal-2022-feta,
    title = ""{FETA}: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue"",
    author = ""Albalak, Alon  and
      Tuan, Yi-Lin  and
      Jandaghi, Pegah  and
      Pryor, Connor  and
      Yoffe, Luke  and
      Ramachandran, Deepak  and
      Getoor, Lise  and
      Pujara, Jay  and
      Wang, William Yang"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.751/"",
    doi = ""10.18653/v1/2022.emnlp-main.751"",
    pages = ""10936--10953"",
    abstract = ""Task transfer, transferring knowledge contained in related tasks, holds the promise of reducing the quantity of labeled data required to fine-tune language models. Dialogue understanding encompasses many diverse tasks, yet task transfer has not been thoroughly studied in conversational AI. This work explores conversational task transfer by introducing FETA: a benchmark for FEw-sample TAsk transfer in open-domain dialogue.FETA contains two underlying sets of conversations upon which there are 10 and 7 tasks annotated, enabling the study of intra-dataset task transfer; task transfer without domain adaptation. We utilize three popular language models and three learning algorithms to analyze the transferability between 132 source-target task pairs and create a baseline for future work.We run experiments in the single- and multi-source settings and report valuable findings, e.g., most performance trends are model-specific, and span extraction and multiple-choice tasks benefit the most from task transfer.In addition to task transfer, FETA can be a valuable resource for future research into the efficiency and generalizability of pre-training datasets and model architectures, as well as for learning settings such as continual and multitask learning.""
}",albalakFETABenchmarkFewsample2022a
IndicXNLI: Evaluating Multilingual Inference for Indian Languages,"While Indic NLP has made rapid advances recently in terms of the availability of corpora and pre-trained models, benchmark datasets on standard NLU tasks are limited. To this end, we introduce INDICXNLI, an NLI dataset for 11 Indic languages. It has been created by high-quality machine translation of the original English XNLI dataset and our analysis attests to the quality of INDICXNLI. By finetuning different pre-trained LMs on this INDICXNLI, we analyze various cross-lingual transfer techniques with respect to the impact of the choice of language models, languages, multi-linguality, mix-language input, etc. These experiments provide us with useful insights into the behaviour of pre-trained models for a diverse set of languages.",https://aclanthology.org/2022.emnlp-main.755,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{aggarwal-etal-2022-indicxnli,
    title = ""{I}ndic{XNLI}: Evaluating Multilingual Inference for {I}ndian Languages"",
    author = ""Aggarwal, Divyanshu  and
      Gupta, Vivek  and
      Kunchukuttan, Anoop"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.755/"",
    doi = ""10.18653/v1/2022.emnlp-main.755"",
    pages = ""10994--11006"",
    abstract = ""While Indic NLP has made rapid advances recently in terms of the availability of corpora and pre-trained models, benchmark datasets on standard NLU tasks are limited. To this end, we introduce INDICXNLI, an NLI dataset for 11 Indic languages. It has been created by high-quality machine translation of the original English XNLI dataset and our analysis attests to the quality of INDICXNLI. By finetuning different pre-trained LMs on this INDICXNLI, we analyze various cross-lingual transfer techniques with respect to the impact of the choice of language models, languages, multi-linguality, mix-language input, etc. These experiments provide us with useful insights into the behaviour of pre-trained models for a diverse set of languages.""
}",aggarwalIndicXNLIEvaluatingMultilingual2022
Agent-Specific Deontic Modality Detection in Legal Language,"Legal documents are typically long and written in legalese, which makes it particularly difficult for laypeople to understand their rights and duties. While natural language understanding technologies can be valuable in supporting such understanding in the legal domain, the limited availability of datasets annotated for deontic modalities in the legal domain, due to the cost of hiring experts and privacy issues, is a bottleneck. To this end, we introduce, LEXDEMOD, a corpus of English contracts annotatedwith deontic modality expressed with respect to a contracting party or agent along with the modal triggers. We benchmark this dataset on two tasks: (i) agent-specific multi-label deontic modality classification, and (ii) agent-specific deontic modality and trigger span detection using Transformer-based (Vaswani et al., 2017) language models. Transfer learning experiments show that the linguistic diversity of modal expressions in LEXDEMOD generalizes reasonably from lease to employment andrental agreements. A small case study indicates that a model trained on LEXDEMOD can detect red flags with high recall. We believe our work offers a new research direction for deontic modality detection in the legal domain.",https://aclanthology.org/2022.emnlp-main.795,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{sancheti-etal-2022-agent,
    title = ""Agent-Specific Deontic Modality Detection in Legal Language"",
    author = ""Sancheti, Abhilasha  and
      Garimella, Aparna  and
      Srinivasan, Balaji Vasan  and
      Rudinger, Rachel"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.795/"",
    doi = ""10.18653/v1/2022.emnlp-main.795"",
    pages = ""11563--11579"",
    abstract = ""Legal documents are typically long and written in legalese, which makes it particularly difficult for laypeople to understand their rights and duties. While natural language understanding technologies can be valuable in supporting such understanding in the legal domain, the limited availability of datasets annotated for deontic modalities in the legal domain, due to the cost of hiring experts and privacy issues, is a bottleneck. To this end, we introduce, LEXDEMOD, a corpus of English contracts annotatedwith deontic modality expressed with respect to a contracting party or agent along with the modal triggers. We benchmark this dataset on two tasks: (i) agent-specific multi-label deontic modality classification, and (ii) agent-specific deontic modality and trigger span detection using Transformer-based (Vaswani et al., 2017) language models. Transfer learning experiments show that the linguistic diversity of modal expressions in LEXDEMOD generalizes reasonably from lease to employment andrental agreements. A small case study indicates that a model trained on LEXDEMOD can detect red flags with high recall. We believe our work offers a new research direction for deontic modality detection in the legal domain.""
}",sanchetiAgentspecificDeonticModality2022
COLD: A Benchmark for Chinese Offensive Language Detection,"Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark –COLD for Chinese offensive language analysis, including a Chinese Offensive Language Dataset –COLDATASET and a baseline detector –COLDETECTOR which is trained on the dataset. We show that the COLD benchmark contributes to Chinese offensive language detection which is challenging for existing resources. We then deploy the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained language models. We first analyze the offensiveness of existing generative models and show that these models inevitably expose varying degrees of offensive issues. Furthermore, we investigate the factors that influence the offensive generations, and we find that anti-bias contents and keywords referring to certain groups or revealing negative attitudes trigger offensive outputs easier.",https://aclanthology.org/2022.emnlp-main.796,2022,emnlp-main,Yes,Language,Benchmark,"@inproceedings{deng-etal-2022-cold,
    title = ""{COLD}: A Benchmark for {C}hinese Offensive Language Detection"",
    author = ""Deng, Jiawen  and
      Zhou, Jingyan  and
      Sun, Hao  and
      Zheng, Chujie  and
      Mi, Fei  and
      Meng, Helen  and
      Huang, Minlie"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.796/"",
    doi = ""10.18653/v1/2022.emnlp-main.796"",
    pages = ""11580--11599"",
    abstract = ""Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark {--}COLD for Chinese offensive language analysis, including a Chinese Offensive Language Dataset {--}COLDATASET and a baseline detector {--}COLDETECTOR which is trained on the dataset. We show that the COLD benchmark contributes to Chinese offensive language detection which is challenging for existing resources. We then deploy the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained language models. We first analyze the offensiveness of existing generative models and show that these models inevitably expose varying degrees of offensive issues. Furthermore, we investigate the factors that influence the offensive generations, and we find that anti-bias contents and keywords referring to certain groups or revealing negative attitudes trigger offensive outputs easier.""
}",dengCOLDBenchmarkChinese2022
MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction,"This paper presents MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each sentence is corrected by three annotators, and their corrections are carefully reviewed by a senior annotator, resulting in 2.3 references per sentence. We conduct experiments with two mainstream CGEC models, i.e., the sequence-to-sequence model and the sequence-to-edit model, both enhanced with large pretrained language models, achieving competitive benchmark performance on previous and our datasets. We also discuss CGEC evaluation methodologies, including the effect of multiple references and using a char-based metric. Our annotation guidelines, data, and code are available at https://github.com/HillZhang1999/MuCGEC.",https://aclanthology.org/2022.naacl-main.227,2022,naacl-main,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2022-mucgec,
    title = ""{M}u{CGEC}: a Multi-Reference Multi-Source Evaluation Dataset for {C}hinese Grammatical Error Correction"",
    author = ""Zhang, Yue  and
      Li, Zhenghua  and
      Bao, Zuyi  and
      Li, Jiacheng  and
      Zhang, Bo  and
      Li, Chen  and
      Huang, Fei  and
      Zhang, Min"",
    editor = ""Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.227/"",
    doi = ""10.18653/v1/2022.naacl-main.227"",
    pages = ""3118--3130"",
    abstract = ""This paper presents MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each sentence is corrected by three annotators, and their corrections are carefully reviewed by a senior annotator, resulting in 2.3 references per sentence. We conduct experiments with two mainstream CGEC models, i.e., the sequence-to-sequence model and the sequence-to-edit model, both enhanced with large pretrained language models, achieving competitive benchmark performance on previous and our datasets. We also discuss CGEC evaluation methodologies, including the effect of multiple references and using a char-based metric. Our annotation guidelines, data, and code are available at \url{https://github.com/HillZhang1999/MuCGEC}.""
}",zhangMuCGECMultireferenceMultisource2022
Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,"In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models’ abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.",https://aclanthology.org/2022.naacl-main.234,2022,naacl-main,Yes,Language,Benchmark,"@inproceedings{chen-gao-2022-curriculum,
    title = ""Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding"",
    author = ""Chen, Zeming  and
      Gao, Qiyue"",
    editor = ""Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.234/"",
    doi = ""10.18653/v1/2022.naacl-main.234"",
    pages = ""3204--3219"",
    abstract = ""In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models' abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.""
}",chenCurriculumBroadcoverageBenchmark2022
Towards Efficient NLP: A Standard Evaluation and A Strong Baseline,"Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.",https://aclanthology.org/2022.naacl-main.240,2022,naacl-main,Yes,Language,Benchmark,"@inproceedings{liu-etal-2022-towards-efficient,
    title = ""Towards Efficient {NLP}: A Standard Evaluation and A Strong Baseline"",
    author = ""Liu, Xiangyang  and
      Sun, Tianxiang  and
      He, Junliang  and
      Wu, Jiawen  and
      Wu, Lingling  and
      Zhang, Xinyu  and
      Jiang, Hao  and
      Cao, Zhao  and
      Huang, Xuanjing  and
      Qiu, Xipeng"",
    editor = ""Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.240/"",
    doi = ""10.18653/v1/2022.naacl-main.240"",
    pages = ""3288--3303"",
    abstract = ""Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.""
}",liuEfficientNLPStandard2022
Benchmarking Intersectional Biases in NLP,"There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.",https://aclanthology.org/2022.naacl-main.263,2022,naacl-main,Yes,Language,Benchmark,"@inproceedings{lalor-etal-2022-benchmarking,
    title = ""Benchmarking Intersectional Biases in {NLP}"",
    author = ""Lalor, John  and
      Yang, Yi  and
      Smith, Kendall  and
      Forsgren, Nicole  and
      Abbasi, Ahmed"",
    editor = ""Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.263/"",
    doi = ""10.18653/v1/2022.naacl-main.263"",
    pages = ""3598--3609"",
    abstract = ""There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.""
}",lalorBenchmarkingIntersectionalBiases2022
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,"When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.",https://aclanthology.org/2022.naacl-main.346,2022,naacl-main,Yes,Language,Benchmark,"@inproceedings{li-etal-2022-quantifying,
    title = ""Quantifying Adaptability in Pre-trained Language Models with 500 Tasks"",
    author = ""Li, Belinda  and
      Yu, Jane  and
      Khabsa, Madian  and
      Zettlemoyer, Luke  and
      Halevy, Alon  and
      Andreas, Jacob"",
    editor = ""Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.346/"",
    doi = ""10.18653/v1/2022.naacl-main.346"",
    pages = ""4696--4715"",
    abstract = ""When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.""
}",liQuantifyingAdaptabilityPretrained2022
Natural Language to Code Generation in Interactive Data Science Notebooks,"Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at https://github.com/google-research/arcade-nl2code/.",https://aclanthology.org/2023.acl-long.9,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{yin-etal-2023-natural,
    title = ""Natural Language to Code Generation in Interactive Data Science Notebooks"",
    author = ""Yin, Pengcheng  and
      Li, Wen-Ding  and
      Xiao, Kefan  and
      Rao, Abhishek  and
      Wen, Yeming  and
      Shi, Kensen  and
      Howland, Joshua  and
      Bailey, Paige  and
      Catasta, Michele  and
      Michalewski, Henryk  and
      Polozov, Oleksandr  and
      Sutton, Charles"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.9/"",
    doi = ""10.18653/v1/2023.acl-long.9"",
    pages = ""126--173"",
    abstract = ""Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at \url{https://github.com/google-research/arcade-nl2code/}.""
}",yinNaturalLanguageCode2023
ALERT: Adapt Language Models to Reasoning Tasks,"Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context. To address this question, we introduce {pasted macro ‘OUR’}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro ‘OUR’}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro ‘OUR’}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",https://aclanthology.org/2023.acl-long.60,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{yu-etal-2023-alert,
    title = ""{ALERT}: Adapt Language Models to Reasoning Tasks"",
    author = ""Yu, Ping  and
      Wang, Tianlu  and
      Golovneva, Olga  and
      AlKhamissi, Badr  and
      Verma, Siddharth  and
      Jin, Zhijing  and
      Ghosh, Gargi  and
      Diab, Mona  and
      Celikyilmaz, Asli"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.60/"",
    doi = ""10.18653/v1/2023.acl-long.60"",
    pages = ""1055--1081"",
    abstract = ""Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context. To address this question, we introduce {\{}pasted macro {\textquoteleft}OUR'{\}}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {\{}pasted macro {\textquoteleft}OUR'{\}}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {\{}pasted macro {\textquoteleft}OUR'{\}}model we further investigate \textit{the role of finetuning}. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.""
}",yuALERTAdaptLanguage2023a
Do language models have coherent mental models of everyday things?,"When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that “the yolk surrounds the shell” is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 “X relation Y?” true/false questions. Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent “parts mental models” (54-59% accurate, 19-43% conditional constraint violation). We propose an extension where we add a constraint satisfaction layer on top of the LM’s raw predictions to apply commonsense constraints. As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20%), suggesting how the incoherence of the LM’s pictures of everyday things can be significantly reduced.",https://aclanthology.org/2023.acl-long.106,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{gu-etal-2023-language,
    title = ""Do language models have coherent mental models of everyday things?"",
    author = ""Gu, Yuling  and
      Dalvi Mishra, Bhavana  and
      Clark, Peter"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.106/"",
    doi = ""10.18653/v1/2023.acl-long.106"",
    pages = ""1892--1913"",
    abstract = ""When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that {\textquotedblleft}the yolk surrounds the shell{\textquotedblright} is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 {\textquotedblleft}X relation Y?{\textquotedblright} true/false questions. Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent {\textquotedblleft}parts mental models{\textquotedblright} (54-59{\%} accurate, 19-43{\%} conditional constraint violation). We propose an extension where we add a constraint satisfaction layer on top of the LM`s raw predictions to apply commonsense constraints. As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20{\%}), suggesting how the incoherence of the LM`s pictures of everyday things can be significantly reduced.""
}",guLanguageModelsHave2023
Bi-Phone: Modeling Inter Language Phonetic Influences in Text,"A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1).We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2.These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing corrupted L2 text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web.We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly. We also introduce a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE. Finally, we also release the SuperGLUE benchmark to promote further research in phonetically robust language models. To the best of our knowledge, FunGLUE is the first benchmark to introduce L1-L2 interactions in text.",https://aclanthology.org/2023.acl-long.145,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{gupta-etal-2023-bi,
    title = ""Bi-Phone: Modeling Inter Language Phonetic Influences in Text"",
    author = ""Gupta, Abhirut  and
      Sai, Ananya B.  and
      Sproat, Richard  and
      Vasilevski, Yuri  and
      Ren, James  and
      Jash, Ambarish  and
      Sodhi, Sukhdeep  and
      Raghuveer, Aravindan"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.145/"",
    doi = ""10.18653/v1/2023.acl-long.145"",
    pages = ""2580--2592"",
    abstract = ""A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1).We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2.These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing corrupted L2 text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web.We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly. We also introduce a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE. Finally, we also release the SuperGLUE benchmark to promote further research in phonetically robust language models. To the best of our knowledge, FunGLUE is the first benchmark to introduce L1-L2 interactions in text.""
}",guptaBiphoneModelingInter2023
MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling,"We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERT-based models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks. Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on {pasted macro ‘BENCHMARK’} and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning methods. The code and datasets are publicly available https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23.",https://aclanthology.org/2023.acl-long.201,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{song-etal-2023-matsci,
    title = ""{M}at{S}ci-{NLP}: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling"",
    author = ""Song, Yu  and
      Miret, Santiago  and
      Liu, Bang"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.201/"",
    doi = ""10.18653/v1/2023.acl-long.201"",
    pages = ""3621--3639"",
    abstract = ""We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERT-based models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks. Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on {\{}pasted macro {\textquoteleft}BENCHMARK'{\}} and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning methods. The code and datasets are publicly available \url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23}.""
}",songMatSciNLPEvaluatingScientific2023
Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,"Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators’ prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.",https://aclanthology.org/2023.acl-long.228,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{liu-etal-2023-revisiting,
    title = ""Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation"",
    author = ""Liu, Yixin  and
      Fabbri, Alex  and
      Liu, Pengfei  and
      Zhao, Yilun  and
      Nan, Linyong  and
      Han, Ruilin  and
      Han, Simeng  and
      Joty, Shafiq  and
      Wu, Chien-Sheng  and
      Xiong, Caiming  and
      Radev, Dragomir"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.228/"",
    doi = ""10.18653/v1/2023.acl-long.228"",
    pages = ""4140--4170"",
    abstract = ""Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.""
}",liuRevisitingGoldStandard2023
Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change,"Reasoning about actions and change (RAC) is essential to understand and interact with the ever-changing environment. Previous AI research has shown the importance of fundamental and indispensable knowledge of actions, i.e., preconditions and effects. However, traditional methods rely on logical formalization which hinders practical applications. With recent transformer-based language models (LMs), reasoning over text is desirable and seemingly feasible, leading to the question of whether LMs can effectively and efficiently learn to solve RAC problems. We propose four essential RAC tasks as a comprehensive textual benchmark and generate problems in a way that minimizes the influence of other linguistic requirements (e.g., grounding) to focus on RAC. The resulting benchmark, TRAC, encompassing problems of various complexities, facilitates a more granular evaluation of LMs, precisely targeting the structural generalization ability much needed for RAC. Experiments with three high-performing transformers indicate that additional efforts are needed to tackle challenges raised by TRAC.",https://aclanthology.org/2023.acl-long.255,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{he-etal-2023-exploring,
    title = ""Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change"",
    author = ""He, Weinan  and
      Huang, Canming  and
      Xiao, Zhanhao  and
      Liu, Yongmei"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.255/"",
    doi = ""10.18653/v1/2023.acl-long.255"",
    pages = ""4629--4643"",
    abstract = ""Reasoning about actions and change (RAC) is essential to understand and interact with the ever-changing environment. Previous AI research has shown the importance of fundamental and indispensable knowledge of actions, i.e., preconditions and effects. However, traditional methods rely on logical formalization which hinders practical applications. With recent transformer-based language models (LMs), reasoning over text is desirable and seemingly feasible, leading to the question of whether LMs can effectively and efficiently learn to solve RAC problems. We propose four essential RAC tasks as a comprehensive textual benchmark and generate problems in a way that minimizes the influence of other linguistic requirements (e.g., grounding) to focus on RAC. The resulting benchmark, TRAC, encompassing problems of various complexities, facilitates a more granular evaluation of LMs, precisely targeting the structural generalization ability much needed for RAC. Experiments with three high-performing transformers indicate that additional efforts are needed to tackle challenges raised by TRAC.""
}",heExploringCapacityPretrained2023a
Revisiting non-English Text Simplification: A Unified Multilingual Benchmark,"Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.",https://aclanthology.org/2023.acl-long.269,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{ryan-etal-2023-revisiting,
    title = ""Revisiting non-{E}nglish Text Simplification: A Unified Multilingual Benchmark"",
    author = ""Ryan, Michael J  and
      Naous, Tarek  and
      Xu, Wei"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.269/"",
    doi = ""10.18653/v1/2023.acl-long.269"",
    pages = ""4898--4927"",
    abstract = ""Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.""
}",ryanRevisitingNonEnglishText2023
Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk,"Language is the principal tool for human communication, in which humor is one of the most attractive parts. Producing natural language like humans using computers, a.k.a, Natural Language Generation (NLG), has been widely used for dialogue systems, chatbots, machine translation, as well as computer-aid creation e.g., idea generations, scriptwriting. However, the humor aspect of natural language is relatively under-investigated, especially in the age of pre-trained language models. In this work, we aim to preliminarily test *whether NLG can generate humor as humans do*. We build a largest dataset consisting of numerous **C**hinese **C**omical **C**rosstalk scripts (called **C**3 in short), which is for a popular Chinese performing art called ‘Xiangsheng’ or ‘相声’ since 1800s.We benchmark various generation approaches including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a human assessment, showing that 1) *large-scale pretraining largely improves crosstalk generation quality*; and 2) *even the scripts generated from the best PLM is far from what we expect*. We conclude humor generation could be largely improved using large-scaled PLMs, but it is still in its infancy. The data and benchmarking code are publicly available in [https://github.com/anonNo2/crosstalk-generation](https://github.com/anonNo2/crosstalk-generation).",https://aclanthology.org/2023.acl-long.419,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{li-etal-2023-language,
    title = ""Can Language Models Make Fun? A Case Study in {C}hinese Comical Crosstalk"",
    author = ""Li, Jianquan  and
      Wu, XiangBo  and
      Liu, Xiaokang  and
      Xie, Qianqian  and
      Tiwari, Prayag  and
      Wang, Benyou"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.419/"",
    doi = ""10.18653/v1/2023.acl-long.419"",
    pages = ""7581--7596"",
    abstract = ""Language is the principal tool for human communication, in which humor is one of the most attractive parts. Producing natural language like humans using computers, a.k.a, Natural Language Generation (NLG), has been widely used for dialogue systems, chatbots, machine translation, as well as computer-aid creation e.g., idea generations, scriptwriting. However, the humor aspect of natural language is relatively under-investigated, especially in the age of pre-trained language models. In this work, we aim to preliminarily test *whether NLG can generate humor as humans do*. We build a largest dataset consisting of numerous **C**hinese **C**omical **C**rosstalk scripts (called **C**3 in short), which is for a popular Chinese performing art called {\textquoteleft}Xiangsheng' or {\textquoteleft}相声' since 1800s.We benchmark various generation approaches including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a human assessment, showing that 1) *large-scale pretraining largely improves crosstalk generation quality*; and 2) *even the scripts generated from the best PLM is far from what we expect*. We conclude humor generation could be largely improved using large-scaled PLMs, but it is still in its infancy. The data and benchmarking code are publicly available in [\url{https://github.com/anonNo2/crosstalk-generation}](\url{https://github.com/anonNo2/crosstalk-generation}).""
}",liCanLanguageModels2023a
READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises,"For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we construct READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input. We designed our annotation pipeline to maximize diversity, for example by instructing the annotators to use diverse input method editors (IMEs) for keyboard noises and recruiting speakers from diverse dialectical groups for speech noises. We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation. As the first large-scale attempt in creating a benchmark with noises geared towards user-generated inputs, we believe that READIN serves as an important complement to existing Chinese NLP benchmarks. The source code and dataset can be obtained from https://github.com/thunlp/READIN.",https://aclanthology.org/2023.acl-long.460,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{si-etal-2023-readin,
    title = ""{READIN}: A {C}hinese Multi-Task Benchmark with Realistic and Diverse Input Noises"",
    author = ""Si, Chenglei  and
      Zhang, Zhengyan  and
      Chen, Yingfa  and
      Wang, Xiaozhi  and
      Liu, Zhiyuan  and
      Sun, Maosong"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.460/"",
    doi = ""10.18653/v1/2023.acl-long.460"",
    pages = ""8272--8285"",
    abstract = ""For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we construct READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input. We designed our annotation pipeline to maximize diversity, for example by instructing the annotators to use diverse input method editors (IMEs) for keyboard noises and recruiting speakers from diverse dialectical groups for speech noises. We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation. As the first large-scale attempt in creating a benchmark with noises geared towards user-generated inputs, we believe that READIN serves as an important complement to existing Chinese NLP benchmarks. The source code and dataset can be obtained from \url{https://github.com/thunlp/READIN}.""
}",siREADINChineseMultitask2023
bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark,"We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at https://bgglue.github.io, and we hope that it will enable further advancements in developing NLU models for Bulgarian.",https://aclanthology.org/2023.acl-long.487,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{hardalov-etal-2023-bgglue,
    title = ""bg{GLUE}: A {B}ulgarian General Language Understanding Evaluation Benchmark"",
    author = ""Hardalov, Momchil  and
      Atanasova, Pepa  and
      Mihaylov, Todor  and
      Angelova, Galia  and
      Simov, Kiril  and
      Osenova, Petya  and
      Stoyanov, Veselin  and
      Koychev, Ivan  and
      Nakov, Preslav  and
      Radev, Dragomir"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.487/"",
    doi = ""10.18653/v1/2023.acl-long.487"",
    pages = ""8733--8759"",
    abstract = ""We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at \url{https://bgglue.github.io}, and we hope that it will enable further advancements in developing NLU models for Bulgarian.""
}",hardalovBgGLUEBulgarianGeneral2023
WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,"We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",https://aclanthology.org/2023.acl-long.507,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{felkner-etal-2023-winoqueer,
    title = ""{W}ino{Q}ueer: A Community-in-the-Loop Benchmark for Anti-{LGBTQ}+ Bias in Large Language Models"",
    author = ""Felkner, Virginia  and
      Chang, Ho-Chun Herbert  and
      Jang, Eugene  and
      May, Jonathan"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.507/"",
    doi = ""10.18653/v1/2023.acl-long.507"",
    pages = ""9126--9140"",
    abstract = ""We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.""
}",felknerWinoQueerCommunityintheloopBenchmark2023
SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models,"Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America.",https://aclanthology.org/2023.acl-long.548,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{jha-etal-2023-seegull,
    title = ""{S}ee{GULL}: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models"",
    author = ""Jha, Akshita  and
      Mostafazadeh Davani, Aida  and
      Reddy, Chandan K  and
      Dave, Shachi  and
      Prabhakaran, Vinodkumar  and
      Dev, Sunipa"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.548/"",
    doi = ""10.18653/v1/2023.acl-long.548"",
    pages = ""9851--9870"",
    abstract = ""Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America.""
}",jhaSeeGULLStereotypeBenchmark2023
MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning,"Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric – Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.",https://aclanthology.org/2023.acl-long.641,2023,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{xu-etal-2023-multiinstruct,
    title = ""{M}ulti{I}nstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning"",
    author = ""Xu, Zhiyang  and
      Shen, Ying  and
      Huang, Lifu"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.641/"",
    doi = ""10.18653/v1/2023.acl-long.641"",
    pages = ""11445--11465"",
    abstract = ""Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric {--} Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.""
}",xuMultiInstructImprovingMultimodal2023
"Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages","Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT.",https://aclanthology.org/2023.acl-long.693,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{doddapaneni-etal-2023-towards,
    title = ""Towards Leaving No {I}ndic Language Behind: Building Monolingual Corpora, Benchmark and Models for {I}ndic Languages"",
    author = ""Doddapaneni, Sumanth  and
      Aralikatte, Rahul  and
      Ramesh, Gowtham  and
      Goyal, Shreya  and
      Khapra, Mitesh M.  and
      Kunchukuttan, Anoop  and
      Kumar, Pratyush"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.693/"",
    doi = ""10.18653/v1/2023.acl-long.693"",
    pages = ""12402--12426"",
    abstract = ""Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at \url{https://github.com/AI4Bharat/IndicBERT}.""
}",doddapaneniLeavingNoIndic2023
Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models,"Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",https://aclanthology.org/2023.acl-long.828,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{tan-etal-2023-towards,
    title = ""Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models"",
    author = ""Tan, Qingyu  and
      Ng, Hwee Tou  and
      Bing, Lidong"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.828/"",
    doi = ""10.18653/v1/2023.acl-long.828"",
    pages = ""14820--14835"",
    abstract = ""Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.""
}",tanBenchmarkingImprovingTemporal2023
Exploring Large Language Models for Classical Philology,"Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5’s decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology.",https://aclanthology.org/2023.acl-long.846,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{riemenschneider-frank-2023-exploring,
    title = ""Exploring Large Language Models for Classical Philology"",
    author = ""Riemenschneider, Frederick  and
      Frank, Anette"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.846/"",
    doi = ""10.18653/v1/2023.acl-long.846"",
    pages = ""15181--15199"",
    abstract = ""Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5`s decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology.""
}",riemenschneiderExploringLargeLanguage2023
XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations,"Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at https://github.com/psunlpgroup/XSemPLR.",https://aclanthology.org/2023.acl-long.887,2023,acl-long,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2023-xsemplr,
    title = ""{XS}em{PLR}: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations"",
    author = ""Zhang, Yusen  and
      Wang, Jun  and
      Wang, Zhiguo  and
      Zhang, Rui"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.887/"",
    doi = ""10.18653/v1/2023.acl-long.887"",
    pages = ""15918--15947"",
    abstract = ""Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at \url{https://github.com/psunlpgroup/XSemPLR}.""
}",zhangXSemPLRCrosslingualSemantic2023
PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English,"Privacy policies provide individuals with information about their rights and how their personal information is handled. Natural language understanding (NLU) technologies can support individuals and practitioners to understand better privacy practices described in lengthy and complex documents. However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices. To this end, we introduce the Privacy Policy Language Understanding Evaluation (PLUE) benchmark, a multi-task benchmark for evaluating the privacy policy language understanding across various tasks. We also collect a large corpus of privacy policies to enable privacy policy domain-specific language model pre-training. We evaluate several generic pre-trained language models and continue pre-training them on the collected corpus. We demonstrate that domain-specific continual pre-training offers performance improvements across all tasks. The code and models are released at https://github.com/JFChi/PLUE.",https://aclanthology.org/2023.acl-short.31,2023,acl-short,Yes,Language,Benchmark,"@inproceedings{chi-etal-2023-plue,
    title = ""{PLUE}: Language Understanding Evaluation Benchmark for Privacy Policies in {E}nglish"",
    author = ""Chi, Jianfeng  and
      Ahmad, Wasi Uddin  and
      Tian, Yuan  and
      Chang, Kai-Wei"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-short.31/"",
    doi = ""10.18653/v1/2023.acl-short.31"",
    pages = ""352--365"",
    abstract = ""Privacy policies provide individuals with information about their rights and how their personal information is handled. Natural language understanding (NLU) technologies can support individuals and practitioners to understand better privacy practices described in lengthy and complex documents. However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices. To this end, we introduce the Privacy Policy Language Understanding Evaluation (PLUE) benchmark, a multi-task benchmark for evaluating the privacy policy language understanding across various tasks. We also collect a large corpus of privacy policies to enable privacy policy domain-specific language model pre-training. We evaluate several generic pre-trained language models and continue pre-training them on the collected corpus. We demonstrate that domain-specific continual pre-training offers performance improvements across all tasks. The code and models are released at \url{https://github.com/JFChi/PLUE}.""
}",chiPLUELanguageUnderstanding2023
Measuring the Effect of Influential Messages on Varying Personas,"Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. We further evaluate the SOTA neural language models with our dataset. The empirical results suggest that the included persona attributes are helpful for the performance of all response dimensions. Our analysis shows that the best-performing models are capable of predicting responses that are consistent with the personas, and as a byproduct, the task formulation also enables many interesting applications in the analysis of social network groups and their opinions, such as the discovery of extreme opinion groups.",https://aclanthology.org/2023.acl-short.48,2023,acl-short,Yes,Language,Benchmark,"@inproceedings{sun-etal-2023-measuring,
    title = ""Measuring the Effect of Influential Messages on Varying Personas"",
    author = ""Sun, Chenkai  and
      Li, Jinning  and
      Chan, Hou Pong  and
      Zhai, ChengXiang  and
      Ji, Heng"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-short.48/"",
    doi = ""10.18653/v1/2023.acl-short.48"",
    pages = ""554--562"",
    abstract = ""Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. We further evaluate the SOTA neural language models with our dataset. The empirical results suggest that the included persona attributes are helpful for the performance of all response dimensions. Our analysis shows that the best-performing models are capable of predicting responses that are consistent with the personas, and as a byproduct, the task formulation also enables many interesting applications in the analysis of social network groups and their opinions, such as the discovery of extreme opinion groups.""
}",sunMeasuringEffectInfluential2023a
ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning,"A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test the latest InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveals the model can correctly reason about negation, but struggles to do so on NLI examples outside of its core pretraining regime.",https://aclanthology.org/2023.acl-short.154,2023,acl-short,Yes,Language,Benchmark,"@inproceedings{she-etal-2023-scone,
    title = ""{S}co{N}e: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning"",
    author = ""She, Jingyuan S.  and
      Potts, Christopher  and
      Bowman, Samuel R.  and
      Geiger, Atticus"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-short.154/"",
    doi = ""10.18653/v1/2023.acl-short.154"",
    pages = ""1803--1821"",
    abstract = ""A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test the latest InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveals the model can correctly reason about negation, but struggles to do so on NLI examples outside of its core pretraining regime.""
}",sheScoNeBenchmarkingNegation2023a
We’re Afraid Language Models Aren’t Modeling Ambiguity,"Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We capture ambiguity in a sentence through its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.",https://aclanthology.org/2023.emnlp-main.51,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{liu-etal-2023-afraid,
    title = ""We`re Afraid Language Models Aren`t Modeling Ambiguity"",
    author = ""Liu, Alisa  and
      Wu, Zhaofeng  and
      Michael, Julian  and
      Suhr, Alane  and
      West, Peter  and
      Koller, Alexander  and
      Swayamdipta, Swabha  and
      Smith, Noah  and
      Choi, Yejin"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.51/"",
    doi = ""10.18653/v1/2023.emnlp-main.51"",
    pages = ""790--807"",
    abstract = ""Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We capture ambiguity in a sentence through its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32{\%} of the time in crowdworker evaluation, compared to 90{\%} for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.""
}",liuWe`reAfraidLanguage2023a
"Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms","Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying commonsense norms: NormLens. NormLens consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a simple yet effective approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code will be released.",https://aclanthology.org/2023.emnlp-main.57,2023,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{han-etal-2023-reading,
    title = ""Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms"",
    author = ""Han, Seungju  and
      Kim, Junhyeok  and
      Hessel, Jack  and
      Jiang, Liwei  and
      Chung, Jiwan  and
      Son, Yejin  and
      Choi, Yejin  and
      Yu, Youngjae"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.57/"",
    doi = ""10.18653/v1/2023.emnlp-main.57"",
    pages = ""894--914"",
    abstract = ""Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying commonsense norms: NormLens. NormLens consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a simple yet effective approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code will be released.""
}",hanReadingBooksGreat2023
QTSumm: Query-Focused Summarization over Tabular Data,"People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users’ information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.",https://aclanthology.org/2023.emnlp-main.74,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zhao-etal-2023-qtsumm,
    title = ""{QTS}umm: Query-Focused Summarization over Tabular Data"",
    author = ""Zhao, Yilun  and
      Qi, Zhenting  and
      Nan, Linyong  and
      Mi, Boyu  and
      Liu, Yixin  and
      Zou, Weijin  and
      Han, Simeng  and
      Chen, Ruizhe  and
      Tang, Xiangru  and
      Xu, Yumo  and
      Radev, Dragomir  and
      Cohan, Arman"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.74/"",
    doi = ""10.18653/v1/2023.emnlp-main.74"",
    pages = ""1157--1172"",
    abstract = ""People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users' information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.""
}",zhaoQTSummQueryfocusedSummarization2023
ALCUNA: Large Language Models Meet New Knowledge,"With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models’ capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs’ ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs’ abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model’s understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.",https://aclanthology.org/2023.emnlp-main.87,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{yin-etal-2023-alcuna,
    title = ""{ALCUNA}: Large Language Models Meet New Knowledge"",
    author = ""Yin, Xunjian  and
      Huang, Baizhou  and
      Wan, Xiaojun"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.87/"",
    doi = ""10.18653/v1/2023.emnlp-main.87"",
    pages = ""1397--1414"",
    abstract = ""With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models' capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs' abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model`s understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.""
}",yinALCUNALargeLanguage2023a
DUnE: Dataset for Unified Editing,"Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model’s knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. “Messi plays for Inter Miami” confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the applications of language models expand, so do the diverse ways in which we wish to edit and refine their outputs. In this study, we broaden the scope of the editing problem to include an array of editing cases such as debiasing and rectifying reasoning errors and define an edit as any natural language expression that solicits a change in the model’s outputs. We are introducing DUnE, an editing benchmark where edits are natural language sentences and propose that DUnE presents a challenging yet relevant task. To substantiate this claim, we conduct an extensive series of experiments testing various editing approaches to address DUnE, demonstrating their respective strengths and weaknesses. We argue that retrieval-augmented language modeling can outperform specialized editing techniques and neither set of approaches has fully solved the generalized editing problem covered by our benchmark.",https://aclanthology.org/2023.emnlp-main.114,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{akyurek-etal-2023-dune,
    title = ""{DU}n{E}: Dataset for Unified Editing"",
    author = {Aky{\""u}rek, Afra  and
      Pan, Eric  and
      Kuwanto, Garry  and
      Wijaya, Derry},
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.114/"",
    doi = ""10.18653/v1/2023.emnlp-main.114"",
    pages = ""1847--1861"",
    abstract = ""Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model`s knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. {\textquotedblleft}Messi plays for Inter Miami{\textquotedblright} confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the applications of language models expand, so do the diverse ways in which we wish to edit and refine their outputs. In this study, we broaden the scope of the editing problem to include an array of editing cases such as debiasing and rectifying reasoning errors and define an edit as any natural language expression that solicits a change in the model`s outputs. We are introducing DUnE, an editing benchmark where edits are natural language sentences and propose that DUnE presents a challenging yet relevant task. To substantiate this claim, we conduct an extensive series of experiments testing various editing approaches to address DUnE, demonstrating their respective strengths and weaknesses. We argue that retrieval-augmented language modeling can outperform specialized editing techniques and neither set of approaches has fully solved the generalized editing problem covered by our benchmark.""
}",akyurekDUnEDatasetUnified2023
OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization,"The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document open aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.",https://aclanthology.org/2023.emnlp-main.121,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{amar-etal-2023-openasp,
    title = ""{O}pen{A}sp: A Benchmark for Multi-document Open Aspect-based Summarization"",
    author = ""Amar, Shmuel  and
      Schiff, Liat  and
      Ernst, Ori  and
      Shefer, Asi  and
      Shapira, Ori  and
      Dagan, Ido"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.121/"",
    doi = ""10.18653/v1/2023.emnlp-main.121"",
    pages = ""1967--1991"",
    abstract = ""The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document open aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.""
}",amarOpenAspBenchmarkMultidocument2023
Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning,"Language model probing is often used to test specific capabilities of models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing. The datasets and code are available on Github.",https://aclanthology.org/2023.emnlp-main.130,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{shivagunde-etal-2023-larger,
    title = ""Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning"",
    author = ""Shivagunde, Namrata  and
      Lialin, Vladislav  and
      Rumshisky, Anna"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.130/"",
    doi = ""10.18653/v1/2023.emnlp-main.130"",
    pages = ""2094--2107"",
    abstract = ""Language model probing is often used to test specific capabilities of models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57{\%} compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6{\%} of them during probing. The datasets and code are available on Github.""
}",shivagundeLargerProbesTell2023
Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think - Introducing AI Detectability Index (ADI),"With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter, signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that “if the content is traditional elements of authorship produced by a machine, the work lacks human authorship and the office will not register it for copyright”. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a lower ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.",https://aclanthology.org/2023.emnlp-main.136,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{chakraborty-etal-2023-counter,
    title = ""Counter {T}uring Test ({CT}2): {AI}-Generated Text Detection is Not as Easy as You May Think - Introducing {AI} Detectability Index ({ADI})"",
    author = ""Chakraborty, Megha  and
      Tonmoy, S.M Towhidul Islam  and
      Zaman, S M Mehedi  and
      Gautam, Shreya  and
      Kumar, Tanay  and
      Sharma, Krish  and
      Barman, Niyar  and
      Gupta, Chandan  and
      Jain, Vinija  and
      Chadha, Aman  and
      Sheth, Amit  and
      Das, Amitava"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.136/"",
    doi = ""10.18653/v1/2023.emnlp-main.136"",
    pages = ""2206--2239"",
    abstract = ""With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter, signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that {\textquotedblleft}if the content is traditional elements of authorship produced by a machine, the work lacks human authorship and the office will not register it for copyright{\textquotedblright}. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a lower ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.""
}",chakrabortyCounterTuringTest2023
TempTabQA: Temporal Question Answering for Semi-Structured Tables,"Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset, we evaluate several state-of-the-art models for temporal reasoning. We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points. Given these results, our dataset has the potential to serve as a challenging benchmark to improve the temporal reasoning capabilities of NLP models.",https://aclanthology.org/2023.emnlp-main.149,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{gupta-etal-2023-temptabqa,
    title = ""{T}emp{T}ab{QA}: Temporal Question Answering for Semi-Structured Tables"",
    author = ""Gupta, Vivek  and
      Kandoi, Pranshu  and
      Vora, Mahek  and
      Zhang, Shuo  and
      He, Yujie  and
      Reinanda, Ridho  and
      Srikumar, Vivek"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.149/"",
    doi = ""10.18653/v1/2023.emnlp-main.149"",
    pages = ""2431--2453"",
    abstract = ""Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset, we evaluate several state-of-the-art models for temporal reasoning. We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points. Given these results, our dataset has the potential to serve as a challenging benchmark to improve the temporal reasoning capabilities of NLP models.""
}",guptaTempTabQATemporalQuestion2023
The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages,"Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately represented in any of the existing benchmarks. To address this gap, we present SPARROW, an extensive multilingual benchmark specifically designed for SM understanding. SPARROW comprises 169 datasets covering 13 task types across six primary categories (e.g., anti-social language detection, emotion recognition). SPARROW datasets encompass 64 different languages originating from 12 language families representing 16 writing scripts. We evaluate the performance of various multilingual pretrained language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT) on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score. Our benchmark is available at: https://github.com/UBC-NLP/SPARROW",https://aclanthology.org/2023.emnlp-main.160,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2023-skipped,
    title = ""The Skipped Beat: A Study of Sociopragmatic Understanding in {LLM}s for 64 Languages"",
    author = ""Zhang, Chiyu  and
      Doan, Khai  and
      Liao, Qisheng  and
      Abdul-Mageed, Muhammad"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.160/"",
    doi = ""10.18653/v1/2023.emnlp-main.160"",
    pages = ""2630--2662"",
    abstract = ""Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately represented in any of the existing benchmarks. To address this gap, we present SPARROW, an extensive multilingual benchmark specifically designed for SM understanding. SPARROW comprises 169 datasets covering 13 task types across six primary categories (e.g., anti-social language detection, emotion recognition). SPARROW datasets encompass 64 different languages originating from 12 language families representing 16 writing scripts. We evaluate the performance of various multilingual pretrained language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT) on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score. Our benchmark is available at: https://github.com/UBC-NLP/SPARROW""
}",zhangSkippedBeatStudy2023
API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs,"Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs’ ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs’ capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca’s tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.",https://aclanthology.org/2023.emnlp-main.187,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{li-etal-2023-api,
    title = ""{API}-Bank: A Comprehensive Benchmark for Tool-Augmented {LLM}s"",
    author = ""Li, Minghao  and
      Zhao, Yingxiu  and
      Yu, Bowen  and
      Song, Feifan  and
      Li, Hangyu  and
      Yu, Haiyang  and
      Li, Zhoujun  and
      Huang, Fei  and
      Li, Yongbin"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.187/"",
    doi = ""10.18653/v1/2023.emnlp-main.187"",
    pages = ""3102--3116"",
    abstract = ""Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca`s tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.""
}",liAPIbankComprehensiveBenchmark2023
Revisiting De-Identification of Electronic Medical Records: Evaluation of Within- and Cross-Hospital Generalization,"The de-identification task aims to detect and remove the protected health information from electronic medical records (EMRs). Previous studies generally focus on the within-hospital setting and achieve great successes, while the cross-hospital setting has been overlooked. This study introduces a new de-identification dataset comprising EMRs from three hospitals in China, creating a benchmark for evaluating both within- and cross-hospital generalization. We find significant domain discrepancy between hospitals. A model with almost perfect within-hospital performance struggles when transferred across hospitals. Further experiments show that pretrained language models and some domain generalization methods can alleviate this problem. We believe that our data and findings will encourage investigations on the generalization of medical NLP models.",https://aclanthology.org/2023.emnlp-main.224,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{liu-etal-2023-revisiting-de,
    title = ""Revisiting De-Identification of Electronic Medical Records: Evaluation of Within- and Cross-Hospital Generalization"",
    author = ""Liu, Yiyang  and
      Li, Jinpeng  and
      Zhu, Enwei"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.224/"",
    doi = ""10.18653/v1/2023.emnlp-main.224"",
    pages = ""3666--3674"",
    abstract = ""The de-identification task aims to detect and remove the protected health information from electronic medical records (EMRs). Previous studies generally focus on the within-hospital setting and achieve great successes, while the cross-hospital setting has been overlooked. This study introduces a new de-identification dataset comprising EMRs from three hospitals in China, creating a benchmark for evaluating both within- and cross-hospital generalization. We find significant domain discrepancy between hospitals. A model with almost perfect within-hospital performance struggles when transferred across hospitals. Further experiments show that pretrained language models and some domain generalization methods can alleviate this problem. We believe that our data and findings will encourage investigations on the generalization of medical NLP models.""
}",liuRevisitingDeidentificationElectronic2023a
ROBBIE: Robust Bias Evaluation of Large Generative Language Models,"As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs.",https://aclanthology.org/2023.emnlp-main.230,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{esiobu-etal-2023-robbie,
    title = ""{ROBBIE}: Robust Bias Evaluation of Large Generative Language Models"",
    author = ""Esiobu, David  and
      Tan, Xiaoqing  and
      Hosseini, Saghar  and
      Ung, Megan  and
      Zhang, Yuchen  and
      Fernandes, Jude  and
      Dwivedi-Yu, Jane  and
      Presani, Eleonora  and
      Williams, Adina  and
      Smith, Eric"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.230/"",
    doi = ""10.18653/v1/2023.emnlp-main.230"",
    pages = ""3764--3814"",
    abstract = ""As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs.""
}",esiobuROBBIERobustBias2023
MEGA: Multilingual Evaluation of Generative AI,"Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.",https://aclanthology.org/2023.emnlp-main.258,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{ahuja-etal-2023-mega,
    title = ""{MEGA}: Multilingual Evaluation of Generative {AI}"",
    author = ""Ahuja, Kabir  and
      Diddee, Harshita  and
      Hada, Rishav  and
      Ochieng, Millicent  and
      Ramesh, Krithika  and
      Jain, Prachi  and
      Nambi, Akshay  and
      Ganu, Tanuja  and
      Segal, Sameer  and
      Ahmed, Mohamed  and
      Bali, Kalika  and
      Sitaram, Sunayana"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.258/"",
    doi = ""10.18653/v1/2023.emnlp-main.258"",
    pages = ""4232--4267"",
    abstract = ""Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.""
}",ahujaMEGAMultilingualEvaluation2023
Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction,"The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F_1 score. Our resources and code will be publicly available.",https://aclanthology.org/2023.emnlp-main.360,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{qi-etal-2023-preserving,
    title = ""Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction"",
    author = ""Qi, Ji  and
      Zhang, Chuchun  and
      Wang, Xiaozhi  and
      Zeng, Kaisheng  and
      Yu, Jifan  and
      Liu, Jinxin  and
      Hou, Lei  and
      Li, Juanzi  and
      Bin, Xu"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.360/"",
    doi = ""10.18653/v1/2023.emnlp-main.360"",
    pages = ""5876--5890"",
    abstract = ""The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 $F_1$ score. Our resources and code will be publicly available.""
}",qiPreservingKnowledgeInvariance2023
HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,"Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5% user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps.",https://aclanthology.org/2023.emnlp-main.397,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{li-etal-2023-halueval,
    title = ""{H}alu{E}val: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models"",
    author = ""Li, Junyi  and
      Cheng, Xiaoxue  and
      Zhao, Xin  and
      Nie, Jian-Yun  and
      Wen, Ji-Rong"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.397/"",
    doi = ""10.18653/v1/2023.emnlp-main.397"",
    pages = ""6449--6464"",
    abstract = ""Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5{\%} user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps.""
}",liHaluEvalLargescaleHallucination2023a
Enabling Large Language Models to Generate Text with Citations,"Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs’ Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions—fluency, correctness, and citation quality—and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement—For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",https://aclanthology.org/2023.emnlp-main.398,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{gao-etal-2023-enabling,
    title = ""Enabling Large Language Models to Generate Text with Citations"",
    author = ""Gao, Tianyu  and
      Yen, Howard  and
      Yu, Jiatong  and
      Chen, Danqi"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.398/"",
    doi = ""10.18653/v1/2023.emnlp-main.398"",
    pages = ""6465--6488"",
    abstract = ""Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions{---}fluency, correctness, and citation quality{---}and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement{---}For example, on the ELI5 dataset, even the best models lack complete citation support 50{\%} of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.""
}",gaoEnablingLargeLanguage2023a
Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models,"Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies. Additionally, we broaden the scope of our evaluation by including in our analysis 2 Code Generation LMs. Beyond broad benchmarking of models and prompts, we also conduct a fine-grained investigation of performance across different categories of temporal tasks. We further analyze the LLMs on varying temporal aspects, offering insights into their proficiency in understanding and predicting the continuity, sequence, and progression of events over time. Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain.",https://aclanthology.org/2023.emnlp-main.418,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{jain-etal-2023-language-models,
    title = ""Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models"",
    author = ""Jain, Raghav  and
      Sojitra, Daivik  and
      Acharya, Arkadeep  and
      Saha, Sriparna  and
      Jatowt, Adam  and
      Dandapat, Sandipan"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.418/"",
    doi = ""10.18653/v1/2023.emnlp-main.418"",
    pages = ""6750--6774"",
    abstract = ""Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies. Additionally, we broaden the scope of our evaluation by including in our analysis 2 Code Generation LMs. Beyond broad benchmarking of models and prompts, we also conduct a fine-grained investigation of performance across different categories of temporal tasks. We further analyze the LLMs on varying temporal aspects, offering insights into their proficiency in understanding and predicting the continuity, sequence, and progression of events over time. Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain.""
}",jainLanguageModelsHave2023
Benchmarking and Improving Text-to-SQL Generation under Ambiguity,"Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-k decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-k. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-k ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.",https://aclanthology.org/2023.emnlp-main.436,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{bhaskar-etal-2023-benchmarking,
    title = ""Benchmarking and Improving Text-to-{SQL} Generation under Ambiguity"",
    author = ""Bhaskar, Adithya  and
      Tomar, Tushar  and
      Sathe, Ashutosh  and
      Sarawagi, Sunita"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.436/"",
    doi = ""10.18653/v1/2023.emnlp-main.436"",
    pages = ""7053--7074"",
    abstract = ""Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-$k$ decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-$k$. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-$k$ ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.""
}",bhaskarBenchmarkingImprovingTexttoSQL2023a
DUMB: A Benchmark for Smart Evaluation of Dutch Models,"We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models. Through a comparison of 14 pre-trained language models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition to highlighting best strategies for training larger Dutch models, DUMB will foster further research on Dutch. A public leaderboard is available at https://dumbench.nl.",https://aclanthology.org/2023.emnlp-main.447,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{de-vries-etal-2023-dumb,
    title = ""{DUMB}: A Benchmark for Smart Evaluation of {D}utch Models"",
    author = ""de Vries, Wietse  and
      Wieling, Martijn  and
      Nissim, Malvina"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.447/"",
    doi = ""10.18653/v1/2023.emnlp-main.447"",
    pages = ""7221--7241"",
    abstract = ""We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models. Through a comparison of 14 pre-trained language models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition to highlighting best strategies for training larger Dutch models, DUMB will foster further research on Dutch. A public leaderboard is available at https://dumbench.nl.""
}",devriesDUMBBenchmarkSmart2023
Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models,"The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",https://aclanthology.org/2023.emnlp-main.468,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{arora-etal-2023-llms,
    title = ""Have {LLM}s Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models"",
    author = ""Arora, Daman  and
      Singh, Himanshu  and
      Mausam"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.468/"",
    doi = ""10.18653/v1/2023.emnlp-main.468"",
    pages = ""7527--7543"",
    abstract = ""The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40{\%}. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.""
}",aroraHaveLLMsAdvanced2023
TheoremQA: A Theorem-driven Question Answering Dataset,"The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models’ capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4’s capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs’ capabilities to solve challenging science problems.",https://aclanthology.org/2023.emnlp-main.489,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{chen-etal-2023-theoremqa,
    title = ""{T}heorem{QA}: A Theorem-driven Question Answering Dataset"",
    author = ""Chen, Wenhu  and
      Yin, Ming  and
      Ku, Max  and
      Lu, Pan  and
      Wan, Yixin  and
      Ma, Xueguang  and
      Xu, Jianyu  and
      Wang, Xinyi  and
      Xia, Tony"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.489/"",
    doi = ""10.18653/v1/2023.emnlp-main.489"",
    pages = ""7889--7901"",
    abstract = ""The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90{\%} accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE{\&}CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4`s capabilities to solve these problems are unparalleled, achieving an accuracy of 51{\%} with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15{\%}, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs' capabilities to solve challenging science problems.""
}",chenTheoremQATheoremdrivenQuestion2023a
Superlim: A Swedish Language Understanding Evaluation Benchmark,"We present Superlim, a multi-task NLP benchmark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. We describe the dataset, the tasks, the leaderboard and report the baseline results yielded by a reference implementation. The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark. We address methodological challenges, such as mitigating the Anglocentric bias when creating datasets for a less-resourced language; choosing the most appropriate measures; documenting the datasets and making the leaderboard convenient and transparent. We also highlight other potential usages of the dataset, such as, for instance, the evaluation of cross-lingual transfer learning.",https://aclanthology.org/2023.emnlp-main.506,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{berdicevskis-etal-2023-superlim,
    title = ""Superlim: A {S}wedish Language Understanding Evaluation Benchmark"",
    author = {Berdicevskis, Aleksandrs  and
      Bouma, Gerlof  and
      Kurtz, Robin  and
      Morger, Felix  and
      {\""O}hman, Joey  and
      Adesam, Yvonne  and
      Borin, Lars  and
      Dann{\'e}lls, Dana  and
      Forsberg, Markus  and
      Isbister, Tim  and
      Lindahl, Anna  and
      Malmsten, Martin  and
      Rekathati, Faton  and
      Sahlgren, Magnus  and
      Volodina, Elena  and
      B{\""o}rjeson, Love  and
      Hengchen, Simon  and
      Tahmasebi, Nina},
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.506/"",
    doi = ""10.18653/v1/2023.emnlp-main.506"",
    pages = ""8137--8153"",
    abstract = ""We present Superlim, a multi-task NLP benchmark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. We describe the dataset, the tasks, the leaderboard and report the baseline results yielded by a reference implementation. The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark. We address methodological challenges, such as mitigating the Anglocentric bias when creating datasets for a less-resourced language; choosing the most appropriate measures; documenting the datasets and making the leaderboard convenient and transparent. We also highlight other potential usages of the dataset, such as, for instance, the evaluation of cross-lingual transfer learning.""
}",berdicevskisSuperlimSwedishLanguage2023
This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models,"Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.",https://aclanthology.org/2023.emnlp-main.531,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{garcia-ferrero-etal-2023-dataset,
    title = ""This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models"",
    author = ""Garc{\'i}a-Ferrero, Iker  and
      Altuna, Bego{\~n}a  and
      Alvez, Javier  and
      Gonzalez-Dios, Itziar  and
      Rigau, German"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.531/"",
    doi = ""10.18653/v1/2023.emnlp-main.531"",
    pages = ""8596--8615"",
    abstract = ""Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.""
}",garcia-ferreroThisNotDataset2023a
"MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation","Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models. Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains, informing their practical applications and future advancements.",https://aclanthology.org/2023.emnlp-main.540,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{he-etal-2023-medeval,
    title = ""{M}ed{E}val: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation"",
    author = ""He, Zexue  and
      Wang, Yu  and
      Yan, An  and
      Liu, Yao  and
      Chang, Eric  and
      Gentili, Amilcare  and
      McAuley, Julian  and
      Hsu, Chun-Nan"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.540/"",
    doi = ""10.18653/v1/2023.emnlp-main.540"",
    pages = ""8725--8744"",
    abstract = ""Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models. Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains, informing their practical applications and future advancements.""
}",heMedEvalMultilevelMultitask2023a
ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization,"Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative dialogues. However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages. To address this language resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first Chinese dataset for benchmarking target-independent stance detection and debate summarization. Our dataset consists of 1,218 real-world debates that were conducted in Chinese on 476 unique topics, containing 2,436 stance-specific summaries and 14,133 fully annotated utterances. Besides providing a versatile testbed for future research, we also conduct an empirical study on the dataset and propose an integrated task. The results show the challenging nature of the dataset and suggest a potential of incorporating stance detection in summarization for argumentative dialogue.",https://aclanthology.org/2023.emnlp-main.582,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zhao-etal-2023-orchid,
    title = ""{ORCHID}: A {C}hinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization"",
    author = ""Zhao, Xiutian  and
      Wang, Ke  and
      Peng, Wei"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.582/"",
    doi = ""10.18653/v1/2023.emnlp-main.582"",
    pages = ""9358--9375"",
    abstract = ""Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative dialogues. However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages. To address this language resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first Chinese dataset for benchmarking target-independent stance detection and debate summarization. Our dataset consists of 1,218 real-world debates that were conducted in Chinese on 476 unique topics, containing 2,436 stance-specific summaries and 14,133 fully annotated utterances. Besides providing a versatile testbed for future research, we also conduct an empirical study on the dataset and propose an integrated task. The results show the challenging nature of the dataset and suggest a potential of incorporating stance detection in summarization for argumentative dialogue.""
}",zhaoORCHIDChineseDebate2023
EpiK-Eval: Evaluation for Language Models as Epistemic Models,"In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents—a crucial ability in numerous applications—remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs’ proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from this study offer insights for developing more robust and reliable LLMs. Our code and benchmark are available at https://github.com/chandar-lab/EpiK-Eval",https://aclanthology.org/2023.emnlp-main.593,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{prato-etal-2023-epik,
    title = ""{E}pi{K}-Eval: Evaluation for Language Models as Epistemic Models"",
    author = ""Prato, Gabriele  and
      Huang, Jerry  and
      Parthasarathi, Prasanna  and
      Sodhani, Shagun  and
      Chandar, Sarath"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.593/"",
    doi = ""10.18653/v1/2023.emnlp-main.593"",
    pages = ""9523--9557"",
    abstract = ""In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents{---}a crucial ability in numerous applications{---}remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from this study offer insights for developing more robust and reliable LLMs. Our code and benchmark are available at https://github.com/chandar-lab/EpiK-Eval""
}",pratoEpiKevalEvaluationLanguage2023a
SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization,"With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8% below estimated human performance, highlighting the gaps in LLMs’ ability to reason about facts and detect inconsistencies when they occur.",https://aclanthology.org/2023.emnlp-main.600,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{laban-etal-2023-summedits,
    title = ""{S}umm{E}dits: Measuring {LLM} Ability at Factual Reasoning Through The Lens of Summarization"",
    author = ""Laban, Philippe  and
      Kryscinski, Wojciech  and
      Agarwal, Divyansh  and
      Fabbri, Alexander  and
      Xiong, Caiming  and
      Joty, Shafiq  and
      Wu, Chien-Sheng"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.600/"",
    doi = ""10.18653/v1/2023.emnlp-main.600"",
    pages = ""9662--9676"",
    abstract = ""With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8{\%} below estimated human performance, highlighting the gaps in LLMs' ability to reason about facts and detect inconsistencies when they occur.""
}",labanSummEditsMeasuringLLM2023a
MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark,"There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple languages.",https://aclanthology.org/2023.emnlp-main.616,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{macko-etal-2023-multitude,
    title = ""{MULTIT}u{DE}: Large-Scale Multilingual Machine-Generated Text Detection Benchmark"",
    author = ""Macko, Dominik  and
      Moro, Robert  and
      Uchendu, Adaku  and
      Lucas, Jason  and
      Yamashita, Michiharu  and
      Pikuliak, Mat{\'u}{\v{s}}  and
      Srba, Ivan  and
      Le, Thai  and
      Lee, Dongwon  and
      Simko, Jakub  and
      Bielikova, Maria"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.616/"",
    doi = ""10.18653/v1/2023.emnlp-main.616"",
    pages = ""9960--9987"",
    abstract = ""There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple languages.""
}",mackoMULTITuDELargescaleMultilingual2023
Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark,"Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand social language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor & sarcasm, offensiveness, sentiment & emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The resources are released at https://github.com/minjechoi/SOCKET.",https://aclanthology.org/2023.emnlp-main.699,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{choi-etal-2023-llms,
    title = ""Do {LLM}s Understand Social Knowledge? Evaluating the Sociability of Large Language Models with {S}oc{KET} Benchmark"",
    author = ""Choi, Minje  and
      Pei, Jiaxin  and
      Kumar, Sagar  and
      Shu, Chang  and
      Jurgens, David"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.699/"",
    doi = ""10.18653/v1/2023.emnlp-main.699"",
    pages = ""11370--11403"",
    abstract = ""Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand social language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor {\&} sarcasm, offensiveness, sentiment {\&} emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The resources are released at https://github.com/minjechoi/SOCKET.""
}",choiLLMsUnderstandSocial2023
TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models,"Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks are mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proof but also evaluates a generative LM’s reasoning ability on formulas and capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from web, annotate the simplification process manually, and translate it into the “Lean” formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we also create three automatically generated training and testing datasets of varying difficulty and distributions. Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM’s including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data, and provide a new tool to study the generative LM’s ability on both formal and mathematical reasoning.",https://aclanthology.org/2023.emnlp-main.711,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{xiong-etal-2023-trigo,
    title = ""{TRIGO}: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models"",
    author = ""Xiong, Jing  and
      Shen, Jianhao  and
      Yuan, Ye  and
      Wang, Haiming  and
      Yin, Yichun  and
      Liu, Zhengying  and
      Li, Lin  and
      Guo, Zhijiang  and
      Cao, Qingxing  and
      Huang, Yinya  and
      Zheng, Chuanyang  and
      Liang, Xiaodan  and
      Zhang, Ming  and
      Liu, Qun"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.711/"",
    doi = ""10.18653/v1/2023.emnlp-main.711"",
    pages = ""11594--11632"",
    abstract = ""Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks are mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proof but also evaluates a generative LM`s reasoning ability on formulas and capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from web, annotate the simplification process manually, and translate it into the {\textquotedblleft}Lean{\textquotedblright} formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we also create three automatically generated training and testing datasets of varying difficulty and distributions. Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM`s including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data, and provide a new tool to study the generative LM`s ability on both formal and mathematical reasoning.""
}",xiongTRIGOBenchmarkingFormal2023a
Can Language Models Understand Physical Concepts?,"Language models (LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is unclear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up parameters of LMs 134\times. Our dataset is available at https://github.com/TobiasLee/VEC.",https://aclanthology.org/2023.emnlp-main.726,2023,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{li-etal-2023-language-models,
    title = ""Can Language Models Understand Physical Concepts?"",
    author = ""Li, Lei  and
      Xu, Jingjing  and
      Dong, Qingxiu  and
      Zheng, Ce  and
      Sun, Xu  and
      Kong, Lingpeng  and
      Liu, Qi"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.726/"",
    doi = ""10.18653/v1/2023.emnlp-main.726"",
    pages = ""11843--11861"",
    abstract = ""Language models (LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is unclear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85{\%} on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up parameters of LMs $134\times$. Our dataset is available at https://github.com/TobiasLee/VEC.""
}",liCanLanguageModels2023
Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,"Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.",https://aclanthology.org/2023.emnlp-main.760,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{koto-etal-2023-large,
    title = ""Large Language Models Only Pass Primary School Exams in {I}ndonesia: A Comprehensive Test on {I}ndo{MMLU}"",
    author = ""Koto, Fajri  and
      Aisyah, Nurul  and
      Li, Haonan  and
      Baldwin, Timothy"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.760/"",
    doi = ""10.18653/v1/2023.emnlp-main.760"",
    pages = ""12359--12374"",
    abstract = ""Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46{\%} of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.""
}",kotoLargeLanguageModels2023
Multilingual Large Language Models Are Not (Yet) Code-Switchers,"Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current “multilingualism’ in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.",https://aclanthology.org/2023.emnlp-main.774,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2023-multilingual,
    title = ""Multilingual Large Language Models Are Not (Yet) Code-Switchers"",
    author = ""Zhang, Ruochen  and
      Cahyawijaya, Samuel  and
      Cruz, Jan Christian Blaise  and
      Winata, Genta  and
      Aji, Alham Fikri"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.774/"",
    doi = ""10.18653/v1/2023.emnlp-main.774"",
    pages = ""12567--12582"",
    abstract = ""Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current {\textquotedblleft}multilingualism' in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.""
}",zhangMultilingualLargeLanguage2023
Doolittle: Benchmarks and Corpora for Academic Writing Formalization,"Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle.",https://aclanthology.org/2023.emnlp-main.809,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{diao-etal-2023-doolittle,
    title = ""Doolittle: Benchmarks and Corpora for Academic Writing Formalization"",
    author = ""Diao, Shizhe  and
      Lei, Yongyu  and
      Pan, Liangming  and
      Fang, Tianqing  and
      Zhou, Wangchunshu  and
      Keh, Sedrick  and
      Kan, Min-Yen  and
      Zhang, Tong"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.809/"",
    doi = ""10.18653/v1/2023.emnlp-main.809"",
    pages = ""13093--13111"",
    abstract = ""Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle.""
}",diaoDoolittleBenchmarksCorpora2023a
BLESS: Benchmarking Large Language Models on Sentence Simplification,"We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics.",https://aclanthology.org/2023.emnlp-main.821,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{kew-etal-2023-bless,
    title = ""{BLESS}: Benchmarking Large Language Models on Sentence Simplification"",
    author = ""Kew, Tannon  and
      Chi, Alison  and
      V{\'a}squez-Rodr{\'i}guez, Laura  and
      Agrawal, Sweta  and
      Aumiller, Dennis  and
      Alva-Manchego, Fernando  and
      Shardlow, Matthew"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.821/"",
    doi = ""10.18653/v1/2023.emnlp-main.821"",
    pages = ""13291--13309"",
    abstract = ""We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics.""
}",kewBLESSBenchmarkingLarge2023a
Can We Edit Multimodal Large Language Models?,"In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights.",https://aclanthology.org/2023.emnlp-main.856,2023,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{cheng-etal-2023-edit,
    title = ""Can We Edit Multimodal Large Language Models?"",
    author = ""Cheng, Siyuan  and
      Tian, Bozhong  and
      Liu, Qingbin  and
      Chen, Xi  and
      Wang, Yongheng  and
      Chen, Huajun  and
      Zhang, Ningyu"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.856/"",
    doi = ""10.18653/v1/2023.emnlp-main.856"",
    pages = ""13877--13888"",
    abstract = ""In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights.""
}",chengCanWeEdit2023
BRAINTEASER: Lateral Thinking Puzzles for Large Language Models,"The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BrainTeaser: a multiple-choice Question Answering task designed to test the model’s ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BrainTeaser based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.",https://aclanthology.org/2023.emnlp-main.885,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{jiang-etal-2023-brainteaser,
    title = ""{BRAINTEASER}: Lateral Thinking Puzzles for Large Language Models"",
    author = ""Jiang, Yifan  and
      Ilievski, Filip  and
      Ma, Kaixin  and
      Sourati, Zhivar"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.885/"",
    doi = ""10.18653/v1/2023.emnlp-main.885"",
    pages = ""14317--14332"",
    abstract = ""The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BrainTeaser: a multiple-choice Question Answering task designed to test the model`s ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BrainTeaser based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.""
}",jiangBRAINTEASERLateralThinking2023
FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,"Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.",https://aclanthology.org/2023.emnlp-main.890,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{kim-etal-2023-fantom,
    title = ""{FANT}o{M}: A Benchmark for Stress-testing Machine Theory of Mind in Interactions"",
    author = ""Kim, Hyunwoo  and
      Sclar, Melanie  and
      Zhou, Xuhui  and
      Bras, Ronan  and
      Kim, Gunhee  and
      Choi, Yejin  and
      Sap, Maarten"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.890/"",
    doi = ""10.18653/v1/2023.emnlp-main.890"",
    pages = ""14397--14413"",
    abstract = ""Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.""
}",kimFANToMBenchmarkStresstesting2023
R2H: Building Multimodal Navigation Helpers that Respond to Help Requests,"Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent’s ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, named SeeRee, and employing a multi-modal large language model in a zero-shot manner. Analysis of the task and method was conducted based on both automatic benchmarking and human evaluations.",https://aclanthology.org/2023.emnlp-main.915,2023,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{fan-etal-2023-r2h,
    title = ""{R}2{H}: Building Multimodal Navigation Helpers that Respond to Help Requests"",
    author = ""Fan, Yue  and
      Gu, Jing  and
      Zheng, Kaizhi  and
      Wang, Xin"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.915/"",
    doi = ""10.18653/v1/2023.emnlp-main.915"",
    pages = ""14803--14819"",
    abstract = ""Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent`s ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, named SeeRee, and employing a multi-modal large language model in a zero-shot manner. Analysis of the task and method was conducted based on both automatic benchmarking and human evaluations.""
}",fanR2HBuildingMultimodal2023
Unveiling the Essence of Poetry: Introducing a Comprehensive Dataset and Benchmark for Poem Summarization,"While research in natural language processing has progressed significantly in creative language generation, the question of whether language models can interpret the intended meaning of creative language largely remains unanswered. Poetry as a creative art form has existed for generations, and summarization of such content requires deciphering the figurative patterns to find out the actual intent and message of the poet. This task can provide the researchers an opportunity to evaluate the creative language interpretation capacity of the language models. Unlike typical text, summarization of poems is a challenging task as poems carry a deeper meaning, which can be easily lost if only the literal meaning is considered. That being said, we propose a new task in the field of natural language understanding called ‘Poem Summarization’. As a starting, we propose the first-ever dataset for this task, named ‘PoemSum’, consisting of 3011 samples of poetry and its corresponding summarized interpretation in the English language. We have benchmarked the performance of different state-of-the-art summarization models and provided observations on their limitations. The dataset and all relevant code used in this work have been made publicly available.",https://aclanthology.org/2023.emnlp-main.920,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{mahbub-etal-2023-unveiling,
    title = ""Unveiling the Essence of Poetry: Introducing a Comprehensive Dataset and Benchmark for Poem Summarization"",
    author = ""Mahbub, Ridwan  and
      Khan, Ifrad  and
      Anuva, Samiha  and
      Shahriar, Md Shihab  and
      Laskar, Md Tahmid Rahman  and
      Ahmed, Sabbir"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.920/"",
    doi = ""10.18653/v1/2023.emnlp-main.920"",
    pages = ""14878--14886"",
    abstract = ""While research in natural language processing has progressed significantly in creative language generation, the question of whether language models can interpret the intended meaning of creative language largely remains unanswered. Poetry as a creative art form has existed for generations, and summarization of such content requires deciphering the figurative patterns to find out the actual intent and message of the poet. This task can provide the researchers an opportunity to evaluate the creative language interpretation capacity of the language models. Unlike typical text, summarization of poems is a challenging task as poems carry a deeper meaning, which can be easily lost if only the literal meaning is considered. That being said, we propose a new task in the field of natural language understanding called {\textquoteleft}Poem Summarization'. As a starting, we propose the first-ever dataset for this task, named {\textquoteleft}PoemSum', consisting of 3011 samples of poetry and its corresponding summarized interpretation in the English language. We have benchmarked the performance of different state-of-the-art summarization models and provided observations on their limitations. The dataset and all relevant code used in this work have been made publicly available.""
}",mahbubUnveilingEssencePoetry2023
CRAB: Assessing the Strength of Causal Relationships Between Real-world Events,"Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of causal relationships of events in narratives. In this work, we present CRAB, a new Causal Reasoning Assessment Benchmark designed to evaluate causal understanding of events in real-world narratives. CRAB contains fine-grained, contextual causality annotations for ~2.7K pairs of real-world events that describe various newsworthy event timelines (e.g., the acquisition of Twitter by Elon Musk). Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task. Motivated by classical causal principles, we also analyze the causal structures of groups of events in CRAB, and find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains. We make our dataset and code available to the research community.",https://aclanthology.org/2023.emnlp-main.940,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{romanou-etal-2023-crab,
    title = ""{CRAB}: Assessing the Strength of Causal Relationships Between Real-world Events"",
    author = ""Romanou, Angelika  and
      Montariol, Syrielle  and
      Paul, Debjit  and
      Laugier, Leo  and
      Aberer, Karl  and
      Bosselut, Antoine"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.940/"",
    doi = ""10.18653/v1/2023.emnlp-main.940"",
    pages = ""15198--15216"",
    abstract = ""Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of causal relationships of events in narratives. In this work, we present CRAB, a new Causal Reasoning Assessment Benchmark designed to evaluate causal understanding of events in real-world narratives. CRAB contains fine-grained, contextual causality annotations for {\textasciitilde}2.7K pairs of real-world events that describe various newsworthy event timelines (e.g., the acquisition of Twitter by Elon Musk). Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task. Motivated by classical causal principles, we also analyze the causal structures of groups of events in CRAB, and find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains. We make our dataset and code available to the research community.""
}",romanouCRABAssessingStrength2023
MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,"The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model’s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.",https://aclanthology.org/2023.emnlp-main.971,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zhong-etal-2023-mquake,
    title = ""{MQ}u{AKE}: Assessing Knowledge Editing in Language Models via Multi-Hop Questions"",
    author = ""Zhong, Zexuan  and
      Wu, Zhengxuan  and
      Manning, Christopher  and
      Potts, Christopher  and
      Chen, Danqi"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.971/"",
    doi = ""10.18653/v1/2023.emnlp-main.971"",
    pages = ""15686--15702"",
    abstract = ""The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model`s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.""
}",zhongMQuAKEAssessingKnowledge2023
Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization,"Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data. In this work, we propose TempoSum, a novel benchmark that contains data samples from 2010 to 2022, to understand the temporal generalization ability of abstractive summarization models. Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Moreover, existing faithfulness enhancement methods cannot reliably improve the faithfulness of summarization models on future data. Finally, we discuss several recommendations to the research community on how to evaluate and improve the temporal generalization capability of text summarization models.",https://aclanthology.org/2023.emnlp-main.1007,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{cheang-etal-2023-lms,
    title = ""Can {LM}s Generalize to Future Data? An Empirical Analysis on Text Summarization"",
    author = ""Cheang, Chi  and
      Chan, Hou  and
      Wong, Derek  and
      Liu, Xuebo  and
      Li, Zhaocong  and
      Sun, Yanming  and
      Liu, Shudong  and
      Chao, Lidia"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.1007/"",
    doi = ""10.18653/v1/2023.emnlp-main.1007"",
    pages = ""16205--16217"",
    abstract = ""Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data. In this work, we propose TempoSum, a novel benchmark that contains data samples from 2010 to 2022, to understand the temporal generalization ability of abstractive summarization models. Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Moreover, existing faithfulness enhancement methods cannot reliably improve the faithfulness of summarization models on future data. Finally, we discuss several recommendations to the research community on how to evaluate and improve the temporal generalization capability of text summarization models.""
}",cheangCanLMsGeneralize2023
A Benchmark for Reasoning with Spatial Prepositions,"Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes original datasets in English and Romanian and aims to probe the limits of reasoning about spatial relations in large language models. We use prompt engineering to study the performance of two families of large language models, PaLM and GPT-3, on our benchmark. Our results show considerable variability in the performance of smaller and larger models, as well as across prompts and languages. However, none of the models reaches human performance.",https://aclanthology.org/2023.emnlp-main.1015,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{comsa-narayanan-2023-benchmark,
    title = ""A Benchmark for Reasoning with Spatial Prepositions"",
    author = ""Comsa, Iulia  and
      Narayanan, Srini"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.1015/"",
    doi = ""10.18653/v1/2023.emnlp-main.1015"",
    pages = ""16328--16335"",
    abstract = ""Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes original datasets in English and Romanian and aims to probe the limits of reasoning about spatial relations in large language models. We use prompt engineering to study the performance of two families of large language models, PaLM and GPT-3, on our benchmark. Our results show considerable variability in the performance of smaller and larger models, as well as across prompts and languages. However, none of the models reaches human performance.""
}",comsaBenchmarkReasoningSpatial2023a
JASMINE: Arabic GPT Models for Few-Shot Learning,"Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset ( 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a wide range of NLP tasks. We aim to responsibly release our models and evaluation benchmark with interested researchers, along with code for experimenting with them.",https://aclanthology.org/2023.emnlp-main.1040,2023,emnlp-main,Yes,Language,Benchmark,"@inproceedings{billah-nagoudi-etal-2023-jasmine,
    title = ""{JASMINE}: {A}rabic {GPT} Models for Few-Shot Learning"",
    author = ""Billah Nagoudi, El Moatez  and
      Abdul-Mageed, Muhammad  and
      Elmadany, AbdelRahim  and
      Inciarte, Alcides  and
      Islam Khondaker, Md Tawkat"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.1040/"",
    doi = ""10.18653/v1/2023.emnlp-main.1040"",
    pages = ""16721--16744"",
    abstract = ""Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset ( 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a wide range of NLP tasks. We aim to responsibly release our models and evaluation benchmark with interested researchers, along with code for experimenting with them.""
}",billahnagoudiJASMINEArabicGPT2023
PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models,"The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models and users. On the one hand, powerful LMs achieve state-of-the-art performance over numerous downstream NLP tasks. On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage. To address these issues, many recent works propose privacy-preserving language models (PPLMs) with differential privacy (DP). Unfortunately, different DP implementations make it challenging for a fair comparison among existing PPLMs. In this paper, we present PrivLM-Bench, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs. Instead of only reporting DP parameters, PrivLM-Bench sheds light on the neglected inference data privacy during actual usage. PrivLM-Bench first clearly defines multi-faceted privacy objectives. Then, PrivLM-Bench constructs a unified pipeline to perform private fine-tuning. Lastly, PrivLM-Bench performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results. The empirical attack results are used to fairly and intuitively evaluate the privacy leakage of various PPLMs. We conduct extensive experiments on three datasets of GLUE for mainstream LMs.",https://aclanthology.org/2024.acl-long.4,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{li-etal-2024-privlm,
    title = ""{P}riv{LM}-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models"",
    author = ""Li, Haoran  and
      Guo, Dadi  and
      Li, Donghao  and
      Fan, Wei  and
      Hu, Qi  and
      Liu, Xin  and
      Chan, Chunkit  and
      Yao, Duanyi  and
      Yao, Yuan  and
      Song, Yangqiu"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.4/"",
    doi = ""10.18653/v1/2024.acl-long.4"",
    pages = ""54--73"",
    abstract = ""The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models and users. On the one hand, powerful LMs achieve state-of-the-art performance over numerous downstream NLP tasks. On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage. To address these issues, many recent works propose privacy-preserving language models (PPLMs) with differential privacy (DP). Unfortunately, different DP implementations make it challenging for a fair comparison among existing PPLMs. In this paper, we present PrivLM-Bench, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs. Instead of only reporting DP parameters, PrivLM-Bench sheds light on the neglected inference data privacy during actual usage. PrivLM-Bench first clearly defines multi-faceted privacy objectives. Then, PrivLM-Bench constructs a unified pipeline to perform private fine-tuning. Lastly, PrivLM-Bench performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results. The empirical attack results are used to fairly and intuitively evaluate the privacy leakage of various PPLMs. We conduct extensive experiments on three datasets of GLUE for mainstream LMs.""
}",liPrivLMbenchMultilevelPrivacy2024
SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs,"Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs’ numerical reasoning and fusion skills.",https://aclanthology.org/2024.acl-long.17,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{hu-etal-2024-sportsmetrics,
    title = ""{S}ports{M}etrics: Blending Text and Numerical Data to Understand Information Fusion in {LLM}s"",
    author = ""Hu, Yebowen  and
      Song, Kaiqiang  and
      Cho, Sangwoo  and
      Wang, Xiaoyang  and
      Foroosh, Hassan  and
      Yu, Dong  and
      Liu, Fei"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.17/"",
    doi = ""10.18653/v1/2024.acl-long.17"",
    pages = ""267--278"",
    abstract = ""Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills.""
}",huSportsMetricsBlendingText2024
FOFO: A Benchmark to Evaluate LLMs’ Format-Following Capability,"This paper presents FoFo, a pioneering benchmark for evaluating large language models’ (LLMs) ability to follow complex, domain-specific formats, a crucial yet under-examined capability for their application as AI agents. Despite LLMs’ advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs’ format-following performance is independent of their content generation quality; and LLMs’ format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo’s role in guiding the selection of domain-specific AI agents. FoFo will be publicly released, contributing a critical tool for advancing LLM evaluation and application.",https://aclanthology.org/2024.acl-long.40,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{xia-etal-2024-fofo,
    title = ""{FOFO}: A Benchmark to Evaluate {LLM}s' Format-Following Capability"",
    author = ""Xia, Congying  and
      Xing, Chen  and
      Du, Jiangshu  and
      Yang, Xinyi  and
      Feng, Yihao  and
      Xu, Ran  and
      Yin, Wenpeng  and
      Xiong, Caiming"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.40/"",
    doi = ""10.18653/v1/2024.acl-long.40"",
    pages = ""680--699"",
    abstract = ""This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet under-examined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo`s role in guiding the selection of domain-specific AI agents. FoFo will be publicly released, contributing a critical tool for advancing LLM evaluation and application.""
}",xiaFOFOBenchmarkEvaluate2024
The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants,"We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.",https://aclanthology.org/2024.acl-long.44,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{bandarkar-etal-2024-belebele,
    title = ""The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants"",
    author = ""Bandarkar, Lucas  and
      Liang, Davis  and
      Muller, Benjamin  and
      Artetxe, Mikel  and
      Shukla, Satya Narayan  and
      Husa, Donald  and
      Goyal, Naman  and
      Krishnan, Abhinandan  and
      Zettlemoyer, Luke  and
      Khabsa, Madian"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.44/"",
    doi = ""10.18653/v1/2024.acl-long.44"",
    pages = ""749--775"",
    abstract = ""We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.""
}",bandarkarBelebeleBenchmarkParallel2024
VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks,"Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on *realistic visually grounded tasks*. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.",https://aclanthology.org/2024.acl-long.50,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{koh-etal-2024-visualwebarena,
    title = ""{V}isual{W}eb{A}rena: Evaluating Multimodal Agents on Realistic Visual Web Tasks"",
    author = ""Koh, Jing Yu  and
      Lo, Robert  and
      Jang, Lawrence  and
      Duvvur, Vikram  and
      Lim, Ming  and
      Huang, Po-Yu  and
      Neubig, Graham  and
      Zhou, Shuyan  and
      Salakhutdinov, Russ  and
      Fried, Daniel"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.50/"",
    doi = ""10.18653/v1/2024.acl-long.50"",
    pages = ""881--905"",
    abstract = ""Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on *realistic visually grounded tasks*. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.""
}",kohVisualWebArenaEvaluatingMultimodal2024
Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation,"In response to the limitations of manual ad creation, significant research has been conducted in the field of automatic ad text generation (ATG). However, the lack of comprehensive benchmarks and well-defined problem sets has made comparing different methods challenging. To tackle these challenges, we standardize the task of ATG and propose a first benchmark dataset, CAMERA, carefully designed and enabling the utilization of multi-modal information and facilitating industry-wise evaluations. Our extensive experiments with a variety of nine baselines, from classical methods to state-of-the-art models including large language models (LLMs), show the current state and the remaining challenges. We also explore how existing metrics in ATG and an LLM-based evaluator align with human evaluations.",https://aclanthology.org/2024.acl-long.54,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{mita-etal-2024-striking,
    title = ""Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation"",
    author = ""Mita, Masato  and
      Murakami, Soichiro  and
      Kato, Akihiko  and
      Zhang, Peinan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.54/"",
    doi = ""10.18653/v1/2024.acl-long.54"",
    pages = ""955--972"",
    abstract = ""In response to the limitations of manual ad creation, significant research has been conducted in the field of automatic ad text generation (ATG). However, the lack of comprehensive benchmarks and well-defined problem sets has made comparing different methods challenging. To tackle these challenges, we standardize the task of ATG and propose a first benchmark dataset, CAMERA, carefully designed and enabling the utilization of multi-modal information and facilitating industry-wise evaluations. Our extensive experiments with a variety of nine baselines, from classical methods to state-of-the-art models including large language models (LLMs), show the current state and the remaining challenges. We also explore how existing metrics in ATG and an LLM-based evaluator align with human evaluations.""
}",mitaStrikingGoldAdvertising2024
TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models,"Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly comprehending the intricacies of the world.Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark.To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena.TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models.We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings.Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning.Besides, LLMs exhibit capability discrepancies across different reasoning categories.Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges.We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning.Code and data are available at https://github.com/zchuz/TimeBench.",https://aclanthology.org/2024.acl-long.66,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{chu-etal-2024-timebench,
    title = ""{T}ime{B}ench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models"",
    author = ""Chu, Zheng  and
      Chen, Jingchang  and
      Chen, Qianglong  and
      Yu, Weijiang  and
      Wang, Haotian  and
      Liu, Ming  and
      Qin, Bing"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.66/"",
    doi = ""10.18653/v1/2024.acl-long.66"",
    pages = ""1204--1228"",
    abstract = ""Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly comprehending the intricacies of the world.Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark.To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena.TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models.We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings.Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning.Besides, LLMs exhibit capability discrepancies across different reasoning categories.Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges.We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning.Code and data are available at https://github.com/zchuz/TimeBench.""
}",chuTimeBenchComprehensiveEvaluation2024
WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models,"To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For benchmarking procedure, to ensure an apples-to-apples comparison, we first adjust each watermarking method’s hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For task selection, we diversify the input and output length to form a five-category taxonomy, covering 9 tasks. (3) For evaluation metric, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate 4 open-source watermarks on 2 LLMs under 2 watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github.com/THU-KEG/WaterBench.",https://aclanthology.org/2024.acl-long.83,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{tu-etal-2024-waterbench,
    title = ""{W}ater{B}ench: Towards Holistic Evaluation of Watermarks for Large Language Models"",
    author = ""Tu, Shangqing  and
      Sun, Yuliang  and
      Bai, Yushi  and
      Yu, Jifan  and
      Hou, Lei  and
      Li, Juanzi"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.83/"",
    doi = ""10.18653/v1/2024.acl-long.83"",
    pages = ""1517--1542"",
    abstract = ""To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For benchmarking procedure, to ensure an apples-to-apples comparison, we first adjust each watermarking method`s hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For task selection, we diversify the input and output length to form a five-category taxonomy, covering 9 tasks. (3) For evaluation metric, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate 4 open-source watermarks on 2 LLMs under 2 watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github.com/THU-KEG/WaterBench.""
}",tuWaterBenchHolisticEvaluation2024
"Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding","The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events.We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.",https://aclanthology.org/2024.acl-long.87,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2024-analyzing,
    title = ""Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding"",
    author = ""Zhang, Zhihan  and
      Cao, Yixin  and
      Ye, Chenchen  and
      Ma, Yunshan  and
      Liao, Lizi  and
      Chua, Tat-Seng"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.87/"",
    doi = ""10.18653/v1/2024.acl-long.87"",
    pages = ""1588--1606"",
    abstract = ""The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events.We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.""
}",zhangAnalyzingTemporalComplex2024
Selene: Pioneering Automated Proof in Software Verification,"Ensuring correctness is a pivotal aspect of software engineering. Among the various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce Selene in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level operating system microkernel, seL4. Selene provides a comprehensive framework for end-to-end proof generation and a lightweight verification environment. Our experimental results with advanced large language models (LLMs), such as GPT-3.5-turbo and GPT-4, highlight the capabilities of LLMs in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by Selene can be mitigated in future research endeavors.",https://aclanthology.org/2024.acl-long.98,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2024-selene,
    title = ""Selene: Pioneering Automated Proof in Software Verification"",
    author = ""Zhang, Lichen  and
      Lu, Shuai  and
      Duan, Nan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.98/"",
    doi = ""10.18653/v1/2024.acl-long.98"",
    pages = ""1776--1789"",
    abstract = ""Ensuring correctness is a pivotal aspect of software engineering. Among the various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce Selene in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level operating system microkernel, seL4. Selene provides a comprehensive framework for end-to-end proof generation and a lightweight verification environment. Our experimental results with advanced large language models (LLMs), such as GPT-3.5-turbo and GPT-4, highlight the capabilities of LLMs in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by Selene can be mitigated in future research endeavors.""
}",zhangSelenePioneeringAutomated2024
ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models,"Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks.",https://aclanthology.org/2024.acl-long.111,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{ren-etal-2024-valuebench,
    title = ""{V}alue{B}ench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models"",
    author = ""Ren, Yuanyi  and
      Ye, Haoran  and
      Fang, Hanjun  and
      Zhang, Xin  and
      Song, Guojie"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.111/"",
    doi = ""10.18653/v1/2024.acl-long.111"",
    pages = ""2015--2040"",
    abstract = ""Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks.""
}",renValueBenchComprehensivelyEvaluating2024
Exploring the Potential of Large Language Models in Computational Argumentation,"Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors.",https://aclanthology.org/2024.acl-long.126,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{chen-etal-2024-exploring-potential,
    title = ""Exploring the Potential of Large Language Models in Computational Argumentation"",
    author = ""Chen, Guizhen  and
      Cheng, Liying  and
      Luu, Anh Tuan  and
      Bing, Lidong"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.126/"",
    doi = ""10.18653/v1/2024.acl-long.126"",
    pages = ""2309--2330"",
    abstract = ""Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors.""
}",chenExploringPotentialLarge2024a
MELA: Multilingual Evaluation of Linguistic Acceptability,"In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability—MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language—Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks.",https://aclanthology.org/2024.acl-long.146,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2024-mela,
    title = ""{MELA}: Multilingual Evaluation of Linguistic Acceptability"",
    author = ""Zhang, Ziyin  and
      Liu, Yikang  and
      Huang, Weifang  and
      Mao, Junyu  and
      Wang, Rui  and
      Hu, Hai"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.146/"",
    doi = ""10.18653/v1/2024.acl-long.146"",
    pages = ""2658--2674"",
    abstract = ""In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability{---}MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language{---}Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks.""
}",zhangMELAMultilingualEvaluation2024
GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers,"Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs’ math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.",https://aclanthology.org/2024.acl-long.163,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{li-etal-2024-gsm,
    title = ""{GSM}-Plus: A Comprehensive Benchmark for Evaluating the Robustness of {LLM}s as Mathematical Problem Solvers"",
    author = ""Li, Qintong  and
      Cui, Leyang  and
      Zhao, Xueliang  and
      Kong, Lingpeng  and
      Bi, Wei"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.163/"",
    doi = ""10.18653/v1/2024.acl-long.163"",
    pages = ""2961--2984"",
    abstract = ""Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.""
}",liGSMplusComprehensiveBenchmark2024
"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding","Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs’ long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.",https://aclanthology.org/2024.acl-long.172,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{bai-etal-2024-longbench,
    title = ""{L}ong{B}ench: A Bilingual, Multitask Benchmark for Long Context Understanding"",
    author = ""Bai, Yushi  and
      Lv, Xin  and
      Zhang, Jiajie  and
      Lyu, Hongchang  and
      Tang, Jiankai  and
      Huang, Zhidian  and
      Du, Zhengxiao  and
      Liu, Xiao  and
      Zeng, Aohan  and
      Hou, Lei  and
      Dong, Yuxiao  and
      Tang, Jie  and
      Li, Juanzi"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.172/"",
    doi = ""10.18653/v1/2024.acl-long.172"",
    pages = ""3119--3137"",
    abstract = ""Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.""
}",baiLongBenchBilingualMultitask2024
Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models,"Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs’ capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored.In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles.Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl’s taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs’ outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.",https://aclanthology.org/2024.acl-long.173,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{chen-etal-2024-dr,
    title = ""{D}r.{A}cademy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models"",
    author = ""Chen, Yuyan  and
      Wu, Chenwei  and
      Yan, Songzhou  and
      Liu, Panjun  and
      Xiao, Yanghua"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.173/"",
    doi = ""10.18653/v1/2024.acl-long.173"",
    pages = ""3138--3167"",
    abstract = ""Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs' capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored.In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles.Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl`s taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs' outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.""
}",chenDrAcademyBenchmarkEvaluating2024
Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark,"This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.",https://aclanthology.org/2024.acl-long.177,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{park-etal-2024-open,
    title = ""Open {K}o-{LLM} Leaderboard: Evaluating Large Language Models in {K}orean with {K}o-H5 Benchmark"",
    author = ""Park, Chanjun  and
      Kim, Hyeonwoo  and
      Kim, Dahyun  and
      Cho, SeongHwan  and
      Kim, Sanghoon  and
      Lee, Sukyung  and
      Kim, Yungi  and
      Lee, Hwalsuk"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.177/"",
    doi = ""10.18653/v1/2024.acl-long.177"",
    pages = ""3220--3234"",
    abstract = ""This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.""
}",parkOpenKoLLMLeaderboard2024
OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,"Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97% on OlympiadBench, with a mere 10.74% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at https://github.com/OpenBMB/OlympiadBench",https://aclanthology.org/2024.acl-long.211,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{he-etal-2024-olympiadbench,
    title = ""{O}lympiad{B}ench: A Challenging Benchmark for Promoting {AGI} with Olympiad-Level Bilingual Multimodal Scientific Problems"",
    author = ""He, Chaoqun  and
      Luo, Renjie  and
      Bai, Yuzhuo  and
      Hu, Shengding  and
      Thai, Zhen  and
      Shen, Junhao  and
      Hu, Jinyi  and
      Han, Xu  and
      Huang, Yujie  and
      Zhang, Yuxiang  and
      Liu, Jie  and
      Qi, Lei  and
      Liu, Zhiyuan  and
      Sun, Maosong"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.211/"",
    doi = ""10.18653/v1/2024.acl-long.211"",
    pages = ""3828--3850"",
    abstract = ""Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97{\%} on OlympiadBench, with a mere 10.74{\%} in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at \url{https://github.com/OpenBMB/OlympiadBench}""
}",heOlympiadBenchChallengingBenchmark2024a
M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection,"The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain and multi-generator corpus of MGTs — M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github.com/mbzuai-nlp/M4GT-Bench.",https://aclanthology.org/2024.acl-long.218,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-m4gt,
    title = ""{M}4{GT}-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection"",
    author = ""Wang, Yuxia  and
      Mansurov, Jonibek  and
      Ivanov, Petar  and
      Su, Jinyan  and
      Shelmanov, Artem  and
      Tsvigun, Akim  and
      Mohammed Afzal, Osama  and
      Mahmoud, Tarek  and
      Puccetti, Giovanni  and
      Arnold, Thomas  and
      Aji, Alham  and
      Habash, Nizar  and
      Gurevych, Iryna  and
      Nakov, Preslav"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.218/"",
    doi = ""10.18653/v1/2024.acl-long.218"",
    pages = ""3964--3992"",
    abstract = ""The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain and multi-generator corpus of MGTs {---} M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github.com/mbzuai-nlp/M4GT-Bench.""
}",wangM4GTbenchEvaluationBenchmark2024
MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation,"Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and codes can be obtained from https://github.com/THU-KEG/MAVEN-Argument.",https://aclanthology.org/2024.acl-long.224,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-maven,
    title = ""{MAVEN}-{ARG}: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation"",
    author = ""Wang, Xiaozhi  and
      Peng, Hao  and
      Guan, Yong  and
      Zeng, Kaisheng  and
      Chen, Jianhui  and
      Hou, Lei  and
      Han, Xu  and
      Lin, Yankai  and
      Liu, Zhiyuan  and
      Xie, Ruobing  and
      Zhou, Jie  and
      Li, Juanzi"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.224/"",
    doi = ""10.18653/v1/2024.acl-long.224"",
    pages = ""4072--4091"",
    abstract = ""Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and codes can be obtained from https://github.com/THU-KEG/MAVEN-Argument.""
}",wangMAVENARGCompletingPuzzle2024
NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,"Complex reasoning ability is one of the most important features of Large Language Models (LLMs). Numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, we introduce a new benchmark NPHardEval. It contains a broad spectrum of 900 algorithmic questions belonging up to the NP-Hard complexity class, offering a rigorous measure of the reasoning ability of LLMs utilizing computational complexity. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.",https://aclanthology.org/2024.acl-long.225,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{fan-etal-2024-nphardeval,
    title = ""{NPH}ard{E}val: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes"",
    author = ""Fan, Lizhou  and
      Hua, Wenyue  and
      Li, Lingyao  and
      Ling, Haoyang  and
      Zhang, Yongfeng"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.225/"",
    doi = ""10.18653/v1/2024.acl-long.225"",
    pages = ""4092--4114"",
    abstract = ""Complex reasoning ability is one of the most important features of Large Language Models (LLMs). Numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, we introduce a new benchmark NPHardEval. It contains a broad spectrum of 900 algorithmic questions belonging up to the NP-Hard complexity class, offering a rigorous measure of the reasoning ability of LLMs utilizing computational complexity. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.""
}",fanNPHardEvalDynamicBenchmark2024
A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains,"Prompting language models to provide step-by-step answers (e.g., “Chain-of-Thought”) is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model’s answer, across a variety of datasets and state-of-the-art language models. Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions. Available at https://reveal-dataset.github.io/ .",https://aclanthology.org/2024.acl-long.254,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{jacovi-etal-2024-chain,
    title = ""A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains"",
    author = ""Jacovi, Alon  and
      Bitton, Yonatan  and
      Bohnet, Bernd  and
      Herzig, Jonathan  and
      Honovich, Or  and
      Tseng, Michael  and
      Collins, Michael  and
      Aharoni, Roee  and
      Geva, Mor"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.254/"",
    doi = ""10.18653/v1/2024.acl-long.254"",
    pages = ""4615--4634"",
    abstract = ""Prompting language models to provide step-by-step answers (e.g., {\textquotedblleft}Chain-of-Thought{\textquotedblright}) is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model`s answer, across a variety of datasets and state-of-the-art language models. Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions. Available at https://reveal-dataset.github.io/ .""
}",jacoviChainofthoughtStrongIts2024
FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models,"The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs’ outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.",https://aclanthology.org/2024.acl-long.257,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{jiang-etal-2024-followbench,
    title = ""{F}ollow{B}ench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models"",
    author = ""Jiang, Yuxin  and
      Wang, Yufei  and
      Zeng, Xingshan  and
      Zhong, Wanjun  and
      Li, Liangyou  and
      Mi, Fei  and
      Shang, Lifeng  and
      Jiang, Xin  and
      Liu, Qun  and
      Wang, Wei"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.257/"",
    doi = ""10.18653/v1/2024.acl-long.257"",
    pages = ""4667--4688"",
    abstract = ""The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs' outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.""
}",jiangFollowBenchMultilevelFinegrained2024
Open Grounded Planning: Challenges and Benchmark Construction,"The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning task–open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning.",https://aclanthology.org/2024.acl-long.272,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{guo-etal-2024-open,
    title = ""Open Grounded Planning: Challenges and Benchmark Construction"",
    author = ""Guo, Shiguang  and
      Deng, Ziliang  and
      Lin, Hongyu  and
      Lu, Yaojie  and
      Han, Xianpei  and
      Sun, Le"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.272/"",
    doi = ""10.18653/v1/2024.acl-long.272"",
    pages = ""4982--5003"",
    abstract = ""The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning task{--}open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning.""
}",guoOpenGroundedPlanning2024
Marathon: A Race Through the Realm of Long Context with Large Language Models,"With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models’ comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce Marathon, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs’ capabilities in understanding and reasoning over extended contexts.",https://aclanthology.org/2024.acl-long.284,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2024-marathon,
    title = ""Marathon: A Race Through the Realm of Long Context with Large Language Models"",
    author = ""Zhang, Lei  and
      Li, Yunshui  and
      Liu, Ziqiang  and
      Yang, Jiaxi  and
      Liu, Junhao  and
      Chen, Longze  and
      Luo, Run  and
      Yang, Min"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.284/"",
    doi = ""10.18653/v1/2024.acl-long.284"",
    pages = ""5201--5217"",
    abstract = ""With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models' comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce Marathon, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs' capabilities in understanding and reasoning over extended contexts.""
}",zhangMarathonRaceRealm2024
UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation,"Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, containing hallucinations generated by LLMs with minimal restrictions. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.",https://aclanthology.org/2024.acl-long.288,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{liang-etal-2024-uhgeval,
    title = ""{UHGE}val: Benchmarking the Hallucination of {C}hinese Large Language Models via Unconstrained Generation"",
    author = ""Liang, Xun  and
      Song, Shichao  and
      Niu, Simin  and
      Li, Zhiyu  and
      Xiong, Feiyu  and
      Tang, Bo  and
      Wang, Yezhaohui  and
      He, Dawei  and
      Peng, Cheng  and
      Wang, Zhonghao  and
      Deng, Haiying"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.288/"",
    doi = ""10.18653/v1/2024.acl-long.288"",
    pages = ""5266--5293"",
    abstract = ""Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, containing hallucinations generated by LLMs with minimal restrictions. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.""
}",liangUHGEvalBenchmarkingHallucination2024a
CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,"Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce **CodeScope**, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers **43 programming languages** and **eight coding tasks**. It evaluates the coding performance of LLMs from three dimensions (perspectives): **length**, **difficulty**, and **efficiency**. To facilitate execution-based evaluations of code generation, we develop **MultiCodeEngine**, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope.",https://aclanthology.org/2024.acl-long.301,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{yan-etal-2024-codescope,
    title = ""{C}ode{S}cope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating {LLM}s on Code Understanding and Generation"",
    author = ""Yan, Weixiang  and
      Liu, Haitian  and
      Wang, Yunkun  and
      Li, Yunzhe  and
      Chen, Qian  and
      Wang, Wen  and
      Lin, Tingyu  and
      Zhao, Weishan  and
      Zhu, Li  and
      Sundaram, Hari  and
      Deng, Shuiguang"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.301/"",
    doi = ""10.18653/v1/2024.acl-long.301"",
    pages = ""5511--5558"",
    abstract = ""Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce **CodeScope**, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers **43 programming languages** and **eight coding tasks**. It evaluates the coding performance of LLMs from three dimensions (perspectives): **length**, **difficulty**, and **efficiency**. To facilitate execution-based evaluations of code generation, we develop **MultiCodeEngine**, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope.""
}",yanCodeScopeExecutionbasedMultilingual2024
Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?,"Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench (Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by \times 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this [link](https://anonymous.4open.science/r/MTI-Bench-6F01).",https://aclanthology.org/2024.acl-long.304,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{son-etal-2024-multi-task,
    title = ""Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?"",
    author = ""Son, Guijin  and
      Baek, SangWon  and
      Nam, Sangdae  and
      Jeong, Ilgyun  and
      Kim, Seungone"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.304/"",
    doi = ""10.18653/v1/2024.acl-long.304"",
    pages = ""5606--5627"",
    abstract = ""Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle \textit{multiple} instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench (Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by $\times 1.46$ times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3{\%} and 12.4{\%} improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this [link](https://anonymous.4open.science/r/MTI-Bench-6F01).""
}",sonMultitaskInferenceCan2024
Benchmarking Data Science Agents,"In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval – a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.",https://aclanthology.org/2024.acl-long.308,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2024-benchmarking-data,
    title = ""Benchmarking Data Science Agents"",
    author = ""Zhang, Yuge  and
      Jiang, Qiyang  and
      XingyuHan, XingyuHan  and
      Chen, Nan  and
      Yang, Yuqing  and
      Ren, Kan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.308/"",
    doi = ""10.18653/v1/2024.acl-long.308"",
    pages = ""5677--5700"",
    abstract = ""In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval {--} a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.""
}",zhangBenchmarkingDataScience2024
EmoBench: Evaluating the Emotional Intelligence of Large Language Models,"Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.",https://aclanthology.org/2024.acl-long.326,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{sabour-etal-2024-emobench,
    title = ""{E}mo{B}ench: Evaluating the Emotional Intelligence of Large Language Models"",
    author = ""Sabour, Sahand  and
      Liu, Siyang  and
      Zhang, Zheyuan  and
      Liu, June  and
      Zhou, Jinfeng  and
      Sunaryo, Alvionna  and
      Lee, Tatia  and
      Mihalcea, Rada  and
      Huang, Minlie"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.326/"",
    doi = ""10.18653/v1/2024.acl-long.326"",
    pages = ""5986--6004"",
    abstract = ""Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.""
}",sabourEmoBenchEvaluatingEmotional2024a
"XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval","Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce *xCodeEval*, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples (16.5 B tokens) from about 7.5 K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual code execution engine, *ExecEval* that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI’s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate to be quite challenging as per the current advancements in language models.",https://aclanthology.org/2024.acl-long.367,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{khan-etal-2024-xcodeeval,
    title = ""{XC}ode{E}val: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"",
    author = ""Khan, Mohammad Abdullah Matin  and
      Bari, M Saiful  and
      Do, Xuan Long  and
      Wang, Weishi  and
      Parvez, Md Rizwan  and
      Joty, Shafiq"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.367/"",
    doi = ""10.18653/v1/2024.acl-long.367"",
    pages = ""6766--6805"",
    abstract = ""Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce *xCodeEval*, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples (16.5 B tokens) from about 7.5 K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual code execution engine, *ExecEval* that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI`s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate to be quite challenging as per the current advancements in language models.""
}",khanXCodeEvalExecutionbasedLarge2024
A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia,"Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model’s internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.",https://aclanthology.org/2024.acl-long.369,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{monea-etal-2024-glitch,
    title = ""A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia"",
    author = ""Monea, Giovanni  and
      Peyrard, Maxime  and
      Josifoski, Martin  and
      Chaudhary, Vishrav  and
      Eisner, Jason  and
      Kiciman, Emre  and
      Palangi, Hamid  and
      Patra, Barun  and
      West, Robert"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.369/"",
    doi = ""10.18653/v1/2024.acl-long.369"",
    pages = ""6828--6844"",
    abstract = ""Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model`s internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.""
}",moneaGlitchMatrixLocating2024a
LaMP: When Large Language Models Meet Personalization,"This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark — a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.",https://aclanthology.org/2024.acl-long.399,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{salemi-etal-2024-lamp,
    title = ""{L}a{MP}: When Large Language Models Meet Personalization"",
    author = ""Salemi, Alireza  and
      Mysore, Sheshera  and
      Bendersky, Michael  and
      Zamani, Hamed"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.399/"",
    doi = ""10.18653/v1/2024.acl-long.399"",
    pages = ""7370--7392"",
    abstract = ""This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark {---} a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.""
}",salemiLaMPWhenLarge2024
MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues,"The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at https://github.com/mtbench101/mt-bench-101.",https://aclanthology.org/2024.acl-long.401,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{bai-etal-2024-mt,
    title = ""{MT}-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues"",
    author = ""Bai, Ge  and
      Liu, Jie  and
      Bu, Xingyuan  and
      He, Yancheng  and
      Liu, Jiaheng  and
      Zhou, Zhanhui  and
      Lin, Zhuoran  and
      Su, Wenbo  and
      Ge, Tiezheng  and
      Zheng, Bo  and
      Ouyang, Wanli"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.401/"",
    doi = ""10.18653/v1/2024.acl-long.401"",
    pages = ""7421--7454"",
    abstract = ""The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at https://github.com/mtbench101/mt-bench-101.""
}",baiMTbench101FinegrainedBenchmark2024
Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models,"Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning.Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5%-15% compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancements in graph data understanding.",https://aclanthology.org/2024.acl-long.404,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{ai-etal-2024-advancement,
    title = ""Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models"",
    author = ""Ai, Qihang  and
      Li, Jiafan  and
      Dai, Jincheng  and
      Zhou, Jianwu  and
      Liu, Lemao  and
      Jiang, Haiyun  and
      Shi, Shuming"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.404/"",
    doi = ""10.18653/v1/2024.acl-long.404"",
    pages = ""7485--7501"",
    abstract = ""Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning.Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5{\%}-15{\%} compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancements in graph data understanding.""
}",aiAdvancementGraphUnderstanding2024
EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models,"We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content in the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision–text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.",https://aclanthology.org/2024.acl-long.420,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{das-etal-2024-exams,
    title = ""{EXAMS}-{V}: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models"",
    author = ""Das, Rocktim  and
      Hristov, Simeon  and
      Li, Haonan  and
      Dimitrov, Dimitar  and
      Koychev, Ivan  and
      Nakov, Preslav"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.420/"",
    doi = ""10.18653/v1/2024.acl-long.420"",
    pages = ""7768--7791"",
    abstract = ""We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content in the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision{--}text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.""
}",dasEXAMSVMultidisciplineMultilingual2024
M^3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought,"Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M^3CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M^3CoT and there is a large gap between VLLMs and human performance in M^3CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M^3CoT will serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.",https://aclanthology.org/2024.acl-long.446,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{chen-etal-2024-m3cot,
    title = ""{M}$^3${C}o{T}: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought"",
    author = ""Chen, Qiguang  and
      Qin, Libo  and
      Zhang, Jin  and
      Chen, Zhi  and
      Xu, Xiao  and
      Che, Wanxiang"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.446/"",
    doi = ""10.18653/v1/2024.acl-long.446"",
    pages = ""8199--8221"",
    abstract = ""Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M$^3$CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M$^3$CoT and there is a large gap between VLLMs and human performance in M$^3$CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M$^3$CoT will serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.""
}",chenM3CoTNovelBenchmark2024
BizBench: A Quantitative Reasoning Benchmark for Business and Finance,"Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models’ ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model’s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs’ limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.",https://aclanthology.org/2024.acl-long.452,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{krumdick-etal-2024-bizbench,
    title = ""{B}iz{B}ench: A Quantitative Reasoning Benchmark for Business and Finance"",
    author = ""Krumdick, Michael  and
      Koncel-Kedziorski, Rik  and
      Lai, Viet Dac  and
      Reddy, Varshini  and
      Lovering, Charles  and
      Tanner, Chris"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.452/"",
    doi = ""10.18653/v1/2024.acl-long.452"",
    pages = ""8309--8332"",
    abstract = ""Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models' ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model`s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs' limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.""
}",krumdickBizBenchQuantitativeReasoning2024a
FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence,"Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.",https://aclanthology.org/2024.acl-long.459,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{joseph-etal-2024-factpico,
    title = ""{F}act{PICO}: Factuality Evaluation for Plain Language Summarization of Medical Evidence"",
    author = {Joseph, Sebastian  and
      Chen, Lily  and
      Trienes, Jan  and
      G{\""o}ke, Hannah  and
      Coers, Monika  and
      Xu, Wei  and
      Wallace, Byron  and
      Li, Junyi Jessy},
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.459/"",
    doi = ""10.18653/v1/2024.acl-long.459"",
    pages = ""8437--8464"",
    abstract = ""Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.""
}",josephFactPICOFactualityEvaluation2024a
OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models,"Neural Theory-of-Mind (N-ToM), machine’s ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters’ psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs’ capabilities of modeling characters’ mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters’ mental states in the psychological world.",https://aclanthology.org/2024.acl-long.466,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{xu-etal-2024-opentom,
    title = ""{O}pen{T}o{M}: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models"",
    author = ""Xu, Hainiu  and
      Zhao, Runcong  and
      Zhu, Lixing  and
      Du, Jinhua  and
      He, Yulan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.466/"",
    doi = ""10.18653/v1/2024.acl-long.466"",
    pages = ""8593--8623"",
    abstract = ""Neural Theory-of-Mind (N-ToM), machine`s ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.""
}",xuOpenToMComprehensiveBenchmark2024
RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations,"Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.",https://aclanthology.org/2024.acl-long.470,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{huang-etal-2024-ravel,
    title = ""{RAVEL}: Evaluating Interpretability Methods on Disentangling Language Model Representations"",
    author = ""Huang, Jing  and
      Wu, Zhengxuan  and
      Potts, Christopher  and
      Geva, Mor  and
      Geiger, Atticus"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.470/"",
    doi = ""10.18653/v1/2024.acl-long.470"",
    pages = ""8669--8687"",
    abstract = ""Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.""
}",huangRAVELEvaluatingInterpretability2024
Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents,"With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction.However, there is a scarcity of benchmarks available for LLM-based mobile agents.Benchmarking these agents generally faces three main challenges:(1) The inefficiency of UI-only operations imposes limitations to task evaluation.(2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents.(3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents.First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion.Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs.To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios.Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform will be released in the future.",https://aclanthology.org/2024.acl-long.478,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{deng-etal-2024-mobile,
    title = ""Mobile-Bench: An Evaluation Benchmark for {LLM}-based Mobile Agents"",
    author = ""Deng, Shihan  and
      Xu, Weikai  and
      Sun, Hongda  and
      Liu, Wei  and
      Tan, Tao  and
      Liujianfeng, Liujianfeng  and
      Li, Ang  and
      Luan, Jian  and
      Wang, Bin  and
      Yan, Rui  and
      Shang, Shuo"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.478/"",
    doi = ""10.18653/v1/2024.acl-long.478"",
    pages = ""8813--8831"",
    abstract = ""With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction.However, there is a scarcity of benchmarks available for LLM-based mobile agents.Benchmarking these agents generally faces three main challenges:(1) The inefficiency of UI-only operations imposes limitations to task evaluation.(2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents.(3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents.First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion.Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs.To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios.Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform will be released in the future.""
}",dengMobilebenchEvaluationBenchmark2024
F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods,"Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs’ fundamental abilities.",https://aclanthology.org/2024.acl-long.507,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{sun-etal-2024-f,
    title = ""{F}-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods"",
    author = ""Sun, Yu  and
      Keyuchen, Keyuchen  and
      Wang, Shujie  and
      Li, Peiji  and
      Guo, Qipeng  and
      Yan, Hang  and
      Qiu, Xipeng  and
      Huang, Xuanjing  and
      Lin, Dahua"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.507/"",
    doi = ""10.18653/v1/2024.acl-long.507"",
    pages = ""9348--9369"",
    abstract = ""Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs' fundamental abilities.""
}",sunFevalAsssessingFundamental2024
MERA: A Comprehensive LLM Evaluation in Russian,"Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). However, despite researchers’ attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce a new instruction benchmark, MERA, oriented towards the FMs’ performance on the Russian language. The benchmark encompasses 21 evaluation tasks for generative models covering 10 skills and is supplied with private answer scoring to prevent data leakage. The paper introduces a methodology to evaluate FMs and LMs in fixed zero- and few-shot instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential ethical concerns and drawbacks.",https://aclanthology.org/2024.acl-long.534,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{fenogenova-etal-2024-mera,
    title = ""{MERA}: A Comprehensive {LLM} Evaluation in {R}ussian"",
    author = ""Fenogenova, Alena  and
      Chervyakov, Artem  and
      Martynov, Nikita  and
      Kozlova, Anastasia  and
      Tikhonova, Maria  and
      Akhmetgareeva, Albina  and
      Emelyanov, Anton  and
      Shevelev, Denis  and
      Lebedev, Pavel  and
      Sinev, Leonid  and
      Isaeva, Ulyana  and
      Kolomeytseva, Katerina  and
      Moskovskiy, Daniil  and
      Goncharova, Elizaveta  and
      Savushkin, Nikita  and
      Mikhailova, Polina  and
      Minaeva, Anastasia  and
      Dimitrov, Denis  and
      Panchenko, Alexander  and
      Markov, Sergey"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.534/"",
    doi = ""10.18653/v1/2024.acl-long.534"",
    pages = ""9920--9948"",
    abstract = ""Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce a new instruction benchmark, MERA, oriented towards the FMs' performance on the Russian language. The benchmark encompasses 21 evaluation tasks for generative models covering 10 skills and is supplied with private answer scoring to prevent data leakage. The paper introduces a methodology to evaluate FMs and LMs in fixed zero- and few-shot instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential ethical concerns and drawbacks.""
}",fenogenovaMERAComprehensiveLLM2024a
NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism,"We present NewsBench, a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. Our constructed benchmark dataset is focused on four facets of writing proficiency and six facets of safety adherence, and it comprises manually and carefully designed 1,267 test samples in the types of multiple choice questions and short answer questions for five editorial tasks in 24 news domains. To measure performances, we propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations. Based on the systematic evaluation framework, we conduct a comprehensive analysis of eleven popular LLMs which can handle Chinese. The experimental results highlight GPT-4 and ERNIE Bot as top performers, yet reveal a relative deficiency in journalistic safety adherence in creative writing tasks. Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations. The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism.",https://aclanthology.org/2024.acl-long.538,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{li-etal-2024-newsbench,
    title = ""{N}ews{B}ench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in {C}hinese Journalism"",
    author = ""Li, Miao  and
      Chen, Ming-Bin  and
      Tang, Bo  and
      ShengbinHou, ShengbinHou  and
      Wang, Pengyu  and
      Deng, Haiying  and
      Li, Zhiyu  and
      Xiong, Feiyu  and
      Mao, Keming  and
      Peng, Cheng  and
      Luo, Yi"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.538/"",
    doi = ""10.18653/v1/2024.acl-long.538"",
    pages = ""9993--10014"",
    abstract = ""We present NewsBench, a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. Our constructed benchmark dataset is focused on four facets of writing proficiency and six facets of safety adherence, and it comprises manually and carefully designed 1,267 test samples in the types of multiple choice questions and short answer questions for five editorial tasks in 24 news domains. To measure performances, we propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations. Based on the systematic evaluation framework, we conduct a comprehensive analysis of eleven popular LLMs which can handle Chinese. The experimental results highlight GPT-4 and ERNIE Bot as top performers, yet reveal a relative deficiency in journalistic safety adherence in creative writing tasks. Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations. The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism.""
}",liNewsBenchSystematicEvaluation2024
SyllabusQA: A Course Logistics Question Answering Dataset,"Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.",https://aclanthology.org/2024.acl-long.557,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{fernandez-etal-2024-syllabusqa,
    title = ""{S}yllabus{QA}: A Course Logistics Question Answering Dataset"",
    author = ""Fernandez, Nigel  and
      Scarlatos, Alexander  and
      Lan, Andrew"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.557/"",
    doi = ""10.18653/v1/2024.acl-long.557"",
    pages = ""10344--10369"",
    abstract = ""Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.""
}",fernandezSyllabusQACourseLogistics2024
AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts,"Legal tasks and datasets are often used as benchmarks for the capabilities of language models. However, openly available annotated datasets are rare. In this paper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts. Together with the data, we present a first baseline for the task of detecting potentially void clauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5. Our results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54. While the fine-tuned models often performed better with regard to precision, GPT-3.5 outperformed the other approaches with regard to recall. An analysis of the errors indicates that one of the main challenges could be the correct interpretation of complex clauses, rather than the decision boundaries of what is permissible and what is not.",https://aclanthology.org/2024.acl-long.559,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{braun-matthes-2024-agb,
    title = ""{AGB}-{DE}: A Corpus for the Automated Legal Assessment of Clauses in {G}erman Consumer Contracts"",
    author = ""Braun, Daniel  and
      Matthes, Florian"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.559/"",
    doi = ""10.18653/v1/2024.acl-long.559"",
    pages = ""10389--10405"",
    abstract = ""Legal tasks and datasets are often used as benchmarks for the capabilities of language models. However, openly available annotated datasets are rare. In this paper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts. Together with the data, we present a first baseline for the task of detecting potentially void clauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5. Our results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54. While the fine-tuned models often performed better with regard to precision, GPT-3.5 outperformed the other approaches with regard to recall. An analysis of the errors indicates that one of the main challenges could be the correct interpretation of complex clauses, rather than the decision boundaries of what is permissible and what is not.""
}",braunAGBDECorpusAutomated2024
CODIS: Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models,"Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner.",https://aclanthology.org/2024.acl-long.573,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{luo-etal-2024-codis,
    title = ""{CODIS}: Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models"",
    author = ""Luo, Fuwen  and
      Chen, Chi  and
      Wan, Zihao  and
      Kang, Zhaolu  and
      Yan, Qidong  and
      Li, Yingjie  and
      Wang, Xiaolong  and
      Wang, Siyu  and
      Wang, Ziyue  and
      Mi, Xiaoyue  and
      Li, Peng  and
      Ma, Ning  and
      Sun, Maosong  and
      Liu, Yang"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.573/"",
    doi = ""10.18653/v1/2024.acl-long.573"",
    pages = ""10639--10659"",
    abstract = ""Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner.""
}",luoCODISBenchmarkingContextdependent2024
CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models,"Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct 12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs.Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge.In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs.",https://aclanthology.org/2024.acl-long.578,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2024-clamber,
    title = ""{CLAMBER}: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models"",
    author = ""Zhang, Tong  and
      Qin, Peixin  and
      Deng, Yang  and
      Huang, Chen  and
      Lei, Wenqiang  and
      Liu, Junhong  and
      Jin, Dingnan  and
      Liang, Hongru  and
      Chua, Tat-Seng"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.578/"",
    doi = ""10.18653/v1/2024.acl-long.578"",
    pages = ""10746--10766"",
    abstract = ""Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct 12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs.Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge.In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs.""
}",zhangCLAMBERBenchmarkIdentifying2024a
PAGED: A Benchmark for Procedural Graphs Extraction from Documents,"Automatic extraction of procedural graphs from documents creates a low-cost way for users to easily understand a complex procedure by skimming visual graphs. Despite the progress in recent studies, it remains unanswered: whether the existing studies have well solved this task (Q1) and whether the emerging large language models (LLMs) can bring new opportunities to this task (Q2). To this end, we propose a new benchmark PAGED, equipped with a large high-quality dataset and standard evaluations. It investigates five state-of-the-art baselines, revealing that they fail to extract optimal procedural graphs well because of their heavy reliance on hand-written rules and limited available data. We further involve three advanced LLMs in PAGED and enhance them with a novel self-refine strategy. The results point out the advantages of LLMs in identifying textual elements and their gaps in building logical structures. We hope PAGED can serve as a major landmark for automatic procedural graph extraction and the investigations in PAGED can offer insights into the research on logic reasoning among non-sequential elements.",https://aclanthology.org/2024.acl-long.583,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{du-etal-2024-paged,
    title = ""{PAGED}: A Benchmark for Procedural Graphs Extraction from Documents"",
    author = ""Du, Weihong  and
      Liao, Wenrui  and
      Liang, Hongru  and
      Lei, Wenqiang"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.583/"",
    doi = ""10.18653/v1/2024.acl-long.583"",
    pages = ""10829--10846"",
    abstract = ""Automatic extraction of procedural graphs from documents creates a low-cost way for users to easily understand a complex procedure by skimming visual graphs. Despite the progress in recent studies, it remains unanswered: whether the existing studies have well solved this task (Q1) and whether the emerging large language models (LLMs) can bring new opportunities to this task (Q2). To this end, we propose a new benchmark PAGED, equipped with a large high-quality dataset and standard evaluations. It investigates five state-of-the-art baselines, revealing that they fail to extract optimal procedural graphs well because of their heavy reliance on hand-written rules and limited available data. We further involve three advanced LLMs in PAGED and enhance them with a novel self-refine strategy. The results point out the advantages of LLMs in identifying textual elements and their gaps in building logical structures. We hope PAGED can serve as a major landmark for automatic procedural graph extraction and the investigations in PAGED can offer insights into the research on logic reasoning among non-sequential elements.""
}",duPAGEDBenchmarkProcedural2024
RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,"Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual case and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. We show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive hallucination detection performance when compared to the existing prompt-based approaches using state-of-the-art LLMs such as GPT-4. Furthermore, the finetuned model can effectively mitigate hallucination in LLM responses.",https://aclanthology.org/2024.acl-long.585,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{niu-etal-2024-ragtruth,
    title = ""{RAGT}ruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models"",
    author = ""Niu, Cheng  and
      Wu, Yuanhao  and
      Zhu, Juno  and
      Xu, Siliang  and
      Shum, KaShun  and
      Zhong, Randy  and
      Song, Juntong  and
      Zhang, Tong"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.585/"",
    doi = ""10.18653/v1/2024.acl-long.585"",
    pages = ""10862--10878"",
    abstract = ""Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual case and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. We show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive hallucination detection performance when compared to the existing prompt-based approaches using state-of-the-art LLMs such as GPT-4. Furthermore, the finetuned model can effectively mitigate hallucination in LLM responses.""
}",niuRAGTruthHallucinationCorpus2024a
IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages,"As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench — the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate stateof-the-art LLMs like GPT-3.5, GPT-4, PaLM2, and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench isavailable at www.github.com/google-researchdatasets/indic-gen-bench",https://aclanthology.org/2024.acl-long.595,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{singh-etal-2024-indicgenbench,
    title = ""{I}ndic{G}en{B}ench: A Multilingual Benchmark to Evaluate Generation Capabilities of {LLM}s on {I}ndic Languages"",
    author = ""Singh, Harman  and
      Gupta, Nitish  and
      Bharadwaj, Shikhar  and
      Tewari, Dinesh  and
      Talukdar, Partha"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.595/"",
    doi = ""10.18653/v1/2024.acl-long.595"",
    pages = ""11047--11073"",
    abstract = ""As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench {---} the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate stateof-the-art LLMs like GPT-3.5, GPT-4, PaLM2, and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench isavailable at www.github.com/google-researchdatasets/indic-gen-bench""
}",singhIndicGenBenchMultilingualBenchmark2024
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations,"We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs’ reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM’s language orientation and the task’s domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs’ memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs’ strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM.",https://aclanthology.org/2024.acl-long.604,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{sun-etal-2024-benchmarking-chinese,
    title = ""Benchmarking {C}hinese Commonsense Reasoning of {LLM}s: From {C}hinese-Specifics to Reasoning-Memorization Correlations"",
    author = ""Sun, Jiaxing  and
      Huang, Weiquan  and
      Wu, Jiang  and
      Gu, Chenya  and
      Li, Wei  and
      Zhang, Songyang  and
      Yan, Hang  and
      He, Conghui"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.604/"",
    doi = ""10.18653/v1/2024.acl-long.604"",
    pages = ""11205--11228"",
    abstract = ""We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM`s language orientation and the task`s domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM.""
}",sunBenchmarkingChineseCommonsense2024
IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning,"Legal systems worldwide are inundated with exponential growth in cases and documents. There is an imminent need to develop NLP and ML techniques for automatically processing and understanding legal documents to streamline the legal system. However, evaluating and comparing various NLP models designed specifically for the legal domain is challenging. This paper addresses this challenge by proposing : Benchmark for Indian Legal Text Understanding and Reasoning. contains monolingual (English, Hindi) and multi-lingual (9 Indian languages) domain-specific tasks that address different aspects of the legal system from the point of view of understanding and reasoning over Indian legal documents. We present baseline models (including LLM-based) for each task, outlining the gap between models and the ground truth. To foster further research in the legal domain, we create a leaderboard (available at: https://exploration-lab.github.io/IL-TUR/ ) where the research community can upload and compare legal text understanding systems.",https://aclanthology.org/2024.acl-long.618,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{joshi-etal-2024-il,
    title = ""{IL}-{TUR}: Benchmark for {I}ndian Legal Text Understanding and Reasoning"",
    author = ""Joshi, Abhinav  and
      Paul, Shounak  and
      Sharma, Akshat  and
      Goyal, Pawan  and
      Ghosh, Saptarshi  and
      Modi, Ashutosh"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.618/"",
    doi = ""10.18653/v1/2024.acl-long.618"",
    pages = ""11460--11499"",
    abstract = ""Legal systems worldwide are inundated with exponential growth in cases and documents. There is an imminent need to develop NLP and ML techniques for automatically processing and understanding legal documents to streamline the legal system. However, evaluating and comparing various NLP models designed specifically for the legal domain is challenging. This paper addresses this challenge by proposing : Benchmark for Indian Legal Text Understanding and Reasoning. contains monolingual (English, Hindi) and multi-lingual (9 Indian languages) domain-specific tasks that address different aspects of the legal system from the point of view of understanding and reasoning over Indian legal documents. We present baseline models (including LLM-based) for each task, outlining the gap between models and the ground truth. To foster further research in the legal domain, we create a leaderboard (available at: https://exploration-lab.github.io/IL-TUR/ ) where the research community can upload and compare legal text understanding systems.""
}",joshiILTURBenchmarkIndian2024
AlignBench: Benchmarking Chinese Alignment of Large Language Models,"Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs’ alignment in Chinese. We tailor a human-in-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references.To ensure references’ correctness, each knowledge-intensive query is accompanied with evidences collected from reliable webpages (including the url and quotation) by our annotators.For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge (CITATION) with Chain-of-Thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability.All evaluation codes and data are publicly available at https://github.com/THUDM/AlignBench",https://aclanthology.org/2024.acl-long.624,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{liu-etal-2024-alignbench,
    title = ""{A}lign{B}ench: Benchmarking {C}hinese Alignment of Large Language Models"",
    author = ""Liu, Xiao  and
      Lei, Xuanyu  and
      Wang, Shengyuan  and
      Huang, Yue  and
      Feng, Andrew  and
      Wen, Bosi  and
      Cheng, Jiale  and
      Ke, Pei  and
      Xu, Yifan  and
      Tam, Weng Lam  and
      Zhang, Xiaohan  and
      Sun, Lichao  and
      Gu, Xiaotao  and
      Wang, Hongning  and
      Zhang, Jing  and
      Huang, Minlie  and
      Dong, Yuxiao  and
      Tang, Jie"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.624/"",
    doi = ""10.18653/v1/2024.acl-long.624"",
    pages = ""11621--11640"",
    abstract = ""Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in Chinese. We tailor a human-in-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references.To ensure references' correctness, each knowledge-intensive query is accompanied with evidences collected from reliable webpages (including the url and quotation) by our annotators.For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge (CITATION) with Chain-of-Thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability.All evaluation codes and data are publicly available at \url{https://github.com/THUDM/AlignBench}""
}",liuAlignBenchBenchmarkingChinese2024a
CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation,"Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in CharacterEval, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.",https://aclanthology.org/2024.acl-long.638,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{tu-etal-2024-charactereval,
    title = ""{C}haracter{E}val: A {C}hinese Benchmark for Role-Playing Conversational Agent Evaluation"",
    author = ""Tu, Quan  and
      Fan, Shilong  and
      Tian, Zihang  and
      Shen, Tianhao  and
      Shang, Shuo  and
      Gao, Xin  and
      Yan, Rui"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.638/"",
    doi = ""10.18653/v1/2024.acl-long.638"",
    pages = ""11836--11850"",
    abstract = ""Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce \textit{CharacterEval}, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. \textit{CharacterEval} employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in \textit{CharacterEval}, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on \textit{CharacterEval} demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.""
}",tuCharacterEvalChineseBenchmark2024
Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation,"We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design Quintd - a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with Quintd. However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80% of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs.",https://aclanthology.org/2024.acl-long.651,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{kasner-dusek-2024-beyond,
    title = ""Beyond Traditional Benchmarks: Analyzing Behaviors of Open {LLM}s on Data-to-Text Generation"",
    author = ""Kasner, Zden{\v{e}}k  and
      Dusek, Ondrej"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.651/"",
    doi = ""10.18653/v1/2024.acl-long.651"",
    pages = ""12045--12072"",
    abstract = ""We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design Quintd - a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with Quintd. However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80{\%} of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs.""
}",kasnerTraditionalBenchmarksAnalyzing2024a
Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends,"Recent advancements in large language models (LLMs) have significantly advanced the capabilities of summarization systems.However, they continue to face a persistent challenge: hallucination. While prior work has extensively examined LLMs in news domains, evaluation of dialogue summarization has primarily focused on BART-based models, resulting in a notable gap in understanding LLM effectiveness.Our work seeks to address this gap by benchmarking LLMs for dialogue summarization faithfulness using human annotations,focusing on identifying and categorizing span-level inconsistencies.Specifically, we evaluate two prominent LLMs: GPT-4 and Alpaca-13B.Our evaluation reveals that LLMs often generate plausible, but not fully supported inferences based on conversation contextual cues, a trait absent in older models. As a result, we propose a refined taxonomy of errors, introducing a novel category termed “Contextual Inference” to address this aspect of LLM behavior. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors effectively. To address this, we introduce two prompt-based approaches for fine-grained error detection. Our methods outperform existing metrics, particularly in identifying the novel “Contextual Inference” error type.",https://aclanthology.org/2024.acl-long.677,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{ramprasad-etal-2024-analyzing,
    title = ""Analyzing {LLM} Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends"",
    author = ""Ramprasad, Sanjana  and
      Ferracane, Elisa  and
      Lipton, Zachary"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.677/"",
    doi = ""10.18653/v1/2024.acl-long.677"",
    pages = ""12549--12561"",
    abstract = ""Recent advancements in large language models (LLMs) have significantly advanced the capabilities of summarization systems.However, they continue to face a persistent challenge: hallucination. While prior work has extensively examined LLMs in news domains, evaluation of dialogue summarization has primarily focused on BART-based models, resulting in a notable gap in understanding LLM effectiveness.Our work seeks to address this gap by benchmarking LLMs for dialogue summarization faithfulness using human annotations,focusing on identifying and categorizing span-level inconsistencies.Specifically, we evaluate two prominent LLMs: GPT-4 and Alpaca-13B.Our evaluation reveals that LLMs often generate plausible, but not fully supported inferences based on conversation contextual cues, a trait absent in older models. As a result, we propose a refined taxonomy of errors, introducing a novel category termed {\textquotedblleft}Contextual Inference{\textquotedblright} to address this aspect of LLM behavior. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors effectively. To address this, we introduce two prompt-based approaches for fine-grained error detection. Our methods outperform existing metrics, particularly in identifying the novel {\textquotedblleft}Contextual Inference{\textquotedblright} error type.""
}",ramprasadAnalyzingLLMBehavior2024a
Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks,"Multimodal large language models (MLLMs) have proven effective in a wide range of tasks that require complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, the success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, even those with large speaker populations, such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed *Peacock*, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce *Henna*, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturally-aware Arabic MLLMs. The GitHub repository for the *Peacock* project is available at [https://github.com/UBC-NLP/peacock](https://github.com/UBC-NLP/peacock).",https://aclanthology.org/2024.acl-long.689,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{alwajih-etal-2024-peacock,
    title = ""Peacock: A Family of {A}rabic Multimodal Large Language Models and Benchmarks"",
    author = ""Alwajih, Fakhraddin  and
      Nagoudi, El Moatez Billah  and
      Bhatia, Gagan  and
      Mohamed, Abdelrahman  and
      Abdul-Mageed, Muhammad"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.689/"",
    doi = ""10.18653/v1/2024.acl-long.689"",
    pages = ""12753--12776"",
    abstract = ""Multimodal large language models (MLLMs) have proven effective in a wide range of tasks that require complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, the success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, even those with large speaker populations, such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed *Peacock*, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce *Henna*, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturally-aware Arabic MLLMs. The GitHub repository for the *Peacock* project is available at [https://github.com/UBC-NLP/peacock](https://github.com/UBC-NLP/peacock).""
}",alwajihPeacockFamilyArabic2024
FinanceMATH: Knowledge-Intensive Math Reasoning in Finance Domains,"We introduce FinanceMath, a novel benchmark designed to evaluate LLMs' capabilities in solving knowledge-intensive math reasoning problems. Compared to prior works, this study features three core advancements. First, FinanceMath includes 1,200 problems with a hybrid of textual and tabular content. These problems require college-level knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment. We also construct a finance-domain knowledge bank and investigate various knowledge integration strategies. Finally, we evaluate a wide spectrum of 44 LLMs with both Chain-of-Thought and Program-of-Thought prompting methods. Our experimental results reveal that the current best-performing system (i.e., GPT-4o) achieves only 60.9% accuracy using CoT prompting, leaving substantial room for improvement. Moreover, while augmenting LLMs with external knowledge can improve model performance (e.g., from 47.5% to 54.5% for Gemini-1.5-Pro), their accuracy remains significantly lower than the estimated human expert performance of 92%. We believe that FinanceMath can advance future research in the area of domain-specific knowledge retrieval and integration, particularly within the context of solving reasoning-intensive tasks.",https://aclanthology.org/2024.acl-long.693,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{zhao-etal-2024-knowledgefmath,
    title = ""FinanceMATH: Knowledge-Intensive Math Reasoning in Finance Domains"",
    author = ""Zhao, Yilun  and
      Liu, Hongjun  and
      Long, Yitao  and
      Zhang, Rui  and
      Zhao, Chen  and
      Cohan, Arman"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.693/"",
    doi = ""10.18653/v1/2024.acl-long.693"",
    pages = ""12841--12858"",
    abstract = ""We introduce FinanceMath, a novel benchmark designed to evaluate LLMs' capabilities in solving knowledge-intensive math reasoning problems. Compared to prior works, this study features three core advancements. First, FinanceMath includes 1,200 problems with a hybrid of textual and tabular content. These problems require college-level knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment. We also construct a finance-domain knowledge bank and investigate various knowledge integration strategies. Finally, we evaluate a wide spectrum of 44 LLMs with both Chain-of-Thought and Program-of-Thought prompting methods. Our experimental results reveal that the current best-performing system (i.e., GPT-4o) achieves only 60.9{\%} accuracy using CoT prompting, leaving substantial room for improvement. Moreover, while augmenting LLMs with external knowledge can improve model performance (e.g., from 47.5{\%} to 54.5{\%} for Gemini-1.5-Pro), their accuracy remains significantly lower than the estimated human expert performance of 92{\%}. We believe that FinanceMath can advance future research in the area of domain-specific knowledge retrieval and integration, particularly within the context of solving reasoning-intensive tasks.""
}",zhaoFinanceMATHKnowledgeintensiveMath2024
API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs,"There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes.",https://aclanthology.org/2024.acl-long.694,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{basu-etal-2024-api,
    title = ""{API}-{BLEND}: A Comprehensive Corpora for Training and Benchmarking {API} {LLM}s"",
    author = ""Basu, Kinjal  and
      Abdelaziz, Ibrahim  and
      Chaudhury, Subhajit  and
      Dan, Soham  and
      Crouse, Maxwell  and
      Munawar, Asim  and
      Austel, Vernon  and
      Kumaravel, Sadhana  and
      Muthusamy, Vinod  and
      Kapanipathi, Pavan  and
      Lastras, Luis"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.694/"",
    doi = ""10.18653/v1/2024.acl-long.694"",
    pages = ""12859--12870"",
    abstract = ""There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes.""
}",basuAPIBLENDComprehensiveCorpora2024
Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?,"Temporal reasoning is fundamental for large language models (LLMs) to comprehend the world. Current temporal reasoning datasets are limited to questions about single or isolated events, falling short in mirroring the realistic temporal characteristics involving concurrent nature and intricate temporal interconnections. In this paper, we introduce CoTempQA, a comprehensive co-temporal Question Answering (QA) benchmark containing four co-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our extensive experiments reveal a significant gap between the performance of current LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced with Chain of Thought (CoT) methodologies, models consistently struggle with our task. In our preliminary exploration, we discovered that mathematical reasoning plays a significant role in handling co-temporal events and proposed a strategy to boost LLMs’ co-temporal reasoning from a mathematical perspective. We hope that our CoTempQA datasets will encourage further advancements in improving the co-temporal reasoning capabilities of LLMs.",https://aclanthology.org/2024.acl-long.703,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{su-etal-2024-living,
    title = ""Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?"",
    author = ""Su, Zhaochen  and
      Li, Juntao  and
      Zhang, Jun  and
      Zhu, Tong  and
      Qu, Xiaoye  and
      Zhou, Pan  and
      Bowen, Yan  and
      Cheng, Yu  and
      Zhang, Min"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.703/"",
    doi = ""10.18653/v1/2024.acl-long.703"",
    pages = ""13014--13033"",
    abstract = ""Temporal reasoning is fundamental for large language models (LLMs) to comprehend the world. Current temporal reasoning datasets are limited to questions about single or isolated events, falling short in mirroring the realistic temporal characteristics involving concurrent nature and intricate temporal interconnections. In this paper, we introduce CoTempQA, a comprehensive co-temporal Question Answering (QA) benchmark containing four co-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our extensive experiments reveal a significant gap between the performance of current LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced with Chain of Thought (CoT) methodologies, models consistently struggle with our task. In our preliminary exploration, we discovered that mathematical reasoning plays a significant role in handling co-temporal events and proposed a strategy to boost LLMs' co-temporal reasoning from a mathematical perspective. We hope that our CoTempQA datasets will encourage further advancements in improving the co-temporal reasoning capabilities of LLMs.""
}",suLivingMomentCan2024
LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments,"Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings.",https://aclanthology.org/2024.acl-long.705,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{chen-etal-2024-llmarena,
    title = ""{LLMA}rena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments"",
    author = ""Chen, Junzhe  and
      Hu, Xuming  and
      Liu, Shuodi  and
      Huang, Shiyu  and
      Tu, Wei-Wei  and
      He, Zhaofeng  and
      Wen, Lijie"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.705/"",
    doi = ""10.18653/v1/2024.acl-long.705"",
    pages = ""13055--13077"",
    abstract = ""Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings.""
}",chenLLMArenaAssessingCapabilities2024
MULFE: A Multi-Level Benchmark for Free Text Model Editing,"Adjusting the outdated behaviors of large langugae models (LLMs) after deployment remains a significant challenge. It motivates the model editing research, which is however mainly explored in a restricted task form with triple-based edit requests. Recent works have initiated a transition to a more practical and unified editing task that takes free-form text as edit requests. However, there are gaps in nuanced benchmark designs and re-evaluation of existing methods. To bridge the gaps, we introduce a multi-level benchmark for free text model editing (MULFE). The benchmark categorizes probe queries into three levels of generalization, ranging from basic literal memory to deeper understanding and reasoning. Based on the benchmark, we conduct extensive experiments across various base models, edit sizes, and editing methods, including adaptations of mainstream locate-and-edit and hypernetwork methods. The results highlight the inconsistent behaviors of edited models on different generalization levels. Higher-level generalization remains a significant challenge. Based on the findings, we propose SIDE, a simple yet effective method based on in-context distillation to enhance the generalization performance. The benchmark dataset and evaluation scripts are publicly available at http://github.com/wchrepo/mulfe.",https://aclanthology.org/2024.acl-long.732,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-mulfe,
    title = ""{MULFE}: A Multi-Level Benchmark for Free Text Model Editing"",
    author = ""Wang, Chenhao  and
      Cao, Pengfei  and
      Jin, Zhuoran  and
      Chen, Yubo  and
      Zeng, Daojian  and
      Liu, Kang  and
      Zhao, Jun"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.732/"",
    doi = ""10.18653/v1/2024.acl-long.732"",
    pages = ""13570--13587"",
    abstract = ""Adjusting the outdated behaviors of large langugae models (LLMs) after deployment remains a significant challenge. It motivates the model editing research, which is however mainly explored in a restricted task form with triple-based edit requests. Recent works have initiated a transition to a more practical and unified editing task that takes free-form text as edit requests. However, there are gaps in nuanced benchmark designs and re-evaluation of existing methods. To bridge the gaps, we introduce a multi-level benchmark for free text model editing (MULFE). The benchmark categorizes probe queries into three levels of generalization, ranging from basic literal memory to deeper understanding and reasoning. Based on the benchmark, we conduct extensive experiments across various base models, edit sizes, and editing methods, including adaptations of mainstream locate-and-edit and hypernetwork methods. The results highlight the inconsistent behaviors of edited models on different generalization levels. Higher-level generalization remains a significant challenge. Based on the findings, we propose SIDE, a simple yet effective method based on in-context distillation to enhance the generalization performance. The benchmark dataset and evaluation scripts are publicly available at http://github.com/wchrepo/mulfe.""
}",wangMULFEMultilevelBenchmark2024
Evaluating Very Long-Term Conversational Memory of LLM Agents,"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 600 turns and 16K tokens on avg., over up to 32 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.",https://aclanthology.org/2024.acl-long.747,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{maharana-etal-2024-evaluating,
    title = ""Evaluating Very Long-Term Conversational Memory of {LLM} Agents"",
    author = ""Maharana, Adyasha  and
      Lee, Dong-Ho  and
      Tulyakov, Sergey  and
      Bansal, Mohit  and
      Barbieri, Francesco  and
      Fang, Yuwei"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.747/"",
    doi = ""10.18653/v1/2024.acl-long.747"",
    pages = ""13851--13870"",
    abstract = ""Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 600 turns and 16K tokens on avg., over up to 32 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.""
}",maharanaEvaluatingVeryLongterm2024
NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms,"The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms – new word forms – over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs’ ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.",https://aclanthology.org/2024.acl-long.749,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zheng-etal-2024-neo,
    title = ""{NEO}-{BENCH}: Evaluating Robustness of Large Language Models with Neologisms"",
    author = ""Zheng, Jonathan  and
      Ritter, Alan  and
      Xu, Wei"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.749/"",
    doi = ""10.18653/v1/2024.acl-long.749"",
    pages = ""13885--13906"",
    abstract = ""The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms {--} new word forms {--} over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.""
}",zhengNEOBENCHEvaluatingRobustness2024a
Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models,"Large vision-language models (LVLMs) excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains.To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension.ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers spanning various scientific domains.Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances open-sourced LVLMs’ mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark.Furthermore, employing ArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs.Evaluation results with state-of-the-art LVLMs underscore their struggle with the nuanced semantics of academic figures, while domain-specific training yields substantial performance gains.Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.",https://aclanthology.org/2024.acl-long.775,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{li-etal-2024-multimodal-arxiv,
    title = ""Multimodal {A}r{X}iv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models"",
    author = ""Li, Lei  and
      Wang, Yuqi  and
      Xu, Runxin  and
      Wang, Peiyi  and
      Feng, Xiachong  and
      Kong, Lingpeng  and
      Liu, Qi"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.775/"",
    doi = ""10.18653/v1/2024.acl-long.775"",
    pages = ""14369--14387"",
    abstract = ""Large vision-language models (LVLMs) excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains.To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension.ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers spanning various scientific domains.Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances open-sourced LVLMs' mathematical reasoning capabilities, achieving a 10.4{\%} absolute accuracy gain on a multimodal mathematical reasoning benchmark.Furthermore, employing ArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs.Evaluation results with state-of-the-art LVLMs underscore their struggle with the nuanced semantics of academic figures, while domain-specific training yields substantial performance gains.Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.""
}",liMultimodalArXivDataset2024
L-Eval: Instituting Standardized Evaluation for Long Context Language Models,"Recently, there has been growing interest in long-context scaling of large language models (LLMs). To facilitate research in this field, we propose L-Eval to institute a more standardized evaluation for Long-Context Language Models (LCLMs) addressing two key aspects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and more than 2,000 human-labeled query-response pairs including diverse task types, domains, and input length (3k~200k tokens). On the other hand, we investigate the effectiveness of evaluation metrics for LCLMs and we show that Length-instruction-enhanced (LIE) evaluation and LLM judges can better correlate with human judgments. We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of a more principled evaluation of these models.",https://aclanthology.org/2024.acl-long.776,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{an-etal-2024-l,
    title = ""{L}-Eval: Instituting Standardized Evaluation for Long Context Language Models"",
    author = ""An, Chenxin  and
      Gong, Shansan  and
      Zhong, Ming  and
      Zhao, Xingjian  and
      Li, Mukai  and
      Zhang, Jun  and
      Kong, Lingpeng  and
      Qiu, Xipeng"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.776/"",
    doi = ""10.18653/v1/2024.acl-long.776"",
    pages = ""14388--14411"",
    abstract = ""Recently, there has been growing interest in long-context scaling of large language models (LLMs). To facilitate research in this field, we propose L-Eval to institute a more standardized evaluation for Long-Context Language Models (LCLMs) addressing two key aspects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and more than 2,000 human-labeled query-response pairs including diverse task types, domains, and input length (3k{\textasciitilde}200k tokens). On the other hand, we investigate the effectiveness of evaluation metrics for LCLMs and we show that Length-instruction-enhanced (LIE) evaluation and LLM judges can better correlate with human judgments. We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of a more principled evaluation of these models.""
}",anLevalInstitutingStandardized2024
CausalGym: Benchmarking causal interpretability methods on linguistic tasks,"Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M–6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler–gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.",https://aclanthology.org/2024.acl-long.785,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{arora-etal-2024-causalgym,
    title = ""{C}ausal{G}ym: Benchmarking causal interpretability methods on linguistic tasks"",
    author = ""Arora, Aryaman  and
      Jurafsky, Dan  and
      Potts, Christopher"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.785/"",
    doi = ""10.18653/v1/2024.acl-long.785"",
    pages = ""14638--14663"",
    abstract = ""Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M{--}6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler{--}gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.""
}",aroraCausalGymBenchmarkingCausal2024
Latxa: An Open Language Model and Evaluation Suite for Basque,"We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,046 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.",https://aclanthology.org/2024.acl-long.799,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{etxaniz-etal-2024-latxa,
    title = ""Latxa: An Open Language Model and Evaluation Suite for {B}asque"",
    author = ""Etxaniz, Julen  and
      Sainz, Oscar  and
      Miguel, Naiara  and
      Aldabe, Itziar  and
      Rigau, German  and
      Agirre, Eneko  and
      Ormazabal, Aitor  and
      Artetxe, Mikel  and
      Soroa, Aitor"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.799/"",
    doi = ""10.18653/v1/2024.acl-long.799"",
    pages = ""14952--14972"",
    abstract = ""We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,046 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.""
}",etxanizLatxaOpenLanguage2024
\inftyBench: Extending Long Context Evaluation Beyond 100K Tokens,"Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose , the first LLM benchmark featuring an average data length surpassing 100K tokens. comprises synthetic and realistic tasks spanning diverse domains in English and Chinese. The tasks in are designed to require an understanding of long dependencies in contexts and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. Based on , we evaluate several state-of-the-art LLMs tailored for processing long contexts. The experimental results indicate that existing long-context LLMs still require significant advancements to process 100K+ contexts effectively. Furthermore, we present three intriguing analyses regarding the behavior of LLMs processing long context. Our code and data is released.",https://aclanthology.org/2024.acl-long.814,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2024-bench,
    title = ""$\infty${B}ench: Extending Long Context Evaluation Beyond 100{K} Tokens"",
    author = ""Zhang, Xinrong  and
      Chen, Yingfa  and
      Hu, Shengding  and
      Xu, Zihang  and
      Chen, Junhao  and
      Hao, Moo  and
      Han, Xu  and
      Thai, Zhen  and
      Wang, Shuo  and
      Liu, Zhiyuan  and
      Sun, Maosong"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.814/"",
    doi = ""10.18653/v1/2024.acl-long.814"",
    pages = ""15262--15277"",
    abstract = ""Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose , the first LLM benchmark featuring an average data length surpassing 100K tokens. comprises synthetic and realistic tasks spanning diverse domains in English and Chinese. The tasks in are designed to require an understanding of long dependencies in contexts and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. Based on , we evaluate several state-of-the-art LLMs tailored for processing long contexts. The experimental results indicate that existing long-context LLMs still require significant advancements to process 100K+ contexts effectively. Furthermore, we present three intriguing analyses regarding the behavior of LLMs processing long context. Our code and data is released.""
}",zhangBenchExtendingLong2024
SafetyBench: Evaluating the Safety of Large Language Models,"With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at https://github.com/thu-coai/SafetyBench. Submission entrance and leaderboard are available at https://llmbench.ai/safety.",https://aclanthology.org/2024.acl-long.830,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2024-safetybench,
    title = ""{S}afety{B}ench: Evaluating the Safety of Large Language Models"",
    author = ""Zhang, Zhexin  and
      Lei, Leqi  and
      Wu, Lindong  and
      Sun, Rui  and
      Huang, Yongkang  and
      Long, Chong  and
      Liu, Xiao  and
      Lei, Xuanyu  and
      Tang, Jie  and
      Huang, Minlie"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.830/"",
    doi = ""10.18653/v1/2024.acl-long.830"",
    pages = ""15537--15553"",
    abstract = ""With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at https://github.com/thu-coai/SafetyBench. Submission entrance and leaderboard are available at https://llmbench.ai/safety.""
}",zhangSafetyBenchEvaluatingSafety2024
M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models,"Managing long sequences has become an important and necessary feature for large language models (LLMs). However, assessing their ability to handle long contexts remains a challenge. This paper introduces M^4LE, a \textbf{M}ulti-ability, \textbf{M}ulti-range, \textbf{M}ulti-task, \textbf{M}ulti-domain benchmark for \textbf{L}ong-context \textbf{E}valuation. It encompasses 36 NLP datasets, covering 11 types of tasks and 12 domains, providing a comprehensive test bed. To address the lack of tasks featuring naturally long sequences, we propose an automatic approach to convert short-sequence tasks into long-sequence scenarios. These scenarios evaluate LLMs’ long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding. This automatic approach allows us to create instances evenly distributed from 1k to 8k input length. Our evaluation of 11 prominent LLMs reveals that 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.",https://aclanthology.org/2024.acl-long.832,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{kwan-etal-2024-m4le,
    title = ""{M}4{LE}: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models"",
    author = ""Kwan, Wai-Chung  and
      Zeng, Xingshan  and
      Wang, Yufei  and
      Sun, Yusen  and
      Li, Liangyou  and
      Jiang, Yuxin  and
      Shang, Lifeng  and
      Liu, Qun  and
      Wong, Kam-Fai"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.832/"",
    doi = ""10.18653/v1/2024.acl-long.832"",
    pages = ""15568--15592"",
    abstract = ""Managing long sequences has become an important and necessary feature for large language models (LLMs). However, assessing their ability to handle long contexts remains a challenge. This paper introduces M$^4$LE, a $\textbf{M}$ulti-ability, $\textbf{M}$ulti-range, $\textbf{M}$ulti-task, $\textbf{M}$ulti-domain benchmark for $\textbf{L}$ong-context $\textbf{E}$valuation. It encompasses 36 NLP datasets, covering 11 types of tasks and 12 domains, providing a comprehensive test bed. To address the lack of tasks featuring naturally long sequences, we propose an automatic approach to convert short-sequence tasks into long-sequence scenarios. These scenarios evaluate LLMs' long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding. This automatic approach allows us to create instances evenly distributed from 1k to 8k input length. Our evaluation of 11 prominent LLMs reveals that 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.""
}",kwanM4LEMultiabilityMultirange2024
ToMBench: Benchmarking Theory of Mind in Large Language Models,"Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of LLMs’ ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.",https://aclanthology.org/2024.acl-long.847,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{chen-etal-2024-tombench,
    title = ""{T}o{MB}ench: Benchmarking Theory of Mind in Large Language Models"",
    author = ""Chen, Zhuang  and
      Wu, Jincenzi  and
      Zhou, Jinfeng  and
      Wen, Bosi  and
      Bi, Guanqun  and
      Jiang, Gongyao  and
      Cao, Yaru  and
      Hu, Mengting  and
      Lai, Yunghwei  and
      Xiong, Zexuan  and
      Huang, Minlie"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.847/"",
    doi = ""10.18653/v1/2024.acl-long.847"",
    pages = ""15959--15983"",
    abstract = ""Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10{\%} points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of LLMs' ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.""
}",chenToMBenchBenchmarkingTheory2024
MultiPICo: Multilingual Perspectivist Irony Corpus,"Recently, several scholars have contributed to the growth of a new theoretical framework in NLP called perspectivism. This approach aimsto leverage data annotated by different individuals to model diverse perspectives that affect their opinions on subjective phenomena such as irony. In this context, we propose MultiPICo, a multilingual perspectivist corpus of ironic short conversations in different languages andlinguistic varieties extracted from Twitter and Reddit. The corpus includes sociodemographic information about its annotators. Our analysis of the annotated corpus shows how different demographic cohorts may significantly disagree on their annotation of irony and how certain cultural factors influence the perception of the phenomenon and the agreement on the annotation. Moreover, we show how disaggregated annotations and rich annotator metadata can be exploited to benchmark the ability of large language models to recognize irony, their positionality with respect to sociodemographic groups, and the efficacy of perspective-taking prompting for irony detection in multiple languages.",https://aclanthology.org/2024.acl-long.849,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{casola-etal-2024-multipico,
    title = ""{M}ulti{PIC}o: Multilingual Perspectivist Irony Corpus"",
    author = ""Casola, Silvia  and
      Frenda, Simona  and
      Lo, Soda Marem  and
      Sezerer, Erhan  and
      Uva, Antonio  and
      Basile, Valerio  and
      Bosco, Cristina  and
      Pedrani, Alessandro  and
      Rubagotti, Chiara  and
      Patti, Viviana  and
      Bernardi, Davide"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.849/"",
    doi = ""10.18653/v1/2024.acl-long.849"",
    pages = ""16008--16021"",
    abstract = ""Recently, several scholars have contributed to the growth of a new theoretical framework in NLP called perspectivism. This approach aimsto leverage data annotated by different individuals to model diverse perspectives that affect their opinions on subjective phenomena such as irony. In this context, we propose MultiPICo, a multilingual perspectivist corpus of ironic short conversations in different languages andlinguistic varieties extracted from Twitter and Reddit. The corpus includes sociodemographic information about its annotators. Our analysis of the annotated corpus shows how different demographic cohorts may significantly disagree on their annotation of irony and how certain cultural factors influence the perception of the phenomenon and the agreement on the annotation. Moreover, we show how disaggregated annotations and rich annotator metadata can be exploited to benchmark the ability of large language models to recognize irony, their positionality with respect to sociodemographic groups, and the efficacy of perspective-taking prompting for irony detection in multiple languages.""
}",casolaMultiPICoMultilingualPerspectivist2024a
AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents,"Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only ~49% of our ‘normal’ tasks and ~30% of ‘challenge’ tasks, while other models solve at least 16% fewer. This highlights the benchmark’s difficulty and AppWorld’s potential to push the frontiers of interactive coding agents.",https://aclanthology.org/2024.acl-long.850,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{trivedi-etal-2024-appworld,
    title = ""{A}pp{W}orld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents"",
    author = ""Trivedi, Harsh  and
      Khot, Tushar  and
      Hartmann, Mareike  and
      Manku, Ruskin  and
      Dong, Vinty  and
      Li, Edward  and
      Gupta, Shashank  and
      Sabharwal, Ashish  and
      Balasubramanian, Niranjan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.850/"",
    doi = ""10.18653/v1/2024.acl-long.850"",
    pages = ""16022--16076"",
    abstract = ""Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of {\textasciitilde}100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only {\textasciitilde}49{\%} of our {\textquoteleft}normal' tasks and {\textasciitilde}30{\%} of {\textquoteleft}challenge' tasks, while other models solve at least 16{\%} fewer. This highlights the benchmark`s difficulty and AppWorld`s potential to push the frontiers of interactive coding agents.""
}",trivediAppWorldControllableWorld2024
MMToM-QA: Multimodal Theory of Mind Question Answering,"Theory of Mind (ToM), the ability to understand people’s mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets – either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person’s mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person’s activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.",https://aclanthology.org/2024.acl-long.851,2024,acl-long,Yes,Multimodal,Benchmark,"@inproceedings{jin-etal-2024-mmtom,
    title = ""{MMT}o{M}-{QA}: Multimodal Theory of Mind Question Answering"",
    author = ""Jin, Chuanyang  and
      Wu, Yutong  and
      Cao, Jing  and
      Xiang, Jiannan  and
      Kuo, Yen-Ling  and
      Hu, Zhiting  and
      Ullman, Tomer  and
      Torralba, Antonio  and
      Tenenbaum, Joshua  and
      Shu, Tianmin"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.851/"",
    doi = ""10.18653/v1/2024.acl-long.851"",
    pages = ""16077--16102"",
    abstract = ""Theory of Mind (ToM), the ability to understand people`s mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets {--} either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person`s mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person`s activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.""
}",jinMMToMQAMultimodalTheory2024
DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents,"Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables. We conduct an extensive evaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting methods, aiming to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that even the current best-performing system (i.e., GPT-4o) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe that DocMath-Eval can serve as a valuable benchmark for evaluating LLMs' capabilities in solving challenging numerical reasoning problems within expert domains.",https://aclanthology.org/2024.acl-long.852,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{zhao-etal-2024-docmath,
    title = ""{D}oc{M}ath-Eval: Evaluating Math Reasoning Capabilities of {LLM}s in Understanding Long and Specialized Documents"",
    author = ""Zhao, Yilun  and
      Long, Yitao  and
      Liu, Hongjun  and
      Kamoi, Ryo  and
      Nan, Linyong  and
      Chen, Lyuhao  and
      Liu, Yixin  and
      Tang, Xiangru  and
      Zhang, Rui  and
      Cohan, Arman"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.852/"",
    doi = ""10.18653/v1/2024.acl-long.852"",
    pages = ""16103--16120"",
    abstract = ""Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables. We conduct an extensive evaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting methods, aiming to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that even the current best-performing system (i.e., GPT-4o) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe that DocMath-Eval can serve as a valuable benchmark for evaluating LLMs' capabilities in solving challenging numerical reasoning problems within expert domains.""
}",zhaoDocMathevalEvaluatingMath2024
LooGLE: Can Long-Context Language Models Understand Long Contexts?,"Large language models (LLMs) are typically limited to processing texts within context window size, which has spurred significant research efforts into enhancing LLMs’ long-context understanding as well as developing high-quality benchmarks to evaluate the ability. However, prior datasets suffer from short comings like short length compared to the context window of modern LLMs; outdated documents that might have data leakage problems; and an emphasis on short dependency tasks only. In this paper, we present LooGLE , a Long Context Generic Language Evaluation benchmark. It features documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning varying dependency ranges in diverse domains. Human annotators meticulously crafted over 1,100 high-quality question-answer (QA) pairs with thorough cross-validation for a most precise assessment of LLMs’ long dependency capabilities. We conduct a comprehensive evaluation of representative LLMs on LooGLE . The results indicate that most LLMs have shockingly bad long context ability and fail to capture long dependencies in the context, even when their context window size is enough to fit the entire document. Our results shed light on enhancing the “true long-context understanding” ability of LLMs instead of merely enlarging their context window.",https://aclanthology.org/2024.acl-long.859,2024,acl-long,Yes,Language,Benchmark,"@inproceedings{li-etal-2024-loogle,
    title = ""{L}oo{GLE}: Can Long-Context Language Models Understand Long Contexts?"",
    author = ""Li, Jiaqi  and
      Wang, Mengmeng  and
      Zheng, Zilong  and
      Zhang, Muhan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.859/"",
    doi = ""10.18653/v1/2024.acl-long.859"",
    pages = ""16304--16333"",
    abstract = ""Large language models (LLMs) are typically limited to processing texts within context window size, which has spurred significant research efforts into enhancing LLMs' long-context understanding as well as developing high-quality benchmarks to evaluate the ability. However, prior datasets suffer from short comings like short length compared to the context window of modern LLMs; outdated documents that might have data leakage problems; and an emphasis on short dependency tasks only. In this paper, we present LooGLE , a Long Context Generic Language Evaluation benchmark. It features documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning varying dependency ranges in diverse domains. Human annotators meticulously crafted over 1,100 high-quality question-answer (QA) pairs with thorough cross-validation for a most precise assessment of LLMs' long dependency capabilities. We conduct a comprehensive evaluation of representative LLMs on LooGLE . The results indicate that most LLMs have shockingly bad long context ability and fail to capture long dependencies in the context, even when their context window size is enough to fit the entire document. Our results shed light on enhancing the {\textquotedblleft}true long-context understanding{\textquotedblright} ability of LLMs instead of merely enlarging their context window.""
}",liLooGLECanLongcontext2024
Can Language Models Serve as Text-Based World Simulators?,"Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM’s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.",https://aclanthology.org/2024.acl-short.1,2024,acl-short,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-language,
    title = ""Can Language Models Serve as Text-Based World Simulators?"",
    author = ""Wang, Ruoyao  and
      Todd, Graham  and
      Xiao, Ziang  and
      Yuan, Xingdi  and
      C{\^o}t{\'e}, Marc-Alexandre  and
      Clark, Peter  and
      Jansen, Peter"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-short.1/"",
    doi = ""10.18653/v1/2024.acl-short.1"",
    pages = ""1--17"",
    abstract = ""Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM`s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.""
}",wangCanLanguageModels2024
"FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models","One type of question that is commonly found in day-to-day scenarios is “fan-out” questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset, along with open-source tools to run models to encourage evaluation.",https://aclanthology.org/2024.acl-short.2,2024,acl-short,Yes,Language,Benchmark,"@inproceedings{zhu-etal-2024-fanoutqa,
    title = ""{F}an{O}ut{QA}: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models"",
    author = ""Zhu, Andrew  and
      Hwang, Alyssa  and
      Dugan, Liam  and
      Callison-Burch, Chris"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-short.2/"",
    doi = ""10.18653/v1/2024.acl-short.2"",
    pages = ""18--37"",
    abstract = ""One type of question that is commonly found in day-to-day scenarios is {\textquotedblleft}fan-out{\textquotedblright} questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset, along with open-source tools to run models to encourage evaluation.""
}",zhuFanOutQAMultihopMultidocument2024
SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark,"The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models’ abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50% to 60% accuracy achieved by the strongest models.",https://aclanthology.org/2024.acl-short.11,2024,acl-short,Yes,Multimodal,Benchmark,"@inproceedings{liang-etal-2024-scemqa,
    title = ""{S}ce{MQA}: A Scientific College Entrance Level Multimodal Question Answering Benchmark"",
    author = ""Liang, Zhenwen  and
      Guo, Kehan  and
      Liu, Gang  and
      Guo, Taicheng  and
      Zhou, Yujun  and
      Yang, Tianyu  and
      Jiao, Jiajun  and
      Pi, Renjie  and
      Zhang, Jipeng  and
      Zhang, Xiangliang"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-short.11/"",
    doi = ""10.18653/v1/2024.acl-short.11"",
    pages = ""109--119"",
    abstract = ""The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50{\%} to 60{\%} accuracy achieved by the strongest models.""
}",liangSceMQAScientificCollege2024
EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models,"The recent rapid development of Large Vision-Language Models (LVLMs) has indicated their potential for embodied tasks. However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs. The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective. Experiments expose the insufficient capacity of current LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs’ embodied spatial understanding.",https://aclanthology.org/2024.acl-short.33,2024,acl-short,Yes,Multimodal,Benchmark,"@inproceedings{du-etal-2024-embspatial,
    title = ""{E}mb{S}patial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models"",
    author = ""Du, Mengfei  and
      Wu, Binhao  and
      Li, Zejun  and
      Huang, Xuanjing  and
      Wei, Zhongyu"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-short.33/"",
    doi = ""10.18653/v1/2024.acl-short.33"",
    pages = ""346--355"",
    abstract = ""The recent rapid development of Large Vision-Language Models (LVLMs) has indicated their potential for embodied tasks. However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs. The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective. Experiments expose the insufficient capacity of current LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs' embodied spatial understanding.""
}",duEmbSpatialbenchBenchmarkingSpatial2024
RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning,"Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs’ capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce *RoTBench*, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model’s resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.",https://aclanthology.org/2024.emnlp-main.19,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{ye-etal-2024-rotbench,
    title = ""{R}o{TB}ench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning"",
    author = ""Ye, Junjie  and
      Wu, Yilong  and
      Gao, Songyang  and
      Huang, Caishuang  and
      Li, Sixian  and
      Li, Guanyu  and
      Fan, Xiaoran  and
      Zhang, Qi  and
      Gui, Tao  and
      Huang, Xuanjing"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.19/"",
    doi = ""10.18653/v1/2024.emnlp-main.19"",
    pages = ""313--333"",
    abstract = ""Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce *RoTBench*, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model`s resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.""
}",yeRoTBenchMultilevelBenchmark2024
Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection,"Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, making them increasingly integral to various applications. However, this capability introduces the risk of prompt injection attacks, where malicious instructions are embedded in the input to trigger unintended actions or content. Understanding the robustness of LLMs against such attacks is critical for ensuring their safe deployment. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks, assessing their ability to discern which instructions to follow and which to disregard. Through extensive experiments with leading instruction-following LLMs, we reveal significant vulnerabilities, particularly in models that mis-follow injected instructions. Our results show that certain models are excessively inclined to prioritize embedded instructions in prompts, often focusing on the latter parts of the prompt without fully understanding the overall context. Conversely, models that exhibit stronger contextual understanding and instruction-following capabilities tend to be more easily compromised by injected instructions. These findings highlight the need to balance improving LLMs’ instruction-following abilities with enhancing their overall comprehension of prompts, to prevent mis-following inappropriate instructions. We hope our analysis provides valuable insights into these vulnerabilities, contributing to the development of more robust solutions in the future.",https://aclanthology.org/2024.emnlp-main.33,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{li-etal-2024-evaluating-instruction,
    title = ""Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection"",
    author = ""Li, Zekun  and
      Peng, Baolin  and
      He, Pengcheng  and
      Yan, Xifeng"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.33/"",
    doi = ""10.18653/v1/2024.emnlp-main.33"",
    pages = ""557--568"",
    abstract = ""Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, making them increasingly integral to various applications. However, this capability introduces the risk of prompt injection attacks, where malicious instructions are embedded in the input to trigger unintended actions or content. Understanding the robustness of LLMs against such attacks is critical for ensuring their safe deployment. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks, assessing their ability to discern which instructions to follow and which to disregard. Through extensive experiments with leading instruction-following LLMs, we reveal significant vulnerabilities, particularly in models that mis-follow injected instructions. Our results show that certain models are excessively inclined to prioritize embedded instructions in prompts, often focusing on the latter parts of the prompt without fully understanding the overall context. Conversely, models that exhibit stronger contextual understanding and instruction-following capabilities tend to be more easily compromised by injected instructions. These findings highlight the need to balance improving LLMs' instruction-following abilities with enhancing their overall comprehension of prompts, to prevent mis-following inappropriate instructions. We hope our analysis provides valuable insights into these vulnerabilities, contributing to the development of more robust solutions in the future.""
}",liEvaluatingInstructionfollowingRobustness2024
Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?,"Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2% but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub.",https://aclanthology.org/2024.emnlp-main.92,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{yang-etal-2024-large-language-models-always,
    title = ""Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?"",
    author = ""Yang, Zhe  and
      Zhang, Yichang  and
      Liu, Tianyu  and
      Yang, Jian  and
      Lin, Junyang  and
      Zhou, Chang  and
      Sui, Zhifang"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.92/"",
    doi = ""10.18653/v1/2024.emnlp-main.92"",
    pages = ""1531--1555"",
    abstract = ""Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2{\%} but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub.""
}",yangCanLargeLanguage2024
Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!,"Humans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability?In response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases.",https://aclanthology.org/2024.emnlp-main.144,2024,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{chung-etal-2024-visual,
    title = ""Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!"",
    author = ""Chung, Jiwan  and
      Lim, Seungwon  and
      Jeon, Jaehyun  and
      Lee, Seungbeen  and
      Yu, Youngjae"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.144/"",
    doi = ""10.18653/v1/2024.emnlp-main.144"",
    pages = ""2452--2469"",
    abstract = ""Humans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability?In response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases.""
}",chungCanVisualLanguage2024
Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?,"State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on multi-hop reasoning—the ability to identify and integrate information from multiple textual sources.Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. We propose a challenging multi-hop reasoning benchmark by generating seemingly plausible multi-hop reasoning chains that ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs and show that their multi-hop reasoning performance is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We also find that—while LLMs tend to ignore misleading lexical cues—misleading reasoning paths indeed present a significant challenge. The code and data are made available at https://github.com/zawedcvg/Are-Large-Language-Models-Attentive-Readers",https://aclanthology.org/2024.emnlp-main.147,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{bhuiya-etal-2024-seemingly,
    title = ""Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?"",
    author = ""Bhuiya, Neeladri  and
      Schlegel, Viktor  and
      Winkler, Stefan"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.147/"",
    doi = ""10.18653/v1/2024.emnlp-main.147"",
    pages = ""2514--2528"",
    abstract = ""State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on multi-hop reasoning{---}the ability to identify and integrate information from multiple textual sources.Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. We propose a challenging multi-hop reasoning benchmark by generating seemingly plausible multi-hop reasoning chains that ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs and show that their multi-hop reasoning performance is affected, as indicated by up to 45{\%} relative decrease in F1 score when presented with such seemingly plausible alternatives. We also find that{---}while LLMs tend to ignore misleading lexical cues{---}misleading reasoning paths indeed present a significant challenge. The code and data are made available at https://github.com/zawedcvg/Are-Large-Language-Models-Attentive-Readers""
}",bhuiyaSeeminglyPlausibleDistractors2024
CUTE: Measuring LLMs’ Understanding of Their Tokens,"Large Language Models (LLMs) show remarkable performance on a wide variety of tasks. Most LLMs split text into multi-character tokens and process them as atomic units without direct access to individual characters. This raises the question: To what extent can LLMs learn orthographic information? To answer this, we propose a new benchmark, CUTE, which features a collection of tasks designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs on CUTE, finding that most of them seem to know the spelling of their tokens, yet fail to use this information effectively to manipulate text, calling into question how much of this knowledge is generalizable.",https://aclanthology.org/2024.emnlp-main.177,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{edman-etal-2024-cute,
    title = ""{CUTE}: Measuring {LLM}s' Understanding of Their Tokens"",
    author = ""Edman, Lukas  and
      Schmid, Helmut  and
      Fraser, Alexander"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.177/"",
    doi = ""10.18653/v1/2024.emnlp-main.177"",
    pages = ""3017--3026"",
    abstract = ""Large Language Models (LLMs) show remarkable performance on a wide variety of tasks. Most LLMs split text into multi-character tokens and process them as atomic units without direct access to individual characters. This raises the question: To what extent can LLMs learn orthographic information? To answer this, we propose a new benchmark, CUTE, which features a collection of tasks designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs on CUTE, finding that most of them seem to know the spelling of their tokens, yet fail to use this information effectively to manipulate text, calling into question how much of this knowledge is generalizable.""
}",edmanCUTEMeasuringLLMs2024
A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models,"Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset, code, and process data are publicly available at https://github.com/Alice1998/URS.",https://aclanthology.org/2024.emnlp-main.210,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-user,
    title = ""A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models"",
    author = ""Wang, Jiayin  and
      Mo, Fengran  and
      Ma, Weizhi  and
      Sun, Peijie  and
      Zhang, Min  and
      Nie, Jian-Yun"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.210/"",
    doi = ""10.18653/v1/2024.emnlp-main.210"",
    pages = ""3588--3612"",
    abstract = ""Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset, code, and process data are publicly available at https://github.com/Alice1998/URS.""
}",wangUsercentricMultiintentBenchmark2024
VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation,"In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons, sketches and scientific figures. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs and (f) comparison with VLMs on rasterized representations. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced.",https://aclanthology.org/2024.emnlp-main.213,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zou-etal-2024-vgbench,
    title = ""{VGB}ench: Evaluating Large Language Models on Vector Graphics Understanding and Generation"",
    author = ""Zou, Bocheng  and
      Cai, Mu  and
      Zhang, Jianrui  and
      Lee, Yong Jae"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.213/"",
    doi = ""10.18653/v1/2024.emnlp-main.213"",
    pages = ""3647--3659"",
    abstract = ""In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons, sketches and scientific figures. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs and (f) comparison with VLMs on rasterized representations. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced.""
}",zouVGBenchEvaluatingLarge2024
I Could’ve Asked That: Reformulating Unanswerable Questions,"When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the time, respectively. Error analysis shows that 62% of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark and the code to reproduce the experiments.",https://aclanthology.org/2024.emnlp-main.242,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zhao-etal-2024-couldve,
    title = ""{I} Could`ve Asked That: Reformulating Unanswerable Questions"",
    author = ""Zhao, Wenting  and
      Gao, Ge  and
      Cardie, Claire  and
      Rush, Alexander M"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.242/"",
    doi = ""10.18653/v1/2024.emnlp-main.242"",
    pages = ""4207--4220"",
    abstract = ""When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26{\%} and 12{\%} of the time, respectively. Error analysis shows that 62{\%} of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark and the code to reproduce the experiments.""
}",zhaoCould`veAskedThat2024a
STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions,"Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3% to 69.8%. Furthermore, we demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191%, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models.",https://aclanthology.org/2024.emnlp-main.243,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{morabito-etal-2024-stop,
    title = ""{STOP}! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions"",
    author = ""Morabito, Robert  and
      Madhusudan, Sangmitra  and
      McDonald, Tyler  and
      Emami, Ali"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.243/"",
    doi = ""10.18653/v1/2024.emnlp-main.243"",
    pages = ""4221--4243"",
    abstract = ""Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3{\%} to 69.8{\%}. Furthermore, we demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191{\%}, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models.""
}",morabitoSTOPBenchmarkingLarge2024
NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian,"Norwegian, spoken by only 5 million population, is under-representative within the most impressive breakthroughs in NLP tasks. To the best of our knowledge, there has not yet been a comprehensive evaluation of the existing language models (LMs) on Norwegian generation tasks during the article writing process. To fill this gap, we 1) compiled the existing Norwegian dataset and pre-trained 4 Norwegian Open Language Models varied from parameter scales and architectures, collectively called NorGLM; 2) introduced a comprehensive benchmark, NLEBench, for evaluating natural language generation capabilities in Norwegian, encompassing translation and human annotation. Based on the investigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5 has limited capability in understanding the Norwegian context; 2) the increase in model parameter scales demonstrates limited impact on the performance of downstream tasks when the pre-training dataset is constrained in size; 3) smaller models also demonstrate the reasoning capability through Chain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be used to verify the generalizability of LLMs on natural language understanding and, meanwhile, test the interconnectedness of these NLP tasks. We share our resources and code for reproducibility under a CC BY-NC 4.0 license.",https://aclanthology.org/2024.emnlp-main.317,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{liu-etal-2024-nlebench,
    title = ""{NLEB}ench+{N}or{GLM}: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in {N}orwegian"",
    author = ""Liu, Peng  and
      Zhang, Lemei  and
      Farup, Terje  and
      Lauvrak, Even W.  and
      Ingvaldsen, Jon Espen  and
      Eide, Simen  and
      Gulla, Jon Atle  and
      Yang, Zhirong"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.317/"",
    doi = ""10.18653/v1/2024.emnlp-main.317"",
    pages = ""5543--5560"",
    abstract = ""Norwegian, spoken by only 5 million population, is under-representative within the most impressive breakthroughs in NLP tasks. To the best of our knowledge, there has not yet been a comprehensive evaluation of the existing language models (LMs) on Norwegian generation tasks during the article writing process. To fill this gap, we 1) compiled the existing Norwegian dataset and pre-trained 4 Norwegian Open Language Models varied from parameter scales and architectures, collectively called NorGLM; 2) introduced a comprehensive benchmark, NLEBench, for evaluating natural language generation capabilities in Norwegian, encompassing translation and human annotation. Based on the investigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5 has limited capability in understanding the Norwegian context; 2) the increase in model parameter scales demonstrates limited impact on the performance of downstream tasks when the pre-training dataset is constrained in size; 3) smaller models also demonstrate the reasoning capability through Chain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be used to verify the generalizability of LLMs on natural language understanding and, meanwhile, test the interconnectedness of these NLP tasks. We share our resources and code for reproducibility under a CC BY-NC 4.0 license.""
}",liuNLEBench+NorGLMComprehensiveEmpirical2024
Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA,"Long-context modeling capabilities of Large Language Models (LLMs) have garnered widespread attention, leading to the emergence of LLMs with ultra-context windows. Meanwhile, benchmarks for evaluating long-context language models are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong’s test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model’s long-context modeling capabilities.",https://aclanthology.org/2024.emnlp-main.322,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-leave,
    title = ""Leave No Document Behind: Benchmarking Long-Context {LLM}s with Extended Multi-Doc {QA}"",
    author = ""Wang, Minzheng  and
      Chen, Longze  and
      Cheng, Fu  and
      Liao, Shengyi  and
      Zhang, Xinghua  and
      Wu, Bingli  and
      Yu, Haiyang  and
      Xu, Nan  and
      Zhang, Lei  and
      Luo, Run  and
      Li, Yunshui  and
      Yang, Min  and
      Huang, Fei  and
      Li, Yongbin"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.322/"",
    doi = ""10.18653/v1/2024.emnlp-main.322"",
    pages = ""5627--5646"",
    abstract = ""Long-context modeling capabilities of Large Language Models (LLMs) have garnered widespread attention, leading to the emergence of LLMs with ultra-context windows. Meanwhile, benchmarks for evaluating long-context language models are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong`s test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model`s long-context modeling capabilities.""
}",wangLeaveNoDocument2024
Do Large Language Models Know How Much They Know?,"Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. Nevertheless, the rapid advancement in their deployment trails a comprehensive understanding of their internal mechanisms, as well as a delineation of their capabilities and limitations. A desired characteristic of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this attribute, we develop a benchmark that challenges these models to enumerate all information they possess on specific topics. This benchmark assesses whether the models recall excessive, insufficient, or the precise amount of required information, thereby indicating their awareness of how much they know about the given topic. Our findings reveal that the emergence of this property varies across different architectures and manifests at diverse rates. However, with sufficient scaling, all tested models are ultimately capable of performing this task. The insights gained from this research advance our understanding of LLMs, shedding light on their operational capabilities and contributing to the ongoing exploration of their intricate dynamics.",https://aclanthology.org/2024.emnlp-main.348,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{prato-etal-2024-large,
    title = ""Do Large Language Models Know How Much They Know?"",
    author = ""Prato, Gabriele  and
      Huang, Jerry  and
      Parthasarathi, Prasanna  and
      Sodhani, Shagun  and
      Chandar, Sarath"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.348/"",
    doi = ""10.18653/v1/2024.emnlp-main.348"",
    pages = ""6054--6070"",
    abstract = ""Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. Nevertheless, the rapid advancement in their deployment trails a comprehensive understanding of their internal mechanisms, as well as a delineation of their capabilities and limitations. A desired characteristic of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this attribute, we develop a benchmark that challenges these models to enumerate all information they possess on specific topics. This benchmark assesses whether the models recall excessive, insufficient, or the precise amount of required information, thereby indicating their awareness of how much they know about the given topic. Our findings reveal that the emergence of this property varies across different architectures and manifests at diverse rates. However, with sufficient scaling, all tested models are ultimately capable of performing this task. The insights gained from this research advance our understanding of LLMs, shedding light on their operational capabilities and contributing to the ongoing exploration of their intricate dynamics.""
}",pratoLargeLanguageModels2024
Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts,"One useful application of NLP models is to support people in reading complex text from unfamiliar domains (e.g., scientific articles). Simplifying the entire text makes it understandable but sometimes removes important details. On the contrary, helping adult readers understand difficult concepts in context can enhance their vocabulary and knowledge. In a preliminary human study, we first identify that lack of context and unfamiliarity with difficult concepts is a major reason for adult readers’ difficulty with domain-specific text. We then introduce targeted concept simplification, a simplification task for rewriting text to help readers comprehend text containing unfamiliar concepts. We also introduce WikiDomains, a new dataset of 22k definitions from 13 academic domains paired with a difficult concept within each definition. We benchmark the performance of open-source and commercial LLMs and a simple dictionary baseline on this task across human judgments of ease of understanding and meaning preservation. Interestingly, our human judges preferred explanations about the difficult concept more than simplifications of the concept phrase. Further, no single model achieved superior performance across all quality dimensions, and automated metrics also show low correlations with human evaluations of concept simplification (~0.2), opening up rich avenues for research on personalized human reading comprehension support.",https://aclanthology.org/2024.emnlp-main.357,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{asthana-etal-2024-evaluating,
    title = ""Evaluating {LLM}s for Targeted Concept Simplification for Domain-Specific Texts"",
    author = ""Asthana, Sumit  and
      Rashkin, Hannah  and
      Clark, Elizabeth  and
      Huot, Fantine  and
      Lapata, Mirella"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.357/"",
    doi = ""10.18653/v1/2024.emnlp-main.357"",
    pages = ""6208--6226"",
    abstract = ""One useful application of NLP models is to support people in reading complex text from unfamiliar domains (e.g., scientific articles). Simplifying the entire text makes it understandable but sometimes removes important details. On the contrary, helping adult readers understand difficult concepts in context can enhance their vocabulary and knowledge. In a preliminary human study, we first identify that lack of context and unfamiliarity with difficult concepts is a major reason for adult readers' difficulty with domain-specific text. We then introduce targeted concept simplification, a simplification task for rewriting text to help readers comprehend text containing unfamiliar concepts. We also introduce WikiDomains, a new dataset of 22k definitions from 13 academic domains paired with a difficult concept within each definition. We benchmark the performance of open-source and commercial LLMs and a simple dictionary baseline on this task across human judgments of ease of understanding and meaning preservation. Interestingly, our human judges preferred explanations about the difficult concept more than simplifications of the concept phrase. Further, no single model achieved superior performance across all quality dimensions, and automated metrics also show low correlations with human evaluations of concept simplification ({\textasciitilde}0.2), opening up rich avenues for research on personalized human reading comprehension support.""
}",asthanaEvaluatingLLMsTargeted2024a
UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models,"Smaller-scale Vision-Language Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the “Uncontextualized Uncommon Objects” (UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs. Code and project details for UOUO can be found at https://zoezheng126.github.io/UOUO-Website/.",https://aclanthology.org/2024.emnlp-main.369,2024,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{pi-etal-2024-uouo,
    title = ""{UOUO}: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models"",
    author = ""Pi, Xinyu  and
      Wu, Mingyuan  and
      Jiang, Jize  and
      Zheng, Haozhen  and
      Tian, Beitong  and
      Zhai, ChengXiang  and
      Nahrstedt, Klara  and
      Hu, Zhiting"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.369/"",
    doi = ""10.18653/v1/2024.emnlp-main.369"",
    pages = ""6432--6441"",
    abstract = ""Smaller-scale Vision-Language Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the {\textquotedblleft}Uncontextualized Uncommon Objects{\textquotedblright} (UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs. Code and project details for UOUO can be found at https://zoezheng126.github.io/UOUO-Website/.""
}",piUOUOUncontextualizedUncommon2024a
Understanding and Mitigating Language Confusion in LLMs,"We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user’s desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation.",https://aclanthology.org/2024.emnlp-main.380,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{marchisio-etal-2024-understanding,
    title = ""Understanding and Mitigating Language Confusion in {LLM}s"",
    author = ""Marchisio, Kelly  and
      Ko, Wei-Yin  and
      Berard, Alexandre  and
      Dehaze, Th{\'e}o  and
      Ruder, Sebastian"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.380/"",
    doi = ""10.18653/v1/2024.emnlp-main.380"",
    pages = ""6653--6677"",
    abstract = ""We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user`s desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation.""
}",marchisioUnderstandingMitigatingLanguage2024
From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models,"Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models’ cultural inclusivity. Still, they have limited coverage of cultures and do not adequately assess cultural diversity across universal and culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures – underscoring the necessity for enhancing multicultural understanding in vision-language models.",https://aclanthology.org/2024.emnlp-main.385,2024,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{bhatia-etal-2024-local,
    title = ""From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models"",
    author = ""Bhatia, Mehar  and
      Ravi, Sahithya  and
      Chinchure, Aditya  and
      Hwang, EunJeong  and
      Shwartz, Vered"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.385/"",
    doi = ""10.18653/v1/2024.emnlp-main.385"",
    pages = ""6763--6782"",
    abstract = ""Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural inclusivity. Still, they have limited coverage of cultures and do not adequately assess cultural diversity across universal and culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures {--} underscoring the necessity for enhancing multicultural understanding in vision-language models.""
}",bhatiaLocalConceptsUniversals2024
FRoG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in LLMs,"Fuzzy reasoning is vital due to the frequent use of imprecise information in daily contexts. However, the ability of current large language models (LLMs) to handle such reasoning remains largely uncharted. In this paper, we introduce a new benchmark, FRoG, for fuzzy reasoning, featuring real-world mathematical word problems that incorporate generalized quantifiers. Our experimental findings reveal that fuzzy reasoning continues to pose significant challenges for LLMs. Moreover, we find that existing methods designed to enhance reasoning do not consistently improve performance in tasks involving fuzzy logic. Additionally, our results show an inverse scaling effect in the performance of LLMs on FRoG. Interestingly, we also demonstrate that strong mathematical reasoning skills are not necessarily indicative of success on our benchmark.",https://aclanthology.org/2024.emnlp-main.411,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{li-etal-2024-frog,
    title = ""{FR}o{G}: Evaluating Fuzzy Reasoning of Generalized Quantifiers in {LLM}s"",
    author = ""Li, Yiyuan  and
      Sun, Shichao  and
      Liu, Pengfei"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.411/"",
    doi = ""10.18653/v1/2024.emnlp-main.411"",
    pages = ""7239--7256"",
    abstract = ""Fuzzy reasoning is vital due to the frequent use of imprecise information in daily contexts. However, the ability of current large language models (LLMs) to handle such reasoning remains largely uncharted. In this paper, we introduce a new benchmark, FRoG, for fuzzy reasoning, featuring real-world mathematical word problems that incorporate generalized quantifiers. Our experimental findings reveal that fuzzy reasoning continues to pose significant challenges for LLMs. Moreover, we find that existing methods designed to enhance reasoning do not consistently improve performance in tasks involving fuzzy logic. Additionally, our results show an inverse scaling effect in the performance of LLMs on FRoG. Interestingly, we also demonstrate that strong mathematical reasoning skills are not necessarily indicative of success on our benchmark.""
}",liFRoGEvaluatingFuzzy2024a
"MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration","Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs’ reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.We utilize two social deduction games alongside three game-theory scenarios to create diverse environments.Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs’ capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37%. Our data and code can be found here https://github.com/cathyxl/MAgIC.",https://aclanthology.org/2024.emnlp-main.416,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{xu-etal-2024-magic,
    title = ""{MA}g{IC}: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration"",
    author = ""Xu, Lin  and
      Hu, Zhiyuan  and
      Zhou, Daquan  and
      Ren, Hongyu  and
      Dong, Zhen  and
      Keutzer, Kurt  and
      Ng, See-Kiong  and
      Feng, Jiashi"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.416/"",
    doi = ""10.18653/v1/2024.emnlp-main.416"",
    pages = ""7315--7332"",
    abstract = ""Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs' reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.We utilize two social deduction games alongside three game-theory scenarios to create diverse environments.Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37{\%}. Our data and code can be found here https://github.com/cathyxl/MAgIC.""
}",xuMAgICInvestigationLarge2024
PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data,"Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors – the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.",https://aclanthology.org/2024.emnlp-main.451,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{watts-etal-2024-pariksha,
    title = ""{PARIKSHA}: A Large-Scale Investigation of Human-{LLM} Evaluator Agreement on Multilingual and Multi-Cultural Data"",
    author = ""Watts, Ishaan  and
      Gumma, Varun  and
      Yadavalli, Aditya  and
      Seshadri, Vivek  and
      Swaminathan, Manohar  and
      Sitaram, Sunayana"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.451/"",
    doi = ""10.18653/v1/2024.emnlp-main.451"",
    pages = ""7900--7932"",
    abstract = ""Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors {--} the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.""
}",wattsPARIKSHALargescaleInvestigation2024
LawBench: Benchmarking Legal Knowledge of Large Language Models,"We present LawBench, the first evaluation benchmark composed of 20 tasks aimed to assess the ability of Large Language Models (LLMs) to perform Chinese legal-related tasks. LawBench is meticulously crafted to enable precise assessment of LLMs’ legal capabilities from three cognitive levels that correspond to the widely accepted Bloom’s cognitive taxonomy. Using LawBench, we present a comprehensive evaluation of 21 popular LLMs and the first comparative analysis of the empirical results in order to reveal their relative strengths and weaknesses. All data, model predictions and evaluation code are accessible from https://github.com/open-compass/LawBench.",https://aclanthology.org/2024.emnlp-main.452,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{fei-etal-2024-lawbench,
    title = ""{L}aw{B}ench: Benchmarking Legal Knowledge of Large Language Models"",
    author = ""Fei, Zhiwei  and
      Shen, Xiaoyu  and
      Zhu, Dawei  and
      Zhou, Fengzhe  and
      Han, Zhuo  and
      Huang, Alan  and
      Zhang, Songyang  and
      Chen, Kai  and
      Yin, Zhixin  and
      Shen, Zongwen  and
      Ge, Jidong  and
      Ng, Vincent"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.452/"",
    doi = ""10.18653/v1/2024.emnlp-main.452"",
    pages = ""7933--7962"",
    abstract = ""We present LawBench, the first evaluation benchmark composed of 20 tasks aimed to assess the ability of Large Language Models (LLMs) to perform Chinese legal-related tasks. LawBench is meticulously crafted to enable precise assessment of LLMs' legal capabilities from three cognitive levels that correspond to the widely accepted Bloom`s cognitive taxonomy. Using LawBench, we present a comprehensive evaluation of 21 popular LLMs and the first comparative analysis of the empirical results in order to reveal their relative strengths and weaknesses. All data, model predictions and evaluation code are accessible from https://github.com/open-compass/LawBench.""
}",feiLawBenchBenchmarkingLegal2024a
Attribute or Abstain: Large Language Models as Long Document Assistants,"LLMs can help humans working with long documents, but are known to hallucinate. *Attribution* can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiments with different approaches to attribution on 5 LLMs of different sizes. We find that *citation*, i.e. response generation and evidence extraction in one step, performs best for large and fine-tuned models, while additional retrieval can help for small, prompted models. We investigate whether the “Lost in the Middle” phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims. We release code and data for further investigation. [Link](https://github.com/UKPLab/arxiv2024-attribute-or-abstain)",https://aclanthology.org/2024.emnlp-main.463,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{buchmann-etal-2024-attribute,
    title = ""Attribute or Abstain: Large Language Models as Long Document Assistants"",
    author = ""Buchmann, Jan  and
      Liu, Xiao  and
      Gurevych, Iryna"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.463/"",
    doi = ""10.18653/v1/2024.emnlp-main.463"",
    pages = ""8113--8140"",
    abstract = ""LLMs can help humans working with long documents, but are known to hallucinate. *Attribution* can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiments with different approaches to attribution on 5 LLMs of different sizes. We find that *citation*, i.e. response generation and evidence extraction in one step, performs best for large and fine-tuned models, while additional retrieval can help for small, prompted models. We investigate whether the {\textquotedblleft}Lost in the Middle{\textquotedblright} phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims. We release code and data for further investigation. [Link](https://github.com/UKPLab/arxiv2024-attribute-or-abstain)""
}",buchmannAttributeAbstainLarge2024a
CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios,"With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research.",https://aclanthology.org/2024.emnlp-main.480,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{ouyang-etal-2024-climedbench,
    title = ""{C}li{M}ed{B}ench: A Large-Scale {C}hinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios"",
    author = ""Ouyang, Zetian  and
      Qiu, Yishuai  and
      Wang, Linlin  and
      De Melo, Gerard  and
      Zhang, Ya  and
      Wang, Yanfeng  and
      He, Liang"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.480/"",
    doi = ""10.18653/v1/2024.emnlp-main.480"",
    pages = ""8428--8438"",
    abstract = ""With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research.""
}",ouyangCliMedBenchLargescaleChinese2024a
“Flex Tape Can’t Fix That”: Bias and Misinformation in Edited Language Models,"Weight-based model editing methods update the parametric knowledge of language models post-training. However, these methods can unintentionally alter unrelated parametric knowledge representations, potentially increasing the risk of harm. In this work, we investigate how weight editing methods unexpectedly amplify model biases after edits. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias amplification of model editing methods for demographic traits such as race, geographic origin, and gender. We use Seesaw-CF to examine the impact of model editing on bias in five large language models. Our results demonstrate that edited models exhibit, to various degrees, more biased behavior for certain demographic groups than before they were edited, specifically becoming less confident in properties for Asian and African subjects. Additionally, editing facts about place of birth, country of citizenship, or gender has particularly negative effects on the model’s knowledge about unrelated properties, such as field of work, a pattern observed across multiple models.",https://aclanthology.org/2024.emnlp-main.494,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{halevy-etal-2024-flex,
    title = ""{\textquotedblleft}Flex Tape Can`t Fix That{\textquotedblright}: Bias and Misinformation in Edited Language Models"",
    author = ""Halevy, Karina H  and
      Sotnikova, Anna  and
      AlKhamissi, Badr  and
      Montariol, Syrielle  and
      Bosselut, Antoine"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.494/"",
    doi = ""10.18653/v1/2024.emnlp-main.494"",
    pages = ""8690--8707"",
    abstract = ""Weight-based model editing methods update the parametric knowledge of language models post-training. However, these methods can unintentionally alter unrelated parametric knowledge representations, potentially increasing the risk of harm. In this work, we investigate how weight editing methods unexpectedly amplify model biases after edits. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias amplification of model editing methods for demographic traits such as race, geographic origin, and gender. We use Seesaw-CF to examine the impact of model editing on bias in five large language models. Our results demonstrate that edited models exhibit, to various degrees, more biased behavior for certain demographic groups than before they were edited, specifically becoming less confident in properties for Asian and African subjects. Additionally, editing facts about place of birth, country of citizenship, or gender has particularly negative effects on the model`s knowledge about unrelated properties, such as field of work, a pattern observed across multiple models.""
}",halevyFlexTapeCan`t2024
AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?,"Language agents, built on top of language models (LMs), are systems that can interact with complex environments, such as the open web. In this work, we examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses. We introduce AssistantBench, a challenging new benchmark consisting of 214 realistic tasks that can be automatically evaluated, covering different scenarios and domains. We find that AssistantBench exposes the limitations of current systems, including language models and retrieval-augmented language models, as no model reaches an accuracy of more than 25 points. While closed-book LMs perform well in terms of accuracy, they exhibit low precision and tend to hallucinate facts. State-of-the-art web agents reach a score of near zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance. Moreover, we analyze failures of current systems and highlight that open web navigation remains a major challenge.",https://aclanthology.org/2024.emnlp-main.505,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{yoran-etal-2024-assistantbench,
    title = ""{A}ssistant{B}ench: Can Web Agents Solve Realistic and Time-Consuming Tasks?"",
    author = ""Yoran, Ori  and
      Amouyal, Samuel Joseph  and
      Malaviya, Chaitanya  and
      Bogin, Ben  and
      Press, Ofir  and
      Berant, Jonathan"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.505/"",
    doi = ""10.18653/v1/2024.emnlp-main.505"",
    pages = ""8938--8968"",
    abstract = ""Language agents, built on top of language models (LMs), are systems that can interact with complex environments, such as the open web. In this work, we examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses. We introduce AssistantBench, a challenging new benchmark consisting of 214 realistic tasks that can be automatically evaluated, covering different scenarios and domains. We find that AssistantBench exposes the limitations of current systems, including language models and retrieval-augmented language models, as no model reaches an accuracy of more than 25 points. While closed-book LMs perform well in terms of accuracy, they exhibit low precision and tend to hallucinate facts. State-of-the-art web agents reach a score of near zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance. Moreover, we analyze failures of current systems and highlight that open web navigation remains a major challenge.""
}",yoranAssistantBenchCanWeb2024
The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention,"Prompt-based “diversity interventions” are commonly adopted to improve the diversity of Text-to-Image (T2I) models depicting individuals with various racial or gender traits. However, will this strategy result in nonfactual demographic distribution, especially when generating real historical figures? In this work, we propose **DemOgraphic FActualIty Representation (DoFaiR)**, a benchmark to systematically quantify the trade-off between using diversity interventions and preserving demographic factuality in T2I models. DoFaiR consists of 756 meticulously fact-checked test instances to reveal the factuality tax of various diversity prompts through an automated evidence-supported evaluation pipeline. Experiments on DoFaiR unveil that diversity-oriented instructions increase the number of different gender and racial groups in DALLE-3’s generations at the cost of historically inaccurate demographic distributions. To resolve this issue, we propose **Fact-Augmented Intervention** (FAI), which instructs a Large Language Model (LLM) to reflect on verbalized or retrieved factual information about gender and racial compositions of generation subjects in history, and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI significantly improves the demographic factuality under diversity interventions while preserving diversity.",https://aclanthology.org/2024.emnlp-main.513,2024,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{wan-etal-2024-factuality,
    title = ""The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention"",
    author = ""Wan, Yixin  and
      Wu, Di  and
      Wang, Haoran  and
      Chang, Kai-Wei"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.513/"",
    doi = ""10.18653/v1/2024.emnlp-main.513"",
    pages = ""9082--9100"",
    abstract = ""Prompt-based {\textquotedblleft}diversity interventions{\textquotedblright} are commonly adopted to improve the diversity of Text-to-Image (T2I) models depicting individuals with various racial or gender traits. However, will this strategy result in nonfactual demographic distribution, especially when generating real historical figures? In this work, we propose **DemOgraphic FActualIty Representation (DoFaiR)**, a benchmark to systematically quantify the trade-off between using diversity interventions and preserving demographic factuality in T2I models. DoFaiR consists of 756 meticulously fact-checked test instances to reveal the factuality tax of various diversity prompts through an automated evidence-supported evaluation pipeline. Experiments on DoFaiR unveil that diversity-oriented instructions increase the number of different gender and racial groups in DALLE-3`s generations at the cost of historically inaccurate demographic distributions. To resolve this issue, we propose **Fact-Augmented Intervention** (FAI), which instructs a Large Language Model (LLM) to reflect on verbalized or retrieved factual information about gender and racial compositions of generation subjects in history, and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI significantly improves the demographic factuality under diversity interventions while preserving diversity.""
}",wanFactualityTaxDiversityintervened2024
RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs,"Minimal pairs are a well-established approach to evaluating the grammatical knowledge of language models. However, existing resources for minimal pairs address a limited number of languages and lack diversity of language-specific grammatical phenomena. This paper introduces the Russian Benchmark of Linguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that differ in grammaticality and isolate a morphological, syntactic, or semantic phenomenon. In contrast to existing benchmarks of linguistic minimal pairs, RuBLiMP is created by applying linguistic perturbations to automatically annotated sentences from open text corpora and decontaminating test data. We describe the data collection protocol and present the results of evaluating 25 language models in various scenarios. We find that the widely used LMs for Russian are sensitive to morphological and agreement-oriented contrasts, but fall behind humans on phenomena requiring the understanding of structural relations, negation, transitivity, and tense. RuBLiMP, the codebase, and other materials are publicly available.",https://aclanthology.org/2024.emnlp-main.522,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{taktasheva-etal-2024-rublimp,
    title = ""{R}u{BL}i{MP}: {R}ussian Benchmark of Linguistic Minimal Pairs"",
    author = ""Taktasheva, Ekaterina  and
      Bazhukov, Maxim  and
      Koncha, Kirill  and
      Fenogenova, Alena  and
      Artemova, Ekaterina  and
      Mikhailov, Vladislav"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.522/"",
    doi = ""10.18653/v1/2024.emnlp-main.522"",
    pages = ""9268--9299"",
    abstract = ""Minimal pairs are a well-established approach to evaluating the grammatical knowledge of language models. However, existing resources for minimal pairs address a limited number of languages and lack diversity of language-specific grammatical phenomena. This paper introduces the Russian Benchmark of Linguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that differ in grammaticality and isolate a morphological, syntactic, or semantic phenomenon. In contrast to existing benchmarks of linguistic minimal pairs, RuBLiMP is created by applying linguistic perturbations to automatically annotated sentences from open text corpora and decontaminating test data. We describe the data collection protocol and present the results of evaluating 25 language models in various scenarios. We find that the widely used LMs for Russian are sensitive to morphological and agreement-oriented contrasts, but fall behind humans on phenomena requiring the understanding of structural relations, negation, transitivity, and tense. RuBLiMP, the codebase, and other materials are publicly available.""
}",taktashevaRuBLiMPRussianBenchmark2024
STORYSUMM: Evaluating Faithfulness in Story Summarization,"Human evaluation has been the gold standard for checking faithfulness in abstractive summarization. However, with a challenging source domain like narrative, multiple annotators can agree a summary is faithful, while missing details that are obvious errors only once pointed out. We therefore introduce a new dataset, StorySumm, comprising LLM summaries of short stories with localized faithfulness labels and error explanations. This benchmark is for evaluation methods, testing whether a given method can detect challenging inconsistencies. Using this dataset, we first show that any one human annotation protocol is likely to miss inconsistencies, and we advocate for pursuing a range of methods when establishing ground truth for a summarization dataset. We finally test recent automatic metrics and find that none of them achieve more than 70% balanced accuracy on this task, demonstrating that it is a challenging benchmark for future work in faithfulness evaluation.",https://aclanthology.org/2024.emnlp-main.557,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{subbiah-etal-2024-storysumm,
    title = ""{STORYSUMM}: Evaluating Faithfulness in Story Summarization"",
    author = ""Subbiah, Melanie  and
      Ladhak, Faisal  and
      Mishra, Akankshya  and
      Adams, Griffin Thomas  and
      Chilton, Lydia  and
      McKeown, Kathleen"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.557/"",
    doi = ""10.18653/v1/2024.emnlp-main.557"",
    pages = ""9988--10005"",
    abstract = ""Human evaluation has been the gold standard for checking faithfulness in abstractive summarization. However, with a challenging source domain like narrative, multiple annotators can agree a summary is faithful, while missing details that are obvious errors only once pointed out. We therefore introduce a new dataset, StorySumm, comprising LLM summaries of short stories with localized faithfulness labels and error explanations. This benchmark is for evaluation methods, testing whether a given method can detect challenging inconsistencies. Using this dataset, we first show that any one human annotation protocol is likely to miss inconsistencies, and we advocate for pursuing a range of methods when establishing ground truth for a summarization dataset. We finally test recent automatic metrics and find that none of them achieve more than 70{\%} balanced accuracy on this task, demonstrating that it is a challenging benchmark for future work in faithfulness evaluation.""
}",subbiahSTORYSUMMEvaluatingFaithfulness2024
DataTales: A Benchmark for Real-World Intelligent Data Narration,"We introduce DataTales, a novel benchmark designed to assess the proficiency of language models in data narration, a task crucial for transforming complex tabular data into accessible narratives. Existing benchmarks often fall short in capturing the requisite analytical complexity for practical applications. DataTales addresses this gap by offering 4.9k financial reports paired with corresponding market data, showcasing the demand for models to create clear narratives and analyze large datasets while understanding specialized terminology in the field. Our findings highlights the significant challenge that language models face in achieving the necessary precision and analytical depth for proficient data narration, suggesting promising avenues for future model development and evaluation methodologies.",https://aclanthology.org/2024.emnlp-main.601,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{yang-etal-2024-datatales,
    title = ""{D}ata{T}ales: A Benchmark for Real-World Intelligent Data Narration"",
    author = ""Yang, Yajing  and
      Liu, Qian  and
      Kan, Min-Yen"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.601/"",
    doi = ""10.18653/v1/2024.emnlp-main.601"",
    pages = ""10764--10788"",
    abstract = ""We introduce DataTales, a novel benchmark designed to assess the proficiency of language models in data narration, a task crucial for transforming complex tabular data into accessible narratives. Existing benchmarks often fall short in capturing the requisite analytical complexity for practical applications. DataTales addresses this gap by offering 4.9k financial reports paired with corresponding market data, showcasing the demand for models to create clear narratives and analyze large datasets while understanding specialized terminology in the field. Our findings highlights the significant challenge that language models face in achieving the necessary precision and analytical depth for proficient data narration, suggesting promising avenues for future model development and evaluation methodologies.""
}",yangDataTalesBenchmarkRealworld2024
"GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization","News summarization in today’s global scene can be daunting with its flood of multilingual content and varied viewpoints from different sources. However, current studies often neglect such real-world scenarios as they tend to focus solely on either single-language or single-document tasks. To bridge this gap, we aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization into a novel task, i.e., MCMS, which encapsulates the real-world requirements all-in-one. Nevertheless, the lack of a benchmark inhibits researchers from adequately studying this invaluable problem. To tackle this, we have meticulously constructed the GLOBESUMM dataset by first collecting a wealth of multilingual news reports and restructuring them into event-centric format. Additionally, we introduce the method of protocol-guided prompting for high-quality and cost-effective reference annotation. In MCMS, we also highlight the challenge of conflicts between news reports, in addition to the issues of redundancies and omissions, further enhancing the complexity of GLOBESUMM. Through extensive experimental analysis, we validate the quality of our dataset and elucidate the inherent challenges of the task. We firmly believe that GLOBESUMM, given its challenging nature, will greatly contribute to the multilingual communities and the evaluation of LLMs.",https://aclanthology.org/2024.emnlp-main.603,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{ye-etal-2024-globesumm,
    title = ""{G}lobe{S}umm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization"",
    author = ""Ye, Yangfan  and
      Feng, Xiachong  and
      Feng, Xiaocheng  and
      Ma, Weitao  and
      Qin, Libo  and
      Xu, Dongliang  and
      Yang, Qing  and
      Liu, Hongtao  and
      Qin, Bing"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.603/"",
    doi = ""10.18653/v1/2024.emnlp-main.603"",
    pages = ""10803--10821"",
    abstract = ""News summarization in today`s global scene can be daunting with its flood of multilingual content and varied viewpoints from different sources. However, current studies often neglect such real-world scenarios as they tend to focus solely on either single-language or single-document tasks. To bridge this gap, we aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization into a novel task, i.e., MCMS, which encapsulates the real-world requirements all-in-one. Nevertheless, the lack of a benchmark inhibits researchers from adequately studying this invaluable problem. To tackle this, we have meticulously constructed the GLOBESUMM dataset by first collecting a wealth of multilingual news reports and restructuring them into event-centric format. Additionally, we introduce the method of protocol-guided prompting for high-quality and cost-effective reference annotation. In MCMS, we also highlight the challenge of conflicts between news reports, in addition to the issues of redundancies and omissions, further enhancing the complexity of GLOBESUMM. Through extensive experimental analysis, we validate the quality of our dataset and elucidate the inherent challenges of the task. We firmly believe that GLOBESUMM, given its challenging nature, will greatly contribute to the multilingual communities and the evaluation of LLMs.""
}",yeGlobeSummChallengingBenchmark2024
Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering,"Users post numerous product-related questions on e-commerce platforms, affecting their purchase decisions. Product-related question answering (PQA) entails utilizing product-related resources to provide precise responses to users. We propose a novel task of Multilingual Cross-market Product-based Question Answering (MCPQA) and define the task as providing answers to product-related questions in a main marketplace by utilizing information from another resource-rich auxiliary marketplace in a multilingual context. We introduce a large-scale dataset comprising over 7 million questions from 17 marketplaces across 11 languages. We then perform automatic translation on the Electronics category of our dataset, naming it as McMarket. We focus on two subtasks: review-based answer generation and product-related question ranking. For each subtask, we label a subset of McMarket using an LLM and further evaluate the quality of the annotations via human assessment. We then conduct experiments to benchmark our dataset, using models ranging from traditional lexical models to LLMs in both single-market and cross-market scenarios across McMarket and the corresponding LLM subset. Results show that incorporating cross-market information significantly enhances performance in both tasks.",https://aclanthology.org/2024.emnlp-main.625,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{yuan-etal-2024-unlocking,
    title = ""Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering"",
    author = ""Yuan, Yifei  and
      Deng, Yang  and
      S{\o}gaard, Anders  and
      Aliannejadi, Mohammad"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.625/"",
    doi = ""10.18653/v1/2024.emnlp-main.625"",
    pages = ""11154--11169"",
    abstract = ""Users post numerous product-related questions on e-commerce platforms, affecting their purchase decisions. Product-related question answering (PQA) entails utilizing product-related resources to provide precise responses to users. We propose a novel task of Multilingual Cross-market Product-based Question Answering (MCPQA) and define the task as providing answers to product-related questions in a main marketplace by utilizing information from another resource-rich auxiliary marketplace in a multilingual context. We introduce a large-scale dataset comprising over 7 million questions from 17 marketplaces across 11 languages. We then perform automatic translation on the Electronics category of our dataset, naming it as McMarket. We focus on two subtasks: review-based answer generation and product-related question ranking. For each subtask, we label a subset of McMarket using an LLM and further evaluate the quality of the annotations via human assessment. We then conduct experiments to benchmark our dataset, using models ranging from traditional lexical models to LLMs in both single-market and cross-market scenarios across McMarket and the corresponding LLM subset. Results show that incorporating cross-market information significantly enhances performance in both tasks.""
}",yuanUnlockingMarketsMultilingual2024
ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models,"Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLM’s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.",https://aclanthology.org/2024.emnlp-main.637,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zhang-etal-2024-toolbehonest,
    title = ""{T}ool{B}e{H}onest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models"",
    author = ""Zhang, Yuxiang  and
      Chen, Jing  and
      Wang, Junjie  and
      Liu, Yaxin  and
      Yang, Cheng  and
      Shi, Chufan  and
      Zhu, Xinyu  and
      Lin, Zihao  and
      Wan, Hanwen  and
      Yang, Yujiu  and
      Sakai, Tetsuya  and
      Feng, Tian  and
      Yamana, Hayato"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.637/"",
    doi = ""10.18653/v1/2024.emnlp-main.637"",
    pages = ""11388--11422"",
    abstract = ""Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLM`s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.""
}",zhangToolBeHonestMultilevelHallucination2024
PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation,"Large language models (LLMs) have revolutionized NLP research. Notably, in-context learning enables their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and time-restricted applications. In this work, we introduce PrExMe, a large-scale Prompt Exploration for Metrics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations. This extensive comparison (1) benchmarks recent open-source LLMs as metrics and (2) explores the stability and variability of different prompting strategies. We discover that, on the one hand, there are scenarios for which prompts are stable. For instance, some LLMs show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores. On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes. For example, changing the requested output format from “0 to 100” to ""-1 to +1” can strongly affect the rankings in our evaluation. Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations.",https://aclanthology.org/2024.emnlp-main.641,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{leiter-eger-2024-prexme,
    title = ""{P}r{E}x{M}e! Large Scale Prompt Exploration of Open Source {LLM}s for Machine Translation and Summarization Evaluation"",
    author = ""Leiter, Christoph  and
      Eger, Steffen"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.641/"",
    doi = ""10.18653/v1/2024.emnlp-main.641"",
    pages = ""11481--11506"",
    abstract = ""Large language models (LLMs) have revolutionized NLP research. Notably, in-context learning enables their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and time-restricted applications. In this work, we introduce \textbf{PrExMe}, a large-scale \textbf{Pr}ompt \textbf{Ex}ploration for \textbf{Me}trics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations. This extensive comparison (1) benchmarks recent open-source LLMs as metrics and (2) explores the stability and variability of different prompting strategies. We discover that, on the one hand, there are scenarios for which prompts are stable. For instance, some LLMs show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores. On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes. For example, changing the requested output format from {\textquotedblleft}0 to 100{\textquotedblright} to ''-1 to +1{\textquotedblright} can strongly affect the rankings in our evaluation. Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations.""
}",leiterPrExMeLargeScale2024
Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models,"In dialogue, the addressee may initially misunderstand the speaker and respond erroneously, often prompting the speaker to correct the misunderstanding in the next turn with a Third Position Repair (TPR). The ability to process and respond appropriately to such repair sequences is thus crucial in conversational AI systems. In this paper, we first collect, analyse, and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences in an instruction-following manipulation task that is, by design, rife with referential ambiguity. We employ this dataset to evaluate several state-of-the-art Vision and Language Models (VLM) across multiple settings, focusing on their capability to process and accurately respond to TPRs and thus recover from miscommunication. We find that, compared to humans, all models significantly underperform in this task. We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generalising better to new scenarios. Our results suggest that these models are not yet ready to be deployed in multi-modal collaborative settings where repairs are common, and highlight the need to design training regimes and objectives that facilitate learning from interaction. Our code and data are available at www.github.com/JChiyah/blockworld-repairs",https://aclanthology.org/2024.emnlp-main.643,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{chiyah-garcia-etal-2024-repairs,
    title = ""Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models"",
    author = ""Chiyah-Garcia, Javier  and
      Suglia, Alessandro  and
      Eshghi, Arash"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.643/"",
    doi = ""10.18653/v1/2024.emnlp-main.643"",
    pages = ""11523--11542"",
    abstract = ""In dialogue, the addressee may initially misunderstand the speaker and respond erroneously, often prompting the speaker to correct the misunderstanding in the next turn with a Third Position Repair (TPR). The ability to process and respond appropriately to such repair sequences is thus crucial in conversational AI systems. In this paper, we first collect, analyse, and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences in an instruction-following manipulation task that is, by design, rife with referential ambiguity. We employ this dataset to evaluate several state-of-the-art Vision and Language Models (VLM) across multiple settings, focusing on their capability to process and accurately respond to TPRs and thus recover from miscommunication. We find that, compared to humans, all models significantly underperform in this task. We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generalising better to new scenarios. Our results suggest that these models are not yet ready to be deployed in multi-modal collaborative settings where repairs are common, and highlight the need to design training regimes and objectives that facilitate learning from interaction. Our code and data are available at www.github.com/JChiyah/blockworld-repairs""
}",chiyah-garciaRepairsBlockWorld2024
SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading,"With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains. One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs. Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs’ ability on solving scientific tasks. SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams. We evaluate the performance of various state-of-the-art LLMs on our new benchmark. Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance. Therefore, we provide human expert grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4% exam grade on average. We also provide detailed comparisons between LLM performance and student performance on SciEx. To enable future evaluation of new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx. Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.",https://aclanthology.org/2024.emnlp-main.647,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{dinh-etal-2024-sciex,
    title = ""{S}ci{E}x: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading"",
    author = {Dinh, Tu Anh  and
      Mullov, Carlos  and
      B{\""a}rmann, Leonard  and
      Li, Zhaolin  and
      Liu, Danni  and
      Rei{\ss}, Simon  and
      Lee, Jueun  and
      Lerzer, Nathan  and
      Gao, Jianfeng  and
      Peller-Konrad, Fabian  and
      R{\""o}ddiger, Tobias  and
      Waibel, Alexander  and
      Asfour, Tamim  and
      Beigl, Michael  and
      Stiefelhagen, Rainer  and
      Dachsbacher, Carsten  and
      B{\""o}hm, Klemens  and
      Niehues, Jan},
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.647/"",
    doi = ""10.18653/v1/2024.emnlp-main.647"",
    pages = ""11592--11610"",
    abstract = ""With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains. One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs. Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs' ability on solving scientific tasks. SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams. We evaluate the performance of various state-of-the-art LLMs on our new benchmark. Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance. Therefore, we provide human expert grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4{\%} exam grade on average. We also provide detailed comparisons between LLM performance and student performance on SciEx. To enable future evaluation of new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx. Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.""
}",dinhSciExBenchmarkingLarge2024a
What Are the Odds? Language Models Are Capable of Probabilistic Reasoning,"Language models (LM) are capable of remarkably complex linguistic tasks; however, numerical reasoning is an area in which they frequently struggle. An important but rarely evaluated form of reasoning is understanding probability distributions. In this paper, we focus on evaluating the probabilistic reasoning capabilities of LMs using idealized and real-world statistical distributions. We perform a systematic evaluation of state-of-the-art LMs on three tasks: estimating percentiles, drawing samples, and calculating probabilities. We evaluate three ways to provide context to LMs 1) anchoring examples from within a distribution or family of distributions, 2) real-world context, 3) summary statistics on which to base a Normal approximation. Models can make inferences about distributions, and can be further aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified. To conduct this work, we developed a comprehensive benchmark distribution dataset with associated question-answer pairs that we have released publicly.",https://aclanthology.org/2024.emnlp-main.654,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{paruchuri-etal-2024-odds,
    title = ""What Are the Odds? Language Models Are Capable of Probabilistic Reasoning"",
    author = ""Paruchuri, Akshay  and
      Garrison, Jake  and
      Liao, Shun  and
      Hernandez, John B  and
      Sunshine, Jacob  and
      Althoff, Tim  and
      Liu, Xin  and
      McDuff, Daniel"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.654/"",
    doi = ""10.18653/v1/2024.emnlp-main.654"",
    pages = ""11712--11733"",
    abstract = ""Language models (LM) are capable of remarkably complex linguistic tasks; however, numerical reasoning is an area in which they frequently struggle. An important but rarely evaluated form of reasoning is understanding probability distributions. In this paper, we focus on evaluating the probabilistic reasoning capabilities of LMs using idealized and real-world statistical distributions. We perform a systematic evaluation of state-of-the-art LMs on three tasks: estimating percentiles, drawing samples, and calculating probabilities. We evaluate three ways to provide context to LMs 1) anchoring examples from within a distribution or family of distributions, 2) real-world context, 3) summary statistics on which to base a Normal approximation. Models can make inferences about distributions, and can be further aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified. To conduct this work, we developed a comprehensive benchmark distribution dataset with associated question-answer pairs that we have released publicly.""
}",paruchuriWhatAreOdds2024
ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment,"We present a comprehensive evaluation of large language models for multilingual readability assessment. Existing evaluation resources lack domain and language diversity, limiting the ability for cross-domain and cross-lingual analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset with human annotations of 9757 sentences in Arabic, English, French, Hindi, and Russian, collected from 112 different data sources. This benchmark will encourage research on developing robust multilingual readability assessment methods. Using ReadMe++, we benchmark multilingual and monolingual language models in the supervised, unsupervised, and few-shot prompting settings. The domain and language diversity in ReadMe++ enable us to test more effective few-shot prompting, and identify shortcomings in state-of-the-art unsupervised methods. Our experiments also reveal exciting results of superior domain generalization and enhanced cross-lingual transfer capabilities by models trained on ReadMe++. We will make our data publicly available and release a python package tool for multilingual sentence readability prediction using our trained models at: https://github.com/tareknaous/readme",https://aclanthology.org/2024.emnlp-main.682,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{naous-etal-2024-readme,
    title = ""{R}ead{M}e++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment"",
    author = ""Naous, Tarek  and
      Ryan, Michael J  and
      Lavrouk, Anton  and
      Chandra, Mohit  and
      Xu, Wei"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.682/"",
    doi = ""10.18653/v1/2024.emnlp-main.682"",
    pages = ""12230--12266"",
    abstract = ""We present a comprehensive evaluation of large language models for multilingual readability assessment. Existing evaluation resources lack domain and language diversity, limiting the ability for cross-domain and cross-lingual analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset with human annotations of 9757 sentences in Arabic, English, French, Hindi, and Russian, collected from 112 different data sources. This benchmark will encourage research on developing robust multilingual readability assessment methods. Using ReadMe++, we benchmark multilingual and monolingual language models in the supervised, unsupervised, and few-shot prompting settings. The domain and language diversity in ReadMe++ enable us to test more effective few-shot prompting, and identify shortcomings in state-of-the-art unsupervised methods. Our experiments also reveal exciting results of superior domain generalization and enhanced cross-lingual transfer capabilities by models trained on ReadMe++. We will make our data publicly available and release a python package tool for multilingual sentence readability prediction using our trained models at: https://github.com/tareknaous/readme""
}",naousReadMeBenchmarkingMultilingual2024
SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories,"Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPER aims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub-problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.",https://aclanthology.org/2024.emnlp-main.702,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{bogin-etal-2024-super,
    title = ""{SUPER}: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories"",
    author = ""Bogin, Ben  and
      Yang, Kejuan  and
      Gupta, Shashank  and
      Richardson, Kyle  and
      Bransom, Erin  and
      Clark, Peter  and
      Sabharwal, Ashish  and
      Khot, Tushar"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.702/"",
    doi = ""10.18653/v1/2024.emnlp-main.702"",
    pages = ""12622--12645"",
    abstract = ""Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPER aims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub-problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3{\%} of the end-to-end set, and 46.1{\%} of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.""
}",boginSUPEREvaluatingAgents2024
AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies,"Humans regularly engage in analogical thinking, relating personal experiences to current situations (X is analogous to Y because of Z). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose AnaloBench, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We collect a set of 340 high quality, human written analogies for use in our benchmark, which constitutes the largest such collection to date. We then test a broad collection of models consisting of 12 open source and 3 proprietary in various sizes and architectures. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.",https://aclanthology.org/2024.emnlp-main.725,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{ye-etal-2024-analobench,
    title = ""{A}nalo{B}ench: Benchmarking the Identification of Abstract and Long-context Analogies"",
    author = ""Ye, Xiao  and
      Wang, Andrew  and
      Choi, Jacob  and
      Lu, Yining  and
      Sharma, Shreya  and
      Shen, Lingfeng  and
      Tiyyala, Vijay Murari  and
      Andrews, Nicholas  and
      Khashabi, Daniel"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.725/"",
    doi = ""10.18653/v1/2024.emnlp-main.725"",
    pages = ""13060--13082"",
    abstract = ""Humans regularly engage in analogical thinking, relating personal experiences to current situations (X is analogous to Y because of Z). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose AnaloBench, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We collect a set of 340 high quality, human written analogies for use in our benchmark, which constitutes the largest such collection to date. We then test a broad collection of models consisting of 12 open source and 3 proprietary in various sizes and architectures. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.""
}",yeAnaloBenchBenchmarkingIdentification2024
DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models,"We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, including Python and SQL, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously designed the evaluation suite to ensure the accuracy and robustness of the evaluation. We developed the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at [link](https://github.com/yiyihum/dabench)",https://aclanthology.org/2024.emnlp-main.748,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{huang-etal-2024-da,
    title = ""{DA}-Code: Agent Data Science Code Generation Benchmark for Large Language Models"",
    author = ""Huang, Yiming  and
      Luo, Jianwen  and
      Yu, Yan  and
      Zhang, Yitong  and
      Lei, Fangyu  and
      Wei, Yifan  and
      He, Shizhu  and
      Huang, Lifu  and
      Liu, Xiao  and
      Zhao, Jun  and
      Liu, Kang"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.748/"",
    doi = ""10.18653/v1/2024.emnlp-main.748"",
    pages = ""13487--13521"",
    abstract = ""We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, including Python and SQL, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously designed the evaluation suite to ensure the accuracy and robustness of the evaluation. We developed the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5{\%} accuracy, leaving ample room for improvement. We release our benchmark at [link](https://github.com/yiyihum/dabench)""
}",huangDAcodeAgentData2024
Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark,"The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs",https://aclanthology.org/2024.emnlp-main.759,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{liu-etal-2024-large,
    title = ""Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark"",
    author = ""Liu, Fenglin  and
      Li, Zheng  and
      Zhou, Hongjian  and
      Yin, Qingyu  and
      Yang, Jingfeng  and
      Tang, Xianfeng  and
      Luo, Chen  and
      Zeng, Ming  and
      Jiang, Haoming  and
      Gao, Yifan  and
      Nigam, Priyanka  and
      Nag, Sreyashi  and
      Yin, Bing  and
      Hua, Yining  and
      Zhou, Xuan  and
      Rohanian, Omid  and
      Thakur, Anshul  and
      Clifton, Lei  and
      Clifton, David A."",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.759/"",
    doi = ""10.18653/v1/2024.emnlp-main.759"",
    pages = ""13696--13710"",
    abstract = ""The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs""
}",liuLargeLanguageModels2024a
Social Bias Probing: Fairness Benchmarking for Language Models,"While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.",https://aclanthology.org/2024.emnlp-main.812,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{marchiori-manerba-etal-2024-social,
    title = ""Social Bias Probing: Fairness Benchmarking for Language Models"",
    author = ""Marchiori Manerba, Marta  and
      Stanczak, Karolina  and
      Guidotti, Riccardo  and
      Augenstein, Isabelle"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.812/"",
    doi = ""10.18653/v1/2024.emnlp-main.812"",
    pages = ""14653--14671"",
    abstract = ""While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.""
}",marchiorimanerbaSocialBiasProbing2024
FinDVer: Explainable Claim Verification over Long and Hybrid-content Financial Documents,"We introduce FinDVer, a comprehensive benchmark specifically designed to evaluate the explainable claim verification capabilities of LLMs in the context of understanding and analyzing long, hybrid-content financial documents. FinDVer contains 4,000 expert-annotated examples across four subsets, each focusing on a type of scenario that frequently arises in real-world financial domains. We assess a broad spectrum of 25 LLMs under long-context and RAG settings. Our results show that even the current best-performing system (i.e., GPT-4o) significantly lags behind human experts. Our detailed findings and insights highlight the strengths and limitations of existing LLMs in this new task. We believe FinDVer can serve as a valuable benchmark for evaluating LLM capabilities in claim verification over complex, expert-domain documents.",https://aclanthology.org/2024.emnlp-main.818,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{zhao-etal-2024-findver,
    title = ""{F}in{DV}er: Explainable Claim Verification over Long and Hybrid-content Financial Documents"",
    author = ""Zhao, Yilun  and
      Long, Yitao  and
      Jiang, Tintin  and
      Wang, Chengye  and
      Chen, Weiyuan  and
      Liu, Hongjun  and
      Tang, Xiangru  and
      Zhang, Yiming  and
      Zhao, Chen  and
      Cohan, Arman"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.818/"",
    doi = ""10.18653/v1/2024.emnlp-main.818"",
    pages = ""14739--14752"",
    abstract = ""We introduce FinDVer, a comprehensive benchmark specifically designed to evaluate the explainable claim verification capabilities of LLMs in the context of understanding and analyzing long, hybrid-content financial documents. FinDVer contains 4,000 expert-annotated examples across four subsets, each focusing on a type of scenario that frequently arises in real-world financial domains. We assess a broad spectrum of 25 LLMs under long-context and RAG settings. Our results show that even the current best-performing system (i.e., GPT-4o) significantly lags behind human experts. Our detailed findings and insights highlight the strengths and limitations of existing LLMs in this new task. We believe FinDVer can serve as a valuable benchmark for evaluating LLM capabilities in claim verification over complex, expert-domain documents.""
}",zhaoFinDVerExplainableClaim2024a
ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities,"Large language models(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model’s reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.",https://aclanthology.org/2024.emnlp-main.833,2024,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{su-etal-2024-actplan,
    title = ""{A}ct{P}lan-1{K}: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities"",
    author = ""Su, Ying  and
      Ling, Zhan  and
      Shi, Haochen  and
      Jiayang, Cheng  and
      Yim, Yauwai  and
      Song, Yangqiu"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.833/"",
    doi = ""10.18653/v1/2024.emnlp-main.833"",
    pages = ""14953--14965"",
    abstract = ""Large language models(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model`s reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.""
}",suActPlan1KBenchmarkingProcedural2024
LitSearch: A Retrieval Benchmark for Scientific Literature Search,"Literature search questions, such as “where can I find research on the evaluation of consistency in generated summaries?” pose significant challenges for modern search engines and retrieval systems. These questions often require a deep understanding of research concepts and the ability to reason over entire articles. In this work, we introduce LitSearch, a retrieval benchmark comprising 597 realistic literature search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions generated by GPT-4 based on paragraphs containing inline citations from research papers and (2) questions about recently published papers, manually written by their authors. All LitSearch questions were manually examined or edited by experts to ensure high quality. We extensively benchmark state-of-the-art retrieval models and also evaluate two LLM-based reranking pipelines. We find a significant performance gap between BM25 and state-of-the-art dense retrievers, with a 24.8% difference in absolute recall@5. The LLM-based reranking strategies further improve the best-performing dense retriever by 4.4%. Additionally, commercial search engines and research tools like Google Search perform poorly on LitSearch, lagging behind the best dense retriever by 32 points. Taken together, these results show that LitSearch is an informative new testbed for retrieval systems while catering to a real-world use case.",https://aclanthology.org/2024.emnlp-main.840,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{ajith-etal-2024-litsearch,
    title = ""{L}it{S}earch: A Retrieval Benchmark for Scientific Literature Search"",
    author = ""Ajith, Anirudh  and
      Xia, Mengzhou  and
      Chevalier, Alexis  and
      Goyal, Tanya  and
      Chen, Danqi  and
      Gao, Tianyu"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.840/"",
    doi = ""10.18653/v1/2024.emnlp-main.840"",
    pages = ""15068--15083"",
    abstract = ""Literature search questions, such as {\textquotedblleft}where can I find research on the evaluation of consistency in generated summaries?{\textquotedblright} pose significant challenges for modern search engines and retrieval systems. These questions often require a deep understanding of research concepts and the ability to reason over entire articles. In this work, we introduce LitSearch, a retrieval benchmark comprising 597 realistic literature search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions generated by GPT-4 based on paragraphs containing inline citations from research papers and (2) questions about recently published papers, manually written by their authors. All LitSearch questions were manually examined or edited by experts to ensure high quality. We extensively benchmark state-of-the-art retrieval models and also evaluate two LLM-based reranking pipelines. We find a significant performance gap between BM25 and state-of-the-art dense retrievers, with a 24.8{\%} difference in absolute recall@5. The LLM-based reranking strategies further improve the best-performing dense retriever by 4.4{\%}. Additionally, commercial search engines and research tools like Google Search perform poorly on LitSearch, lagging behind the best dense retriever by 32 points. Taken together, these results show that LitSearch is an informative new testbed for retrieval systems while catering to a real-world use case.""
}",ajithLitSearchRetrievalBenchmark2024
AKEW: Assessing Knowledge Editing in the Wild,"Knowledge editing injects knowledge updates into language models to keep them correct and up-to-date. However, its current evaluations deviate significantly from practice: their knowledge updates solely consist of structured facts derived from meticulously crafted datasets, instead of practical sources—unstructured texts like news articles, and they often overlook practical real-world knowledge updates. To address these issues, in this paper we propose AKEW (Assessing Knowledge Editing in the Wild), a new practical benchmark for knowledge editing. AKEW fully covers three editing settings of knowledge updates: structured facts, unstructured texts as facts, and extracted triplets. It further introduces new datasets featuring both counterfactual and real-world knowledge updates. Through extensive experiments, we demonstrate the considerable gap between state-of-the-art knowledge-editing methods and practical scenarios. Our analyses further highlight key insights to motivate future research for practical knowledge editing.",https://aclanthology.org/2024.emnlp-main.843,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{wu-etal-2024-akew,
    title = ""{AKEW}: Assessing Knowledge Editing in the Wild"",
    author = ""Wu, Xiaobao  and
      Pan, Liangming  and
      Wang, William Yang  and
      Luu, Anh Tuan"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.843/"",
    doi = ""10.18653/v1/2024.emnlp-main.843"",
    pages = ""15118--15133"",
    abstract = ""Knowledge editing injects knowledge updates into language models to keep them correct and up-to-date. However, its current evaluations deviate significantly from practice: their knowledge updates solely consist of structured facts derived from meticulously crafted datasets, instead of practical sources{---}unstructured texts like news articles, and they often overlook practical real-world knowledge updates. To address these issues, in this paper we propose AKEW (Assessing Knowledge Editing in the Wild), a new practical benchmark for knowledge editing. AKEW fully covers three editing settings of knowledge updates: structured facts, unstructured texts as facts, and extracted triplets. It further introduces new datasets featuring both counterfactual and real-world knowledge updates. Through extensive experiments, we demonstrate the considerable gap between state-of-the-art knowledge-editing methods and practical scenarios. Our analyses further highlight key insights to motivate future research for practical knowledge editing.""
}",wuAKEWAssessingKnowledge2024
CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation,"Evaluating the degree of reproduction of copyright-protected content by language models (LMs) is of significant interest to the AI and legal communities. Although both literal and non-literal similarities are considered by courts when assessing the degree of reproduction, prior research has focused only on literal similarities. To bridge this gap, we introduce CopyBench, a benchmark designed to measure both literal and non-literal copying in LM generations. Using copyrighted fiction books as text sources, we provide automatic evaluation protocols to assess literal and non-literal copying, balanced against the model utility in terms of the ability to recall facts from the copyrighted works and generate fluent completions. We find that, although literal copying is relatively rare, two types of non-literal copying—event copying and character copying—occur even in models as small as 7B parameters. Larger models demonstrate significantly more copying, with literal copying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3% to 5.9% when comparing Llama3-8B and 70B models, respectively. We further evaluate the effectiveness of current strategies for mitigating copying and show that (1) training-time alignment can reduce literal copying but may increase non-literal copying, and (2) current inference-time mitigation methods primarily reduce literal but not non-literal copying.",https://aclanthology.org/2024.emnlp-main.844,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{chen-etal-2024-copybench,
    title = ""{C}opy{B}ench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation"",
    author = ""Chen, Tong  and
      Asai, Akari  and
      Mireshghallah, Niloofar  and
      Min, Sewon  and
      Grimmelmann, James  and
      Choi, Yejin  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke  and
      Koh, Pang Wei"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.844/"",
    doi = ""10.18653/v1/2024.emnlp-main.844"",
    pages = ""15134--15158"",
    abstract = ""Evaluating the degree of reproduction of copyright-protected content by language models (LMs) is of significant interest to the AI and legal communities. Although both literal and non-literal similarities are considered by courts when assessing the degree of reproduction, prior research has focused only on literal similarities. To bridge this gap, we introduce CopyBench, a benchmark designed to measure both literal and non-literal copying in LM generations. Using copyrighted fiction books as text sources, we provide automatic evaluation protocols to assess literal and non-literal copying, balanced against the model utility in terms of the ability to recall facts from the copyrighted works and generate fluent completions. We find that, although literal copying is relatively rare, two types of non-literal copying{---}event copying and character copying{---}occur even in models as small as 7B parameters. Larger models demonstrate significantly more copying, with literal copying rates increasing from 0.2{\%} to 10.5{\%} and non-literal copying from 2.3{\%} to 5.9{\%} when comparing Llama3-8B and 70B models, respectively. We further evaluate the effectiveness of current strategies for mitigating copying and show that (1) training-time alignment can reduce literal copying but may increase non-literal copying, and (2) current inference-time mitigation methods primarily reduce literal but not non-literal copying.""
}",chenCopyBenchMeasuringLiteral2024
AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction,"Large Language Models (LLMs) can interact with the real world by connecting with versatile external APIs, resulting in better problem-solving and task automation capabilities. Previous research primarily either focuses on APIs with limited arguments from a single source or overlooks the complex dependency relationship between different APIs. However, it is essential to utilize multiple APIs collaboratively from various sources, especially for complex user instructions. In this paper, we introduce MetaBench, the first benchmark to evaluate LLMs’ ability to plan and execute multiple APIs from various sources in order to complete the user’s task. Specifically, we consider two significant challenges in multiple APIs: 1) graph structures: some APIs can be executed independently while others need to be executed one by one, resulting in graph-like execution order; and 2) permission constraints: which source is authorized to execute the API call. We have experimental results on 9 distinct LLMs; e.g., GPT-4o achieves only a 2.0% success rate at the most complex instruction, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning and finetuning. Our code and data are publicly available at https://github.com/ruleGreen/AppBench.",https://aclanthology.org/2024.emnlp-main.856,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-appbench,
    title = ""{A}pp{B}ench: Planning of Multiple {API}s from Various {APP}s for Complex User Instruction"",
    author = ""Wang, Hongru  and
      Wang, Rui  and
      Xue, Boyang  and
      Xia, Heming  and
      Cao, Jingtao  and
      Liu, Zeming  and
      Pan, Jeff Z.  and
      Wong, Kam-Fai"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.856/"",
    doi = ""10.18653/v1/2024.emnlp-main.856"",
    pages = ""15322--15336"",
    abstract = ""Large Language Models (LLMs) can interact with the real world by connecting with versatile external APIs, resulting in better problem-solving and task automation capabilities. Previous research primarily either focuses on APIs with limited arguments from a single source or overlooks the complex dependency relationship between different APIs. However, it is essential to utilize multiple APIs collaboratively from various sources, especially for complex user instructions. In this paper, we introduce MetaBench, the first benchmark to evaluate LLMs' ability to plan and execute multiple APIs from various sources in order to complete the user`s task. Specifically, we consider two significant challenges in multiple APIs: 1) graph structures: some APIs can be executed independently while others need to be executed one by one, resulting in graph-like execution order; and 2) permission constraints: which source is authorized to execute the API call. We have experimental results on 9 distinct LLMs; e.g., GPT-4o achieves only a 2.0{\%} success rate at the most complex instruction, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning and finetuning. Our code and data are publicly available at \url{https://github.com/ruleGreen/AppBench}.""
}",wangAppBenchPlanningMultiple2024
ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?,"Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code.",https://aclanthology.org/2024.emnlp-main.859,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{waghjale-etal-2024-ecco,
    title = ""{ECCO}: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?"",
    author = ""Waghjale, Siddhant  and
      Veerendranath, Vishruth  and
      Wang, Zhiruo  and
      Fried, Daniel"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.859/"",
    doi = ""10.18653/v1/2024.emnlp-main.859"",
    pages = ""15362--15376"",
    abstract = ""Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code.""
}",waghjaleECCOCanWe2024
Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators,"Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science communicators. Unlike existing benchmarks, our approach emphasizes assessing these models on scientific question-answering tasks that require a nuanced understanding and awareness of answerability. We introduce a novel dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria. We benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families. While most open-access models significantly underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects. We also find that even the GPT models exhibit a general incompetence in reliably verifying LLM responses. Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.",https://aclanthology.org/2024.emnlp-main.889,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{bajpai-etal-2024-llms,
    title = ""Can {LLM}s replace Neil de{G}rasse Tyson? Evaluating the Reliability of {LLM}s as Science Communicators"",
    author = ""Bajpai, Prasoon  and
      Chatterjee, Niladri  and
      Dutta, Subhabrata  and
      Chakraborty, Tanmoy"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.889/"",
    doi = ""10.18653/v1/2024.emnlp-main.889"",
    pages = ""15895--15912"",
    abstract = ""Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science communicators. Unlike existing benchmarks, our approach emphasizes assessing these models on scientific question-answering tasks that require a nuanced understanding and awareness of answerability. We introduce a novel dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria. We benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families. While most open-access models significantly underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects. We also find that even the GPT models exhibit a general incompetence in reliably verifying LLM responses. Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.""
}",bajpaiCanLLMsReplace2024
The Instinctive Bias: Spurious Images lead to Illusion in MLLMs,"Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from visual illusion. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the visual illusion level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs’ robustness in the presence of misleading images. The code and datasets are available at https://github.com/MasaiahHan/CorrelationQA.",https://aclanthology.org/2024.emnlp-main.904,2024,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{han-etal-2024-instinctive,
    title = ""The Instinctive Bias: Spurious Images lead to Illusion in {MLLM}s"",
    author = ""Han, Tianyang  and
      Lian, Qing  and
      Pan, Rui  and
      Pi, Renjie  and
      Zhang, Jipeng  and
      Diao, Shizhe  and
      Lin, Yong  and
      Zhang, Tong"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.904/"",
    doi = ""10.18653/v1/2024.emnlp-main.904"",
    pages = ""16163--16177"",
    abstract = ""Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from visual illusion. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the visual illusion level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs' robustness in the presence of misleading images. The code and datasets are available at https://github.com/MasaiahHan/CorrelationQA.""
}",hanInstinctiveBiasSpurious2024
Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?,"Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques. Our code and data are available: https://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.",https://aclanthology.org/2024.emnlp-main.923,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{he-etal-2024-trust,
    title = ""Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?"",
    author = ""He, Jianfeng  and
      Yang, Runing  and
      Yu, Linlin  and
      Li, Changbin  and
      Jia, Ruoxi  and
      Chen, Feng  and
      Jin, Ming  and
      Lu, Chang-Tien"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.923/"",
    doi = ""10.18653/v1/2024.emnlp-main.923"",
    pages = ""16514--16575"",
    abstract = ""Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques. Our code and data are available: https://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.""
}",heCanWeTrust2024a
Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis,"In this study, we introduce ANGST, a novel, first of its kind benchmark for depression-anxiety comorbidity classification from social media posts. Unlike contemporary datasets that often oversimplify the intricate interplay between different mental health disorders by treating them as isolated conditions, ANGST enables multi-label classification, allowing each post to be simultaneously identified as indicating depression and/or anxiety. Comprising 2876 meticulously annotated posts by expert psychologists and an additional 7667 silver-labeled posts, ANGST posits a more representative sample of online mental health discourse. Moreover, we benchmark ANGST using various state-of-the-art language models, ranging from Mental-BERT to GPT-4. Our results provide significant insights into the capabilities and limitations of these models in complex diagnostic scenarios. While GPT-4 generally outperforms other models, none achieve an F1 score exceeding 72% in multi-class comorbid classification, underscoring the ongoing challenges in applying language models to mental health diagnostics.",https://aclanthology.org/2024.emnlp-main.931,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{hengle-etal-2024-still,
    title = ""Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis"",
    author = ""Hengle, Amey  and
      Kulkarni, Atharva  and
      Patankar, Shantanu Deepak  and
      Chandrasekaran, Madhumitha  and
      D{'}silva, Sneha  and
      Jacob, Jemima S.  and
      Gupta, Rashmi"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.931/"",
    doi = ""10.18653/v1/2024.emnlp-main.931"",
    pages = ""16698--16721"",
    abstract = ""In this study, we introduce ANGST, a novel, first of its kind benchmark for depression-anxiety comorbidity classification from social media posts. Unlike contemporary datasets that often oversimplify the intricate interplay between different mental health disorders by treating them as isolated conditions, ANGST enables multi-label classification, allowing each post to be simultaneously identified as indicating depression and/or anxiety. Comprising 2876 meticulously annotated posts by expert psychologists and an additional 7667 silver-labeled posts, ANGST posits a more representative sample of online mental health discourse. Moreover, we benchmark ANGST using various state-of-the-art language models, ranging from Mental-BERT to GPT-4. Our results provide significant insights into the capabilities and limitations of these models in complex diagnostic scenarios. While GPT-4 generally outperforms other models, none achieve an F1 score exceeding 72{\%} in multi-class comorbid classification, underscoring the ongoing challenges in applying language models to mental health diagnostics.""
}",hengleStillNotQuite2024
LLM-Evolve: Evaluation for LLM’s Evolving Capability on Benchmarks,"The advancement of large language models (LLMs) has extended their use to dynamic and interactive real-world applications, where models engage continuously with their environment and potentially enhance their performance over time. Most existing LLM benchmarks evaluate LLMs on i.i.d. tasks, overlooking their ability to learn iteratively from past experiences. Our paper bridges this evaluation gap by proposing a novel framework, LLM-Evolve, which extends established benchmarks to sequential problem-solving settings. LLM-Evolve evaluates LLMs over multiple rounds, providing feedback after each round to build a demonstration memory that the models can query in future tasks. We applied LLM-Evolve to the MMLU, GSM8K, and AgentBench benchmarks, testing 8 state-of-the-art open-source and closed-source models. Results show that LLMs can achieve performance improvements of up to 17% by learning from past interactions, with the quality of retrieval algorithms and feedback significantly influencing this capability. These insights advocate for more understanding and benchmarks for LLMs’ performance in evolving interactive scenarios.",https://aclanthology.org/2024.emnlp-main.940,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{you-etal-2024-llm,
    title = ""{LLM}-Evolve: Evaluation for {LLM}`s Evolving Capability on Benchmarks"",
    author = ""You, Jiaxuan  and
      Liu, Mingjie  and
      Prabhumoye, Shrimai  and
      Patwary, Mostofa  and
      Shoeybi, Mohammad  and
      Catanzaro, Bryan"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.940/"",
    doi = ""10.18653/v1/2024.emnlp-main.940"",
    pages = ""16937--16942"",
    abstract = ""The advancement of large language models (LLMs) has extended their use to dynamic and interactive real-world applications, where models engage continuously with their environment and potentially enhance their performance over time. Most existing LLM benchmarks evaluate LLMs on i.i.d. tasks, overlooking their ability to learn iteratively from past experiences. Our paper bridges this evaluation gap by proposing a novel framework, LLM-Evolve, which extends established benchmarks to sequential problem-solving settings. LLM-Evolve evaluates LLMs over multiple rounds, providing feedback after each round to build a demonstration memory that the models can query in future tasks. We applied LLM-Evolve to the MMLU, GSM8K, and AgentBench benchmarks, testing 8 state-of-the-art open-source and closed-source models. Results show that LLMs can achieve performance improvements of up to 17{\%} by learning from past interactions, with the quality of retrieval algorithms and feedback significantly influencing this capability. These insights advocate for more understanding and benchmarks for LLMs' performance in evolving interactive scenarios.""
}",youLLMevolveEvaluationLLM`s2024
Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models,"We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 3rd graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks. The implementation of Mathador-LM benchmark is available at https://github.com/IST-DASLab/Mathador-LM.",https://aclanthology.org/2024.emnlp-main.946,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{kurtic-etal-2024-mathador,
    title = ""Mathador-{LM}: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models"",
    author = ""Kurtic, Eldar  and
      Moeini, Amir  and
      Alistarh, Dan"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.946/"",
    doi = ""10.18653/v1/2024.emnlp-main.946"",
    pages = ""17020--17027"",
    abstract = ""We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 3rd graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks. The implementation of Mathador-LM benchmark is available at https://github.com/IST-DASLab/Mathador-LM.""
}",kurticMathadorLMDynamicBenchmark2024
One Thousand and One Pairs: A “novel” challenge for long-context language models,"Synthetic long-context LLM benchmarks (e.g., “needle-in-the-haystack”) test only surface-level retrieval capabilities; but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest pair accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.",https://aclanthology.org/2024.emnlp-main.948,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{karpinska-etal-2024-one,
    title = ""One Thousand and One Pairs: A {\textquotedblleft}novel{\textquotedblright} challenge for long-context language models"",
    author = ""Karpinska, Marzena  and
      Thai, Katherine  and
      Lo, Kyle  and
      Goyal, Tanya  and
      Iyyer, Mohit"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.948/"",
    doi = ""10.18653/v1/2024.emnlp-main.948"",
    pages = ""17048--17085"",
    abstract = ""Synthetic long-context LLM benchmarks (e.g., {\textquotedblleft}needle-in-the-haystack{\textquotedblright}) test only surface-level retrieval capabilities; but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest pair accuracy at 55.8{\%}. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.""
}",karpinskaOneThousandOne2024a
NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition,"Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model quality. To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels. These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted. However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation. To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors. We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their achievable upper bound. We release NoiseBench for both English and German to the research community.",https://aclanthology.org/2024.emnlp-main.1011,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{merdjanovska-etal-2024-noisebench,
    title = ""{N}oise{B}ench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition"",
    author = ""Merdjanovska, Elena  and
      Aynetdinov, Ansar  and
      Akbik, Alan"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1011/"",
    doi = ""10.18653/v1/2024.emnlp-main.1011"",
    pages = ""18182--18198"",
    abstract = ""Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model quality. To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels. These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted. However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation. To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors. We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their achievable upper bound. We release NoiseBench for both English and German to the research community.""
}",merdjanovskaNoiseBenchBenchmarkingImpact2024
GuardBench: A Large-Scale Benchmark for Guardrail Models,"Generative AI systems powered by Large Language Models have become increasingly popular in recent years. Lately, due to the risk of providing users with unsafe information, the adoption of those systems in safety-critical domains has raised significant concerns. To respond to this situation, input-output filters, commonly called guardrail models, have been proposed to complement other measures, such as model alignment. Unfortunately, the lack of a standard benchmark for guardrail models poses significant evaluation issues and makes it hard to compare results across scientific publications. To fill this gap, we introduce GuardBench, a large-scale benchmark for guardrail models comprising 40 safety evaluation datasets. To facilitate the adoption of GuardBench, we release a Python library providing an automated evaluation pipeline built on top of it. With our benchmark, we also share the first large-scale prompt moderation datasets in German, French, Italian, and Spanish. To assess the current state-of-the-art, we conduct an extensive comparison of recent guardrail models and show that a general-purpose instruction-following model of comparable size achieves competitive results without the need for specific fine-tuning.",https://aclanthology.org/2024.emnlp-main.1022,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{bassani-sanchez-2024-guardbench,
    title = ""{G}uard{B}ench: A Large-Scale Benchmark for Guardrail Models"",
    author = ""Bassani, Elias  and
      Sanchez, Ignacio"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1022/"",
    doi = ""10.18653/v1/2024.emnlp-main.1022"",
    pages = ""18393--18409"",
    abstract = ""Generative AI systems powered by Large Language Models have become increasingly popular in recent years. Lately, due to the risk of providing users with unsafe information, the adoption of those systems in safety-critical domains has raised significant concerns. To respond to this situation, input-output filters, commonly called guardrail models, have been proposed to complement other measures, such as model alignment. Unfortunately, the lack of a standard benchmark for guardrail models poses significant evaluation issues and makes it hard to compare results across scientific publications. To fill this gap, we introduce GuardBench, a large-scale benchmark for guardrail models comprising 40 safety evaluation datasets. To facilitate the adoption of GuardBench, we release a Python library providing an automated evaluation pipeline built on top of it. With our benchmark, we also share the first large-scale prompt moderation datasets in German, French, Italian, and Spanish. To assess the current state-of-the-art, we conduct an extensive comparison of recent guardrail models and show that a general-purpose instruction-following model of comparable size achieves competitive results without the need for specific fine-tuning.""
}",bassaniGuardBenchLargescaleBenchmark2024
Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model,"Although most current large multimodal models (LMMs) can already understand photos of natural scenes and portraits, their understanding of abstract images, e.g., charts, maps, or layouts, and visual reasoning capabilities remains quite rudimentary. They often struggle with simple daily tasks, such as reading time from a clock, understanding a flowchart, or planning a route using a road map. In light of this, we design a multi-modal self-instruct, utilizing large language models and their code capabilities to synthesize massive abstract images and visual reasoning instructions across daily scenarios. Our strategy effortlessly creates a multimodal benchmark with 11,193 instructions for eight visual scenarios: charts, tables, simulated maps, dashboards, flowcharts, relation graphs, floor plans, and visual puzzles. This benchmark, constructed with simple lines and geometric elements, exposes the shortcomings of most advanced LMMs like GPT-4V and Llava in abstract image understanding, spatial relations reasoning, and visual element induction. Besides, to verify the quality of our synthetic data, we fine-tune an LMM using 62,476 synthetic chart, table and road map instructions. The results demonstrate improved chart understanding and map navigation performance, and also demonstrate potential benefits for other visual reasoning tasks.",https://aclanthology.org/2024.emnlp-main.1072,2024,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{zhang-etal-2024-multimodal,
    title = ""Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model"",
    author = ""Zhang, Wenqi  and
      Cheng, Zhenglin  and
      He, Yuanyu  and
      Wang, Mengna  and
      Shen, Yongliang  and
      Tan, Zeqi  and
      Hou, Guiyang  and
      He, Mingqian  and
      Ma, Yanna  and
      Lu, Weiming  and
      Zhuang, Yueting"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1072/"",
    doi = ""10.18653/v1/2024.emnlp-main.1072"",
    pages = ""19228--19252"",
    abstract = ""Although most current large multimodal models (LMMs) can already understand photos of natural scenes and portraits, their understanding of abstract images, e.g., charts, maps, or layouts, and visual reasoning capabilities remains quite rudimentary. They often struggle with simple daily tasks, such as reading time from a clock, understanding a flowchart, or planning a route using a road map. In light of this, we design a multi-modal self-instruct, utilizing large language models and their code capabilities to synthesize massive abstract images and visual reasoning instructions across daily scenarios. Our strategy effortlessly creates a multimodal benchmark with 11,193 instructions for eight visual scenarios: charts, tables, simulated maps, dashboards, flowcharts, relation graphs, floor plans, and visual puzzles. \textbf{This benchmark, constructed with simple lines and geometric elements, exposes the shortcomings of most advanced LMMs} like GPT-4V and Llava in abstract image understanding, spatial relations reasoning, and visual element induction. Besides, to verify the quality of our synthetic data, we fine-tune an LMM using 62,476 synthetic chart, table and road map instructions. The results demonstrate improved chart understanding and map navigation performance, and also demonstrate potential benefits for other visual reasoning tasks.""
}",zhangMultimodalSelfinstructSynthetic2024
CaT-Bench: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans,"Understanding the abilities of LLMs to reason about natural language plans, such as instructional text and recipes, is critical to reliably using them in decision-making systems. A fundamental aspect of plans is the temporal order in which their steps need to be executed, which reflects the underlying causal dependencies between them. We introduce CaT-Bench, a benchmark of Step Order Prediction questions, which test whether a step must necessarily occur before or after another in cooking recipe plans. We use this to evaluate how well frontier LLMs understand causal and temporal dependencies. We find that SOTA LLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased towards predicting dependence more often, perhaps relying on temporal order of steps as a heuristic. While prompting for explanations and using few-shot examples improve performance, the best F1 result is only 0.73. Further, human evaluation of explanations along with answer correctness show that, on average, humans do not agree with model reasoning. Surprisingly, we also find that explaining after answering leads to better performance than normal chain-of-thought prompting, and LLM answers are not consistent across questions about the same step pairs. Overall, results show that LLMs’ ability to detect dependence between steps has significant room for improvement.",https://aclanthology.org/2024.emnlp-main.1077,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{lal-etal-2024-cat,
    title = ""{C}a{T}-Bench: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans"",
    author = ""Lal, Yash Kumar  and
      Cohen, Vanya  and
      Chambers, Nathanael  and
      Balasubramanian, Niranjan  and
      Mooney, Ray"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1077/"",
    doi = ""10.18653/v1/2024.emnlp-main.1077"",
    pages = ""19336--19354"",
    abstract = ""Understanding the abilities of LLMs to reason about natural language plans, such as instructional text and recipes, is critical to reliably using them in decision-making systems. A fundamental aspect of plans is the temporal order in which their steps need to be executed, which reflects the underlying causal dependencies between them. We introduce CaT-Bench, a benchmark of Step Order Prediction questions, which test whether a step must necessarily occur before or after another in cooking recipe plans. We use this to evaluate how well frontier LLMs understand causal and temporal dependencies. We find that SOTA LLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased towards predicting dependence more often, perhaps relying on temporal order of steps as a heuristic. While prompting for explanations and using few-shot examples improve performance, the best F1 result is only 0.73. Further, human evaluation of explanations along with answer correctness show that, on average, humans do not agree with model reasoning. Surprisingly, we also find that explaining after answering leads to better performance than normal chain-of-thought prompting, and LLM answers are not consistent across questions about the same step pairs. Overall, results show that LLMs' ability to detect dependence between steps has significant room for improvement.""
}",lalCaTbenchBenchmarkingLanguage2024
Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues,"Personality recognition aims to identify the personality traits implied in user data such as dialogues and social media posts. Current research predominantly treats personality recognition as a classification task, failing to reveal the supporting evidence for the recognized personality. In this paper, we propose a novel task named Explainable Personality Recognition, aiming to reveal the reasoning process as supporting evidence of the personality trait. Inspired by personality theories, personality traits are made up of stable patterns of personality state, where the states are short-term characteristic patterns of thoughts, feelings, and behaviors in a concrete situation at a specific moment in time. We propose an explainable personality recognition framework called Chain-of-Personality-Evidence (CoPE), which involves a reasoning process from specific contexts to short-term personality states to long-term personality traits. Furthermore, based on the CoPE framework, we construct an explainable personality recognition dataset from dialogues, PersonalityEvd. We introduce two explainable personality state recognition and explainable personality trait recognition tasks, which require models to recognize the personality state and trait labels and their corresponding support evidence. Our extensive experiments based on Large Language Models on the two tasks show that revealing personality traits is very challenging and we present some insights for future research. We will release our dataset and source code to facilitate further studies in this direction.",https://aclanthology.org/2024.emnlp-main.1115,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{sun-etal-2024-revealing,
    title = ""Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues"",
    author = ""Sun, Lei  and
      Zhao, Jinming  and
      Jin, Qin"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1115/"",
    doi = ""10.18653/v1/2024.emnlp-main.1115"",
    pages = ""19988--20002"",
    abstract = ""Personality recognition aims to identify the personality traits implied in user data such as dialogues and social media posts. Current research predominantly treats personality recognition as a classification task, failing to reveal the supporting evidence for the recognized personality. In this paper, we propose a novel task named Explainable Personality Recognition, aiming to reveal the reasoning process as supporting evidence of the personality trait. Inspired by personality theories, personality traits are made up of stable patterns of personality state, where the states are short-term characteristic patterns of thoughts, feelings, and behaviors in a concrete situation at a specific moment in time. We propose an explainable personality recognition framework called Chain-of-Personality-Evidence (CoPE), which involves a reasoning process from specific contexts to short-term personality states to long-term personality traits. Furthermore, based on the CoPE framework, we construct an explainable personality recognition dataset from dialogues, PersonalityEvd. We introduce two explainable personality state recognition and explainable personality trait recognition tasks, which require models to recognize the personality state and trait labels and their corresponding support evidence. Our extensive experiments based on Large Language Models on the two tasks show that revealing personality traits is very challenging and we present some insights for future research. We will release our dataset and source code to facilitate further studies in this direction.""
}",sunRevealingPersonalityTraits2024
MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models,"Large language models (LLMs) are increasingly used for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks mainly focus on single-turn evaluations, overlooking the models’ capabilities in multi-turn interactions. To address this gap, we introduce , a comprehensive benchmark to evaluate the multi-turn conversational abilities of LLMs. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or creating new examples using GPT-4 with a human-in-the-loop process to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 10 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks. We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models’ fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance.",https://aclanthology.org/2024.emnlp-main.1124,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{kwan-etal-2024-mt,
    title = ""{MT}-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models"",
    author = ""Kwan, Wai-Chung  and
      Zeng, Xingshan  and
      Jiang, Yuxin  and
      Wang, Yufei  and
      Li, Liangyou  and
      Shang, Lifeng  and
      Jiang, Xin  and
      Liu, Qun  and
      Wong, Kam-Fai"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1124/"",
    doi = ""10.18653/v1/2024.emnlp-main.1124"",
    pages = ""20153--20177"",
    abstract = ""Large language models (LLMs) are increasingly used for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks mainly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions. To address this gap, we introduce , a comprehensive benchmark to evaluate the multi-turn conversational abilities of LLMs. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or creating new examples using GPT-4 with a human-in-the-loop process to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 10 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks. We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance.""
}",kwanMTevalMultiturnCapabilities2024
Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models,"As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types — propositional, first-order, and non-monotonic consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs such as GPT-4, ChatGPT, Gemini-Pro, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs.",https://aclanthology.org/2024.emnlp-main.1160,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{patel-etal-2024-multi,
    title = ""Multi-{L}ogi{E}val: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models"",
    author = ""Patel, Nisarg  and
      Kulkarni, Mohith  and
      Parmar, Mihir  and
      Budhiraja, Aashna  and
      Nakamura, Mutsumi  and
      Varshney, Neeraj  and
      Baral, Chitta"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1160/"",
    doi = ""10.18653/v1/2024.emnlp-main.1160"",
    pages = ""20856--20879"",
    abstract = ""As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types {---} propositional, first-order, and non-monotonic consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs such as GPT-4, ChatGPT, Gemini-Pro, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of {\textasciitilde}68{\%} at depth-1 to {\textasciitilde}43{\%} at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs.""
}",patelMultiLogiEvalEvaluatingMultistep2024
Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game,"The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect438 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice humanplayers. Our results show that even the best-performing LLM, Claude 3.5 Sonnet, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 18% of the games. Novice and expert players perform better than Claude 3.5 Sonnet, with expert human players significantly outperforming it. We create a taxonomy of the knowledge types required to successfully cluster and categorize words in the Connections game. We find that while LLMs are decent at categorizing words based on semantic relations they struggle with other types of knowledge such as Encyclopedic Knowledge, Multiword Expressions or knowledge that combines both Word Form and Meaning. Our results establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.",https://aclanthology.org/2024.emnlp-main.1182,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{samdarshi-etal-2024-connecting,
    title = ""Connecting the Dots: Evaluating Abstract Reasoning Capabilities of {LLM}s Using the {N}ew {Y}ork {T}imes Connections Word Game"",
    author = ""Samdarshi, Prisha  and
      Mustafa, Mariam  and
      Kulkarni, Anushka  and
      Rothkopf, Raven  and
      Chakrabarty, Tuhin  and
      Muresan, Smaranda"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1182/"",
    doi = ""10.18653/v1/2024.emnlp-main.1182"",
    pages = ""21219--21236"",
    abstract = ""The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect438 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice humanplayers. Our results show that even the best-performing LLM, Claude 3.5 Sonnet, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 18{\%} of the games. Novice and expert players perform better than Claude 3.5 Sonnet, with expert human players significantly outperforming it. We create a taxonomy of the knowledge types required to successfully cluster and categorize words in the Connections game. We find that while LLMs are decent at categorizing words based on semantic relations they struggle with other types of knowledge such as Encyclopedic Knowledge, Multiword Expressions or knowledge that combines both Word Form and Meaning. Our results establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.""
}",samdarshiConnectingDotsEvaluating2024
An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models,"Large Multimodal Models (LMMs) have achieved strong performance across a range of vision and language tasks. However, their spatial reasoning capabilities are under-investigated. In this paper, we construct a novel VQA dataset, Spatial-MM, to comprehensively study LMMs’ spatial understanding and reasoning capabilities. Our analyses on object-relationship and multi-hop reasoning reveal several important findings. Firstly, bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs’ spatial reasoning. Secondly, LMMs struggle more with questions posed from the human perspective than the camera perspective about the image. Thirdly, chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations. Moreover, spatial reasoning steps are much less accurate than non-spatial ones across MLLMs. Lastly, our perturbation analysis on GQA-spatial reveals that LMMs are much stronger at basic object detection than complex spatial reasoning. We believe our new benchmark dataset and in-depth analyses can spark further research on LMMs spatial reasoning.",https://aclanthology.org/2024.emnlp-main.1195,2024,emnlp-main,Yes,Multimodal,Benchmark,"@inproceedings{shiri-etal-2024-empirical,
    title = ""An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models"",
    author = ""Shiri, Fatemeh  and
      Guo, Xiao-Yu  and
      Far, Mona Golestan  and
      Yu, Xin  and
      Haf, Reza  and
      Li, Yuan-Fang"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1195/"",
    doi = ""10.18653/v1/2024.emnlp-main.1195"",
    pages = ""21440--21455"",
    abstract = ""Large Multimodal Models (LMMs) have achieved strong performance across a range of vision and language tasks. However, their spatial reasoning capabilities are under-investigated. In this paper, we construct a novel VQA dataset, Spatial-MM, to comprehensively study LMMs' spatial understanding and reasoning capabilities. Our analyses on object-relationship and multi-hop reasoning reveal several important findings. Firstly, bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs' spatial reasoning. Secondly, LMMs struggle more with questions posed from the human perspective than the camera perspective about the image. Thirdly, chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations. Moreover, spatial reasoning steps are much less accurate than non-spatial ones across MLLMs. Lastly, our perturbation analysis on GQA-spatial reveals that LMMs are much stronger at basic object detection than complex spatial reasoning. We believe our new benchmark dataset and in-depth analyses can spark further research on LMMs spatial reasoning.""
}",shiriEmpiricalAnalysisSpatial2024
The Greatest Good Benchmark: Measuring LLMs’ Alignment with Utilitarian Moral Dilemmas,"The question of how to make decisions that maximise the well-being of all persons is very relevant to design language models that are beneficial to humanity and free from harm. We introduce the Greatest Good Benchmark to evaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis across 15 diverse LLMs reveals consistently encoded moral preferences that diverge from established moral theories and lay population moral standards. Most LLMs have a marked preference for impartial beneficence and rejection of instrumental harm. These findings showcase the ‘artificial moral compass’ of LLMs, offering insights into their moral alignment.",https://aclanthology.org/2024.emnlp-main.1224,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{marraffini-etal-2024-greatest,
    title = ""The Greatest Good Benchmark: Measuring {LLM}s' Alignment with Utilitarian Moral Dilemmas"",
    author = ""Marraffini, Giovanni Franco Gabriel  and
      Cotton, Andr{\'e}s  and
      Hsueh, Noe Fabian  and
      Fridman, Axel  and
      Wisznia, Juan  and
      Corro, Luciano Del"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1224/"",
    doi = ""10.18653/v1/2024.emnlp-main.1224"",
    pages = ""21950--21959"",
    abstract = ""The question of how to make decisions that maximise the well-being of all persons is very relevant to design language models that are beneficial to humanity and free from harm. We introduce the Greatest Good Benchmark to evaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis across 15 diverse LLMs reveals consistently encoded moral preferences that diverge from established moral theories and lay population moral standards. Most LLMs have a marked preference for impartial beneficence and rejection of instrumental harm. These findings showcase the {\textquoteleft}artificial moral compass' of LLMs, offering insights into their moral alignment.""
}",marraffiniGreatestGoodBenchmark2024
FOLIO: Natural Language Reasoning with First-Order Logic,"Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.",https://aclanthology.org/2024.emnlp-main.1229,2024,emnlp-main,Yes,Language,Benchmark,"@inproceedings{han-etal-2024-folio,
    title = ""{FOLIO}: Natural Language Reasoning with First-Order Logic"",
    author = ""Han, Simeng  and
      Schoelkopf, Hailey  and
      Zhao, Yilun  and
      Qi, Zhenting  and
      Riddell, Martin  and
      Zhou, Wenfei  and
      Coady, James  and
      Peng, David  and
      Qiao, Yujie  and
      Benson, Luke  and
      Sun, Lucy  and
      Wardle-Solano, Alexander  and
      Szab{\'o}, Hannah  and
      Zubova, Ekaterina  and
      Burtell, Matthew  and
      Fan, Jonathan  and
      Liu, Yixin  and
      Wong, Brian  and
      Sailor, Malcolm  and
      Ni, Ansong  and
      Nan, Linyong  and
      Kasai, Jungo  and
      Yu, Tao  and
      Zhang, Rui  and
      Fabbri, Alexander  and
      Kryscinski, Wojciech Maciej  and
      Yavuz, Semih  and
      Liu, Ye  and
      Lin, Xi Victoria  and
      Joty, Shafiq  and
      Zhou, Yingbo  and
      Xiong, Caiming  and
      Ying, Rex  and
      Cohan, Arman  and
      Radev, Dragomir"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.1229/"",
    doi = ""10.18653/v1/2024.emnlp-main.1229"",
    pages = ""22017--22031"",
    abstract = ""Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.""
}",hanFOLIONaturalLanguage2024
Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?,"Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.",https://aclanthology.org/2024.naacl-long.18,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{sun-etal-2024-head,
    title = ""Head-to-Tail: How Knowledgeable are Large Language Models ({LLM}s)? {A}.{K}.{A}. Will {LLM}s Replace Knowledge Graphs?"",
    author = ""Sun, Kai  and
      Xu, Yifan  and
      Zha, Hanwen  and
      Liu, Yue  and
      Dong, Xin Luna"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.18/"",
    doi = ""10.18653/v1/2024.naacl-long.18"",
    pages = ""311--325"",
    abstract = ""Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.""
}",sunHeadtotailHowKnowledgeable2024a
Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles,"Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.",https://aclanthology.org/2024.naacl-long.32,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{huang-etal-2024-embrace,
    title = ""Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles"",
    author = ""Huang, Kung-Hsiang  and
      Laban, Philippe  and
      Fabbri, Alexander  and
      Choubey, Prafulla Kumar  and
      Joty, Shafiq  and
      Xiong, Caiming  and
      Wu, Chien-Sheng"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.32/"",
    doi = ""10.18653/v1/2024.naacl-long.32"",
    pages = ""570--593"",
    abstract = ""Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40{\%} of the diverse information on average.""
}",huangEmbraceDivergenceRicher2024
An Examination of the Compositionality of Large Generative Vision-Language Models,"With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics ( VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a **SyntaxBias Score**, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely **S**ynt**A**ctically **DE**-biased benchmark (SADE). Our study provides an unbiased benchmark for the compositionality of GVLMs, facilitating future research in this direction. Code and dataset are available at https://github.com/TeleeMa/SADE.",https://aclanthology.org/2024.naacl-long.39,2024,naacl-long,Yes,Multimodal,Benchmark,"@inproceedings{ma-etal-2024-examination,
    title = ""An Examination of the Compositionality of Large Generative Vision-Language Models"",
    author = ""Ma, Teli  and
      Li, Rong  and
      Liang, Junwei"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.39/"",
    doi = ""10.18653/v1/2024.naacl-long.39"",
    pages = ""692--705"",
    abstract = ""With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics ( VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a **SyntaxBias Score**, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely **S**ynt**A**ctically **DE**-biased benchmark (SADE). Our study provides an unbiased benchmark for the compositionality of GVLMs, facilitating future research in this direction. Code and dataset are available at https://github.com/TeleeMa/SADE.""
}",maExaminationCompositionalityLarge2024
MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference,"The task of scientific Natural Language Inference (NLI) involves predicting the semantic relation between two sentences extracted from research articles. This task was recently proposed along with a new dataset called SciNLI derived from papers published in the computational linguistics domain. In this paper, we aim to introduce diversity in the scientific NLI task and present MSciNLI, a dataset containing 132,320 sentence pairs extracted from five new scientific domains. The availability of multiple domains makes it possible to study domain shift for scientific NLI. We establish strong baselines on MSciNLI by fine-tuning Pre-trained Language Models (PLMs) and prompting Large Language Models (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21% and 51.77%, respectively, illustrating that MSciNLI is challenging for both types of models. Furthermore, we show that domain shift degrades the performance of scientific NLI models which demonstrates the diverse characteristics of different domains in our dataset. Finally, we use both scientific NLI datasets in an intermediate task transfer learning setting and show that they can improve the performance of downstream tasks in the scientific domain. We make our dataset and code available on Github.",https://aclanthology.org/2024.naacl-long.90,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{sadat-caragea-2024-mscinli,
    title = ""{MS}ci{NLI}: A Diverse Benchmark for Scientific Natural Language Inference"",
    author = ""Sadat, Mobashir  and
      Caragea, Cornelia"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.90/"",
    doi = ""10.18653/v1/2024.naacl-long.90"",
    pages = ""1610--1629"",
    abstract = ""The task of scientific Natural Language Inference (NLI) involves predicting the semantic relation between two sentences extracted from research articles. This task was recently proposed along with a new dataset called SciNLI derived from papers published in the computational linguistics domain. In this paper, we aim to introduce diversity in the scientific NLI task and present MSciNLI, a dataset containing 132,320 sentence pairs extracted from five new scientific domains. The availability of multiple domains makes it possible to study domain shift for scientific NLI. We establish strong baselines on MSciNLI by fine-tuning Pre-trained Language Models (PLMs) and prompting Large Language Models (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21{\%} and 51.77{\%}, respectively, illustrating that MSciNLI is challenging for both types of models. Furthermore, we show that domain shift degrades the performance of scientific NLI models which demonstrates the diverse characteristics of different domains in our dataset. Finally, we use both scientific NLI datasets in an intermediate task transfer learning setting and show that they can improve the performance of downstream tasks in the scientific domain. We make our dataset and code available on Github.""
}",sadatMSciNLIDiverseBenchmark2024a
Toward Informal Language Processing: Knowledge of Slang in Large Language Models,"Recent advancement in large language models (LLMs) has offered a strong potential for natural language systems to process informal language. A representative form of informal language is slang, used commonly in daily conversations and online social media. To date, slang has not been comprehensively evaluated in LLMs due partly to the absence of a carefully designed and publicly accessible benchmark. Using movie subtitles, we construct a dataset that supports evaluation on a diverse set of tasks pertaining to automatic processing of slang. For both evaluation and finetuning, we show the effectiveness of our dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences. We also show how our dataset can be used to probe the output distributions of LLMs for interpretive insights. We find that while LLMs such as GPT-4 achieve good performance in a zero-shot setting, smaller BERT-like models finetuned on our dataset achieve comparable performance. Furthermore, we show that our dataset enables finetuning of LLMs such as GPT-3.5 that achieve substantially better performance than strong zero-shot baselines. Our work offers a comprehensive evaluation and a high-quality benchmark on English slang based on the OpenSubtitles corpus, serving both as a publicly accessible resource and a platform for applying tools for informal language processing.",https://aclanthology.org/2024.naacl-long.94,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{sun-etal-2024-toward,
    title = ""Toward Informal Language Processing: Knowledge of Slang in Large Language Models"",
    author = ""Sun, Zhewei  and
      Hu, Qian  and
      Gupta, Rahul  and
      Zemel, Richard  and
      Xu, Yang"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.94/"",
    doi = ""10.18653/v1/2024.naacl-long.94"",
    pages = ""1683--1701"",
    abstract = ""Recent advancement in large language models (LLMs) has offered a strong potential for natural language systems to process informal language. A representative form of informal language is slang, used commonly in daily conversations and online social media. To date, slang has not been comprehensively evaluated in LLMs due partly to the absence of a carefully designed and publicly accessible benchmark. Using movie subtitles, we construct a dataset that supports evaluation on a diverse set of tasks pertaining to automatic processing of slang. For both evaluation and finetuning, we show the effectiveness of our dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences. We also show how our dataset can be used to probe the output distributions of LLMs for interpretive insights. We find that while LLMs such as GPT-4 achieve good performance in a zero-shot setting, smaller BERT-like models finetuned on our dataset achieve comparable performance. Furthermore, we show that our dataset enables finetuning of LLMs such as GPT-3.5 that achieve substantially better performance than strong zero-shot baselines. Our work offers a comprehensive evaluation and a high-quality benchmark on English slang based on the OpenSubtitles corpus, serving both as a publicly accessible resource and a platform for applying tools for informal language processing.""
}",sunInformalLanguageProcessing2024
BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer,"Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.",https://aclanthology.org/2024.naacl-long.100,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{asai-etal-2024-buffet,
    title = ""{BUFFET}: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer"",
    author = ""Asai, Akari  and
      Kudugunta, Sneha  and
      Yu, Xinyan  and
      Blevins, Terra  and
      Gonen, Hila  and
      Reid, Machel  and
      Tsvetkov, Yulia  and
      Ruder, Sebastian  and
      Hajishirzi, Hannaneh"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.100/"",
    doi = ""10.18653/v1/2024.naacl-long.100"",
    pages = ""1771--1800"",
    abstract = ""Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.""
}",asaiBUFFETBenchmarkingLarge2024
"MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks","There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.",https://aclanthology.org/2024.naacl-long.143,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{ahuja-etal-2024-megaverse,
    title = ""{MEGAVERSE}: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks"",
    author = ""Ahuja, Sanchit  and
      Aggarwal, Divyanshu  and
      Gumma, Varun  and
      Watts, Ishaan  and
      Sathe, Ashutosh  and
      Ochieng, Millicent  and
      Hada, Rishav  and
      Jain, Prachi  and
      Ahmed, Mohamed  and
      Bali, Kalika  and
      Sitaram, Sunayana"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.143/"",
    doi = ""10.18653/v1/2024.naacl-long.143"",
    pages = ""2598--2637"",
    abstract = ""There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.""
}",ahujaMEGAVERSEBenchmarkingLarge2024
PatentEval: Understanding Errors in Patent Generation,"In this work, we introduce a comprehensive error typology specifically designed for evaluating two distinct tasks in machine-generated patent texts: claims-to-abstract generation, and the generation of the next claim given previous ones. We have also developed a benchmark, PatentEval, for systematically assessing language models in this context. Our study includes a comparative analysis, annotated by humans, of various models. These range from those specifically adapted during training for tasks within the patent domain to the latest general-purpose large language models (LLMs). Furthermore, we explored and evaluated some metrics to approximate human judgments in patent text evaluation, analyzing the extent to which these metrics align with expert assessments. These approaches provide valuable insights into the capabilities and limitations of current language models in the specialized field of patent text generation.",https://aclanthology.org/2024.naacl-long.147,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{zuo-etal-2024-patenteval,
    title = ""{P}atent{E}val: Understanding Errors in Patent Generation"",
    author = ""Zuo, You  and
      Gerdes, Kim  and
      Clergerie, {\'E}ric  and
      Sagot, Beno{\^i}t"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.147/"",
    doi = ""10.18653/v1/2024.naacl-long.147"",
    pages = ""2687--2710"",
    abstract = ""In this work, we introduce a comprehensive error typology specifically designed for evaluating two distinct tasks in machine-generated patent texts: claims-to-abstract generation, and the generation of the next claim given previous ones. We have also developed a benchmark, PatentEval, for systematically assessing language models in this context. Our study includes a comparative analysis, annotated by humans, of various models. These range from those specifically adapted during training for tasks within the patent domain to the latest general-purpose large language models (LLMs). Furthermore, we explored and evaluated some metrics to approximate human judgments in patent text evaluation, analyzing the extent to which these metrics align with expert assessments. These approaches provide valuable insights into the capabilities and limitations of current language models in the specialized field of patent text generation.""
}",zuoPatentEvalUnderstandingErrors2024
GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models,"The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities can cause hallucinations. Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent with human preferences for RE quality. Last, we made a comprehensive evaluation of fourteen leading LLMs using GenRES across document, bag, and sentence level RE datasets, respectively, to set the benchmark for future research in GRE",https://aclanthology.org/2024.naacl-long.155,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{jiang-etal-2024-genres,
    title = ""{G}en{RES}: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models"",
    author = ""Jiang, Pengcheng  and
      Lin, Jiacheng  and
      Wang, Zifeng  and
      Sun, Jimeng  and
      Han, Jiawei"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.155/"",
    doi = ""10.18653/v1/2024.naacl-long.155"",
    pages = ""2820--2837"",
    abstract = ""The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities can cause hallucinations. Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent with human preferences for RE quality. Last, we made a comprehensive evaluation of fourteen leading LLMs using GenRES across document, bag, and sentence level RE datasets, respectively, to set the benchmark for future research in GRE""
}",jiangGenRESRethinkingEvaluation2024a
Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks,"The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these “ground truth” weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods identify neurons that are not specific to a single memorized sequence.",https://aclanthology.org/2024.naacl-long.176,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{chang-etal-2024-localization,
    title = ""Do Localization Methods Actually Localize Memorized Data in {LLM}s? A Tale of Two Benchmarks"",
    author = ""Chang, Ting-Yun  and
      Thomason, Jesse  and
      Jia, Robin"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.176/"",
    doi = ""10.18653/v1/2024.naacl-long.176"",
    pages = ""3190--3211"",
    abstract = ""The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these {\textquotedblleft}ground truth{\textquotedblright} weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods identify neurons that are not specific to a single memorized sequence.""
}",changLocalizationMethodsActually2024a
Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks,"Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs’ capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models’ long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs’ long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.",https://aclanthology.org/2024.naacl-long.205,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-ada,
    title = ""{A}da-{LE}val: Evaluating long-context {LLM}s with length-adaptable benchmarks"",
    author = ""Wang, Chonghua  and
      Duan, Haodong  and
      Zhang, Songyang  and
      Lin, Dahua  and
      Chen, Kai"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.205/"",
    doi = ""10.18653/v1/2024.naacl-long.205"",
    pages = ""3712--3724"",
    abstract = ""Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.""
}",wangAdaLEvalEvaluatingLongcontext2024
XNLIeu: a dataset for cross-lingual NLI in Basque,"XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.",https://aclanthology.org/2024.naacl-long.234,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{heredia-etal-2024-xnlieu,
    title = ""{XNLI}eu: a dataset for cross-lingual {NLI} in {B}asque"",
    author = ""Heredia, Maite  and
      Etxaniz, Julen  and
      Zulaika, Muitze  and
      Saralegi, Xabier  and
      Barnes, Jeremy  and
      Soroa, Aitor"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.234/"",
    doi = ""10.18653/v1/2024.naacl-long.234"",
    pages = ""4177--4188"",
    abstract = ""XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.""
}",herediaXNLIeuDatasetCrosslingual2024
TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization,"Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence- level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model’s size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.",https://aclanthology.org/2024.naacl-long.251,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{tang-etal-2024-tofueval,
    title = ""{T}ofu{E}val: Evaluating Hallucinations of {LLM}s on Topic-Focused Dialogue Summarization"",
    author = ""Tang, Liyan  and
      Shalyminov, Igor  and
      Wong, Amy  and
      Burnsky, Jon  and
      Vincent, Jake  and
      Yang, Yu{'}an  and
      Singh, Siffi  and
      Feng, Song  and
      Song, Hwanjun  and
      Su, Hang  and
      Sun, Lijia  and
      Zhang, Yi  and
      Mansour, Saab  and
      McKeown, Kathleen"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.251/"",
    doi = ""10.18653/v1/2024.naacl-long.251"",
    pages = ""4455--4480"",
    abstract = ""Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence- level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model`s size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.""
}",tangTofuEvalEvaluatingHallucinations2024
Flames: Benchmarking Value Alignment of LLMs in Chinese,"The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and ‘topping the chart’ in these evaluations, there is still a significant gap in LLMs’ deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findings indicate that all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions. We also develop a lightweight specified scorer capable of scoring LLMs across multiple dimensions to efficiently evaluate new models on the benchmark. The complexity of Flames has far exceeded existing benchmarks, setting a new challenge for contemporary LLMs and highlighting the need for further alignment of LLMs. Our benchmark is publicly available at https://github.com/AIFlames/Flames.",https://aclanthology.org/2024.naacl-long.256,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{huang-etal-2024-flames,
    title = ""Flames: Benchmarking Value Alignment of {LLM}s in {C}hinese"",
    author = ""Huang, Kexin  and
      Liu, Xiangyang  and
      Guo, Qianyu  and
      Sun, Tianxiang  and
      Sun, Jiawei  and
      Wang, Yaru  and
      Zhou, Zeyang  and
      Wang, Yixu  and
      Teng, Yan  and
      Qiu, Xipeng  and
      Wang, Yingchun  and
      Lin, Dahua"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.256/"",
    doi = ""10.18653/v1/2024.naacl-long.256"",
    pages = ""4551--4591"",
    abstract = ""The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and {\textquoteleft}topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findings indicate that all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions. We also develop a lightweight specified scorer capable of scoring LLMs across multiple dimensions to efficiently evaluate new models on the benchmark. The complexity of Flames has far exceeded existing benchmarks, setting a new challenge for contemporary LLMs and highlighting the need for further alignment of LLMs. Our benchmark is publicly available at https://github.com/AIFlames/Flames.""
}",huangFlamesBenchmarkingValue2024
MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification,"We introduce MAFALDA, a benchmark for fallacy classification that merges and unites previous fallacy datasets. It comes with a taxonomy that aligns, refines, and unifies existing classifications of fallacies. We further provide a manual annotation of a part of the dataset together with manual explanations for each annotation. We propose a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity. We then evaluate several language models under a zero-shot learning setting and human performances on MAFALDA to assess their capability to detect and classify fallacies.",https://aclanthology.org/2024.naacl-long.270,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{helwe-etal-2024-mafalda,
    title = ""{MAFALDA}: A Benchmark and Comprehensive Study of Fallacy Detection and Classification"",
    author = ""Helwe, Chadi  and
      Calamai, Tom  and
      Paris, Pierre-Henri  and
      Clavel, Chlo{\'e}  and
      Suchanek, Fabian"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.270/"",
    doi = ""10.18653/v1/2024.naacl-long.270"",
    pages = ""4810--4845"",
    abstract = ""We introduce MAFALDA, a benchmark for fallacy classification that merges and unites previous fallacy datasets. It comes with a taxonomy that aligns, refines, and unifies existing classifications of fallacies. We further provide a manual annotation of a part of the dataset together with manual explanations for each annotation. We propose a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity. We then evaluate several language models under a zero-shot learning setting and human performances on MAFALDA to assess their capability to detect and classify fallacies.""
}",helweMAFALDABenchmarkComprehensive2024a
SportQA: A Benchmark for Sports Understanding in Large Language Models,"A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs. The dataset is available at https://github.com/haotianxia/SportQA",https://aclanthology.org/2024.naacl-long.283,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{xia-etal-2024-sportqa,
    title = ""{S}port{QA}: A Benchmark for Sports Understanding in Large Language Models"",
    author = ""Xia, Haotian  and
      Yang, Zhengbang  and
      Wang, Yuqing  and
      Tracy, Rhys  and
      Zhao, Yun  and
      Huang, Dongdong  and
      Chen, Zezhi  and
      Zhu, Yan  and
      Wang, Yuan-fang  and
      Shen, Weining"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.283/"",
    doi = ""10.18653/v1/2024.naacl-long.283"",
    pages = ""5061--5081"",
    abstract = ""A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs. The dataset is available at https://github.com/haotianxia/SportQA""
}",xiaSportQABenchmarkSports2024a
Carpe diem: On the Evaluation of World Knowledge in Lifelong Language Models,"The dynamic nature of knowledge in an ever-changing world presents challenges for language models trained on static data; the model in the real world often requires not only acquiring new knowledge but also overwriting outdated information into updated ones. To study the ability of language models for these time-dependent dynamics in human language, we introduce a novel task, EvolvingQA, a temporally evolving question-answering benchmark designed for training and evaluating LMs on an evolving Wikipedia database. The construction of EvolvingQA is automated with our pipeline using large language models. We uncover that existing continual learning baselines suffer from updating and removing outdated knowledge. Our analysis suggests that models fail to rectify knowledge due to small weight gradients. In addition, we elucidate that language models particularly struggle to reflect the change of numerical or temporal information. Our work aims to model the dynamic nature of real-world information, suggesting faithful evaluations of the evolution-adaptability of language models. Our data construction code and dataset files are available at https://github.com/kimyuji/EvolvingQA_benchmark.",https://aclanthology.org/2024.naacl-long.302,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{kim-etal-2024-carpe,
    title = ""Carpe diem: On the Evaluation of World Knowledge in Lifelong Language Models"",
    author = ""Kim, Yujin  and
      Yoon, Jaehong  and
      Ye, Seonghyeon  and
      Bae, Sangmin  and
      Ho, Namgyu  and
      Hwang, Sung Ju  and
      Yun, Se-Young"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.302/"",
    doi = ""10.18653/v1/2024.naacl-long.302"",
    pages = ""5401--5415"",
    abstract = ""The dynamic nature of knowledge in an ever-changing world presents challenges for language models trained on static data; the model in the real world often requires not only acquiring new knowledge but also overwriting outdated information into updated ones. To study the ability of language models for these time-dependent dynamics in human language, we introduce a novel task, EvolvingQA, a temporally evolving question-answering benchmark designed for training and evaluating LMs on an evolving Wikipedia database. The construction of EvolvingQA is automated with our pipeline using large language models. We uncover that existing continual learning baselines suffer from updating and removing outdated knowledge. Our analysis suggests that models fail to rectify knowledge due to small weight gradients. In addition, we elucidate that language models particularly struggle to reflect the change of numerical or temporal information. Our work aims to model the dynamic nature of real-world information, suggesting faithful evaluations of the evolution-adaptability of language models. Our data construction code and dataset files are available at https://github.com/kimyuji/EvolvingQA{\_}benchmark.""
}",kimCarpeDiemEvaluation2024
Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers,"The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works such as RankGPT have demonstrated that the LLMs have better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.",https://aclanthology.org/2024.naacl-long.319,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-large,
    title = ""Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of {LLM}s as Rankers"",
    author = ""Wang, Yuan  and
      Wu, Xuyang  and
      Wu, Hsin-Tai  and
      Tao, Zhiqiang  and
      Fang, Yi"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.319/"",
    doi = ""10.18653/v1/2024.naacl-long.319"",
    pages = ""5712--5724"",
    abstract = ""The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works such as RankGPT have demonstrated that the LLMs have better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.""
}",wangLargeLanguageModels2024
DialogBench: Evaluating LLMs as Human-like Dialogue Systems,"Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities by leveraging instruction tuning,which refreshes human impressions of dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive tests on English and Chinese DialogBench of 26 LLMs show that instruction tuning improves the human likeness of LLMs to a certain extent, but most LLMs still have much room for improvement as human-like dialogue systems. Interestingly, results also show that the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life.",https://aclanthology.org/2024.naacl-long.341,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{ou-etal-2024-dialogbench,
    title = ""{D}ialog{B}ench: Evaluating {LLM}s as Human-like Dialogue Systems"",
    author = ""Ou, Jiao  and
      Lu, Junda  and
      Liu, Che  and
      Tang, Yihong  and
      Zhang, Fuzheng  and
      Zhang, Di  and
      Gai, Kun"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.341/"",
    doi = ""10.18653/v1/2024.naacl-long.341"",
    pages = ""6137--6170"",
    abstract = ""Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities by leveraging instruction tuning,which refreshes human impressions of dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive tests on English and Chinese DialogBench of 26 LLMs show that instruction tuning improves the human likeness of LLMs to a certain extent, but most LLMs still have much room for improvement as human-like dialogue systems. Interestingly, results also show that the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life.""
}",ouDialogBenchEvaluatingLLMs2024
CMB: A Comprehensive Medical Benchmark in Chinese,"Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in contextual incongruities to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB.",https://aclanthology.org/2024.naacl-long.343,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{wang-etal-2024-cmb,
    title = ""{CMB}: A Comprehensive Medical Benchmark in {C}hinese"",
    author = ""Wang, Xidong  and
      Chen, Guiming  and
      Dingjie, Song  and
      Zhiyi, Zhang  and
      Chen, Zhihong  and
      Xiao, Qingying  and
      Chen, Junying  and
      Jiang, Feng  and
      Li, Jianquan  and
      Wan, Xiang  and
      Wang, Benyou  and
      Li, Haizhou"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.343/"",
    doi = ""10.18653/v1/2024.naacl-long.343"",
    pages = ""6184--6205"",
    abstract = ""Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in \textit{contextual incongruities} to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB.""
}",wangCMBComprehensiveMedical2024a
SuperGLEBer: German Language Understanding Evaluation Benchmark,"We assemble a broad Natural Language Understanding benchmark suite for the German language and consequently evaluate a wide array of existing German-capable models in order to create a better understanding of the current state of German LLMs. Our benchmark consists of 29 different tasks ranging over different types such as document classification, sequence tagging, sentence similarity, and question answering, on which we evaluate 10 different German-pretrained models, thereby charting the landscape of German LLMs. In our comprehensive evaluation we find that encoder models are a good choice for most tasks, but also that the largest encoder model does not necessarily perform best for all tasks. We make our benchmark suite and a leaderboard publically available at https://supergleber.professor-x.de and encourage the community to contribute new tasks and evaluate more models on it (https://github.com/LSX-UniWue/SuperGLEBer).",https://aclanthology.org/2024.naacl-long.438,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{pfister-hotho-2024-supergleber,
    title = ""{S}uper{GLEB}er: {G}erman Language Understanding Evaluation Benchmark"",
    author = ""Pfister, Jan  and
      Hotho, Andreas"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.438/"",
    doi = ""10.18653/v1/2024.naacl-long.438"",
    pages = ""7904--7923"",
    abstract = ""We assemble a broad Natural Language Understanding benchmark suite for the German language and consequently evaluate a wide array of existing German-capable models in order to create a better understanding of the current state of German LLMs. Our benchmark consists of 29 different tasks ranging over different types such as document classification, sequence tagging, sentence similarity, and question answering, on which we evaluate 10 different German-pretrained models, thereby charting the landscape of German LLMs. In our comprehensive evaluation we find that encoder models are a good choice for most tasks, but also that the largest encoder model does not necessarily perform best for all tasks. We make our benchmark suite and a leaderboard publically available at https://supergleber.professor-x.de and encourage the community to contribute new tasks and evaluate more models on it (https://github.com/LSX-UniWue/SuperGLEBer).""
}",pfisterSuperGLEBerGermanLanguage2024
BUST: Benchmark for the evaluation of detectors of LLM-Generated Text,"We introduce BUST, a comprehensive benchmark designed to evaluate detectors of texts generated by instruction-tuned large language models (LLMs). Unlike previous benchmarks, our focus lies on evaluating the performance of detector systems, acknowledging the inevitable influence of the underlying tasks and different LLM generators. Our benchmark dataset consists of 25K texts from humans and 7 LLMs responding to instructions across 10 tasks from 3 diverse sources. Using the benchmark, we evaluated 5 detectors and found substantial performance variance across tasks. A meta-analysis of the dataset characteristics was conducted to guide the examination of detector performance. The dataset was analyzed using diverse metrics assessing linguistic features like fluency and coherence, readability scores, and writer attitudes, such as emotions, convincingness, and persuasiveness. Features impacting detector performance were investigated with surrogate models, revealing emotional content in texts enhanced some detectors, yet the most effective detector demonstrated consistent performance, irrespective of writer’s attitudes and text styles. Our approach focused on investigating relationships between the detectors’ performance and two key factors: text characteristics and LLM generators. We believe BUST will provide valuable insights into selecting detectors tailored to specific text styles and tasks and facilitate a more practical and in-depth investigation of detection systems for LLM-generated text.",https://aclanthology.org/2024.naacl-long.444,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{cornelius-etal-2024-bust,
    title = ""{BUST}: Benchmark for the evaluation of detectors of {LLM}-Generated Text"",
    author = ""Cornelius, Joseph  and
      Lithgow-Serrano, Oscar  and
      Mitrovic, Sandra  and
      Dolamic, Ljiljana  and
      Rinaldi, Fabio"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.444/"",
    doi = ""10.18653/v1/2024.naacl-long.444"",
    pages = ""8029--8057"",
    abstract = ""We introduce BUST, a comprehensive benchmark designed to evaluate detectors of texts generated by instruction-tuned large language models (LLMs). Unlike previous benchmarks, our focus lies on evaluating the performance of detector systems, acknowledging the inevitable influence of the underlying tasks and different LLM generators. Our benchmark dataset consists of 25K texts from humans and 7 LLMs responding to instructions across 10 tasks from 3 diverse sources. Using the benchmark, we evaluated 5 detectors and found substantial performance variance across tasks. A meta-analysis of the dataset characteristics was conducted to guide the examination of detector performance. The dataset was analyzed using diverse metrics assessing linguistic features like fluency and coherence, readability scores, and writer attitudes, such as emotions, convincingness, and persuasiveness. Features impacting detector performance were investigated with surrogate models, revealing emotional content in texts enhanced some detectors, yet the most effective detector demonstrated consistent performance, irrespective of writer`s attitudes and text styles. Our approach focused on investigating relationships between the detectors' performance and two key factors: text characteristics and LLM generators. We believe BUST will provide valuable insights into selecting detectors tailored to specific text styles and tasks and facilitate a more practical and in-depth investigation of detection systems for LLM-generated text.""
}",corneliusBUSTBenchmarkEvaluation2024
IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context,"The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India’s unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersectional biases along three intersectional dimensions. Our dataset contains 800 sentence pairs and 300 tuples for bias measurement across different demographics. The dataset is available in English and Hindi, providing a size comparable to existing benchmark datasets. Furthermore, using IndiBias we compare ten different language models on multiple bias measurement metrics. We observed that the language models exhibit more bias across a majority of the intersectional groups. All the scripts utilized and datasets created in this study are publicly available.",https://aclanthology.org/2024.naacl-long.487,2024,naacl-long,Yes,Language,Benchmark,"@inproceedings{sahoo-etal-2024-indibias,
    title = ""{I}ndi{B}ias: A Benchmark Dataset to Measure Social Biases in Language Models for {I}ndian Context"",
    author = ""Sahoo, Nihar  and
      Kulkarni, Pranamya  and
      Ahmad, Arif  and
      Goyal, Tanu  and
      Asad, Narjis  and
      Garimella, Aparna  and
      Bhattacharyya, Pushpak"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-long.487/"",
    doi = ""10.18653/v1/2024.naacl-long.487"",
    pages = ""8786--8806"",
    abstract = ""The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India`s unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersectional biases along three intersectional dimensions. Our dataset contains 800 sentence pairs and 300 tuples for bias measurement across different demographics. The dataset is available in English and Hindi, providing a size comparable to existing benchmark datasets. Furthermore, using IndiBias we compare ten different language models on multiple bias measurement metrics. We observed that the language models exhibit more bias across a majority of the intersectional groups. All the scripts utilized and datasets created in this study are publicly available.""
}",sahooIndiBiasBenchmarkDataset2024
Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?,"Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs’ proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions, coverage, formatting, reasoning, comprehension, pragmatics, and hallucination, highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.",https://aclanthology.org/2024.naacl-short.2,2024,naacl-short,Yes,Language,Benchmark,"@inproceedings{tang-etal-2024-struc,
    title = ""Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?"",
    author = ""Tang, Xiangru  and
      Zong, Yiming  and
      Phang, Jason  and
      Zhao, Yilun  and
      Zhou, Wangchunshu  and
      Cohan, Arman  and
      Gerstein, Mark"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-short.2/"",
    doi = ""10.18653/v1/2024.naacl-short.2"",
    pages = ""12--34"",
    abstract = ""Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs' proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions, coverage, formatting, reasoning, comprehension, pragmatics, and hallucination, highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.""
}",tangStrucbenchAreLarge2024
Do Vision-Language Models Understand Compound Nouns?,"Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., *lab coat*) as well as they understand nouns (e.g., *lab*)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs’ limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25% on Compun. Code and benchmark are available.",https://aclanthology.org/2024.naacl-short.43,2024,naacl-short,Yes,Multimodal,Benchmark,"@inproceedings{kumar-etal-2024-vision,
    title = ""Do Vision-Language Models Understand Compound Nouns?"",
    author = ""Kumar, Sonal  and
      Ghosh, Sreyan  and
      Sakshi, S  and
      Tyagi, Utkarsh  and
      Manocha, Dinesh"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-short.43/"",
    doi = ""10.18653/v1/2024.naacl-short.43"",
    pages = ""519--527"",
    abstract = ""Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., *lab coat*) as well as they understand nouns (e.g., *lab*)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25{\%} on Compun. Code and benchmark are available.""
}",kumarVisionlanguageModelsUnderstand2024
MuLan: A Study of Fact Mutability in Language Models,"Facts are subject to contingencies and can be true or false in different circumstances. One such contingency is time, wherein some facts mutate over a given period, e.g., the president of a country or the winner of a championship. Trustworthy language models ideally identify mutable facts as such and process them accordingly. We create MuLan, a benchmark for evaluating the ability of English language models to anticipate time-contingency, covering both 1:1 and 1:N relations. We hypothesize that mutable facts are encoded differently than immutable ones, hence being easier to update. In a detailed evaluation of six popular large language models, we consistently find differences in the LLMs’ confidence, representations, and update behavior, depending on the mutability of a fact. Our findings should inform future work on the injection of and induction of time-contingent knowledge to/from LLMs.",https://aclanthology.org/2024.naacl-short.67,2024,naacl-short,Yes,Language,Benchmark,"@inproceedings{fierro-etal-2024-mulan,
    title = ""{M}u{L}an: A Study of Fact Mutability in Language Models"",
    author = ""Fierro, Constanza  and
      Garneau, Nicolas  and
      Bugliarello, Emanuele  and
      Kementchedjhieva, Yova  and
      S{\o}gaard, Anders"",
    editor = ""Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven"",
    booktitle = ""Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.naacl-short.67/"",
    doi = ""10.18653/v1/2024.naacl-short.67"",
    pages = ""762--771"",
    abstract = ""Facts are subject to contingencies and can be true or false in different circumstances. One such contingency is time, wherein some facts mutate over a given period, e.g., the president of a country or the winner of a championship. Trustworthy language models ideally identify mutable facts as such and process them accordingly. We create MuLan, a benchmark for evaluating the ability of English language models to anticipate time-contingency, covering both 1:1 and 1:N relations. We hypothesize that mutable facts are encoded differently than immutable ones, hence being easier to update. In a detailed evaluation of six popular large language models, we consistently find differences in the LLMs' confidence, representations, and update behavior, depending on the mutability of a fact. Our findings should inform future work on the injection of and induction of time-contingent knowledge to/from LLMs.""
}",fierroMuLanStudyFact2024a
