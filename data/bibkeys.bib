@inproceedings{abdelnabiCooperationCompetitionMaliciousness2024,
  title = {Cooperation, Competition, and Maliciousness: {{LLM-stakeholders}} Interactive Negotiation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Abdelnabi, Sahar and Gomaa, Amr and Sivaprasad, Sarath and Sch{\"o}nherr, Lea and Fritz, Mario},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {83548--83599},
  publisher = {Curran Associates, Inc.},
  join_key = {136}
}

@inproceedings{abdinKITABEvaluatingLLMs2024,
  title = {{{KITAB}}: {{Evaluating LLMs}} on Constraint Satisfaction for Information Retrieval},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Abdin, Marah I and Gunasekar, Suriya and Chandrasekaran, Varun and Li, Jerry and Yuksekgonul, Mert and Peshawaria, Rahee Ghosh and Naik, Ranjita and Nushi, Besmira},
  year = {2024},
  join_key = {47}
}

@inproceedings{aggarwalIndicXNLIEvaluatingMultilingual2022,
  title = {{{IndicXNLI}}: {{Evaluating}} Multilingual Inference for {{Indian}} Languages},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Aggarwal, Divyanshu {and} Gupta, Vivek {and} Kunchukuttan},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {10994--11006},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.755},
  abstract = {While Indic NLP has made rapid advances recently in terms of the availability of corpora and pre-trained models, benchmark datasets on standard NLU tasks are limited. To this end, we introduce INDICXNLI, an NLI dataset for 11 Indic languages. It has been created by high-quality machine translation of the original English XNLI dataset and our analysis attests to the quality of INDICXNLI. By finetuning different pre-trained LMs on this INDICXNLI, we analyze various cross-lingual transfer techniques with respect to the impact of the choice of language models, languages, multi-linguality, mix-language input, etc. These experiments provide us with useful insights into the behaviour of pre-trained models for a diverse set of languages.},
  join_key = {271}
}

@inproceedings{agrawalLargeLanguageModels2022,
  title = {Large Language Models Are Few-Shot Clinical Information Extractors},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Agrawal, Monica {and} Hegselmann, Stefan {and} Lang},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {1998--2022},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.130},
  abstract = {A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.},
  join_key = {259}
}

@inproceedings{ahujaMEGAMultilingualEvaluation2023,
  title = {{{MEGA}}: {{Multilingual}} Evaluation of Generative {{AI}}},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Ahuja, Kabir {and} Diddee, Harshita {and} Hada},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {4232--4267},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.258},
  abstract = {Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.},
  join_key = {313}
}

@inproceedings{ahujaMEGAVERSEBenchmarkingLarge2024,
  title = {{{MEGAVERSE}}: {{Benchmarking}} Large Language Models across Languages, Modalities, Models and Tasks},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Ahuja, Sanchit {and} Aggarwal, Divyanshu {and} Gumma},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {2598--2637},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.143},
  abstract = {There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.},
  join_key = {502}
}

@inproceedings{aiAdvancementGraphUnderstanding2024,
  title = {Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Ai, Qihang {and} Li, Jiafan {and} Dai},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {7485--7501},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.404},
  abstract = {Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning.Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5\%-15\% compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancements in graph data understanding.},
  join_key = {380}
}

@inproceedings{ajithLitSearchRetrievalBenchmark2024,
  title = {{{LitSearch}}: A Retrieval Benchmark for Scientific Literature Search},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Ajith, Anirudh {and} Xia, Mengzhou {and} Chevalier},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {15068--15083},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.840},
  abstract = {Literature search questions, such as ``where can I find research on the evaluation of consistency in generated summaries?'' pose significant challenges for modern search engines and retrieval systems. These questions often require a deep understanding of research concepts and the ability to reason over entire articles. In this work, we introduce LitSearch, a retrieval benchmark comprising 597 realistic literature search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions generated by GPT-4 based on paragraphs containing inline citations from research papers and (2) questions about recently published papers, manually written by their authors. All LitSearch questions were manually examined or edited by experts to ensure high quality. We extensively benchmark state-of-the-art retrieval models and also evaluate two LLM-based reranking pipelines. We find a significant performance gap between BM25 and state-of-the-art dense retrievers, with a 24.8\% difference in absolute recall@5. The LLM-based reranking strategies further improve the best-performing dense retriever by 4.4\%. Additionally, commercial search engines and research tools like Google Search perform poorly on LitSearch, lagging behind the best dense retriever by 32 points. Taken together, these results show that LitSearch is an informative new testbed for retrieval systems while catering to a real-world use case.},
  join_key = {473}
}

@inproceedings{akhbariSETLEXSEMCHALLENGEUsing2024,
  title = {{{SETLEXSEM CHALLENGE}}: {{Using}} Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Akhbari, Bardiya and Gawali, Manish and Dronen, Nicholas A.},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {50381--50400},
  publisher = {Curran Associates, Inc.},
  join_key = {205}
}

@inproceedings{akyurekDUnEDatasetUnified2023,
  title = {{{DUnE}}: {{Dataset}} for Unified Editing},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Aky{\"u}rek, Afra {and} Pan, Eric {and} Kuwanto},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {1847--1861},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.114},
  abstract = {Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model`s knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. ``Messi plays for Inter Miami'' confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the applications of language models expand, so do the diverse ways in which we wish to edit and refine their outputs. In this study, we broaden the scope of the editing problem to include an array of editing cases such as debiasing and rectifying reasoning errors and define an edit as any natural language expression that solicits a change in the model`s outputs. We are introducing DUnE, an editing benchmark where edits are natural language sentences and propose that DUnE presents a challenging yet relevant task. To substantiate this claim, we conduct an extensive series of experiments testing various editing approaches to address DUnE, demonstrating their respective strengths and weaknesses. We argue that retrieval-augmented language modeling can outperform specialized editing techniques and neither set of approaches has fully solved the generalized editing problem covered by our benchmark.},
  join_key = {304}
}

@inproceedings{alamCTIBenchBenchmarkEvaluating2024,
  title = {{{CTIBench}}: A Benchmark for Evaluating Llms in Cyber Threat Intelligence},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Alam, Md Tanvirul and Bhusal, Dipkamal and Nguyen, Le and Rastogi, Nidhi},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {50805--50825},
  publisher = {Curran Associates, Inc.},
  join_key = {140}
}

@inproceedings{albalakFETABenchmarkFewsample2022a,
  title = {{{FETA}}: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Albalak, Alon {and} Tuan, Yi-Lin {and} Jandaghi},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {10936--10953},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.751},
  abstract = {Task transfer, transferring knowledge contained in related tasks, holds the promise of reducing the quantity of labeled data required to fine-tune language models. Dialogue understanding encompasses many diverse tasks, yet task transfer has not been thoroughly studied in conversational AI. This work explores conversational task transfer by introducing FETA: a benchmark for FEw-sample TAsk transfer in open-domain dialogue.FETA contains two underlying sets of conversations upon which there are 10 and 7 tasks annotated, enabling the study of intra-dataset task transfer; task transfer without domain adaptation. We utilize three popular language models and three learning algorithms to analyze the transferability between 132 source-target task pairs and create a baseline for future work.We run experiments in the single- and multi-source settings and report valuable findings, e.g., most performance trends are model-specific, and span extraction and multiple-choice tasks benefit the most from task transfer.In addition to task transfer, FETA can be a valuable resource for future research into the efficiency and generalizability of pre-training datasets and model architectures, as well as for learning settings such as continual and multitask learning.},
  join_key = {270}
}

@inproceedings{alwajihPeacockFamilyArabic2024,
  title = {Peacock: A Family of {{Arabic}} Multimodal Large Language Models and Benchmarks},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Alwajih, Fakhraddin {and} Nagoudi, El Moatez Billah {and} Bhatia},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {12753--12776},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.689},
  abstract = {Multimodal large language models (MLLMs) have proven effective in a wide range of tasks that require complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, the success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, even those with large speaker populations, such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed *Peacock*, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce *Henna*, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturally-aware Arabic MLLMs. The GitHub repository for the *Peacock* project is available at [https://github.com/UBC-NLP/peacock](https://github.com/UBC-NLP/peacock).},
  join_key = {404}
}

@inproceedings{amarOpenAspBenchmarkMultidocument2023,
  title = {{{OpenAsp}}: A Benchmark for Multi-Document Open Aspect-Based Summarization},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Amar, Shmuel {and} Schiff, Liat {and} Ernst},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {1967--1991},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.121},
  abstract = {The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document open aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.},
  join_key = {305}
}

@inproceedings{anLevalInstitutingStandardized2024,
  title = {L-Eval: {{Instituting}} Standardized Evaluation for Long Context Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {An, Chenxin {and} Gong, Shansan {and} Zhong},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {14388--14411},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.776},
  abstract = {Recently, there has been growing interest in long-context scaling of large language models (LLMs). To facilitate research in this field, we propose L-Eval to institute a more standardized evaluation for Long-Context Language Models (LCLMs) addressing two key aspects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and more than 2,000 human-labeled query-response pairs including diverse task types, domains, and input length (3k{\textasciitilde}200k tokens). On the other hand, we investigate the effectiveness of evaluation metrics for LCLMs and we show that Length-instruction-enhanced (LIE) evaluation and LLM judges can better correlate with human judgments. We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of a more principled evaluation of these models.},
  join_key = {413}
}

@inproceedings{aroraCausalGymBenchmarkingCausal2024,
  title = {{{CausalGym}}: {{Benchmarking}} Causal Interpretability Methods on Linguistic Tasks},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Arora, Aryaman {and} Jurafsky, Dan {and} Potts},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {14638--14663},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.785},
  abstract = {Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.},
  join_key = {414}
}

@inproceedings{aroraHaveLLMsAdvanced2023,
  title = {Have {{LLMs}} Advanced Enough? {{A}} Challenging Problem Solving Benchmark for Large Language Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Arora, Daman {and} Singh, Himanshu {and} Mausam},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {7527--7543},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.468},
  abstract = {The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.},
  join_key = {320}
}

@inproceedings{asaiBUFFETBenchmarkingLarge2024,
  title = {{{BUFFET}}: {{Benchmarking}} Large Language Models for Few-Shot Cross-Lingual Transfer},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Asai, Akari {and} Kudugunta, Sneha {and} Yu},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {1771--1800},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.100},
  abstract = {Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.},
  join_key = {501}
}

@inproceedings{asthanaEvaluatingLLMsTargeted2024a,
  title = {Evaluating {{LLMs}} for Targeted Concept Simplification for Domain-Specific Texts},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Asthana, Sumit {and} Rashkin, Hannah {and} Clark},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {6208--6226},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.357},
  abstract = {One useful application of NLP models is to support people in reading complex text from unfamiliar domains (e.g., scientific articles). Simplifying the entire text makes it understandable but sometimes removes important details. On the contrary, helping adult readers understand difficult concepts in context can enhance their vocabulary and knowledge. In a preliminary human study, we first identify that lack of context and unfamiliarity with difficult concepts is a major reason for adult readers' difficulty with domain-specific text. We then introduce targeted concept simplification, a simplification task for rewriting text to help readers comprehend text containing unfamiliar concepts. We also introduce WikiDomains, a new dataset of 22k definitions from 13 academic domains paired with a difficult concept within each definition. We benchmark the performance of open-source and commercial LLMs and a simple dictionary baseline on this task across human judgments of ease of understanding and meaning preservation. Interestingly, our human judges preferred explanations about the difficult concept more than simplifications of the concept phrase. Further, no single model achieved superior performance across all quality dimensions, and automated metrics also show low correlations with human evaluations of concept simplification ({\textasciitilde}0.2), opening up rich avenues for research on personalized human reading comprehension support.},
  join_key = {442}
}

@inproceedings{athiwaratkunMultilingualEvaluationCode2023,
  title = {Multi-Lingual Evaluation of Code Generation Models},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Athiwaratkun, Ben and Gouda, Sanjay Krishna and Wang, Zijian and Li, Xiaopeng and Tian, Yuchen and Tan, Ming and Ahmad, Wasi Uddin and Wang, Shiqi and Sun, Qing and Shang, Mingyue and Gonugondla, Sujan Kumar and Ding, Hantian and Kumar, Varun and Fulton, Nathan and Farahani, Arash and Jain, Siddhartha and Giaquinto, Robert and Qian, Haifeng and Ramanathan, Murali Krishna and Nallapati, Ramesh and Ray, Baishakhi and Bhatia, Parminder and Sengupta, Sudipta and Roth, Dan and Xiang, Bing},
  year = {2023},
  join_key = {32}
}

@inproceedings{augustyniakThisWayDesigning2022,
  title = {This Is the Way: Designing and Compiling {{LEPISZCZE}}, a Comprehensive {{NLP}} Benchmark for {{Polish}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Augustyniak, Lukasz and Tagowski, Kamil and Sawczyn, Albert and Janiak, Denis and Bartusiak, Roman and Szymczak, Adrian and Janz, Arkadiusz and Szyma{\'n}ski, Piotr and W{\k a}troba, Marcin and Morzy, Miko{\l}aj and Kajdanowicz, Tomasz and Piasecki, Maciej},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {21805--21818},
  publisher = {Curran Associates, Inc.},
  join_key = {73}
}

@inproceedings{baiLongBenchBilingualMultitask2024,
  title = {{{LongBench}}: A Bilingual, Multitask Benchmark for Long Context Understanding},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Bai, Yushi {and} Lv, Xin {and} Zhang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {3119--3137},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.172},
  abstract = {Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.},
  join_key = {360}
}

@inproceedings{baiMTbench101FinegrainedBenchmark2024,
  title = {{{MT-bench-101}}: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Bai, Ge {and} Liu, Jie {and} Bu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {7421--7454},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.401},
  abstract = {The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at https://github.com/mtbench101/mt-bench-101.},
  join_key = {379}
}

@inproceedings{bajpaiCanLLMsReplace2024,
  title = {Can {{LLMs}} Replace Neil {{deGrasse}} Tyson? {{Evaluating}} the Reliability of {{LLMs}} as Science Communicators},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Bajpai, Prasoon {and} Chatterjee, Niladri {and} Dutta},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {15895--15912},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.889},
  abstract = {Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science communicators. Unlike existing benchmarks, our approach emphasizes assessing these models on scientific question-answering tasks that require a nuanced understanding and awareness of answerability. We introduce a novel dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria. We benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families. While most open-access models significantly underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects. We also find that even the GPT models exhibit a general incompetence in reliably verifying LLM responses. Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.},
  join_key = {478}
}

@inproceedings{bandarkarBelebeleBenchmarkParallel2024,
  title = {The Belebele Benchmark: A Parallel Reading Comprehension Dataset in 122 Language Variants},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Bandarkar, Lucas {and} Liang, Davis {and} Muller},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {749--775},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.44},
  abstract = {We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.},
  join_key = {349}
}

@inproceedings{bassaniGuardBenchLargescaleBenchmark2024,
  title = {{{GuardBench}}: A Large-Scale Benchmark for Guardrail Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Bassani, Elias {and} Sanchez, Ignacio},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {18393--18409},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1022},
  abstract = {Generative AI systems powered by Large Language Models have become increasingly popular in recent years. Lately, due to the risk of providing users with unsafe information, the adoption of those systems in safety-critical domains has raised significant concerns. To respond to this situation, input-output filters, commonly called guardrail models, have been proposed to complement other measures, such as model alignment. Unfortunately, the lack of a standard benchmark for guardrail models poses significant evaluation issues and makes it hard to compare results across scientific publications. To fill this gap, we introduce GuardBench, a large-scale benchmark for guardrail models comprising 40 safety evaluation datasets. To facilitate the adoption of GuardBench, we release a Python library providing an automated evaluation pipeline built on top of it. With our benchmark, we also share the first large-scale prompt moderation datasets in German, French, Italian, and Spanish. To assess the current state-of-the-art, we conduct an extensive comparison of recent guardrail models and show that a general-purpose instruction-following model of comparable size achieves competitive results without the need for specific fine-tuning.},
  join_key = {486}
}

@inproceedings{basuAPIBLENDComprehensiveCorpora2024,
  title = {{{API-BLEND}}: A Comprehensive Corpora for Training and Benchmarking {{API LLMs}}},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Basu, Kinjal {and} Abdelaziz, Ibrahim {and} Chaudhury},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {12859--12870},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.694},
  abstract = {There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes.},
  join_key = {406}
}

@inproceedings{beanLINGOLYBenchmarkOlympiadlevel2024a,
  title = {{{LINGOLY}}: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bean, Andrew and Hellsten, Simi and Mayne, Harry and Magomere, Jabez and A., Ethan and Chi, Ryan and Hale, Scott A. and Kirk, Hannah Rose},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {26224--26237},
  publisher = {Curran Associates, Inc.},
  join_key = {172}
}

@inproceedings{berdicevskisSuperlimSwedishLanguage2023,
  title = {Superlim: A {{Swedish}} Language Understanding Evaluation Benchmark},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Berdicevskis, Aleksandrs {and} Bouma, Gerlof {and} Kurtz},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {8137--8153},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.506},
  abstract = {We present Superlim, a multi-task NLP benchmark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. We describe the dataset, the tasks, the leaderboard and report the baseline results yielded by a reference implementation. The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark. We address methodological challenges, such as mitigating the Anglocentric bias when creating datasets for a less-resourced language; choosing the most appropriate measures; documenting the datasets and making the leaderboard convenient and transparent. We also highlight other potential usages of the dataset, such as, for instance, the evaluation of cross-lingual transfer learning.},
  join_key = {322}
}

@inproceedings{bhargavaDiscoSenseCommonsenseReasoning2022,
  title = {{{DiscoSense}}: {{Commonsense}} Reasoning with Discourse Connectives},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Bhargava, Prajjwal {and} Ng, Vincent},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {10295--10310},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.703},
  abstract = {We present DiscoSense, a benchmark for commonsense reasoning via understanding a wide variety of discourse connectives. We generate compelling distractors in DiscoSense using Conditional Adversarial Filtering, an extension of Adversarial Filtering that employs conditional generation. We show that state-of-the-art pre-trained language models struggle to perform well on DiscoSense, which makes this dataset ideal for evaluating next-generation commonsense reasoning systems.},
  join_key = {269}
}

@inproceedings{bhaskarBenchmarkingImprovingTexttoSQL2023a,
  title = {Benchmarking and Improving Text-to-{{SQL}} Generation under Ambiguity},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Bhaskar, Adithya {and} Tomar, Tushar {and} Sathe},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {7053--7074},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.436},
  abstract = {Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-k decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-k. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-k ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.},
  join_key = {318}
}

@inproceedings{bhatiaLocalConceptsUniversals2024,
  title = {From Local Concepts to Universals: {{Evaluating}} the Multicultural Understanding of Vision-Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Bhatia, Mehar {and} Ravi, Sahithya {and} Chinchure},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {6763--6782},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.385},
  abstract = {Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural inclusivity. Still, they have limited coverage of cultures and do not adequately assess cultural diversity across universal and culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures -- underscoring the necessity for enhancing multicultural understanding in vision-language models.},
  join_key = {445}
}

@inproceedings{bhuiyaSeeminglyPlausibleDistractors2024,
  title = {Seemingly Plausible Distractors in Multi-Hop Reasoning: {{Are}} Large Language Models Attentive Readers?},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Bhuiya, Neeladri {and} Schlegel, Viktor {and} Winkler},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {2514--2528},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.147},
  abstract = {State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on multi-hop reasoning---the ability to identify and integrate information from multiple textual sources.Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. We propose a challenging multi-hop reasoning benchmark by generating seemingly plausible multi-hop reasoning chains that ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs and show that their multi-hop reasoning performance is affected, as indicated by up to 45\% relative decrease in F1 score when presented with such seemingly plausible alternatives. We also find that---while LLMs tend to ignore misleading lexical cues---misleading reasoning paths indeed present a significant challenge. The code and data are made available at https://github.com/zawedcvg/Are-Large-Language-Models-Attentive-Readers},
  join_key = {433}
}

@inproceedings{billahnagoudiJASMINEArabicGPT2023,
  title = {{{JASMINE}}: {{Arabic GPT}} Models for Few-Shot Learning},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Billah Nagoudi, El Moatez {and} Abdul-Mageed, Muhammad {and} Elmadany},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {16721--16744},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.1040},
  abstract = {Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset ( 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a wide range of NLP tasks. We aim to responsibly release our models and evaluation benchmark with interested researchers, along with code for experimenting with them.},
  join_key = {345}
}

@inproceedings{bitton-guettaVisualRiddlesCommonsense2024,
  title = {Visual Riddles: A Commonsense and World Knowledge Challenge for Large Vision and Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {{Bitton-Guetta}, Nitzan and Slobodkin, Aviv and Maimon, Aviya and Habba, Eliya and Rassin, Royi and Bitton, Yonatan and Szpektor, Idan and Globerson, Amir and Elovici, Yuval},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {139561--139588},
  publisher = {Curran Associates, Inc.},
  join_key = {224}
}

@inproceedings{bittonVisITbenchDynamicBenchmark2023,
  title = {{{VisIT-bench}}: A Dynamic Benchmark for Evaluating Instruction-Following Vision-and-Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bitton, Yonatan and Bansal, Hritik and Hessel, Jack and Shao, Rulin and Zhu, Wanrong and Awadalla, Anas and Gardner, Josh and Taori, Rohan and Schmidt, Ludwig},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {26898--26922},
  publisher = {Curran Associates, Inc.},
  join_key = {107}
}

@inproceedings{bittonWinoGAViLGamifiedAssociation2022a,
  title = {{{WinoGAViL}}: {{Gamified}} Association Benchmark to Challenge Vision-and-Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bitton, Yonatan and Bitton Guetta, Nitzan and Yosef, Ron and Elovici, Yuval and Bansal, Mohit and Stanovsky, Gabriel and Schwartz, Roy},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {26549--26564},
  publisher = {Curran Associates, Inc.},
  join_key = {75}
}

@inproceedings{boginSUPEREvaluatingAgents2024,
  title = {{{SUPER}}: {{Evaluating}} Agents on Setting up and Executing Tasks from Research Repositories},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Bogin, Ben {and} Yang, Kejuan {and} Gupta},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {12622--12645},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.702},
  abstract = {Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPER aims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub-problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3\% of the end-to-end set, and 46.1\% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.},
  join_key = {466}
}

@inproceedings{boisvertWorkArenaCompositionalPlanning2024,
  title = {{{WorkArena}}++: {{Towards}} Compositional Planning and Reasoning-Based Common Knowledge Work Tasks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Boisvert, L{\'e}o and Thakkar, Megh and Gasse, Maxime and Caccia, Massimo and De Chezelles, Thibault Le Sellier and Cappart, Quentin and Chapados, Nicolas and Lacoste, Alexandre and Drouin, Alexandre},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {5996--6051},
  publisher = {Curran Associates, Inc.},
  join_key = {236}
}

@inproceedings{braunAGBDECorpusAutomated2024,
  title = {{{AGB-DE}}: A Corpus for the Automated Legal Assessment of Clauses in {{German}} Consumer Contracts},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Braun, Daniel {and} Matthes, Florian},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {10389--10405},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.559},
  abstract = {Legal tasks and datasets are often used as benchmarks for the capabilities of language models. However, openly available annotated datasets are rare. In this paper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts. Together with the data, we present a first baseline for the task of detecting potentially void clauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5. Our results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54. While the fine-tuned models often performed better with regard to precision, GPT-3.5 outperformed the other approaches with regard to recall. An analysis of the errors indicates that one of the main challenges could be the correct interpretation of complex clauses, rather than the decision boundaries of what is permissible and what is not.},
  join_key = {392}
}

@inproceedings{buchmannAttributeAbstainLarge2024a,
  title = {Attribute or Abstain: {{Large}} Language Models as Long Document Assistants},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Buchmann, Jan {and} Liu, Xiao {and} Gurevych},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {8113--8140},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.463},
  abstract = {LLMs can help humans working with long documents, but are known to hallucinate. *Attribution* can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiments with different approaches to attribution on 5 LLMs of different sizes. We find that *citation*, i.e. response generation and evidence extraction in one step, performs best for large and fine-tuned models, while additional retrieval can help for small, prompted models. We investigate whether the ``Lost in the Middle'' phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims. We release code and data for further investigation. [Link](https://github.com/UKPLab/arxiv2024-attribute-or-abstain)},
  join_key = {450}
}

@inproceedings{campagnanoSRL4ESemanticRole2022,
  title = {{{SRL4E}} -- {{Semantic Role Labeling}} for {{Emotions}}: {{A Unified Evaluation Framework}}},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Campagnano, Cesare {and} Conia, Simone {and} Navigli},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {4586--4601},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.314},
  abstract = {In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause. To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of automatic systems not only to distinguish between emotions, but also to capture their semantic constituents. However, currently available gold datasets are heterogeneous in size, domain, format, splits, emotion categories and role labels, making comparisons across different works difficult and hampering progress in the area. In this paper, we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions (SRL4E), in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme. We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task, hoping to provide the tools to facilitate studies in this complex area.},
  join_key = {254}
}

@inproceedings{caoSpider2vHowFar2024,
  title = {Spider2-v: {{How}} Far Are Multimodal Agents from Automating Data Science and Engineering Workflows?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Cao, Ruisheng and Lei, Fangyu and Wu, Haoyuan and Chen, Jixuan and Fu, Yeqiao and Gao, Hongcheng and Xiong, Xinzhuang and Zhang, Hanchong and Mao, Yuchen and Hu, Wenjing and Xie, Tianbao and Xu, Hongshen and Zhang, Danyang and Wang, Sida and Sun, Ruoxi and Yin, Pengcheng and Xiong, Caiming and Ni, Ansong and Liu, Qian and Zhong, Victor and Chen, Lu and Yu, Kai and Yu, Tao},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {107703--107744},
  publisher = {Curran Associates, Inc.},
  join_key = {209}
}

@inproceedings{caoWenMindComprehensiveBenchmark2024a,
  title = {{{WenMind}}: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Cao, Jiahuan and Liu, Yang and Shi, Yongxin and Ding, Kai and Jin, Lianwen},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {51358--51410},
  publisher = {Curran Associates, Inc.},
  join_key = {228}
}

@inproceedings{caoWorstPromptPerformance2024,
  title = {On the Worst Prompt Performance of Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Cao, Bowen and Cai, Deng and Zhang, Zhisong and Zou, Yuexian and Lam, Wai},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {69022--69042},
  publisher = {Curran Associates, Inc.},
  join_key = {196}
}

@inproceedings{casolaMultiPICoMultilingualPerspectivist2024a,
  title = {{{MultiPICo}}: {{Multilingual}} Perspectivist Irony Corpus},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Casola, Silvia {and} Frenda, Simona {and} Lo},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {16008--16021},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.849},
  abstract = {Recently, several scholars have contributed to the growth of a new theoretical framework in NLP called perspectivism. This approach aimsto leverage data annotated by different individuals to model diverse perspectives that affect their opinions on subjective phenomena such as irony. In this context, we propose MultiPICo, a multilingual perspectivist corpus of ironic short conversations in different languages andlinguistic varieties extracted from Twitter and Reddit. The corpus includes sociodemographic information about its annotators. Our analysis of the annotated corpus shows how different demographic cohorts may significantly disagree on their annotation of irony and how certain cultural factors influence the perception of the phenomenon and the agreement on the annotation. Moreover, we show how disaggregated annotations and rich annotator metadata can be exploited to benchmark the ability of large language models to recognize irony, their positionality with respect to sociodemographic groups, and the efficacy of perspective-taking prompting for irony detection in multiple languages.},
  join_key = {420}
}

@inproceedings{castillo-boladoPromptsDynamicConversational2024,
  title = {Beyond Prompts: {{Dynamic}} Conversational Benchmarking of Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {{Castillo-Bolado}, David and Davidson, Joseph and Gray, Finlay and Rosa, Marek},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {42528--42565},
  publisher = {Curran Associates, Inc.},
  join_key = {124}
}

@inproceedings{chakrabortyCounterTuringTest2023,
  title = {Counter {{Turing}} Test ({{CT2}}): {{AI-generated}} Text Detection Is Not as Easy as You May Think - Introducing {{AI}} Detectability Index ({{ADI}})},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Chakraborty, Megha {and} Tonmoy, S.M Towhidul Islam {and} Zaman},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {2206--2239},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.136},
  abstract = {With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter, signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that ``if the content is traditional elements of authorship produced by a machine, the work lacks human authorship and the office will not register it for copyright''. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a lower ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.},
  join_key = {307}
}

@inproceedings{chalkidisFairLexMultilingualBenchmark2022,
  title = {{{FairLex}}: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Chalkidis, Ilias {and} Pasini, Tommaso {and} Zhang},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {4389--4406},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.301},
  abstract = {We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.},
  join_key = {253}
}

@inproceedings{changDrspiderDiagnosticEvaluation2023,
  title = {Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-{{SQL}} Robustness},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Chang, Shuaichen and Wang, Jun and Dong, Mingwen and Pan, Lin and Zhu, Henghui and Li, Alexander Hanbo and Lan, Wuwei and Zhang, Sheng and Jiang, Jiarong and Lilien, Joseph and Ash, Steve and Wang, William Yang and Wang, Zhiguo and Castelli, Vittorio and Ng, Patrick and Xiang, Bing},
  year = {2023},
  join_key = {29}
}

@inproceedings{changLocalizationMethodsActually2024a,
  title = {Do Localization Methods Actually Localize Memorized Data in {{LLMs}}? {{A}} Tale of Two Benchmarks},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Chang, Ting-Yun {and} Thomason, Jesse {and} Jia},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {3190--3211},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.176},
  abstract = {The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these ``ground truth'' weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods identify neurons that are not specific to a single memorized sequence.},
  join_key = {505}
}

@inproceedings{chaoJailbreakBenchOpenRobustness2024,
  title = {{{JailbreakBench}}: {{An}} Open Robustness Benchmark for Jailbreaking Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chao, Patrick and Debenedetti, Edoardo and Robey, Alexander and Andriushchenko, Maksym and Croce, Francesco and Sehwag, Vikash and Dobriban, Edgar and Flammarion, Nicolas and Pappas, George J. and Tram{\`e}r, Florian and Hassani, Hamed and Wong, Eric},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {55005--55029},
  publisher = {Curran Associates, Inc.},
  join_key = {165}
}

@inproceedings{cheangCanLMsGeneralize2023,
  title = {Can {{LMs}} Generalize to Future Data? {{An}} Empirical Analysis on Text Summarization},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Cheang, Chi {and} Chan, Hou {and} Wong},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {16205--16217},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.1007},
  abstract = {Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data. In this work, we propose TempoSum, a novel benchmark that contains data samples from 2010 to 2022, to understand the temporal generalization ability of abstractive summarization models. Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Moreover, existing faithfulness enhancement methods cannot reliably improve the faithfulness of summarization models on future data. Finally, we discuss several recommendations to the research community on how to evaluate and improve the temporal generalization capability of text summarization models.},
  join_key = {343}
}

@inproceedings{chenAreWeRight2024,
  title = {Are We on the Right Way for Evaluating Large Vision-Language Models?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and Zhao, Feng},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {27056--27087},
  publisher = {Curran Associates, Inc.},
  join_key = {115}
}

@inproceedings{chenBenchmarkingRobustnessAdaptation2023,
  title = {Benchmarking Robustness of Adaptation Methods on Pre-Trained Vision-Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Shuo and Gu, Jindong and Han, Zhen and Ma, Yunpu and Torr, Philip and Tresp, Volker},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {51758--51777},
  publisher = {Curran Associates, Inc.},
  join_key = {80}
}

@inproceedings{chenCoINBenchmarkContinual2024,
  title = {{{CoIN}}: A Benchmark of Continual Instruction Tuning for Multimodel Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Cheng and Zhu, Junchen and Luo, Xu and Shen, Heng Tao and Song, Jingkuan and Gao, Lianli},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {57817--57840},
  publisher = {Curran Associates, Inc.},
  join_key = {133}
}

@inproceedings{chenCopyBenchMeasuringLiteral2024,
  title = {{{CopyBench}}: {{Measuring}} Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Chen, Tong {and} Asai, Akari {and} Mireshghallah},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {15134--15158},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.844},
  abstract = {Evaluating the degree of reproduction of copyright-protected content by language models (LMs) is of significant interest to the AI and legal communities. Although both literal and non-literal similarities are considered by courts when assessing the degree of reproduction, prior research has focused only on literal similarities. To bridge this gap, we introduce CopyBench, a benchmark designed to measure both literal and non-literal copying in LM generations. Using copyrighted fiction books as text sources, we provide automatic evaluation protocols to assess literal and non-literal copying, balanced against the model utility in terms of the ability to recall facts from the copyrighted works and generate fluent completions. We find that, although literal copying is relatively rare, two types of non-literal copying---event copying and character copying---occur even in models as small as 7B parameters. Larger models demonstrate significantly more copying, with literal copying rates increasing from 0.2\% to 10.5\% and non-literal copying from 2.3\% to 5.9\% when comparing Llama3-8B and 70B models, respectively. We further evaluate the effectiveness of current strategies for mitigating copying and show that (1) training-time alignment can reduce literal copying but may increase non-literal copying, and (2) current inference-time mitigation methods primarily reduce literal but not non-literal copying.},
  join_key = {475}
}

@inproceedings{chenCrosscareAssessingHealthcare2024,
  title = {Cross-Care: {{Assessing}} the Healthcare Implications of Pre-Training Data on Language Model Bias},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Shan and Gallifant, Jack and Gao, Mingye and Moreira, Pedro and Munch, Nikolaj and Muthukkumar, Ajay and Rajan, Arvind and Kolluri, Jaya and Fiske, Amelia and Hastings, Janna and Aerts, Hugo and Anthony, Brian and Celi, Leo Anthony and La Cava, William G. and Bitterman, Danielle S.},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {23756--23795},
  publisher = {Curran Associates, Inc.},
  join_key = {139}
}

@inproceedings{chenCurriculumBroadcoverageBenchmark2022,
  title = {Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Chen, Zeming {and} Gao, Qiyue},
  editor = {Carpuat, Marine {and} de Marneffe, Marie-Catherine {and} Meza Ruiz},
  year = {2022},
  month = jul,
  pages = {3204--3219},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.234},
  abstract = {In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models' abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.},
  join_key = {275}
}

@inproceedings{chenDrAcademyBenchmarkEvaluating2024,
  title = {Dr.{{Academy}}: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Chen, Yuyan {and} Wu, Chenwei {and} Yan},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {3138--3167},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.173},
  abstract = {Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs' capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored.In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles.Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl`s taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs' outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.},
  join_key = {361}
}

@inproceedings{chenExploringPotentialLarge2024a,
  title = {Exploring the Potential of Large Language Models in Computational Argumentation},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Chen, Guizhen {and} Cheng, Liying {and} Luu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {2309--2330},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.126},
  abstract = {Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors.},
  join_key = {357}
}

@inproceedings{chenFELMBenchmarkingFactuality2023a,
  title = {{{FELM}}: {{Benchmarking}} Factuality Evaluation of Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {{chen}, shiqi and Zhao, Yiran and Zhang, Jinghan and Chern, I-Chun and Gao, Siyang and Liu, Pengfei and He, Junxian},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {44502--44523},
  publisher = {Curran Associates, Inc.},
  join_key = {90}
}

@inproceedings{chengCanWeEdit2023,
  title = {Can We Edit Multimodal Large Language Models?},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Cheng, Siyuan {and} Tian, Bozhong {and} Liu},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {13877--13888},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.856},
  abstract = {In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights.},
  join_key = {336}
}

@inproceedings{chenGMAImmbenchComprehensiveMultimodal2024,
  title = {{{GMAI-mmbench}}: A Comprehensive Multimodal Evaluation Benchmark towards General Medical {{AI}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Pengcheng and Ye, Jin and Wang, Guoan and Li, Yanjun and Deng, Zhongying and Li, Wei and Li, Tianbin and Duan, Haodong and Huang, Ziyan and Su, Yanzhou and Wang, Benyou and Zhang, Shaoting and Fu, Bin and Cai, Jianfei and Zhuang, Bohan and Seibel, Eric J and Qiao, Yu and He, Junjun},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {94327--94427},
  publisher = {Curran Associates, Inc.},
  join_key = {158}
}

@inproceedings{chenLLMArenaAssessingCapabilities2024,
  title = {{{LLMArena}}: {{Assessing}} Capabilities of Large Language Models in Dynamic Multi-Agent Environments},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Chen, Junzhe {and} Hu, Xuming {and} Liu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {13055--13077},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.705},
  abstract = {Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings.},
  join_key = {408}
}

@inproceedings{chenM3CoTNovelBenchmark2024,
  title = {{{M}}{$^{3}$}{{CoT}}: A Novel Benchmark for Multi-Domain Multi-Step Multi-Modal Chain-of-Thought},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Chen, Qiguang {and} Qin, Libo {and} Zhang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {8199--8221},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.446},
  abstract = {Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M{$^3$}CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M{$^3$}CoT and there is a large gap between VLLMs and human performance in M{$^3$}CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M{$^3$}CoT will serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.},
  join_key = {382}
}

@inproceedings{chenMLLMasajudgeAssessingMultimodal2024,
  title = {{{MLLM-as-a-judge}}: {{Assessing}} Multimodal {{LLM-as-a-judge}} with Vision-Language Benchmark},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Chen, Dongping and Chen, Ruoxi and Zhang, Shilin and Wang, Yaochen and Liu, Yinuo and Zhou, Huichi and Zhang, Qihui and Wan, Yao and Zhou, Pan and Sun, Lichao},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {6562--6595},
  publisher = {PMLR},
  abstract = {Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, a closer examination reveals persistent challenges in the evaluative capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: https://mllm-judge.github.io/.},
  join_key = {21}
}

@inproceedings{chenPremiseOrderMatters2024,
  title = {Premise Order Matters in Reasoning with Large Language Models},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Chen, Xinyun and Chi, Ryan Andrew and Wang, Xuezhi and Zhou, Denny},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {6596--6620},
  publisher = {PMLR},
  abstract = {Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that even if the model performance is decent on the optimal order, permuting the premise order can cause a performance drop of over 30},
  join_key = {20}
}

@inproceedings{chenTheoremQATheoremdrivenQuestion2023a,
  title = {{{TheoremQA}}: A Theorem-Driven Question Answering Dataset},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Chen, Wenhu {and} Yin, Ming {and} Ku},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {7889--7901},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.489},
  abstract = {The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90\% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE\&CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4`s capabilities to solve these problems are unparalleled, achieving an accuracy of 51\% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15\%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs' capabilities to solve challenging science problems.},
  join_key = {321}
}

@inproceedings{chenToMBenchBenchmarkingTheory2024,
  title = {{{ToMBench}}: {{Benchmarking}} Theory of Mind in Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Chen, Zhuang {and} Wu, Jincenzi {and} Zhou},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {15959--15983},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.847},
  abstract = {Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10\% points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of LLMs' ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.},
  join_key = {419}
}

@inproceedings{chenWeakevalstrongEvaluatingEliciting2024,
  title = {Weak-Eval-Strong: {{Evaluating}} and Eliciting Lateral Thinking of Llms with Situation Puzzles},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Qi and Zhang, Bowen and Wang, Gang and Wu, Qi},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {79642--79665},
  publisher = {Curran Associates, Inc.},
  join_key = {227}
}

@inproceedings{chevalierLanguageModelsScience2024,
  title = {Language Models as Science Tutors},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Chevalier, Alexis and Geng, Jiayi and Wettig, Alexander and Chen, Howard and Mizera, Sebastian and Annala, Toni and Aragon, Max and Fanlo, Arturo Rodriguez and Frieder, Simon and Machado, Simon and Prabhakar, Akshara and Thieu, Ellie and Wang, Jiachen T. and Wang, Zirui and Wu, Xindi and Xia, Mengzhou and Xia, Wenhan and Yu, Jiatong and Zhu, Junjie and Ren, Zhiyong and Arora, Sanjeev and Chen, Danqi},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {8310--8335},
  publisher = {PMLR},
  abstract = {NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized in math have a 32K-token context window, and they excel at TutorEval while performing strongly on GSM8K and MATH. Our datasets build on open-source materials, and we release our models, data, and evaluations publicly.},
  join_key = {19}
}

@inproceedings{chiPLUELanguageUnderstanding2023,
  title = {{{PLUE}}: {{Language}} Understanding Evaluation Benchmark for Privacy Policies in {{English}}},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Chi, Jianfeng {and} Ahmad, Wasi Uddin {and} Tian},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {352--365},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-short.31},
  abstract = {Privacy policies provide individuals with information about their rights and how their personal information is handled. Natural language understanding (NLU) technologies can support individuals and practitioners to understand better privacy practices described in lengthy and complex documents. However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices. To this end, we introduce the Privacy Policy Language Understanding Evaluation (PLUE) benchmark, a multi-task benchmark for evaluating the privacy policy language understanding across various tasks. We also collect a large corpus of privacy policies to enable privacy policy domain-specific language model pre-training. We evaluate several generic pre-trained language models and continue pre-training them on the collected corpus. We demonstrate that domain-specific continual pre-training offers performance improvements across all tasks. The code and models are released at {$<$}a href="https://github.com/JFChi/PLUE"{$>$}https://github.com/JFChi/PLUE{$<$}/a{$>$}.},
  join_key = {297}
}

@inproceedings{chiyah-garciaRepairsBlockWorld2024,
  title = {Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {{Chiyah-Garcia}, Javier {and} Suglia, Alessandro {and} Eshghi},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {11523--11542},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.643},
  abstract = {In dialogue, the addressee may initially misunderstand the speaker and respond erroneously, often prompting the speaker to correct the misunderstanding in the next turn with a Third Position Repair (TPR). The ability to process and respond appropriately to such repair sequences is thus crucial in conversational AI systems. In this paper, we first collect, analyse, and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences in an instruction-following manipulation task that is, by design, rife with referential ambiguity. We employ this dataset to evaluate several state-of-the-art Vision and Language Models (VLM) across multiple settings, focusing on their capability to process and accurately respond to TPRs and thus recover from miscommunication. We find that, compared to humans, all models significantly underperform in this task. We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generalising better to new scenarios. Our results suggest that these models are not yet ready to be deployed in multi-modal collaborative settings where repairs are common, and highlight the need to design training regimes and objectives that facilitate learning from interaction. Our code and data are available at www.github.com/JChiyah/blockworld-repairs},
  join_key = {462}
}

@inproceedings{choiLLMsUnderstandSocial2023,
  title = {Do {{LLMs}} Understand Social Knowledge? {{Evaluating}} the Sociability of Large Language Models with {{SocKET}} Benchmark},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Choi, Minje {and} Pei, Jiaxin {and} Kumar},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {11370--11403},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.699},
  abstract = {Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand social language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor \& sarcasm, offensiveness, sentiment \& emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The resources are released at https://github.com/minjechoi/SOCKET.},
  join_key = {329}
}

@inproceedings{choiLoTabenchBenchmarkingLanguageoriented2024,
  title = {{{LoTa-bench}}: {{Benchmarking}} Language-Oriented Task Planners for Embodied Agents},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Choi, Jae-Woo and Yoon, Youngwoo and Ong, Hyobin and Kim, Jaehong and Jang, Minsu},
  year = {2024},
  join_key = {51}
}

@inproceedings{chungCanVisualLanguage2024,
  title = {Can Visual Language Models Resolve Textual Ambiguity with Visual Cues? {{Let}} Visual Puns Tell You!},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Chung, Jiwan {and} Lim, Seungwon {and} Jeon},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {2452--2469},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.144},
  abstract = {Humans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability?In response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases.},
  join_key = {432}
}

@inproceedings{chuTimeBenchComprehensiveEvaluation2024,
  title = {{{TimeBench}}: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Chu, Zheng {and} Chen, Jingchang {and} Chen},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {1204--1228},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.66},
  abstract = {Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly comprehending the intricacies of the world.Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark.To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena.TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models.We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings.Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning.Besides, LLMs exhibit capability discrepancies across different reasoning categories.Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges.We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning.Code and data are available at https://github.com/zchuz/TimeBench.},
  join_key = {352}
}

@inproceedings{coda-fornoCogBenchLargeLanguage2024,
  title = {{{CogBench}}: A Large Language Model Walks into a Psychology Lab},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {{Coda-Forno}, Julian and Binz, Marcel and Wang, Jane X and Schulz, Eric},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {9076--9108},
  publisher = {PMLR},
  abstract = {Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 40 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.},
  join_key = {18}
}

@inproceedings{comsaBenchmarkReasoningSpatial2023a,
  title = {A Benchmark for Reasoning with Spatial Prepositions},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Comsa, Iulia {and} Narayanan, Srini},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {16328--16335},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.1015},
  abstract = {Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes original datasets in English and Romanian and aims to probe the limits of reasoning about spatial relations in large language models. We use prompt engineering to study the performance of two families of large language models, PaLM and GPT-3, on our benchmark. Our results show considerable variability in the performance of smaller and larger models, as well as across prompts and languages. However, none of the models reaches human performance.},
  join_key = {344}
}

@inproceedings{corneliusBUSTBenchmarkEvaluation2024,
  title = {{{BUST}}: {{Benchmark}} for the Evaluation of Detectors of {{LLM-Generated Text}}},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Cornelius, Joseph {and} Lithgow-Serrano, Oscar {and} Mitrovic},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {8029--8057},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.444},
  abstract = {We introduce BUST, a comprehensive benchmark designed to evaluate detectors of texts generated by instruction-tuned large language models (LLMs). Unlike previous benchmarks, our focus lies on evaluating the performance of detector systems, acknowledging the inevitable influence of the underlying tasks and different LLM generators. Our benchmark dataset consists of 25K texts from humans and 7 LLMs responding to instructions across 10 tasks from 3 diverse sources. Using the benchmark, we evaluated 5 detectors and found substantial performance variance across tasks. A meta-analysis of the dataset characteristics was conducted to guide the examination of detector performance. The dataset was analyzed using diverse metrics assessing linguistic features like fluency and coherence, readability scores, and writer attitudes, such as emotions, convincingness, and persuasiveness. Features impacting detector performance were investigated with surrogate models, revealing emotional content in texts enhanced some detectors, yet the most effective detector demonstrated consistent performance, irrespective of writer`s attitudes and text styles. Our approach focused on investigating relationships between the detectors' performance and two key factors: text characteristics and LLM generators. We believe BUST will provide valuable insights into selecting detectors tailored to specific text styles and tasks and facilitate a more practical and in-depth investigation of detection systems for LLM-generated text.},
  join_key = {517}
}

@inproceedings{dasEXAMSVMultidisciplineMultilingual2024,
  title = {{{EXAMS-V}}: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Das, Rocktim {and} Hristov, Simeon {and} Li},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {7768--7791},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.420},
  abstract = {We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content in the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision--text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.},
  join_key = {381}
}

@inproceedings{davidsonEvaluatingLanguageModel2024,
  title = {Evaluating Language Model Agency through Negotiations},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Davidson, Tim Ruben and Veselovsky, Veniamin and Kosinski, Michal and West, Robert},
  year = {2024},
  join_key = {43}
}

@inproceedings{dengCOLDBenchmarkChinese2022,
  title = {{{COLD}}: A Benchmark for {{Chinese}} Offensive Language Detection},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Deng, Jiawen {and} Zhou, Jingyan {and} Sun},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {11580--11599},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.796},
  abstract = {Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark --COLD for Chinese offensive language analysis, including a Chinese Offensive Language Dataset --COLDATASET and a baseline detector --COLDETECTOR which is trained on the dataset. We show that the COLD benchmark contributes to Chinese offensive language detection which is challenging for existing resources. We then deploy the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained language models. We first analyze the offensiveness of existing generative models and show that these models inevitably expose varying degrees of offensive issues. Furthermore, we investigate the factors that influence the offensive generations, and we find that anti-bias contents and keywords referring to certain groups or revealing negative attitudes trigger offensive outputs easier.},
  join_key = {273}
}

@inproceedings{dengMobilebenchEvaluationBenchmark2024,
  title = {Mobile-Bench: {{An}} Evaluation Benchmark for {{LLM-based}} Mobile Agents},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Deng, Shihan {and} Xu, Weikai {and} Sun},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {8813--8831},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.478},
  abstract = {With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction.However, there is a scarcity of benchmarks available for LLM-based mobile agents.Benchmarking these agents generally faces three main challenges:(1) The inefficiency of UI-only operations imposes limitations to task evaluation.(2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents.(3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents.First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion.Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs.To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios.Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform will be released in the future.},
  join_key = {387}
}

@inproceedings{dengNewTermBenchmarkingRealtime2024a,
  title = {{{NewTerm}}: {{Benchmarking}} Real-Time New Terms for Large Language Models with Annual Updates},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Deng, Hexuan and Jiao, Wenxiang and Liu, Xuebo and Zhang, Min and Tu, Zhaopeng},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {35760--35795},
  publisher = {Curran Associates, Inc.},
  join_key = {193}
}

@inproceedings{devriesDUMBBenchmarkSmart2023,
  title = {{{DUMB}}: A Benchmark for Smart Evaluation of {{Dutch}} Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {{de Vries}, Wietse {and} Wieling, Martijn {and} Nissim},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {7221--7241},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.447},
  abstract = {We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models. Through a comparison of 14 pre-trained language models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition to highlighting best strategies for training larger Dutch models, DUMB will foster further research on Dutch. A public leaderboard is available at https://dumbench.nl.},
  join_key = {319}
}

@inproceedings{diaoDoolittleBenchmarksCorpora2023a,
  title = {Doolittle: {{Benchmarks}} and Corpora for Academic Writing Formalization},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Diao, Shizhe {and} Lei, Yongyu {and} Pan},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {13093--13111},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.809},
  abstract = {Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle.},
  join_key = {334}
}

@inproceedings{dingEasy2HardbenchStandardizedDifficulty2024,
  title = {{{Easy2Hard-bench}}: {{Standardized}} Difficulty Labels for Profiling {{LLM}} Performance and Generalization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ding, Mucong and Deng, Chenghao and Choo, Jocelyn and Wu, Zichu and Agrawal, Aakriti and Schwarzschild, Avi and Zhou, Tianyi and Goldstein, Tom and Langford, John and Anandkumar, Anima and Huang, Furong},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {44323--44365},
  publisher = {Curran Associates, Inc.},
  join_key = {147}
}

@inproceedings{dinhSciExBenchmarkingLarge2024a,
  title = {{{SciEx}}: {{Benchmarking}} Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Dinh, Tu Anh {and} Mullov, Carlos {and} B{\"a}rmann},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {11592--11610},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.647},
  abstract = {With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains. One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs. Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs' ability on solving scientific tasks. SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams. We evaluate the performance of various state-of-the-art LLMs on our new benchmark. Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance. Therefore, we provide human expert grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4\% exam grade on average. We also provide detailed comparisons between LLM performance and student performance on SciEx. To enable future evaluation of new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx. Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.},
  join_key = {463}
}

@inproceedings{doddapaneniLeavingNoIndic2023,
  title = {Towards Leaving No {{Indic}} Language behind: {{Building}} Monolingual Corpora, Benchmark and Models for {{Indic}} Languages},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Doddapaneni, Sumanth {and} Aralikatte, Rahul {and} Ramesh},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {12402--12426},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.693},
  abstract = {Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at {$<$}a href="https://github.com/AI4Bharat/IndicBERT"{$>$}https://github.com/AI4Bharat/IndicBERT{$<$}/a{$>$}.},
  join_key = {293}
}

@inproceedings{drouinWorkArenaHowCapable2024,
  title = {{{WorkArena}}: {{How}} Capable Are Web Agents at Solving Common Knowledge Work Tasks?},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Drouin, Alexandre and Gasse, Maxime and Caccia, Massimo and Laradji, Issam H. and Del Verme, Manuel and Marty, Tom and Vazquez, David and Chapados, Nicolas and Lacoste, Alexandre},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {11642--11662},
  publisher = {PMLR},
  abstract = {We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 33 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.},
  join_key = {17}
}

@inproceedings{duEmbSpatialbenchBenchmarkingSpatial2024,
  title = {{{EmbSpatial-bench}}: {{Benchmarking}} Spatial Understanding for Embodied Tasks with Large Vision-Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Du, Mengfei {and} Wu, Binhao {and} Li},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {346--355},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-short.33},
  abstract = {The recent rapid development of Large Vision-Language Models (LVLMs) has indicated their potential for embodied tasks. However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs. The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective. Experiments expose the insufficient capacity of current LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs' embodied spatial understanding.},
  join_key = {428}
}

@inproceedings{duMercuryCodeEfficiency2024,
  title = {Mercury: A Code Efficiency Benchmark for Code Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Du, Mingzhe and Tuan, Luu Anh and Ji, Bin and Liu, Qian and Ng, See-Kiong},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {16601--16622},
  publisher = {Curran Associates, Inc.},
  join_key = {182}
}

@inproceedings{dumpalaSUGARCREPEDatasetVisionlanguage2024,
  title = {{{SUGARCREPE}}++ Dataset: {{Vision-language}} Model Sensitivity to Semantic and Lexical Alterations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Dumpala, Sri Harsha and Jaiswal, Aman and Sastry, Chandramouli and Milios, Evangelos and Oore, Sageev and Sajjad, Hassan},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {17972--18018},
  publisher = {Curran Associates, Inc.},
  join_key = {215}
}

@inproceedings{duPAGEDBenchmarkProcedural2024,
  title = {{{PAGED}}: A Benchmark for Procedural Graphs Extraction from Documents},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Du, Weihong {and} Liao, Wenrui {and} Liang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {10829--10846},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.583},
  abstract = {Automatic extraction of procedural graphs from documents creates a low-cost way for users to easily understand a complex procedure by skimming visual graphs. Despite the progress in recent studies, it remains unanswered: whether the existing studies have well solved this task (Q1) and whether the emerging large language models (LLMs) can bring new opportunities to this task (Q2). To this end, we propose a new benchmark PAGED, equipped with a large high-quality dataset and standard evaluations. It investigates five state-of-the-art baselines, revealing that they fail to extract optimal procedural graphs well because of their heavy reliance on hand-written rules and limited available data. We further involve three advanced LLMs in PAGED and enhance them with a novel self-refine strategy. The results point out the advantages of LLMs in identifying textual elements and their gaps in building logical structures. We hope PAGED can serve as a major landmark for automatic procedural graph extraction and the investigations in PAGED can offer insights into the research on logic reasoning among non-sequential elements.},
  join_key = {395}
}

@inproceedings{edmanCUTEMeasuringLLMs2024,
  title = {{{CUTE}}: {{Measuring LLMs}}' Understanding of Their Tokens},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Edman, Lukas {and} Schmid, Helmut {and} Fraser},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {3017--3026},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.177},
  abstract = {Large Language Models (LLMs) show remarkable performance on a wide variety of tasks. Most LLMs split text into multi-character tokens and process them as atomic units without direct access to individual characters. This raises the question: To what extent can LLMs learn orthographic information? To answer this, we propose a new benchmark, CUTE, which features a collection of tasks designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs on CUTE, finding that most of them seem to know the spelling of their tokens, yet fail to use this information effectively to manipulate text, calling into question how much of this knowledge is generalizable.},
  join_key = {434}
}

@inproceedings{esiobuROBBIERobustBias2023,
  title = {{{ROBBIE}}: {{Robust}} Bias Evaluation of Large Generative Language Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Esiobu, David {and} Tan, Xiaoqing {and} Hosseini},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {3764--3814},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.230},
  abstract = {As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs.},
  join_key = {312}
}

@inproceedings{etxanizLatxaOpenLanguage2024,
  title = {Latxa: {{An}} Open Language Model and Evaluation Suite for {{Basque}}},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Etxaniz, Julen {and} Sainz, Oscar {and} Miguel},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {14952--14972},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.799},
  abstract = {We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,046 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.},
  join_key = {415}
}

@inproceedings{fanNPHardEvalDynamicBenchmark2024,
  title = {{{NPHardEval}}: {{Dynamic}} Benchmark on Reasoning Ability of Large Language Models via Complexity Classes},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Fan, Lizhou {and} Hua, Wenyue {and} Li},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {4092--4114},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.225},
  abstract = {Complex reasoning ability is one of the most important features of Large Language Models (LLMs). Numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, we introduce a new benchmark NPHardEval. It contains a broad spectrum of 900 algorithmic questions belonging up to the NP-Hard complexity class, offering a rigorous measure of the reasoning ability of LLMs utilizing computational complexity. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.},
  join_key = {366}
}

@inproceedings{fanR2HBuildingMultimodal2023,
  title = {{{R2H}}: {{Building}} Multimodal Navigation Helpers That Respond to Help Requests},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Fan, Yue {and} Gu, Jing {and} Zheng},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {14803--14819},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.915},
  abstract = {Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent`s ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, named SeeRee, and employing a multi-modal large language model in a zero-shot manner. Analysis of the task and method was conducted based on both automatic benchmarking and human evaluations.},
  join_key = {339}
}

@inproceedings{feiLawBenchBenchmarkingLegal2024a,
  title = {{{LawBench}}: {{Benchmarking}} Legal Knowledge of Large Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Fei, Zhiwei {and} Shen, Xiaoyu {and} Zhu},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {7933--7962},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.452},
  abstract = {We present LawBench, the first evaluation benchmark composed of 20 tasks aimed to assess the ability of Large Language Models (LLMs) to perform Chinese legal-related tasks. LawBench is meticulously crafted to enable precise assessment of LLMs' legal capabilities from three cognitive levels that correspond to the widely accepted Bloom`s cognitive taxonomy. Using LawBench, we present a comprehensive evaluation of 21 popular LLMs and the first comparative analysis of the empirical results in order to reveal their relative strengths and weaknesses. All data, model predictions and evaluation code are accessible from https://github.com/open-compass/LawBench.},
  join_key = {449}
}

@inproceedings{felknerWinoQueerCommunityintheloopBenchmark2023,
  title = {{{WinoQueer}}: A Community-in-the-Loop Benchmark for Anti-{{LGBTQ}}+ Bias in Large Language Models},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Felkner, Virginia {and} Chang, Ho-Chun Herbert {and} Jang},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {9126--9140},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.507},
  abstract = {We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.},
  join_key = {290}
}

@inproceedings{fenogenovaMERAComprehensiveLLM2024a,
  title = {{{MERA}}: A Comprehensive {{LLM}} Evaluation in {{Russian}}},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Fenogenova, Alena {and} Chervyakov, Artem {and} Martynov},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {9920--9948},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.534},
  abstract = {Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce a new instruction benchmark, MERA, oriented towards the FMs' performance on the Russian language. The benchmark encompasses 21 evaluation tasks for generative models covering 10 skills and is supplied with private answer scoring to prevent data leakage. The paper introduces a methodology to evaluate FMs and LMs in fixed zero- and few-shot instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential ethical concerns and drawbacks.},
  join_key = {389}
}

@inproceedings{fernandezSyllabusQACourseLogistics2024,
  title = {{{SyllabusQA}}: A Course Logistics Question Answering Dataset},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Fernandez, Nigel {and} Scarlatos, Alexander {and} Lan},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {10344--10369},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.557},
  abstract = {Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.},
  join_key = {391}
}

@inproceedings{fierroMuLanStudyFact2024a,
  title = {{{MuLan}}: A Study of Fact Mutability in Language Models},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 2: {{Short}} Papers)},
  author = {Fierro, Constanza {and} Garneau, Nicolas {and} Bugliarello},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {762--771},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-short.67},
  abstract = {Facts are subject to contingencies and can be true or false in different circumstances. One such contingency is time, wherein some facts mutate over a given period, e.g., the president of a country or the winner of a championship. Trustworthy language models ideally identify mutable facts as such and process them accordingly. We create MuLan, a benchmark for evaluating the ability of English language models to anticipate time-contingency, covering both 1:1 and 1:N relations. We hypothesize that mutable facts are encoded differently than immutable ones, hence being easier to update. In a detailed evaluation of six popular large language models, we consistently find differences in the LLMs' confidence, representations, and update behavior, depending on the mutability of a fact. Our findings should inform future work on the injection of and induction of time-contingent knowledge to/from LLMs.},
  join_key = {521}
}

@inproceedings{flachsGrammaticalErrorCorrection2020,
  title = {Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Flachs, Simon {and} Lacroix, Oph{\'e}lie {and} Yannakoudakis},
  editor = {Webber, Bonnie {and} Cohn, Trevor {and} He},
  year = {2020},
  month = nov,
  pages = {8467--8478},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.680},
  abstract = {Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.},
  join_key = {241}
}

@inproceedings{friederMathematicalCapabilitiesChatGPT2023a,
  title = {Mathematical Capabilities of {{ChatGPT}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Frieder, Simon and Pinchetti, Luca and Chevalier, Chevalier and Griffiths, Ryan-Rhys and Salvatori, Tommaso and Lukasiewicz, Thomas and Petersen, Philipp and Berner, Julius},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {27699--27744},
  publisher = {Curran Associates, Inc.},
  join_key = {98}
}

@inproceedings{gandhiUnderstandingSocialReasoning2023,
  title = {Understanding Social Reasoning in Language Models with Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Gandhi, Kanishk and Fraenken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {13518--13529},
  publisher = {Curran Associates, Inc.},
  join_key = {106}
}

@inproceedings{gaoEnablingLargeLanguage2023a,
  title = {Enabling Large Language Models to Generate Text with Citations},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Gao, Tianyu {and} Yen, Howard {and} Yu},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {6465--6488},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.398},
  abstract = {Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions---fluency, correctness, and citation quality---and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement---For example, on the ELI5 dataset, even the best models lack complete citation support 50\% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.},
  join_key = {316}
}

@inproceedings{garcia-ferreroThisNotDataset2023a,
  title = {This Is Not a Dataset: A Large Negation Benchmark to Challenge Large Language Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {{Garc{\'i}a-Ferrero}, Iker {and} Altuna, Bego{\~n}a {and} Alvez},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {8596--8615},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.531},
  abstract = {Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.},
  join_key = {323}
}

@inproceedings{geTGEA20Largescale2022,
  title = {{{TGEA}} 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ge, Huibin and Zhao, Xiaohu and Liu, Chuang and Zeng, Yulong and Liu, Qun and Xiong, Deyi},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {31612--31626},
  publisher = {Curran Associates, Inc.},
  join_key = {72}
}

@inproceedings{gharaeeBIOSCAN5MMultimodalDataset2024,
  title = {{{BIOSCAN-5M}}: A Multimodal Dataset for Insect Biodiversity},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Gharaee, Zahra and Lowe, Scott C. and Gong, ZeMing and Arias, Pablo Millan and Pellegrino, Nicholas and Wang, Austin T. and Haurum, Joakim Bruslund and Zarubiieva, Iuliia and Kari, Lila and Steinke, Dirk and Taylor, Graham W. and Fieguth, Paul and Chang, Angel X.},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {36285--36313},
  publisher = {Curran Associates, Inc.},
  join_key = {125}
}

@inproceedings{ghoshEPiCEmployingProverbs2022,
  title = {{{ePiC}}: {{Employing}} Proverbs in Context as a Benchmark for Abstract Language Understanding},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Ghosh, Sayan {and} Srivastava, Shashank},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {3989--4004},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.276},
  abstract = {While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored. Here, we introduce a high-quality crowdsourced dataset of narratives for employing proverbs in context as a benchmark for abstract language understanding. The dataset provides fine-grained annotation of aligned spans between proverbs and narratives, and contains minimal lexical overlaps between narratives and proverbs, ensuring that models need to go beyond surface-level reasoning to succeed. We explore three tasks: (1) proverb recommendation and alignment prediction, (2) narrative generation for a given proverb and topic, and (3) identifying narratives with similar motifs. Our experiments show that neural language models struggle on these tasks compared to humans, and these tasks pose multiple learning challenges.},
  join_key = {252}
}

@inproceedings{gingOpenendedVQABenchmarking2024,
  title = {Open-Ended {{VQA}} Benchmarking of {{Vision-Language}} Models by Exploiting {{Classification}} Datasets and Their Semantic Hierarchy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Ging, Simon and Bravo, Maria Alejandra and Brox, Thomas},
  year = {2024},
  join_key = {58}
}

@inproceedings{gongEvaluationLLMsSyntaxaware2024,
  title = {Evaluation of {{LLMs}} on Syntax-Aware Code Fill-in-the-Middle Tasks},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Gong, Linyuan and Wang, Sida and Elhoushi, Mostafa and Cheung, Alvin},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {15907--15928},
  publisher = {PMLR},
  abstract = {We introduce Syntax-Aware Fill-in-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com.},
  join_key = {16}
}

@inproceedings{guhaLegalBenchCollaborativelyBuilt2023,
  title = {{{LegalBench}}: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Guha, Neel and Nyarko, Julian and Ho, Daniel and R{\'e}, Christopher and Chilton, Adam and K, Aditya and {Chohlas-Wood}, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and Talisman, Dmitry and Hoque, Enam and Surani, Faiz and Fagan, Frank and Sarfaty, Galit and Dickinson, Gregory and Porat, Haggai and Hegland, Jason and Wu, Jessica and Nudell, Joe and Niklaus, Joel and Nay, John and Choi, Jonathan and Tobia, Kevin and Hagan, Margaret and Ma, Megan and Livermore, Michael and {Rasumov-Rahe}, Nikon and Holzenberger, Nils and Kolt, Noam and Henderson, Peter and Rehaag, Sean and Goel, Sharad and Gao, Shang and Williams, Spencer and Gandhi, Sunny and Zur, Tom and Iyer, Varun and Li, Zehua},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {44123--44279},
  publisher = {Curran Associates, Inc.},
  join_key = {95}
}

@inproceedings{guLanguageModelsHave2023,
  title = {Do Language Models Have Coherent Mental Models of Everyday Things?},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Gu, Yuling {and} Dalvi Mishra, Bhavana {and} Clark},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {1892--1913},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.106},
  abstract = {When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that ``the yolk surrounds the shell'' is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 ``X relation Y?'' true/false questions. Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent ``parts mental models'' (54-59\% accurate, 19-43\% conditional constraint violation). We propose an extension where we add a constraint satisfaction layer on top of the LM`s raw predictions to apply commonsense constraints. As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20\%), suggesting how the incoherence of the LM`s pictures of everyday things can be significantly reduced.},
  join_key = {281}
}

@inproceedings{guoCanLlmsSolve2024,
  title = {Can Llms Solve Molecule Puzzles? {{A}} Multimodal Benchmark for Molecular Structure Elucidation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Guo, Kehan and Nan, Bozhao and Zhou, Yujun and Guo, Taicheng and Guo, Zhichun and Surve, Mihir and Liang, Zhenwen and Chawla, Nitesh V. and Wiest, Olaf and Zhang, Xiangliang},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {134721--134746},
  publisher = {Curran Associates, Inc.},
  join_key = {128}
}

@inproceedings{guoOpenGroundedPlanning2024,
  title = {Open Grounded Planning: {{Challenges}} and Benchmark Construction},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Guo, Shiguang {and} Deng, Ziliang {and} Lin},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {4982--5003},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.272},
  abstract = {The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning task--open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning.},
  join_key = {369}
}

@inproceedings{guoRedCodeRiskyCode2024,
  title = {{{RedCode}}: {{Risky}} Code Execution and Generation Benchmark for Code Agents},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Guo, Chengquan and Liu, Xun and Xie, Chulin and Zhou, Andy and Zeng, Yi and Lin, Zinan and Song, Dawn and Li, Bo},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {106190--106236},
  publisher = {Curran Associates, Inc.},
  join_key = {199}
}

@inproceedings{guoWhatCanLarge2023,
  title = {What Can {{Large Language Models}} Do in Chemistry? {{A}} Comprehensive Benchmark on Eight Tasks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Guo, Taicheng and Guo, kehan and Nan, Bozhao and Liang, Zhenwen and Guo, Zhichun and Chawla, Nitesh and Wiest, Olaf and Zhang, Xiangliang},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {59662--59688},
  publisher = {Curran Associates, Inc.},
  join_key = {109}
}

@inproceedings{guptaBiphoneModelingInter2023,
  title = {Bi-Phone: {{Modeling}} Inter Language Phonetic Influences in Text},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Gupta, Abhirut {and} Sai, Ananya B. {and} Sproat},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {2580--2592},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.145},
  abstract = {A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1).We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2.These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing corrupted L2 text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web.We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly. We also introduce a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE. Finally, we also release the SuperGLUE benchmark to promote further research in phonetically robust language models. To the best of our knowledge, FunGLUE is the first benchmark to introduce L1-L2 interactions in text.},
  join_key = {282}
}

@inproceedings{guptaTempTabQATemporalQuestion2023,
  title = {{{TempTabQA}}: {{Temporal}} Question Answering for Semi-Structured Tables},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Gupta, Vivek {and} Kandoi, Pranshu {and} Vora},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {2431--2453},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.149},
  abstract = {Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset, we evaluate several state-of-the-art models for temporal reasoning. We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points. Given these results, our dataset has the potential to serve as a challenging benchmark to improve the temporal reasoning capabilities of NLP models.},
  join_key = {308}
}

@inproceedings{halevyFlexTapeCan`t2024,
  title = {``{{Flex}} Tape Can`t Fix That'': {{Bias}} and Misinformation in Edited Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Halevy, Karina H {and} Sotnikova, Anna {and} AlKhamissi},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {8690--8707},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.494},
  abstract = {Weight-based model editing methods update the parametric knowledge of language models post-training. However, these methods can unintentionally alter unrelated parametric knowledge representations, potentially increasing the risk of harm. In this work, we investigate how weight editing methods unexpectedly amplify model biases after edits. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias amplification of model editing methods for demographic traits such as race, geographic origin, and gender. We use Seesaw-CF to examine the impact of model editing on bias in five large language models. Our results demonstrate that edited models exhibit, to various degrees, more biased behavior for certain demographic groups than before they were edited, specifically becoming less confident in properties for Asian and African subjects. Additionally, editing facts about place of birth, country of citizenship, or gender has particularly negative effects on the model`s knowledge about unrelated properties, such as field of work, a pattern observed across multiple models.},
  join_key = {452}
}

@inproceedings{hallVisoGenderDatasetBenchmarking2023,
  title = {{{VisoGender}}: {{A}} Dataset for Benchmarking Gender Bias in Image-Text Pronoun Resolution},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hall, Siobhan Mackenzie and Gon{\c c}alves Abrantes, Fernanda and Zhu, Hanwen and Sodunke, Grace and Shtedritski, Aleksandar and Kirk, Hannah Rose},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {63687--63723},
  publisher = {Curran Associates, Inc.},
  join_key = {108}
}

@inproceedings{hanFOLIONaturalLanguage2024,
  title = {{{FOLIO}}: {{Natural}} Language Reasoning with First-Order Logic},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Han, Simeng {and} Schoelkopf, Hailey {and} Zhao},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {22017--22031},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1229},
  abstract = {Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.},
  join_key = {495}
}

@inproceedings{hanInstinctiveBiasSpurious2024,
  title = {The Instinctive Bias: {{Spurious}} Images Lead to Illusion in {{MLLMs}}},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Han, Tianyang {and} Lian, Qing {and} Pan},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {16163--16177},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.904},
  abstract = {Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from visual illusion. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the visual illusion level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs' robustness in the presence of misleading images. The code and datasets are available at https://github.com/MasaiahHan/CorrelationQA.},
  join_key = {479}
}

@inproceedings{hanMedSafetyBenchEvaluatingImproving2024,
  title = {{{MedSafetyBench}}: {{Evaluating}} and Improving the Medical Safety of Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Han, Tessa and Kumar, Aounon and Agarwal, Chirag and Lakkaraju, Himabindu},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {33423--33454},
  publisher = {Curran Associates, Inc.},
  join_key = {178}
}

@inproceedings{hanReadingBooksGreat2023,
  title = {Reading Books Is Great, but Not If You Are Driving! {{Visually}} Grounded Reasoning about Defeasible Commonsense Norms},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Han, Seungju {and} Kim, Junhyeok {and} Hessel},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {894--914},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.57},
  abstract = {Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying commonsense norms: NormLens. NormLens consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a simple yet effective approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code will be released.},
  join_key = {301}
}

@inproceedings{hardalovBgGLUEBulgarianGeneral2023,
  title = {{{bgGLUE}}: A {{Bulgarian}} General Language Understanding Evaluation Benchmark},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Hardalov, Momchil {and} Atanasova, Pepa {and} Mihaylov},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {8733--8759},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.487},
  abstract = {We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at {$<$}a href="https://bgglue.github.io"{$>$}https://bgglue.github.io{$<$}/a{$>$}, and we hope that it will enable further advancements in developing NLU models for Bulgarian.},
  join_key = {289}
}

@inproceedings{hareshClevrSkillsCompositionalLanguage2024,
  title = {{{ClevrSkills}}: {{Compositional}} Language and Visual Reasoning in Robotics},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Haresh, Sanjay and Dijkman, Daniel and Bhattacharyya, Apratim and Memisevic, Roland},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {38235--38266},
  publisher = {Curran Associates, Inc.},
  join_key = {132}
}

@inproceedings{hauserLargeLanguageModelsExpertlevel2024,
  title = {Large Language {{ModelsExpert-level}} Global History Knowledge Benchmark ({{HiST-LLM}})},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hauser, Jakob and Kondor, Daniel and Reddish, Jenny and Benam, Majid and Cioni, Enrico and Villa, Federica and Bennett, James S. and Hoyer, Daniel and Francois, Pieter and Turchin, Peter and {del Rio-Chanona}, R. Maria},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {32336--32369},
  publisher = {Curran Associates, Inc.},
  join_key = {169}
}

@inproceedings{hawkinsInvestigatingRepresentationsVerb2020,
  title = {Investigating Representations of Verb Bias in Neural Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Hawkins, Robert {and} Yamakoshi, Takateru {and} Griffiths},
  editor = {Webber, Bonnie {and} Cohn, Trevor {and} He},
  year = {2020},
  month = nov,
  pages = {4653--4663},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.376},
  abstract = {Languages typically provide more than one grammatical construction to express certain types of messages. A speaker`s choice of construction is known to depend on multiple factors, including the choice of main verb -- a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.},
  join_key = {238}
}

@inproceedings{heCanWeTrust2024a,
  title = {Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {He, Jianfeng {and} Yang, Runing {and} Yu},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {16514--16575},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.923},
  abstract = {Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques. Our code and data are available: https://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.},
  join_key = {480}
}

@inproceedings{heExploringCapacityPretrained2023a,
  title = {Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {He, Weinan {and} Huang, Canming {and} Xiao},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {4629--4643},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.255},
  abstract = {Reasoning about actions and change (RAC) is essential to understand and interact with the ever-changing environment. Previous AI research has shown the importance of fundamental and indispensable knowledge of actions, i.e., preconditions and effects. However, traditional methods rely on logical formalization which hinders practical applications. With recent transformer-based language models (LMs), reasoning over text is desirable and seemingly feasible, leading to the question of whether LMs can effectively and efficiently learn to solve RAC problems. We propose four essential RAC tasks as a comprehensive textual benchmark and generate problems in a way that minimizes the influence of other linguistic requirements (e.g., grounding) to focus on RAC. The resulting benchmark, TRAC, encompassing problems of various complexities, facilitates a more granular evaluation of LMs, precisely targeting the structural generalization ability much needed for RAC. Experiments with three high-performing transformers indicate that additional efforts are needed to tackle challenges raised by TRAC.},
  join_key = {285}
}

@inproceedings{helweMAFALDABenchmarkComprehensive2024a,
  title = {{{MAFALDA}}: A Benchmark and Comprehensive Study of Fallacy Detection and Classification},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Helwe, Chadi {and} Calamai, Tom {and} Paris},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {4810--4845},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.270},
  abstract = {We introduce MAFALDA, a benchmark for fallacy classification that merges and unites previous fallacy datasets. It comes with a taxonomy that aligns, refines, and unifies existing classifications of fallacies. We further provide a manual annotation of a part of the dataset together with manual explanations for each annotation. We propose a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity. We then evaluate several language models under a zero-shot learning setting and human performances on MAFALDA to assess their capability to detect and classify fallacies.},
  join_key = {510}
}

@inproceedings{heMedEvalMultilevelMultitask2023a,
  title = {{{MedEval}}: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {He, Zexue {and} Wang, Yu {and} Yan},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {8725--8744},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.540},
  abstract = {Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models. Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains, informing their practical applications and future advancements.},
  join_key = {324}
}

@inproceedings{hendrycksAligningAIShared2020a,
  title = {Aligning {{AI With Shared Human Values}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
  year = {2020},
  month = oct,
  urldate = {2025-04-11},
  abstract = {We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.},
  join_key = {28},
  langid = {english}
}

@inproceedings{hengleStillNotQuite2024,
  title = {Still Not Quite There! {{Evaluating}} Large Language Models for Comorbid Mental Health Diagnosis},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Hengle, Amey {and} Kulkarni, Atharva {and} Patankar},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {16698--16721},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.931},
  abstract = {In this study, we introduce ANGST, a novel, first of its kind benchmark for depression-anxiety comorbidity classification from social media posts. Unlike contemporary datasets that often oversimplify the intricate interplay between different mental health disorders by treating them as isolated conditions, ANGST enables multi-label classification, allowing each post to be simultaneously identified as indicating depression and/or anxiety. Comprising 2876 meticulously annotated posts by expert psychologists and an additional 7667 silver-labeled posts, ANGST posits a more representative sample of online mental health discourse. Moreover, we benchmark ANGST using various state-of-the-art language models, ranging from Mental-BERT to GPT-4. Our results provide significant insights into the capabilities and limitations of these models in complex diagnostic scenarios. While GPT-4 generally outperforms other models, none achieve an F1 score exceeding 72\% in multi-class comorbid classification, underscoring the ongoing challenges in applying language models to mental health diagnostics.},
  join_key = {481}
}

@inproceedings{heOlympiadBenchChallengingBenchmark2024a,
  title = {{{OlympiadBench}}: A Challenging Benchmark for Promoting {{AGI}} with Olympiad-Level Bilingual Multimodal Scientific Problems},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {He, Chaoqun {and} Luo, Renjie {and} Bai},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {3828--3850},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.211},
  abstract = {Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97\% on OlympiadBench, with a mere 10.74\% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at {$<$}a href="https://github.com/OpenBMB/OlympiadBench"{$>$}https://github.com/OpenBMB/OlympiadBench{$<$}/a{$>$}},
  join_key = {363}
}

@inproceedings{herediaXNLIeuDatasetCrosslingual2024,
  title = {{{XNLIeu}}: A Dataset for Cross-Lingual {{NLI}} in {{Basque}}},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Heredia, Maite {and} Etxaniz, Julen {and} Zulaika},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {4177--4188},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.234},
  abstract = {XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.},
  join_key = {507}
}

@inproceedings{heTGEAErrorannotatedDataset2021a,
  title = {{{TGEA}}: {{An}} Error-Annotated Dataset and Benchmark Tasks for {{TextGeneration}} from Pretrained Language Models},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {He, Jie {and} Peng, Bo {and} Liao},
  editor = {Zong, Chengqing {and} Xia, Fei {and} Li},
  year = {2021},
  month = aug,
  pages = {6012--6025},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.469},
  abstract = {In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.},
  join_key = {242}
}

@inproceedings{houWikiContradictBenchmarkEvaluating2024a,
  title = {{{WikiContradict}}: A Benchmark for Evaluating Llms on Real-World Knowledge Conflicts from Wikipedia},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hou, Yufang and Pascale, Alessandra and {Carnerero-Cano}, Javier and Tchrakian, Tigran and Marinescu, Radu and Daly, Elizabeth and Padhi, Inkit and Sattigeri, Prasanna},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {109701--109747},
  publisher = {Curran Associates, Inc.},
  join_key = {233}
}

@inproceedings{hoWikiWhyAnsweringExplaining2023,
  title = {{{WikiWhy}}: {{Answering}} and Explaining Cause-and-Effect Questions},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Ho, Matthew and Sharma, Aditya and Chang, Justin and Saxon, Michael and Levy, Sharon and Lu, Yujie and Wang, William Yang},
  year = {2023},
  join_key = {37}
}

@inproceedings{hsiehSugarCrepeFixingHackable2023,
  title = {{{SugarCrepe}}: {{Fixing}} Hackable Benchmarks for Vision-Language Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hsieh, Cheng-Yu and Zhang, Jieyu and Ma, Zixian and Kembhavi, Aniruddha and Krishna, Ranjay},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {31096--31116},
  publisher = {Curran Associates, Inc.},
  join_key = {104}
}

@inproceedings{huangCevalMultilevelMultidiscipline2023a,
  title = {C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and {lei}, jiayi and Fu, Yao and Sun, Maosong and He, Junxian},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {62991--63010},
  publisher = {Curran Associates, Inc.},
  join_key = {85}
}

@inproceedings{huangConMeRethinkingEvaluation2024,
  title = {{{ConMe}}: {{Rethinking}} Evaluation of Compositional Reasoning for Modern Vlms},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Huang, Irene and Lin, Wei and Mirza, M. Jehanzeb and Hansen, Jacob A. and Doveh, Sivan and Butoi, Victor Ion and Herzig, Roei and Arbelle, Assaf and Kuehne, Hilde and Darrell, Trevor and Gan, Chuang and Oliva, Aude and Feris, Rogerio and Karlinsky, Leonid},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {22927--22946},
  publisher = {Curran Associates, Inc.},
  join_key = {134}
}

@inproceedings{huangDAcodeAgentData2024,
  title = {{{DA-code}}: {{Agent}} Data Science Code Generation Benchmark for Large Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Huang, Yiming {and} Luo, Jianwen {and} Yu},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {13487--13521},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.748},
  abstract = {We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, including Python and SQL, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously designed the evaluation suite to ensure the accuracy and robustness of the evaluation. We developed the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5\% accuracy, leaving ample room for improvement. We release our benchmark at [link](https://github.com/yiyihum/dabench)},
  join_key = {468}
}

@inproceedings{huangEffiBenchBenchmarkingEfficiency2024,
  title = {{{EffiBench}}: {{Benchmarking}} the Efficiency of Automatically Generated Code},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Huang, Dong and Qing, Yuhao and Shang, Weiyi and Cui, Heming and Zhang, Jie M.},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {11506--11544},
  publisher = {Curran Associates, Inc.},
  join_key = {148}
}

@inproceedings{huangEmbraceDivergenceRicher2024,
  title = {Embrace Divergence for Richer Insights: A Multi-Document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Huang, Kung-Hsiang {and} Laban, Philippe {and} Fabbri},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {570--593},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.32},
  abstract = {Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40\% of the diverse information on average.},
  join_key = {497}
}

@inproceedings{huangFlamesBenchmarkingValue2024,
  title = {Flames: {{Benchmarking}} Value Alignment of {{LLMs}} in {{Chinese}}},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Huang, Kexin {and} Liu, Xiangyang {and} Guo},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {4551--4591},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.256},
  abstract = {The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and `topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findings indicate that all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions. We also develop a lightweight specified scorer capable of scoring LLMs across multiple dimensions to efficiently evaluate new models on the benchmark. The complexity of Flames has far exceeded existing benchmarks, setting a new challenge for contemporary LLMs and highlighting the need for further alignment of LLMs. Our benchmark is publicly available at https://github.com/AIFlames/Flames.},
  join_key = {509}
}

@inproceedings{huangMetaLogicLogicalReasoning2022,
  title = {{{MetaLogic}}: {{Logical}} Reasoning Explanations with Fine-Grained Structure},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Huang, Yinya {and} Zhang, Hongming {and} Hong},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {4698--4724},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.310},
  abstract = {In this paper, we propose a comprehensive benchmark to investigate models' logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models' performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.},
  join_key = {264}
}

@inproceedings{huangMetaToolBenchmarkLarge2024,
  title = {{{MetaTool}} Benchmark for Large Language Models: {{Deciding}} Whether to Use Tools and Which to Use},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Huang, Yue and Shi, Jiawen and Li, Yuan and Fan, Chenrui and Wu, Siyuan and Zhang, Qihui and Liu, Yixin and Zhou, Pan and Wan, Yao and Gong, Neil Zhenqiang and Sun, Lichao},
  year = {2024},
  join_key = {53}
}

@inproceedings{huangMLAgentBenchEvaluatingLanguage2024,
  title = {{{MLAgentBench}}: {{Evaluating}} Language Agents on Machine Learning Experimentation},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Huang, Qian and Vora, Jian and Liang, Percy and Leskovec, Jure},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {20271--20309},
  publisher = {PMLR},
  abstract = {A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with 37.5},
  join_key = {13}
}

@inproceedings{huangOlympicArenaBenchmarkingMultidiscipline2024a,
  title = {{{OlympicArena}}: {{Benchmarking}} Multi-Discipline Cognitive Reasoning for Superintelligent {{AI}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Huang, Zhen and Wang, Zengzhi and Xia, Shijie and Li, Xuefeng and Zou, Haoyang and Xu, Ruijie and Fan, Run-Ze and Ye, Lyumanshan and Chern, Ethan and Ye, Yixin and Zhang, Yikai and Yang, Yuqing and Wu, Ting and Wang, Binjie and Sun, Shichao and Xiao, Yang and Li, Yiyuan and Zhou, Fan and Chern, Steffi and Qin, Yiwei and Ma, Yan and Su, Jiadi and Liu, Yixiu and Zheng, Yuxiang and Zhang, Shaoting and Lin, Dahua and Qiao, Yu and Liu, Pengfei},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {19209--19253},
  publisher = {Curran Associates, Inc.},
  join_key = {195}
}

@inproceedings{huangPositionTrustLLMTrustworthiness2024,
  title = {Position: {{TrustLLM}}: {{Trustworthiness}} in Large Language Models},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and Sun, Hanchi and Liu, Zhengliang and Liu, Yixin and Wang, Yijue and Zhang, Zhikun and Vidgen, Bertie and Kailkhura, Bhavya and Xiong, Caiming and Xiao, Chaowei and Li, Chunyuan and Xing, Eric P. and Huang, Furong and Liu, Hao and Ji, Heng and Wang, Hongyi and Zhang, Huan and Yao, Huaxiu and Kellis, Manolis and Zitnik, Marinka and Jiang, Meng and Bansal, Mohit and Zou, James and Pei, Jian and Liu, Jian and Gao, Jianfeng and Han, Jiawei and Zhao, Jieyu and Tang, Jiliang and Wang, Jindong and Vanschoren, Joaquin and Mitchell, John and Shu, Kai and Xu, Kaidi and Chang, Kai-Wei and He, Lifang and Huang, Lifu and Backes, Michael and Gong, Neil Zhenqiang and Yu, Philip S. and Chen, Pin-Yu and Gu, Quanquan and Xu, Ran and Ying, Rex and Ji, Shuiwang and Jana, Suman and Chen, Tianlong and Liu, Tianming and Zhou, Tianyi and Wang, William Yang and Li, Xiang and Zhang, Xiangliang and Wang, Xiao and Xie, Xing and Chen, Xun and Wang, Xuyu and Liu, Yan and Ye, Yanfang and Cao, Yinzhi and Chen, Yong and Zhao, Yue},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {20166--20270},
  publisher = {PMLR},
  abstract = {Large language models (LLMs) have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and capability (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones, suggesting that open-source models can achieve high levels of trustworthiness without additional mechanisms like moderator, offering valuable insights for developers in this field. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Besides these observations, we've uncovered key insights into the multifaceted trustworthiness in LLMs. We emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. We advocate that the establishment of an AI alliance between industry, academia, the open-source community to foster collaboration is imperative to advance the trustworthiness of LLMs.},
  join_key = {14}
}

@inproceedings{huangRAVELEvaluatingInterpretability2024,
  title = {{{RAVEL}}: {{Evaluating}} Interpretability Methods on Disentangling Language Model Representations},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Huang, Jing {and} Wu, Zhengxuan {and} Potts},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {8669--8687},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.470},
  abstract = {Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.},
  join_key = {386}
}

@inproceedings{huangVLKEBLargeVisionlanguage2024,
  title = {{{VLKEB}}: A Large Vision-Language Model Knowledge Editing Benchmark},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Huang, Han and Zhong, Haitian and Yu, Tao and Liu, Qiang and Wu, Shu and Wang, Liang and Tan, Tieniu},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {9257--9280},
  publisher = {Curran Associates, Inc.},
  join_key = {225}
}

@inproceedings{huInfiAgentDABenchEvaluatingAgents2024,
  title = {{{InfiAgent-DABench}}: {{Evaluating}} Agents on Data Analysis Tasks},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Hu, Xueyu and Zhao, Ziyu and Wei, Shuang and Chai, Ziwei and Ma, Qianli and Wang, Guoyin and Wang, Xuwu and Su, Jing and Xu, Jingjing and Zhu, Ming and Cheng, Yao and Yuan, Jianbo and Li, Jiwei and Kuang, Kun and Yang, Yang and Yang, Hongxia and Wu, Fei},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {19544--19572},
  publisher = {PMLR},
  abstract = {In this paper, we introduce InfiAgent-DABench, the first benchmark specifically designed to evaluate LLM-based agents on data analysis tasks. Agents need to solve these tasks end-to-end by interacting with an execution environment. This benchmark contains DAEval, a dataset consisting of 603 data analysis questions derived from 124 CSV files, and an agent framework which incorporates LLMs to serve as data analysis agents for both serving and evaluating. Since data analysis questions are often open-ended and hard to evaluate without human supervision, we adopt a format-prompting technique to convert each question into a closed-form format so that they can be automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the current challenges encountered in data analysis tasks. In addition, building upon our agent framework, we develop a specialized agent, DAAgent, which surpasses GPT-3.5 by 3.9},
  join_key = {15}
}

@inproceedings{huiUDABenchmarkSuite2024a,
  title = {{{UDA}}: A Benchmark Suite for Retrieval Augmented Generation in Real-World Document Analysis},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hui, Yulong and Lu, Yao and Zhang, Huanchen},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {67200--67217},
  publisher = {Curran Associates, Inc.},
  join_key = {221}
}

@inproceedings{huSportsMetricsBlendingText2024,
  title = {{{SportsMetrics}}: {{Blending}} Text and Numerical Data to Understand Information Fusion in {{LLMs}}},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Hu, Yebowen {and} Song, Kaiqiang {and} Cho},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {267--278},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.17},
  abstract = {Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills.},
  join_key = {347}
}

@inproceedings{huUnderstandingFactualKnowledge2024,
  title = {Towards Understanding Factual Knowledge of Large Language Models},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Hu, Xuming and Chen, Junzhe and Li, Xiaochuan and Guo, Yufei and Wen, Lijie and Yu, Philip S. and Guo, Zhijiang},
  year = {2024},
  join_key = {63}
}

@inproceedings{hwangMultitaskBenchmarkKorean2022,
  title = {A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hwang, Wonseok and Lee, Dongjun and Cho, Kyoungyeon and Lee, Hanuhl and Seo, Minjoon},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {32537--32551},
  publisher = {Curran Associates, Inc.},
  join_key = {67}
}

@inproceedings{itoGeneralizationCapacityNeural2024,
  title = {On the Generalization Capacity of Neural Networks during Generic Multimodal Reasoning},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Ito, Takuya and Dan, Soham and Rigotti, Mattia and Kozloski, James and Campbell, Murray},
  year = {2024},
  join_key = {57}
}

@inproceedings{jacoviChainofthoughtStrongIts2024,
  title = {A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Jacovi, Alon {and} Bitton, Yonatan {and} Bohnet},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {4615--4634},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.254},
  abstract = {Prompting language models to provide step-by-step answers (e.g., ``Chain-of-Thought'') is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model`s answer, across a variety of datasets and state-of-the-art language models. Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions. Available at https://reveal-dataset.github.io/ .},
  join_key = {367}
}

@inproceedings{jainLanguageModelsHave2023,
  title = {Do Language Models Have a Common Sense Regarding Time? {{Revisiting}} Temporal Commonsense Reasoning in the Era of Large Language Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Jain, Raghav {and} Sojitra, Daivik {and} Acharya},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {6750--6774},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.418},
  abstract = {Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies. Additionally, we broaden the scope of our evaluation by including in our analysis 2 Code Generation LMs. Beyond broad benchmarking of models and prompts, we also conduct a fine-grained investigation of performance across different categories of temporal tasks. We further analyze the LLMs on varying temporal aspects, offering insights into their proficiency in understanding and predicting the continuity, sequence, and progression of events over time. Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain.},
  join_key = {317}
}

@inproceedings{jainR2ETurningAny2024,
  title = {{{R2E}}: {{Turning}} Any Github Repository into a Programming Agent Environment},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Jain, Naman and Shetty, Manish and Zhang, Tianjun and Han, King and Sen, Koushik and Stoica, Ion},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {21196--21224},
  publisher = {PMLR},
  abstract = {While Large Language Models' (LLMs) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI coding agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R2E code is available at https://r2e.dev/},
  join_key = {12}
}

@inproceedings{jangTemporalWikiLifelongBenchmark2022a,
  title = {{{TemporalWiki}}: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Jang, Joel {and} Ye, Seonghyeon {and} Lee},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {6237--6250},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.418},
  abstract = {Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM`s ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.},
  join_key = {266}
}

@inproceedings{jhaSeeGULLStereotypeBenchmark2023,
  title = {{{SeeGULL}}: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Jha, Akshita {and} Mostafazadeh Davani, Aida {and} Reddy},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {9851--9870},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.548},
  abstract = {Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America.},
  join_key = {291}
}

@inproceedings{jiangBRAINTEASERLateralThinking2023,
  title = {{{BRAINTEASER}}: {{Lateral}} Thinking Puzzles for Large Language Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Jiang, Yifan {and} Ilievski, Filip {and} Ma},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {14317--14332},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.885},
  abstract = {The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BrainTeaser: a multiple-choice Question Answering task designed to test the model`s ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BrainTeaser based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.},
  join_key = {337}
}

@inproceedings{jiangFollowBenchMultilevelFinegrained2024,
  title = {{{FollowBench}}: A Multi-Level Fine-Grained Constraints Following Benchmark for Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Jiang, Yuxin {and} Wang, Yufei {and} Zeng},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {4667--4688},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.257},
  abstract = {The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs' outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.},
  join_key = {368}
}

@inproceedings{jiangGenRESRethinkingEvaluation2024a,
  title = {{{GenRES}}: {{Rethinking}} Evaluation for Generative Relation Extraction in the Era of Large Language Models},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Jiang, Pengcheng {and} Lin, Jiacheng {and} Wang},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {2820--2837},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.155},
  abstract = {The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities can cause hallucinations. Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent with human preferences for RE quality. Last, we made a comprehensive evaluation of fourteen leading LLMs using GenRES across document, bag, and sentence level RE datasets, respectively, to set the benchmark for future research in GRE},
  join_key = {504}
}

@inproceedings{jiangXFACTRMultilingualFactual2020a,
  title = {X-{{FACTR}}: {{Multilingual}} Factual Knowledge Retrieval from Pretrained Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Jiang, Zhengbao {and} Anastasopoulos, Antonios {and} Araki},
  editor = {Webber, Bonnie {and} Cohn, Trevor {and} He},
  year = {2020},
  month = nov,
  pages = {5943--5959},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.479},
  abstract = {Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as ``Punta Cana is located in \_.'' However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at {$<$}a href="https://x-factr.github.io"{$>$}https://x-factr.github.io{$<$}/a{$>$}.},
  join_key = {240}
}

@inproceedings{jiLargeLanguageModels2024,
  title = {Large Language Models as Automated Aligners for Benchmarking Vision-Language Models},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Ji, Yuanfeng and GE, Chongjian and Kong, Weikai and Xie, Enze and Liu, Zhengying and Li, Zhenguo and Luo, Ping},
  year = {2024},
  join_key = {49}
}

@inproceedings{jinCanLargeLanguage2024,
  title = {Can Large Language Models Infer Causation from Correlation?},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Jin, Zhijing and Liu, Jiarui and LYU, Zhiheng and Poff, Spencer and Sachan, Mrinmaya and Mihalcea, Rada and Diab, Mona T. and Sch{\"o}lkopf, Bernhard},
  year = {2024},
  join_key = {41}
}

@inproceedings{jinJailbreakingLargeLanguage2024,
  title = {Jailbreaking Large Language Models against Moderation Guardrails via Cipher Characters},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Jin, Haibo and Zhou, Andy and Menke, Joe D. and Wang, Haohan},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {59408--59435},
  publisher = {Curran Associates, Inc.},
  join_key = {166}
}

@inproceedings{jinMARPLEBenchmarkLonghorizon2024,
  title = {{{MARPLE}}: A Benchmark for Long-Horizon Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Jin, Emily and Huang, Zhuoyi and Fr{\"a}nken, Jan-Philipp and Liu, Weiyu and Cha, Hannah and Brockbank, Erik and Wu, Sarah and Zhang, Ruohan and Wu, Jiajun and Gerstenberg, Tobias},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {108824--108850},
  publisher = {Curran Associates, Inc.},
  join_key = {174}
}

@inproceedings{jinMMToMQAMultimodalTheory2024,
  title = {{{MMToM-QA}}: {{Multimodal}} Theory of Mind Question Answering},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Jin, Chuanyang {and} Wu, Yutong {and} Cao},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {16077--16102},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.851},
  abstract = {Theory of Mind (ToM), the ability to understand people`s mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets -- either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person`s mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person`s activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.},
  join_key = {422}
}

@inproceedings{jinRWKUBenchmarkingRealworld2024a,
  title = {{{RWKU}}: {{Benchmarking}} Real-World Knowledge Unlearning for Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {{jin}, Zhuoran and Cao, Pengfei and Wang, Chenhao and He, Zhitao and Yuan, Hongbang and Li, Jiachun and Chen, Yubo and Liu, Kang and Zhao, Jun},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {98213--98263},
  publisher = {Curran Associates, Inc.},
  join_key = {202}
}

@inproceedings{jinShoppingMMLUMassive2024,
  title = {Shopping {{MMLU}}: A Massive Multi-Task Online Shopping Benchmark for Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Jin, Yilun and Li, Zheng and Zhang, Chenwei and Cao, Tianyu and Gao, Yifan and Jayarao, Pratik and Li, Mao and Liu, Xin and Sarkhel, Ritesh and Tang, Xianfeng and Wang, Haodong and Wang, Zhengyang and Xu, Wenju and Yang, Jingfeng and Yin, Qingyu and Li, Xian and Nigam, Priyanka and Xu, Yi and Chen, Kai and Yang, Qiang and Jiang, Meng and Yin, Bing},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {18062--18089},
  publisher = {Curran Associates, Inc.},
  join_key = {207}
}

@inproceedings{josephFactPICOFactualityEvaluation2024a,
  title = {{{FactPICO}}: {{Factuality}} Evaluation for Plain Language Summarization of Medical Evidence},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Joseph, Sebastian {and} Chen, Lily {and} Trienes},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {8437--8464},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.459},
  abstract = {Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.},
  join_key = {384}
}

@inproceedings{joshiILTURBenchmarkIndian2024,
  title = {{{IL-TUR}}: {{Benchmark}} for {{Indian}} Legal Text Understanding and Reasoning},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Joshi, Abhinav {and} Paul, Shounak {and} Sharma},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {11460--11499},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.618},
  abstract = {Legal systems worldwide are inundated with exponential growth in cases and documents. There is an imminent need to develop NLP and ML techniques for automatically processing and understanding legal documents to streamline the legal system. However, evaluating and comparing various NLP models designed specifically for the legal domain is challenging. This paper addresses this challenge by proposing : Benchmark for Indian Legal Text Understanding and Reasoning. contains monolingual (English, Hindi) and multi-lingual (9 Indian languages) domain-specific tasks that address different aspects of the legal system from the point of view of understanding and reasoning over Indian legal documents. We present baseline models (including LLM-based) for each task, outlining the gap between models and the ground truth. To foster further research in the legal domain, we create a leaderboard (available at: https://exploration-lab.github.io/IL-TUR/ ) where the research community can upload and compare legal text understanding systems.},
  join_key = {399}
}

@inproceedings{kalyanWikiDONewBenchmark2024a,
  title = {{{WikiDO}}: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kalyan, T Pavan and Pasi, Piyush Singh and Dharod, Sahil Nilesh and Motiwala, Azeem Azaz and Jyothi, Preethi and Chaudhary, Aditi and Srinivasan, Krishna},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {140812--140827},
  publisher = {Curran Associates, Inc.},
  join_key = {234}
}

@inproceedings{kannenAestheticsCulturalCompetence2024,
  title = {Beyond Aesthetics: {{Cultural}} Competence in Text-to-Image Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kannen, Nithish and Ahmad, Arif and Andreetto, Marco and Prabhakaran, Vinodkumar and Prabhu, Utsav and Dieng, Adji Bousso and Bhattacharyya, Pushpak and Dave, Shachi},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {13716--13747},
  publisher = {Curran Associates, Inc.},
  join_key = {123}
}

@inproceedings{karpinskaOneThousandOne2024a,
  title = {One {{Thousand}} and {{One Pairs}}: {{A}} ``Novel'' Challenge for Long-Context Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Karpinska, Marzena {and} Thai, Katherine {and} Lo},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {17048--17085},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.948},
  abstract = {Synthetic long-context LLM benchmarks (e.g., ``needle-in-the-haystack'') test only surface-level retrieval capabilities; but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest pair accuracy at 55.8\%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.},
  join_key = {484}
}

@inproceedings{kasaiRealTimeQAWhats2023,
  title = {{{RealTime QA}}: {{Whats}} the Answer Right Now?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kasai, Jungo and Sakaguchi, Keisuke and {takahashi}, yoichi and Le Bras, Ronan and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {49025--49043},
  publisher = {Curran Associates, Inc.},
  join_key = {102}
}

@inproceedings{kasnerTraditionalBenchmarksAnalyzing2024a,
  title = {Beyond Traditional Benchmarks: {{Analyzing}} Behaviors of Open {{LLMs}} on Data-to-Text Generation},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kasner, Zden{\v e}k {and} Dusek, Ondrej},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {12045--12072},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.651},
  abstract = {We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design Quintd - a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with Quintd. However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80\% of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs.},
  join_key = {402}
}

@inproceedings{kazemiBoardgameQADatasetNatural2023a,
  title = {{{BoardgameQA}}: A Dataset for Natural Language Reasoning with Contradictory Information},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kazemi, Mehran and Yuan, Quan and Bhatia, Deepti and Kim, Najoung and Xu, Xin and Imbrasaite, Vaiva and Ramachandran, Deepak},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {39052--39074},
  publisher = {Curran Associates, Inc.},
  join_key = {81}
}

@inproceedings{kesenViLMAZeroshotBenchmark2024,
  title = {{{ViLMA}}: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Kesen, Ilker and Pedrotti, Andrea and Dogan, Mustafa and Cafagna, Michele and Acikgoz, Emre Can and Parcalabescu, Letitia and Calixto, Iacer and Frank, Anette and Gatt, Albert and Erdem, Aykut and Erdem, Erkut},
  year = {2024},
  join_key = {64}
}

@inproceedings{kewBLESSBenchmarkingLarge2023a,
  title = {{{BLESS}}: {{Benchmarking}} Large Language Models on Sentence Simplification},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Kew, Tannon {and} Chi, Alison {and} V{\'a}squez-Rodr{\'i}guez},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {13291--13309},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.821},
  abstract = {We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics.},
  join_key = {335}
}

@inproceedings{khandekarMedCalcbenchEvaluatingLarge2024a,
  title = {{{MedCalc-bench}}: {{Evaluating}} Large Language Models for Medical Calculations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Khandekar, Nikhil and Jin, Qiao and Xiong, Guangzhi and Dunn, Soren and Applebaum, Serina S and Anwar, Zain and {Sarfo-Gyamfi}, Maame and Safranek, Conrad W and Anwar, Abid A and Zhang, Andrew and Gilson, Aidan and Singer, Maxwell B and Dave, Amisha and Taylor, Andrew and Zhang, Aidong and Chen, Qingyu and Lu, Zhiyong},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {84730--84745},
  publisher = {Curran Associates, Inc.},
  join_key = {175}
}

@inproceedings{khanXCodeEvalExecutionbasedLarge2024,
  title = {{{XCodeEval}}: {{An}} Execution-Based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Khan, Mohammad Abdullah Matin {and} Bari, M Saiful {and} Do},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {6766--6805},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.367},
  abstract = {Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce *xCodeEval*, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples (16.5 B tokens) from about 7.5 K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual code execution engine, *ExecEval* that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI`s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate to be quite challenging as per the current advancements in language models.},
  join_key = {376}
}

@inproceedings{kimCarpeDiemEvaluation2024,
  title = {Carpe Diem: {{On}} the Evaluation of World Knowledge in Lifelong Language Models},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Kim, Yujin {and} Yoon, Jaehong {and} Ye},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {5401--5415},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.302},
  abstract = {The dynamic nature of knowledge in an ever-changing world presents challenges for language models trained on static data; the model in the real world often requires not only acquiring new knowledge but also overwriting outdated information into updated ones. To study the ability of language models for these time-dependent dynamics in human language, we introduce a novel task, EvolvingQA, a temporally evolving question-answering benchmark designed for training and evaluating LMs on an evolving Wikipedia database. The construction of EvolvingQA is automated with our pipeline using large language models. We uncover that existing continual learning baselines suffer from updating and removing outdated knowledge. Our analysis suggests that models fail to rectify knowledge due to small weight gradients. In addition, we elucidate that language models particularly struggle to reflect the change of numerical or temporal information. Our work aims to model the dynamic nature of real-world information, suggesting faithful evaluations of the evolution-adaptability of language models. Our data construction code and dataset files are available at https://github.com/kimyuji/EvolvingQA\_benchmark.},
  join_key = {512}
}

@inproceedings{kimFANToMBenchmarkStresstesting2023,
  title = {{{FANToM}}: A Benchmark for Stress-Testing Machine Theory of Mind in Interactions},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Kim, Hyunwoo {and} Sclar, Melanie {and} Zhou},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {14397--14413},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.890},
  abstract = {Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.},
  join_key = {338}
}

@inproceedings{kohVisualWebArenaEvaluatingMultimodal2024,
  title = {{{VisualWebArena}}: {{Evaluating}} Multimodal Agents on Realistic Visual Web Tasks},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Koh, Jing Yu {and} Lo, Robert {and} Jang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {881--905},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.50},
  abstract = {Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on *realistic visually grounded tasks*. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.},
  join_key = {350}
}

@inproceedings{konIaCevalCodeGeneration2024,
  title = {{{IaC-eval}}: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kon, Patrick Tser Jern and Liu, Jiachen and Qiu, Yiming and Fan, Weijun and He, Ting and Lin, Lei and Zhang, Haoran and Park, Owen M. and Elengikal, George S. and Kang, Yuxin and Chen, Ang and Chowdhury, Mosharaf and Lee, Myungjin and Wang, Xinyu},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {134488--134506},
  publisher = {Curran Associates, Inc.},
  join_key = {162}
}

@inproceedings{kotoLargeLanguageModels2023,
  title = {Large Language Models Only Pass Primary School Exams in {{Indonesia}}: A Comprehensive Test on {{IndoMMLU}}},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Koto, Fajri {and} Aisyah, Nurul {and} Li},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {12359--12374},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.760},
  abstract = {Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46\% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.},
  join_key = {332}
}

@inproceedings{kotturSIMMC20Taskoriented2021,
  title = {{{SIMMC}} 2.0: A Task-Oriented Dialog Dataset for Immersive Multimodal Conversations},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Kottur, Satwik {and} Moon, Seungwhan {and} Geramifard},
  editor = {Moens, Marie-Francine {and} Huang, Xuanjing {and} Specia},
  year = {2021},
  month = nov,
  pages = {4903--4912},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.401},
  abstract = {Next generation task-oriented dialog systems need to understand conversational contexts with their perceived surroundings, to effectively help users in the real-world multimodal environment. Existing task-oriented dialog datasets aimed towards virtual assistance fall short and do not situate the dialog in the user`s multimodal context. To overcome, we present a new dataset for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which includes 11K task-oriented user{\textexclamdown}-{\textquestiondown}assistant dialogs (117K utterances) in the shopping domain, grounded in immersive and photo-realistic scenes. The dialogs are collection using a two-phase pipeline: (1) A novel multimodal dialog simulator generates simulated dialog flows, with an emphasis on diversity and richness of interactions, (2) Manual paraphrasing of generating utterances to draw from natural language distribution. We provide an in-depth analysis of the collected dataset, and describe in detail the four main benchmark tasks we propose for SIMMC 2.0. Our baseline model, powered by the state-of-the-art language model, shows promising results, and highlights new challenges and directions for the community to study.},
  join_key = {245}
}

@inproceedings{krojerAreDiffusionModels2023a,
  title = {Are Diffusion Models Vision-and-Language Reasoners?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Krojer, Benno and {Poole-Dayan}, Elinor and Voleti, Vikram and Pal, Chris and Reddy, Siva},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {8385--8405},
  publisher = {Curran Associates, Inc.},
  join_key = {77}
}

@inproceedings{krojerImageRetrievalContextual2022,
  title = {Image Retrieval from Contextual Descriptions},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Krojer, Benno {and} Adlakha, Vaibhav {and} Vineet},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {3426--3440},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.241},
  abstract = {The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on ImageCoDe.Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that ImageCoDE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.},
  join_key = {249}
}

@inproceedings{krumdickBizBenchQuantitativeReasoning2024a,
  title = {{{BizBench}}: A Quantitative Reasoning Benchmark for Business and Finance},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Krumdick, Michael {and} Koncel-Kedziorski, Rik {and} Lai},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {8309--8332},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.452},
  abstract = {Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models' ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model`s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs' limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.},
  join_key = {383}
}

@inproceedings{kumarVisionlanguageModelsUnderstand2024,
  title = {Do Vision-Language Models Understand Compound Nouns?},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 2: {{Short}} Papers)},
  author = {Kumar, Sonal {and} Ghosh, Sreyan {and} Sakshi},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {519--527},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-short.43},
  abstract = {Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., *lab coat*) as well as they understand nouns (e.g., *lab*)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25\% on Compun. Code and benchmark are available.},
  join_key = {520}
}

@inproceedings{kuratovBABILongTestingLimits2024,
  title = {{{BABILong}}: {{Testing}} the Limits of Llms with Long Context Reasoning-in-a-Haystack},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Rodkin, Ivan and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {106519--106554},
  publisher = {Curran Associates, Inc.},
  join_key = {118}
}

@inproceedings{kurticMathadorLMDynamicBenchmark2024,
  title = {Mathador-{{LM}}: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Kurtic, Eldar {and} Moeini, Amir {and} Alistarh},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {17020--17027},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.946},
  abstract = {We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 3rd graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks. The implementation of Mathador-LM benchmark is available at https://github.com/IST-DASLab/Mathador-LM.},
  join_key = {483}
}

@inproceedings{kwanM4LEMultiabilityMultirange2024,
  title = {{{M4LE}}: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kwan, Wai-Chung {and} Zeng, Xingshan {and} Wang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {15568--15592},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.832},
  abstract = {Managing long sequences has become an important and necessary feature for large language models (LLMs). However, assessing their ability to handle long contexts remains a challenge. This paper introduces M{$^4$}LE, a \textbf{M}ulti-ability, \textbf{M}ulti-range, \textbf{M}ulti-task, \textbf{M}ulti-domain benchmark for \textbf{L}ong-context \textbf{E}valuation. It encompasses 36 NLP datasets, covering 11 types of tasks and 12 domains, providing a comprehensive test bed. To address the lack of tasks featuring naturally long sequences, we propose an automatic approach to convert short-sequence tasks into long-sequence scenarios. These scenarios evaluate LLMs' long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding. This automatic approach allows us to create instances evenly distributed from 1k to 8k input length. Our evaluation of 11 prominent LLMs reveals that 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.},
  join_key = {418}
}

@inproceedings{kwanMTevalMultiturnCapabilities2024,
  title = {{{MT-eval}}: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Kwan, Wai-Chung {and} Zeng, Xingshan {and} Jiang},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {20153--20177},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1124},
  abstract = {Large language models (LLMs) are increasingly used for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks mainly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions. To address this gap, we introduce , a comprehensive benchmark to evaluate the multi-turn conversational abilities of LLMs. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or creating new examples using GPT-4 with a human-in-the-loop process to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 10 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks. We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance.},
  join_key = {490}
}

@inproceedings{kweonEHRNoteQALLMBenchmark2024a,
  title = {{{EHRNoteQA}}: {{An LLM}} Benchmark for Real-World Clinical Practice Using Discharge Summaries},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kweon, Sunjun and Kim, Jiyoun and Kwak, Heeyoung and Cha, Dongchul and Yoon, Hangyul and Kim, Kwanghyun and Yang, Jeewon and Won, Seunghyun and Choi, Edward},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {124575--124611},
  publisher = {Curran Associates, Inc.},
  join_key = {149}
}

@inproceedings{labanSummEditsMeasuringLLM2023a,
  title = {{{SummEdits}}: {{Measuring LLM}} Ability at Factual Reasoning through the Lens of Summarization},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Laban, Philippe {and} Kryscinski, Wojciech {and} Agarwal},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {9662--9676},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.600},
  abstract = {With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8\% below estimated human performance, highlighting the gaps in LLMs' ability to reason about facts and detect inconsistencies when they occur.},
  join_key = {327}
}

@inproceedings{laineMeMyselfAI2024,
  title = {Me, Myself, and {{AI}}: {{The}} Situational Awareness Dataset ({{SAD}}) for Llms},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Laine, Rudolf and Chughtai, Bilal and Betley, Jan and Hariharan, Kaivalya and Scheurer, J{\'e}r{\'e}my and Balesni, Mikita and Hobbhahn, Marius and Meinke, Alexander and Evans, Owain},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {64010--64118},
  publisher = {Curran Associates, Inc.},
  join_key = {180}
}

@inproceedings{lalCaTbenchBenchmarkingLanguage2024,
  title = {{{CaT-bench}}: {{Benchmarking}} Language Model Understanding of Causal and Temporal Dependencies in Plans},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Lal, Yash Kumar {and} Cohen, Vanya {and} Chambers},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {19336--19354},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1077},
  abstract = {Understanding the abilities of LLMs to reason about natural language plans, such as instructional text and recipes, is critical to reliably using them in decision-making systems. A fundamental aspect of plans is the temporal order in which their steps need to be executed, which reflects the underlying causal dependencies between them. We introduce CaT-Bench, a benchmark of Step Order Prediction questions, which test whether a step must necessarily occur before or after another in cooking recipe plans. We use this to evaluate how well frontier LLMs understand causal and temporal dependencies. We find that SOTA LLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased towards predicting dependence more often, perhaps relying on temporal order of steps as a heuristic. While prompting for explanations and using few-shot examples improve performance, the best F1 result is only 0.73. Further, human evaluation of explanations along with answer correctness show that, on average, humans do not agree with model reasoning. Surprisingly, we also find that explaining after answering leads to better performance than normal chain-of-thought prompting, and LLM answers are not consistent across questions about the same step pairs. Overall, results show that LLMs' ability to detect dependence between steps has significant room for improvement.},
  join_key = {488}
}

@inproceedings{lalorBenchmarkingIntersectionalBiases2022,
  title = {Benchmarking Intersectional Biases in {{NLP}}},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Lalor, John {and} Yang, Yi {and} Smith},
  editor = {Carpuat, Marine {and} de Marneffe, Marie-Catherine {and} Meza Ruiz},
  year = {2022},
  month = jul,
  pages = {3598--3609},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.263},
  abstract = {There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.},
  join_key = {277}
}

@inproceedings{lanCriticEvalEvaluatingLargescale2024a,
  title = {{{CriticEval}}: {{Evaluating}} Large-Scale Language Model as Critic},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Lan, Tian and Zhang, Wenwei and Xu, Chen and Huang, Heyan and Lin, Dahua and Chen, Kai and Mao, Xian-Ling},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {66907--66960},
  publisher = {Curran Associates, Inc.},
  join_key = {138}
}

@inproceedings{leeQASAAdvancedQuestion2023,
  title = {{{QASA}}: {{Advanced}} Question Answering on Scientific Articles},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  author = {Lee, Yoonjoo and Lee, Kyungjae and Park, Sunghyun and Hwang, Dasol and Kim, Jaehyeon and Lee, Hong-In and Lee, Moontae},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  year = {2023-07-23/2023-07-29},
  series = {Proceedings of Machine Learning Research},
  volume = {202},
  pages = {19036--19052},
  publisher = {PMLR},
  abstract = {Reasoning is the crux of intellectual thinking. While question answering (QA) tasks are prolific with various computational models and benchmark datasets, they mostly tackle factoid or shallow QA without asking deeper understanding. Dual process theory asserts that human reasoning consists of associative thinking to collect relevant pieces of knowledge and logical reasoning to consciously conclude grounding on evidential rationale. Based on our intensive think-aloud study that revealed the three types of questions: surface, testing, and deep questions, we first propose the QASA benchmark that consists of 1798 novel question answering pairs that require full-stack reasoning on scientific articles in AI and ML fields. Then we propose the QASA approach that tackles the full-stack reasoning with large language models via associative selection, evidential rationale-generation, and systematic composition. Our experimental results show that QASA's full-stack inference outperforms the state-of-the-art InstructGPT by a big margin. We also find that rationale-generation is critical for the performance gain, claiming how we should rethink advanced question answering. The dataset is available at https://github.com/lgresearch/QASA.},
  join_key = {25}
}

@inproceedings{leeVHELMHolisticEvaluation2024,
  title = {{{VHELM}}: A Holistic Evaluation of Vision Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Lee, Tony and Tu, Haoqin and Wong, Chi Heem and Zheng, Wenhao and Zhou, Yiyang and Mai, Yifan and Roberts, Josselin Somerville and Yasunaga, Michihiro and Yao, Huaxiu and Xie, Cihang and Liang, Percy},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {140632--140666},
  publisher = {Curran Associates, Inc.},
  join_key = {223}
}

@inproceedings{leiterPrExMeLargeScale2024,
  title = {{{PrExMe}}! {{Large}} Scale Prompt Exploration of Open Source {{LLMs}} for Machine Translation and Summarization Evaluation},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Leiter, Christoph {and} Eger, Steffen},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {11481--11506},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.641},
  abstract = {Large language models (LLMs) have revolutionized NLP research. Notably, in-context learning enables their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and time-restricted applications. In this work, we introduce \textbf{PrExMe}, a large-scale \textbf{Pr}ompt \textbf{Ex}ploration for \textbf{Me}trics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations. This extensive comparison (1) benchmarks recent open-source LLMs as metrics and (2) explores the stability and variability of different prompting strategies. We discover that, on the one hand, there are scenarios for which prompts are stable. For instance, some LLMs show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores. On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes. For example, changing the requested output format from ``0 to 100'' to ''-1 to +1'' can strongly affect the rankings in our evaluation. Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations.},
  join_key = {461}
}

@inproceedings{levySafeTextBenchmarkExploring2022,
  title = {{{SafeText}}: A Benchmark for Exploring Physical Safety in Language Models},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Levy, Sharon {and} Allaway, Emily {and} Subbiah},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {2407--2421},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.154},
  abstract = {Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.},
  join_key = {262}
}

@inproceedings{liangSceMQAScientificCollege2024,
  title = {{{SceMQA}}: A Scientific College Entrance Level Multimodal Question Answering Benchmark},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Liang, Zhenwen {and} Guo, Kehan {and} Liu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {109--119},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-short.11},
  abstract = {The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50\% to 60\% accuracy achieved by the strongest models.},
  join_key = {427}
}

@inproceedings{liangUHGEvalBenchmarkingHallucination2024a,
  title = {{{UHGEval}}: {{Benchmarking}} the Hallucination of {{Chinese}} Large Language Models via Unconstrained Generation},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Liang, Xun {and} Song, Shichao {and} Niu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {5266--5293},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.288},
  abstract = {Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, containing hallucinations generated by LLMs with minimal restrictions. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.},
  join_key = {371}
}

@inproceedings{liAPIbankComprehensiveBenchmark2023,
  title = {{{API-bank}}: A Comprehensive Benchmark for Tool-Augmented {{LLMs}}},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Li, Minghao {and} Zhao, Yingxiu {and} Yu},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {3102--3116},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.187},
  abstract = {Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca`s tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.},
  join_key = {310}
}

@inproceedings{liCanLanguageModels2023,
  title = {Can Language Models Understand Physical Concepts?},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Li, Lei {and} Xu, Jingjing {and} Dong},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {11843--11861},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.726},
  abstract = {Language models (LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is unclear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85\% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up parameters of LMs 134{\texttimes}. Our dataset is available at https://github.com/TobiasLee/VEC.},
  join_key = {331}
}

@inproceedings{liCanLanguageModels2023a,
  title = {Can Language Models Make Fun? {{A}} Case Study in {{Chinese}} Comical Crosstalk},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Li, Jianquan {and} Wu, XiangBo {and} Liu},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {7581--7596},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.419},
  abstract = {Language is the principal tool for human communication, in which humor is one of the most attractive parts. Producing natural language like humans using computers, a.k.a, Natural Language Generation (NLG), has been widely used for dialogue systems, chatbots, machine translation, as well as computer-aid creation e.g., idea generations, scriptwriting. However, the humor aspect of natural language is relatively under-investigated, especially in the age of pre-trained language models. In this work, we aim to preliminarily test *whether NLG can generate humor as humans do*. We build a largest dataset consisting of numerous **C**hinese **C**omical **C**rosstalk scripts (called **C**3 in short), which is for a popular Chinese performing art called `Xiangsheng' or `' since 1800s.We benchmark various generation approaches including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a human assessment, showing that 1) *large-scale pretraining largely improves crosstalk generation quality*; and 2) *even the scripts generated from the best PLM is far from what we expect*. We conclude humor generation could be largely improved using large-scaled PLMs, but it is still in its infancy. The data and benchmarking code are publicly available in [{$<$}a href="https://github.com/anonNo2/crosstalk-generation"{$>$}https://github.com/anonNo2/crosstalk-generation{$<$}/a{$>$}]({$<$}a href="https://github.com/anonNo2/crosstalk-generation"{$>$}https://github.com/anonNo2/crosstalk-generation{$<$}/a{$>$}).},
  join_key = {287}
}

@inproceedings{liCanLargeLanguage2024,
  title = {Can Large Language Models Analyze Graphs like Professionals? {{A}} Benchmark, Datasets and Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Xin and Chen, Weize and Chu, Qizhi and Li, Haopeng and Sun, Zhaojun and Li, Ran and Qian, Chen and Wei, Yiwei and Liu, Zhiyuan and Shi, Chuan and Sun, Maosong and Yang, Cheng},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {141045--141070},
  publisher = {Curran Associates, Inc.},
  join_key = {127}
}

@inproceedings{liCanLLMAlready2023a,
  title = {Can {{LLM}} Already Serve as a Database Interface? {{A}} Big Bench for Large-Scale Database Grounded Text-to-Sqls},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Geng, Ruiying and Huo, Nan and Zhou, Xuanhe and Chenhao, Ma and Li, Guoliang and Chang, Kevin and Huang, Fei and Cheng, Reynold and Li, Yongbin},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {42330--42357},
  publisher = {Curran Associates, Inc.},
  join_key = {83}
}

@inproceedings{liDataCompLMSearchNext2024,
  title = {{{DataComp-LM}}: {{In}} Search of the next Generation of Training Sets for Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Jeffrey and Fang, Alex and Smyrnis, Georgios and Ivgi, Maor and Jordan, Matt and Gadre, Samir and Bansal, Hritik and Guha, Etash and Keh, Sedrick and Arora, Kushal and Garg, Saurabh and Xin, Rui and Muennighoff, Niklas and Heckel, Reinhard and Mercat, Jean and Chen, Mayee and Gururangan, Suchin and Wortsman, Mitchell and Albalak, Alon and Bitton, Yonatan and Nezhurina, Marianna and Abbas, Amro and Hsieh, Cheng-Yu and Ghosh, Dhruba and Gardner, Josh and Kilian, Maciej and Zhang, Hanlin and Shao, Rulin and Pratt, Sarah and Sanyal, Sunny and Ilharco, Gabriel and Daras, Giannis and Marathe, Kalyani and Gokaslan, Aaron and Zhang, Jieyu and Chandu, Khyathi and Nguyen, Thao and Vasiljevic, Igor and Kakade, Sham and Song, Shuran and Sanghavi, Sujay and Faghri, Fartash and Oh, Sewoong and Zettlemoyer, Luke and Lo, Kyle and {El-Nouby}, Alaaeldin and Pouransari, Hadi and Toshev, Alexander and Wang, Stephanie and Groeneveld, Dirk and Soldaini, Luca and Koh, Pang Wei and Jitsev, Jenia and Kollar, Thomas and Dimakis, Alexandros G. and Carmon, Yair and Dave, Achal and Schmidt, Ludwig and Shankar, Vaishaal},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {14200--14282},
  publisher = {Curran Associates, Inc.},
  join_key = {143}
}

@inproceedings{liDiplomatDialogueDataset2023,
  title = {Diplomat: A Dialogue Dataset for Situated {{PragMATic}} Reasoning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Hengli and Zhu, Song-Chun and Zheng, Zilong},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {46856--46884},
  publisher = {Curran Associates, Inc.},
  join_key = {88}
}

@inproceedings{liEmbodiedAgentInterface2024,
  title = {Embodied Agent Interface: {{Benchmarking}} Llms for Embodied Decision Making},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Manling and Zhao, Shiyu and Wang, Qineng and Wang, Kangrui and Zhou, Yu and Srivastava, Sanjana and Gokmen, Cem and Lee, Tony and Li, Li Erran and Zhang, Ruohan and Liu, Weiyu and Liang, Percy and {Fei-Fei}, Li and Mao, Jiayuan and Wu, Jiajun},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {100428--100534},
  publisher = {Curran Associates, Inc.},
  join_key = {150}
}

@inproceedings{liEvaluatingInstructionfollowingRobustness2024,
  title = {Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Li, Zekun {and} Peng, Baolin {and} He},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {557--568},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.33},
  abstract = {Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, making them increasingly integral to various applications. However, this capability introduces the risk of prompt injection attacks, where malicious instructions are embedded in the input to trigger unintended actions or content. Understanding the robustness of LLMs against such attacks is critical for ensuring their safe deployment. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks, assessing their ability to discern which instructions to follow and which to disregard. Through extensive experiments with leading instruction-following LLMs, we reveal significant vulnerabilities, particularly in models that mis-follow injected instructions. Our results show that certain models are excessively inclined to prioritize embedded instructions in prompts, often focusing on the latter parts of the prompt without fully understanding the overall context. Conversely, models that exhibit stronger contextual understanding and instruction-following capabilities tend to be more easily compromised by injected instructions. These findings highlight the need to balance improving LLMs' instruction-following abilities with enhancing their overall comprehension of prompts, to prevent mis-following inappropriate instructions. We hope our analysis provides valuable insights into these vulnerabilities, contributing to the development of more robust solutions in the future.},
  join_key = {430}
}

@inproceedings{liEvoCodeBenchEvolvingCode2024,
  title = {{{EvoCodeBench}}: {{An}} Evolving Code Generation Benchmark with Domain-Specific Evaluations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Jia and Li, Ge and Zhang, Xuanming and Zhao, Yunfei and Dong, Yihong and Jin, Zhi and Li, Binhua and Huang, Fei and Li, Yongbin},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {57619--57641},
  publisher = {Curran Associates, Inc.},
  join_key = {152}
}

@inproceedings{liFIREDatasetFeedback2024,
  title = {{{FIRE}}: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Pengxiang and Gao, Zhi and Zhang, Bofei and Yuan, Tao and Wu, Yuwei and Harandi, Mehrtash and Jia, Yunde and Zhu, Song-Chun and Li, Qing},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {101618--101640},
  publisher = {Curran Associates, Inc.},
  join_key = {155}
}

@inproceedings{liFRoGEvaluatingFuzzy2024a,
  title = {{{FRoG}}: {{Evaluating}} Fuzzy Reasoning of Generalized Quantifiers in {{LLMs}}},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Li, Yiyuan {and} Sun, Shichao {and} Liu},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {7239--7256},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.411},
  abstract = {Fuzzy reasoning is vital due to the frequent use of imprecise information in daily contexts. However, the ability of current large language models (LLMs) to handle such reasoning remains largely uncharted. In this paper, we introduce a new benchmark, FRoG, for fuzzy reasoning, featuring real-world mathematical word problems that incorporate generalized quantifiers. Our experimental findings reveal that fuzzy reasoning continues to pose significant challenges for LLMs. Moreover, we find that existing methods designed to enhance reasoning do not consistently improve performance in tasks involving fuzzy logic. Additionally, our results show an inverse scaling effect in the performance of LLMs on FRoG. Interestingly, we also demonstrate that strong mathematical reasoning skills are not necessarily indicative of success on our benchmark.},
  join_key = {446}
}

@inproceedings{liGLBenchComprehensiveBenchmark2024,
  title = {{{GLBench}}: A Comprehensive Benchmark for Graph with Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Yuhan and Wang, Peisong and Zhu, Xiao and Chen, Aochuan and Jiang, Haiyun and Cai, Deng and Chan, Victor Wai Kin and Li, Jia},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {42349--42368},
  publisher = {Curran Associates, Inc.},
  join_key = {157}
}

@inproceedings{liGSMplusComprehensiveBenchmark2024,
  title = {{{GSM-plus}}: A Comprehensive Benchmark for Evaluating the Robustness of {{LLMs}} as Mathematical Problem Solvers},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Li, Qintong {and} Cui, Leyang {and} Zhao},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {2961--2984},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.163},
  abstract = {Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.},
  join_key = {359}
}

@inproceedings{liHaluEvalLargescaleHallucination2023a,
  title = {{{HaluEval}}: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Li, Junyi {and} Cheng, Xiaoxue {and} Zhao},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {6449--6464},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.397},
  abstract = {Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5\% user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps.},
  join_key = {315}
}

@inproceedings{liInfiBenchEvaluatingQuestionanswering2024,
  title = {{{InfiBench}}: {{Evaluating}} the Question-Answering Capabilities of Code Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Linyi and Geng, Shijie and Li, Zhenwen and He, Yibo and Yu, Hao and Hua, Ziyue and Ning, Guanghan and Wang, Siwei and Xie, Tao and Yang, Hongxia},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {128668--128698},
  publisher = {Curran Associates, Inc.},
  join_key = {163}
}

@inproceedings{liLexEvalComprehensiveChinese2024a,
  title = {{{LexEval}}: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Haitao and Chen, You and Ai, Qingyao and Wu, Yueyue and Zhang, Ruizhe and Liu, Yiqun},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {25061--25094},
  publisher = {Curran Associates, Inc.},
  join_key = {171}
}

@inproceedings{liLooGLECanLongcontext2024,
  title = {{{LooGLE}}: {{Can}} Long-Context Language Models Understand Long Contexts?},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Li, Jiaqi {and} Wang, Mengmeng {and} Zheng},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {16304--16333},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.859},
  abstract = {Large language models (LLMs) are typically limited to processing texts within context window size, which has spurred significant research efforts into enhancing LLMs' long-context understanding as well as developing high-quality benchmarks to evaluate the ability. However, prior datasets suffer from short comings like short length compared to the context window of modern LLMs; outdated documents that might have data leakage problems; and an emphasis on short dependency tasks only. In this paper, we present LooGLE , a Long Context Generic Language Evaluation benchmark. It features documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning varying dependency ranges in diverse domains. Human annotators meticulously crafted over 1,100 high-quality question-answer (QA) pairs with thorough cross-validation for a most precise assessment of LLMs' long dependency capabilities. We conduct a comprehensive evaluation of representative LLMs on LooGLE . The results indicate that most LLMs have shockingly bad long context ability and fail to capture long dependencies in the context, even when their context window size is enough to fit the entire document. Our results shed light on enhancing the ``true long-context understanding'' ability of LLMs instead of merely enlarging their context window.},
  join_key = {424}
}

@inproceedings{liMediQQuestionaskingLlms2024a,
  title = {{{MediQ}}: {{Question-asking}} Llms and a Benchmark for Reliable Interactive Clinical Reasoning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Shuyue Stella and Balachandran, Vidhisha and Feng, Shangbin and Ilgen, Jonathan S. and Pierson, Emma and Koh, Pang Wei and Tsvetkov, Yulia},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {28858--28888},
  publisher = {Curran Associates, Inc.},
  join_key = {176}
}

@inproceedings{liMembershipInferenceAttacks2024,
  title = {Membership Inference Attacks against Large Vision-Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Zhan and Wu, Yongtao and Chen, Yihang and Tonin, Francesco and Abad Rocamora, Elias and Cevher, Volkan},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {98645--98674},
  publisher = {Curran Associates, Inc.},
  join_key = {179}
}

@inproceedings{liMEQABenchmarkMultihop2024,
  title = {{{MEQA}}: A Benchmark for Multi-Hop Event-Centric Question Answering with Explanations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Ruosen and Wang, Zimu and Tran, Son Quoc and Xia, Lei and Du, Xinya},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {126835--126862},
  publisher = {Curran Associates, Inc.},
  join_key = {181}
}

@inproceedings{liMultimodalArXivDataset2024,
  title = {Multimodal {{ArXiv}}: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Li, Lei {and} Wang, Yuqi {and} Xu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {14369--14387},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.775},
  abstract = {Large vision-language models (LVLMs) excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains.To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension.ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers spanning various scientific domains.Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances open-sourced LVLMs' mathematical reasoning capabilities, achieving a 10.4\% absolute accuracy gain on a multimodal mathematical reasoning benchmark.Furthermore, employing ArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs.Evaluation results with state-of-the-art LVLMs underscore their struggle with the nuanced semantics of academic figures, while domain-specific training yields substantial performance gains.Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.},
  join_key = {412}
}

@inproceedings{liNaturalBenchEvaluatingVisionlanguage2024,
  title = {{{NaturalBench}}: {{Evaluating}} Vision-Language Models on Natural Adversarial Samples},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Baiqi and Lin, Zhiqiu and Peng, Wenxuan and Nyandwi, Jean de Dieu and Jiang, Daniel and Ma, Zixian and Khanuja, Simran and Krishna, Ranjay and Neubig, Graham and Ramanan, Deva},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {17044--17068},
  publisher = {Curran Associates, Inc.},
  join_key = {191}
}

@inproceedings{liNewsBenchSystematicEvaluation2024,
  title = {{{NewsBench}}: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in {{Chinese}} Journalism},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Li, Miao {and} Chen, Ming-Bin {and} Tang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {9993--10014},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.538},
  abstract = {We present NewsBench, a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. Our constructed benchmark dataset is focused on four facets of writing proficiency and six facets of safety adherence, and it comprises manually and carefully designed 1,267 test samples in the types of multiple choice questions and short answer questions for five editorial tasks in 24 news domains. To measure performances, we propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations. Based on the systematic evaluation framework, we conduct a comprehensive analysis of eleven popular LLMs which can handle Chinese. The experimental results highlight GPT-4 and ERNIE Bot as top performers, yet reveal a relative deficiency in journalistic safety adherence in creative writing tasks. Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations. The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism.},
  join_key = {390}
}

@inproceedings{linghuMultimodalSituatedReasoning2024,
  title = {Multi-Modal Situated Reasoning in {{3D}} Scenes},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Linghu, Xiongkun and Huang, Jiangyong and Niu, Xuesong and Ma, Xiaojian and Jia, Baoxiong and Huang, Siyuan},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {140903--140936},
  publisher = {Curran Associates, Inc.},
  join_key = {189}
}

@inproceedings{linTruthfulQAMeasuringHow2022,
  title = {{{TruthfulQA}}: {{Measuring}} How Models Mimic Human Falsehoods},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Lin, Stephanie {and} Hilton, Jacob {and} Evans},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {3214--3252},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.229},
  abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
  join_key = {248}
}

@inproceedings{liPrivLMbenchMultilevelPrivacy2024,
  title = {{{PrivLM-bench}}: A Multi-Level Privacy Evaluation Benchmark for Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Li, Haoran {and} Guo, Dadi {and} Li},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {54--73},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.4},
  abstract = {The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models and users. On the one hand, powerful LMs achieve state-of-the-art performance over numerous downstream NLP tasks. On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage. To address these issues, many recent works propose privacy-preserving language models (PPLMs) with differential privacy (DP). Unfortunately, different DP implementations make it challenging for a fair comparison among existing PPLMs. In this paper, we present PrivLM-Bench, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs. Instead of only reporting DP parameters, PrivLM-Bench sheds light on the neglected inference data privacy during actual usage. PrivLM-Bench first clearly defines multi-faceted privacy objectives. Then, PrivLM-Bench constructs a unified pipeline to perform private fine-tuning. Lastly, PrivLM-Bench performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results. The empirical attack results are used to fairly and intuitively evaluate the privacy leakage of various PPLMs. We conduct extensive experiments on three datasets of GLUE for mainstream LMs.},
  join_key = {346}
}

@inproceedings{liQuantifyingAdaptabilityPretrained2022,
  title = {Quantifying Adaptability in Pre-Trained Language Models with 500 Tasks},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Li, Belinda {and} Yu, Jane {and} Khabsa},
  editor = {Carpuat, Marine {and} de Marneffe, Marie-Catherine {and} Meza Ruiz},
  year = {2022},
  month = jul,
  pages = {4696--4715},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.346},
  abstract = {When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.},
  join_key = {278}
}

@inproceedings{liskaStreamingQABenchmarkAdaptation2022,
  title = {{{StreamingQA}}: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  author = {Liska, Adam and Kocisky, Tomas and Gribovskaya, Elena and Terzi, Tayfun and Sezener, Eren and Agrawal, Devang and De Masson D'Autume, Cyprien and Scholtes, Tim and Zaheer, Manzil and Young, Susannah and {Gilsenan-Mcmahon}, Ellen and Austin, Sophia and Blunsom, Phil and Lazaridou, Angeliki},
  editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  year = {2022-07-17/2022-07-23},
  series = {Proceedings of Machine Learning Research},
  volume = {162},
  pages = {13604--13622},
  publisher = {PMLR},
  abstract = {Knowledge and language understanding of models evaluated through question answering (QA) has been usually studied on static snapshots of knowledge, like Wikipedia. However, our world is dynamic, evolves over time, and our models' knowledge becomes outdated. To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM under-perform those with a retrained LM. For questions about higher-frequency named entities, parametric updates are particularly beneficial. In our dynamic world, the StreamingQA dataset enables a more realistic evaluation of QA models, and our experiments highlight several promising directions for future research.},
  join_key = {27}
}

@inproceedings{liTEGDBComprehensiveDataset2024,
  title = {{{TEG-DB}}: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Zhuofeng and Gou, Zixing and Zhang, Xiangnan and Liu, Zhongyuan and Li, Sirui and Hu, Yuntong and Ling, Chen and Zhang, Zheng and Zhao, Liang},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {60980--60998},
  publisher = {Curran Associates, Inc.},
  join_key = {219}
}

@inproceedings{liuAgentBenchEvaluatingLLMs2024,
  title = {{{AgentBench}}: {{Evaluating LLMs}} as Agents},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
  year = {2024},
  join_key = {39}
}

@inproceedings{liuAlignBenchBenchmarkingChinese2024a,
  title = {{{AlignBench}}: {{Benchmarking Chinese}} Alignment of Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Liu, Xiao {and} Lei, Xuanyu {and} Wang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {11621--11640},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.624},
  abstract = {Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in Chinese. We tailor a human-in-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references.To ensure references' correctness, each knowledge-intensive query is accompanied with evidences collected from reliable webpages (including the url and quotation) by our annotators.For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge (CITATION) with Chain-of-Thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability.All evaluation codes and data are publicly available at {$<$}a href="https://github.com/THUDM/AlignBench"{$>$}https://github.com/THUDM/AlignBench{$<$}/a{$>$}},
  join_key = {400}
}

@inproceedings{liuAsEPBenchmarkingDeep2024,
  title = {{{AsEP}}: {{Benchmarking}} Deep Learning Methods for Antibody-Specific Epitope Prediction},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Liu, Chunan and Denzler, Lilian and Chen, Yihong and Martin, Andrew and Paige, Brooks},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {11700--11734},
  publisher = {Curran Associates, Inc.},
  join_key = {117}
}

@inproceedings{liuBenchmarkingLargeLanguage2023a,
  title = {Benchmarking Large Language Models on Cmexam - a Comprehensive Chinese Medical Exam Dataset},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Liu, Junling and Zhou, Peilin and Hua, Yining and Chong, Dading and Tian, Zhongyu and Liu, Andrew and Wang, Helin and You, Chenyu and Guo, Zhenhua and ZHU, {\relax LEI} and Li, Michael Lingzhi},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {52430--52452},
  publisher = {Curran Associates, Inc.},
  join_key = {79}
}

@inproceedings{liuConvBenchMultiturnConversation2024,
  title = {{{ConvBench}}: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Liu, Shuo and Ying, Kaining and Zhang, Hao and Yang, Yue and Lin, Yuqi and Zhang, Tianle and Li, Chuanhao and Qiao, Yu and Luo, Ping and Shao, Wenqi and Zhang, Kaipeng},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {100734--100782},
  publisher = {Curran Associates, Inc.},
  join_key = {135}
}

@inproceedings{liuEfficientNLPStandard2022,
  title = {Towards Efficient {{NLP}}: A Standard Evaluation and a Strong Baseline},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Liu, Xiangyang {and} Sun, Tianxiang {and} He},
  editor = {Carpuat, Marine {and} de Marneffe, Marie-Catherine {and} Meza Ruiz},
  year = {2022},
  month = jul,
  pages = {3288--3303},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.240},
  abstract = {Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.},
  join_key = {276}
}

@inproceedings{liuExposingAttentionGlitches2023,
  title = {Exposing Attention Glitches with Flip-Flop Language Modeling},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Liu, Bingbin and Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {25549--25583},
  publisher = {Curran Associates, Inc.},
  join_key = {89}
}

@inproceedings{liuGeneralLoopInvariant2024,
  title = {Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Liu, Chang and Wu, Xiwei and Feng, Yuan and Cao, Qinxiang and Yan, Junchi},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {129120--129145},
  publisher = {Curran Associates, Inc.},
  join_key = {220}
}

@inproceedings{liuLargeLanguageModels2024a,
  title = {Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Liu, Fenglin {and} Li, Zheng {and} Zhou},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {13696--13710},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.759},
  abstract = {The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs},
  join_key = {469}
}

@inproceedings{liuMMDUMultiturnMultiimage2024,
  title = {{{MMDU}}: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for Lvlms},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Liu, Ziyu and Chu, Tao and Zang, Yuhang and Wei, Xilin and Dong, Xiaoyi and Zhang, Pan and Liang, Zijian and Xiong, Yuanjun and Qiao, Yu and Lin, Dahua and Wang, Jiaqi},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {8698--8733},
  publisher = {Curran Associates, Inc.},
  join_key = {183}
}

@inproceedings{liuNLEBench+NorGLMComprehensiveEmpirical2024,
  title = {{{NLEBench}}+{{NorGLM}}: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in {{Norwegian}}},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Liu, Peng {and} Zhang, Lemei {and} Farup},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {5543--5560},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.317},
  abstract = {Norwegian, spoken by only 5 million population, is under-representative within the most impressive breakthroughs in NLP tasks. To the best of our knowledge, there has not yet been a comprehensive evaluation of the existing language models (LMs) on Norwegian generation tasks during the article writing process. To fill this gap, we 1) compiled the existing Norwegian dataset and pre-trained 4 Norwegian Open Language Models varied from parameter scales and architectures, collectively called NorGLM; 2) introduced a comprehensive benchmark, NLEBench, for evaluating natural language generation capabilities in Norwegian, encompassing translation and human annotation. Based on the investigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5 has limited capability in understanding the Norwegian context; 2) the increase in model parameter scales demonstrates limited impact on the performance of downstream tasks when the pre-training dataset is constrained in size; 3) smaller models also demonstrate the reasoning capability through Chain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be used to verify the generalizability of LLMs on natural language understanding and, meanwhile, test the interconnectedness of these NLP tasks. We share our resources and code for reproducibility under a CC BY-NC 4.0 license.},
  join_key = {439}
}

@inproceedings{liuRepoBenchBenchmarkingRepositorylevel2024,
  title = {{{RepoBench}}: {{Benchmarking}} Repository-Level Code Auto-Completion Systems},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Liu, Tianyang and Xu, Canwen and McAuley, Julian},
  year = {2024},
  join_key = {60}
}

@inproceedings{liuRevisitingDeidentificationElectronic2023a,
  title = {Revisiting De-Identification of Electronic Medical Records: {{Evaluation}} of within- and Cross-Hospital Generalization},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Liu, Yiyang {and} Li, Jinpeng {and} Zhu},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {3666--3674},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.224},
  abstract = {The de-identification task aims to detect and remove the protected health information from electronic medical records (EMRs). Previous studies generally focus on the within-hospital setting and achieve great successes, while the cross-hospital setting has been overlooked. This study introduces a new de-identification dataset comprising EMRs from three hospitals in China, creating a benchmark for evaluating both within- and cross-hospital generalization. We find significant domain discrepancy between hospitals. A model with almost perfect within-hospital performance struggles when transferred across hospitals. Further experiments show that pretrained language models and some domain generalization methods can alleviate this problem. We believe that our data and findings will encourage investigations on the generalization of medical NLP models.},
  join_key = {311}
}

@inproceedings{liuRevisitingGoldStandard2023,
  title = {Revisiting the Gold Standard: {{Grounding}} Summarization Evaluation with Robust Human Evaluation},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Liu, Yixin {and} Fabbri, Alex {and} Liu},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {4140--4170},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.228},
  abstract = {Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.},
  join_key = {284}
}

@inproceedings{liuWe`reAfraidLanguage2023a,
  title = {We`re Afraid Language Models Aren`t Modeling Ambiguity},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Liu, Alisa {and} Wu, Zhaofeng {and} Michael},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {790--807},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.51},
  abstract = {Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We capture ambiguity in a sentence through its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32\% of the time in crowdworker evaluation, compared to 90\% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.},
  join_key = {300}
}

@inproceedings{liVRSBenchVersatileVisionlanguage2024,
  title = {{{VRSBench}}: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Xiang and Ding, Jian and Elhoseiny, Mohamed},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {3229--3242},
  publisher = {Curran Associates, Inc.},
  join_key = {226}
}

@inproceedings{liWhenLlmsMeet2024,
  title = {When Llms Meet Cunning Texts: A Fallacy Understanding Benchmark for Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Yinghui and Zhou, Qingyu and Luo, Yuanzhen and Ma, Shirong and Li, Yangning and Zheng, Hai-Tao and Hu, Xuming and Yu, Philip S.},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {112433--112458},
  publisher = {Curran Associates, Inc.},
  join_key = {230}
}

@inproceedings{liWMDPBenchmarkMeasuring2024,
  title = {The {{WMDP}} Benchmark: {{Measuring}} and Reducing Malicious Use with Unlearning},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D. and Dombrowski, Ann-Kathrin and Goel, Shashwat and Mukobi, Gabriel and {Helm-Burger}, Nathan and Lababidi, Rassin and Justen, Lennart and Liu, Andrew Bo and Chen, Michael and Barrass, Isabelle and Zhang, Oliver and Zhu, Xiaoyuan and Tamirisa, Rishub and Bharathi, Bhrugu and {Herbert-Voss}, Ariel and Breuer, Cort B and Zou, Andy and Mazeika, Mantas and Wang, Zifan and Oswal, Palash and Lin, Weiran and Hunt, Adam Alfred and {Tienken-Harder}, Justin and Shih, Kevin Y. and Talley, Kemper and Guan, John and Steneker, Ian and Campbell, David and Jokubaitis, Brad and Basart, Steven and Fitz, Stephen and Kumaraguru, Ponnurangam and Karmakar, Kallol Krishna and Tupakula, Uday and Varadharajan, Vijay and Shoshitaishvili, Yan and Ba, Jimmy and Esvelt, Kevin M. and Wang, Alexandr and Hendrycks, Dan},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {28525--28550},
  publisher = {PMLR},
  abstract = {The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private and restricted to a narrow range of malicious use scenarios, which limits further research into reducing malicious use. To fill these gaps, we release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai.},
  join_key = {11}
}

@inproceedings{luLearnExplainMultimodal2022,
  title = {Learn to Explain: {{Multimodal}} Reasoning via Thought Chains for Science Question Answering},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {2507--2521},
  publisher = {Curran Associates, Inc.},
  join_key = {69}
}

@inproceedings{luMathVistaEvaluatingMathematical2024,
  title = {{{MathVista}}: {{Evaluating}} Mathematical Reasoning of Foundation Models in Visual Contexts},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  year = {2024},
  join_key = {52}
}

@inproceedings{luoCODISBenchmarkingContextdependent2024,
  title = {{{CODIS}}: {{Benchmarking}} Context-Dependent Visual Comprehension for Multimodal Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Luo, Fuwen {and} Chen, Chi {and} Wan},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {10639--10659},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.573},
  abstract = {Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner.},
  join_key = {393}
}

@inproceedings{luoMMMRSMultimodalMultiGSD2024,
  title = {{{MMM-RS}}: A Multi-Modal, Multi-{{GSD}}, Multi-Scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Luo, Jialin and Wang, Yuanzhi and Gu, Ziqi and Qiu, Yide and Yao, Shuaizhen and Wang, Fuyun and Xu, Chunyan and Zhang, Wenhua and Wang, Dan and Cui, Zhen},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {12151--12163},
  publisher = {Curran Associates, Inc.},
  join_key = {186}
}

@inproceedings{luWebLINXRealworldWebsite2024,
  title = {{{WebLINX}}: {{Real-world}} Website Navigation with Multi-Turn Dialogue},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Lu, Xing Han and Kasner, Zden{\v e}k and Reddy, Siva},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {33007--33056},
  publisher = {PMLR},
  abstract = {We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings.},
  join_key = {10}
}

@inproceedings{luWildVisionEvaluatingVisionlanguage2024,
  title = {{{WildVision}}: {{Evaluating}} Vision-Language Models in the Wild with Human Preferences},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Lu, Yujie and Jiang, Dongfu and Chen, Wenhu and Wang, William Yang and Choi, Yejin and Lin, Bill Yuchen},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {48224--48255},
  publisher = {Curran Associates, Inc.},
  join_key = {235}
}

@inproceedings{lyuMMScanMultimodal3D2024,
  title = {{{MMScan}}: A Multi-Modal {{3D}} Scene Dataset with Hierarchical Grounded Language Annotations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Lyu, Ruiyuan and Lin, Jingli and Wang, Tai and Yang, Shuai and Mao, Xiaohan and Chen, Yilun and Xu, Runsen and Huang, Haifeng and Zhu, Chenming and Lin, Dahua and Pang, Jiangmiao},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {50898--50924},
  publisher = {Curran Associates, Inc.},
  join_key = {187}
}

@inproceedings{maAgentBoardAnalyticalEvaluation2024,
  title = {{{AgentBoard}}: {{An}} Analytical Evaluation Board of Multi-Turn {{LLM}} Agents},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ma, Chang and Zhang, Junlei and Zhu, Zhihao and Yang, Cheng and Yang, Yujiu and Jin, Yaohui and Lan, Zhenzhong and Kong, Lingpeng and He, Junxian},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {74325--74362},
  publisher = {Curran Associates, Inc.},
  join_key = {112}
}

@inproceedings{mackoMULTITuDELargescaleMultilingual2023,
  title = {{{MULTITuDE}}: {{Large-scale}} Multilingual Machine-Generated Text Detection Benchmark},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Macko, Dominik {and} Moro, Robert {and} Uchendu},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {9960--9987},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.616},
  abstract = {There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple languages.},
  join_key = {328}
}

@inproceedings{madanRevisitingFewshotObject2024a,
  title = {Revisiting Few-Shot Object Detection with Vision-Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Madan, Anish and Peri, Neehar and Kong, Shu and Ramanan, Deva},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {19547--19560},
  publisher = {Curran Associates, Inc.},
  join_key = {201}
}

@inproceedings{maExaminationCompositionalityLarge2024,
  title = {An Examination of the Compositionality of Large Generative Vision-Language Models},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Ma, Teli {and} Li, Rong {and} Liang},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {692--705},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.39},
  abstract = {With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics ( VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a **SyntaxBias Score**, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely **S**ynt**A**ctically **DE**-biased benchmark (SADE). Our study provides an unbiased benchmark for the compositionality of GVLMs, facilitating future research in this direction. Code and dataset are available at https://github.com/TeleeMa/SADE.},
  join_key = {498}
}

@inproceedings{magnussonPalomaBenchmarkEvaluating2024a,
  title = {Paloma: A Benchmark for Evaluating Language Model Fit},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Magnusson, Ian and Bhagia, Akshita and Hofmann, Valentin and Soldaini, Luca and Jha, Ananya Harsh and Tafjord, Oyvind and Schwenk, Dustin and Walsh, Evan Pete and Elazar, Yanai and Lo, Kyle and Groeneveld, Dirk and Beltagy, Iz and Hajishirzi, Hannaneh and Smith, Noah A. and Richardson, Kyle and Dodge, Jesse},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {64338--64376},
  publisher = {Curran Associates, Inc.},
  join_key = {198}
}

@inproceedings{maharanaEvaluatingVeryLongterm2024,
  title = {Evaluating Very Long-Term Conversational Memory of {{LLM}} Agents},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Maharana, Adyasha {and} Lee, Dong-Ho {and} Tulyakov},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {13851--13870},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.747},
  abstract = {Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 600 turns and 16K tokens on avg., over up to 32 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.},
  join_key = {410}
}

@inproceedings{mahbubUnveilingEssencePoetry2023,
  title = {Unveiling the Essence of Poetry: {{Introducing}} a Comprehensive Dataset and Benchmark for Poem Summarization},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Mahbub, Ridwan {and} Khan, Ifrad {and} Anuva},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {14878--14886},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.920},
  abstract = {While research in natural language processing has progressed significantly in creative language generation, the question of whether language models can interpret the intended meaning of creative language largely remains unanswered. Poetry as a creative art form has existed for generations, and summarization of such content requires deciphering the figurative patterns to find out the actual intent and message of the poet. This task can provide the researchers an opportunity to evaluate the creative language interpretation capacity of the language models. Unlike typical text, summarization of poems is a challenging task as poems carry a deeper meaning, which can be easily lost if only the literal meaning is considered. That being said, we propose a new task in the field of natural language understanding called `Poem Summarization'. As a starting, we propose the first-ever dataset for this task, named `PoemSum', consisting of 3011 samples of poetry and its corresponding summarized interpretation in the English language. We have benchmarked the performance of different state-of-the-art summarization models and provided observations on their limitations. The dataset and all relevant code used in this work have been made publicly available.},
  join_key = {340}
}

@inproceedings{maiapoloTinyBenchmarksEvaluatingLLMs2024,
  title = {{{tinyBenchmarks}}: Evaluating {{LLMs}} with Fewer Examples},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Maia Polo, Felipe and Weber, Lucas and Choshen, Leshem and Sun, Yuekai and Xu, Gongjun and Yurochkin, Mikhail},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {34303--34326},
  publisher = {PMLR},
  abstract = {The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.},
  join_key = {9}
}

@inproceedings{maLargeLanguageModels2024,
  title = {Large Language Models Play {{StarCraft II}}:Benchmarks and a Chain of Summarization Approach},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ma, Weiyu and Mi, Qirui and Zeng, Yongcheng and Yan, Xue and Wu, Yuqiao and Lin, Runji and Zhang, Haifeng and Wang, Jun},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {133386--133442},
  publisher = {Curran Associates, Inc.},
  join_key = {170}
}

@inproceedings{maMMLONGBENCHDOCBenchmarkingLongcontext2024,
  title = {{{MMLONGBENCH-DOC}}: {{Benchmarking}} Long-Context Document Understanding with Visualizations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and Zhang, Pan and Pan, Liangming and Jiang, Yu-Gang and Wang, Jiaqi and Cao, Yixin and Sun, Aixin},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {95963--96010},
  publisher = {Curran Associates, Inc.},
  join_key = {184}
}

@inproceedings{marchiorimanerbaSocialBiasProbing2024,
  title = {Social Bias Probing: {{Fairness}} Benchmarking for Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Marchiori Manerba, Marta {and} Stanczak, Karolina {and} Guidotti},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {14653--14671},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.812},
  abstract = {While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.},
  join_key = {470}
}

@inproceedings{marchisioUnderstandingMitigatingLanguage2024,
  title = {Understanding and Mitigating Language Confusion in {{LLMs}}},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Marchisio, Kelly {and} Ko, Wei-Yin {and} Berard},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {6653--6677},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.380},
  abstract = {We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user`s desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation.},
  join_key = {444}
}

@inproceedings{marinBENDBenchmarkingDNA2024,
  title = {{{BEND}}: {{Benchmarking DNA}} Language Models on Biologically Meaningful Tasks},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Marin, Frederikke Isa and Teufel, Felix and Horlacher, Marc and Madsen, Dennis and Pultz, Dennis and Winther, Ole and Boomsma, Wouter},
  year = {2024},
  join_key = {40}
}

@inproceedings{marraffiniGreatestGoodBenchmark2024,
  title = {The Greatest Good Benchmark: {{Measuring LLMs}}' Alignment with Utilitarian Moral Dilemmas},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Marraffini, Giovanni Franco Gabriel {and} Cotton, Andr{\'e}s {and} Hsueh},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {21950--21959},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1224},
  abstract = {The question of how to make decisions that maximise the well-being of all persons is very relevant to design language models that are beneficial to humanity and free from harm. We introduce the Greatest Good Benchmark to evaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis across 15 diverse LLMs reveals consistently encoded moral preferences that diverge from established moral theories and lay population moral standards. Most LLMs have a marked preference for impartial beneficence and rejection of instrumental harm. These findings showcase the `artificial moral compass' of LLMs, offering insights into their moral alignment.},
  join_key = {494}
}

@inproceedings{maruNibblingHardCore2022,
  title = {Nibbling at the {{Hard Core}} of {{Word Sense Disambiguation}}},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Maru, Marco {and} Conia, Simone {and} Bevilacqua},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {4724--4737},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.324},
  abstract = {With state-of-the-art systems having finally attained estimated human performance, Word Sense Disambiguation (WSD) has now joined the array of Natural Language Processing tasks that have seemingly been solved, thanks to the vast amounts of knowledge encoded into Transformer-based pre-trained language models. And yet, if we look below the surface of raw figures, it is easy to realize that current approaches still make trivial mistakes that a human would never make. In this work, we provide evidence showing why the F1 score metric should not simply be taken at face value and present an exhaustive analysis of the errors that seven of the most representative state-of-the-art systems for English all-words WSD make on traditional evaluation benchmarks. In addition, we produce and release a collection of test sets featuring (a) an amended version of the standard evaluation benchmark that fixes its lexical and semantic inaccuracies, (b) 42D, a challenge set devised to assess the resilience of systems with respect to least frequent word senses and senses not seen at training time, and (c) hardEN, a challenge set made up solely of instances which none of the investigated state-of-the-art systems can solve. We make all of the test sets and model predictions available to the research community at {$<$}a href="https://github.com/SapienzaNLP/wsd-hard-benchmark"{$>$}https://github.com/SapienzaNLP/wsd-hard-benchmark{$<$}/a{$>$}.},
  join_key = {255}
}

@inproceedings{maSpreadsheetBenchChallengingReal2024,
  title = {{{SpreadsheetBench}}: {{Towards}} Challenging Real World Spreadsheet Manipulation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ma, Zeyao and Zhang, Bohan and Zhang, Jing and Yu, Jifan and Zhang, Xiaokang and Zhang, Xiaohan and Luo, Sijia and Wang, Xi and Tang, Jie},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {94871--94908},
  publisher = {Curran Associates, Inc.},
  join_key = {210}
}

@inproceedings{mathaiKGymPlatformDataset2024,
  title = {{{kGym}}: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mathai, Alex and Huang, Chenxi and Maniatis, Petros and Nogikh, Aleksandr and Ivan{\v c}i{\'c}, Franjo and Yang, Junfeng and Ray, Baishakhi},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {78053--78078},
  publisher = {Curran Associates, Inc.},
  join_key = {168}
}

@inproceedings{merdjanovskaNoiseBenchBenchmarkingImpact2024,
  title = {{{NoiseBench}}: {{Benchmarking}} the Impact of Real Label Noise on Named Entity Recognition},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Merdjanovska, Elena {and} Aynetdinov, Ansar {and} Akbik},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {18182--18198},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1011},
  abstract = {Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model quality. To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels. These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted. However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation. To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors. We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their achievable upper bound. We release NoiseBench for both English and German to the research community.},
  join_key = {485}
}

@inproceedings{mialonGAIABenchmarkGeneral2024,
  title = {{{GAIA}}: A Benchmark for General {{AI}} Assistants},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  year = {2024},
  join_key = {45}
}

@inproceedings{mireshghallahCanLLMsKeep2024,
  title = {Can {{LLMs}} Keep a Secret? {{Testing}} Privacy Implications of Language Models via Contextual Integrity Theory},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi, Yejin},
  year = {2024},
  join_key = {42}
}

@inproceedings{mirzaeeSPARTQATextualQuestion2021,
  title = {{{SPARTQA}}: A Textual Question Answering Benchmark for Spatial Reasoning},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Mirzaee, Roshanak {and} Rajaby Faghihi, Hossein {and} Ning},
  editor = {Toutanova, Kristina {and} Rumshisky, Anna {and} Zettlemoyer},
  year = {2021},
  month = jun,
  pages = {4582--4598},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.naacl-main.364},
  abstract = {This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs' capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.},
  join_key = {247}
}

@inproceedings{mishraNumGLUESuiteFundamental2022a,
  title = {{{NumGLUE}}: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Mishra, Swaroop {and} Mitra, Arindam {and} Varshney},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {3505--3523},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.246},
  abstract = {Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 \%). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 \% on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.},
  join_key = {250}
}

@inproceedings{mitaStrikingGoldAdvertising2024,
  title = {Striking Gold in Advertising: {{Standardization}} and Exploration of Ad Text Generation},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Mita, Masato {and} Murakami, Soichiro {and} Kato},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {955--972},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.54},
  abstract = {In response to the limitations of manual ad creation, significant research has been conducted in the field of automatic ad text generation (ATG). However, the lack of comprehensive benchmarks and well-defined problem sets has made comparing different methods challenging. To tackle these challenges, we standardize the task of ATG and propose a first benchmark dataset, CAMERA, carefully designed and enabling the utilization of multi-modal information and facilitating industry-wise evaluations. Our extensive experiments with a variety of nine baselines, from classical methods to state-of-the-art models including large language models (LLMs), show the current state and the remaining challenges. We also explore how existing metrics in ATG and an LLM-based evaluator align with human evaluations.},
  join_key = {351}
}

@inproceedings{moneaGlitchMatrixLocating2024a,
  title = {A Glitch in the Matrix? {{Locating}} and Detecting Language Model Grounding with Fakepedia},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Monea, Giovanni {and} Peyrard, Maxime {and} Josifoski},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {6828--6844},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.369},
  abstract = {Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model`s internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.},
  join_key = {377}
}

@inproceedings{monteiroRepLiQAQuestionansweringDataset2024a,
  title = {{{RepLiQA}}: A Question-Answering Dataset for Benchmarking Llms on Unseen Reference Content},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Monteiro, Jo{\~a}o and No{\"e}l, Pierre-Andr{\'e} and Marcotte, {\'E}tienne and Rajeswar, Sai and Zantedeschi, Valentina and V{\'a}zquez, David and Chapados, Nicolas and Pal, Christopher and Taslakian, Perouz},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {24242--24276},
  publisher = {Curran Associates, Inc.},
  join_key = {200}
}

@inproceedings{morabitoSTOPBenchmarkingLarge2024,
  title = {{{STOP}}! {{Benchmarking}} Large Language Models with Sensitivity Testing on Offensive Progressions},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Morabito, Robert {and} Madhusudan, Sangmitra {and} McDonald},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {4221--4243},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.243},
  abstract = {Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3\% to 69.8\%. Furthermore, we demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191\%, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models.},
  join_key = {438}
}

@inproceedings{mouSGbenchEvaluatingLLM2024,
  title = {{{SG-bench}}: {{Evaluating LLM}} Safety Generalization across Diverse Tasks and Prompt Types},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mou, Yutao and Zhang, Shikun and Ye, Wei},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {123032--123054},
  publisher = {Curran Associates, Inc.},
  join_key = {206}
}

@inproceedings{mundlerSWTbenchTestingValidating2024a,
  title = {{{SWT-bench}}: {{Testing}} and Validating Real-World Bug-Fixes with Code Agents},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {M{\"u}ndler, Niels and M{\"u}ller, Mark Niklas and He, Jingxuan and Vechev, Martin},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {81857--81887},
  publisher = {Curran Associates, Inc.},
  join_key = {216}
}

@inproceedings{myungBLEnDBenchmarkLlms2024a,
  title = {{{BLEnD}}: A Benchmark for Llms on Everyday Knowledge in Diverse Cultures and Languages},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Myung, Junho and Lee, Nayeon and Zhou, Yi and Jin, Jiho and Putri, Rifki Afina and Antypas, Dimosthenis and Borkakoty, Hsuvas and Kim, Eunsu and {Perez-Almendros}, Carla and Ayele, Abinew Ali and {Guti{\'e}rrez-Basulto}, V{\'{\i}}ctor and {Ib{\'a}{\~n}ez-Garc{\'{\i}}a}, Yazm{\'{\i}}n and Lee, Hwaran and Muhammad, Shamsuddeen Hassan and Park, Kiwoong and Rzayev, Anar Sabuhi and White, Nina and Yimam, Seid Muhie and Pilehvar, Mohammad Taher and Ousidhoum, Nedjma and {Camacho-Collados}, Jose and Oh, Alice},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {78104--78146},
  publisher = {Curran Associates, Inc.},
  join_key = {126}
}

@inproceedings{nangiaCrowSpairsChallengeDataset2020,
  title = {{{CrowS-pairs}}: A Challenge Dataset for Measuring Social Biases in Masked Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Nangia, Nikita {and} Vania, Clara {and} Bhalerao},
  editor = {Webber, Bonnie {and} Cohn, Trevor {and} He},
  year = {2020},
  month = nov,
  pages = {1953--1967},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.154},
  abstract = {Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.},
  join_key = {237}
}

@inproceedings{naousReadMeBenchmarkingMultilingual2024,
  title = {{{ReadMe}}++: {{Benchmarking}} Multilingual Language Models for Multi-Domain Readability Assessment},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Naous, Tarek {and} Ryan, Michael J {and} Lavrouk},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {12230--12266},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.682},
  abstract = {We present a comprehensive evaluation of large language models for multilingual readability assessment. Existing evaluation resources lack domain and language diversity, limiting the ability for cross-domain and cross-lingual analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset with human annotations of 9757 sentences in Arabic, English, French, Hindi, and Russian, collected from 112 different data sources. This benchmark will encourage research on developing robust multilingual readability assessment methods. Using ReadMe++, we benchmark multilingual and monolingual language models in the supervised, unsupervised, and few-shot prompting settings. The domain and language diversity in ReadMe++ enable us to test more effective few-shot prompting, and identify shortcomings in state-of-the-art unsupervised methods. Our experiments also reveal exciting results of superior domain generalization and enhanced cross-lingual transfer capabilities by models trained on ReadMe++. We will make our data publicly available and release a python package tool for multilingual sentence readability prediction using our trained models at: https://github.com/tareknaous/readme},
  join_key = {465}
}

@inproceedings{nasirGameTraversalBenchmarkEvaluatingPlanning2024a,
  title = {{{GameTraversalBenchmark}}: {{Evaluating}} Planning Abilities of Large Language Models through Traversing {{2D}} Game Maps},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Nasir, Muhammad Umair and James, Steven and Togelius, Julian},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {31813--31827},
  publisher = {Curran Associates, Inc.},
  join_key = {156}
}

@inproceedings{niuRAGTruthHallucinationCorpus2024a,
  title = {{{RAGTruth}}: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Niu, Cheng {and} Wu, Yuanhao {and} Zhu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {10862--10878},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.585},
  abstract = {Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual case and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. We show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive hallucination detection performance when compared to the existing prompt-based approaches using state-of-the-art LLMs such as GPT-4. Furthermore, the finetuned model can effectively mitigate hallucination in LLM responses.},
  join_key = {396}
}

@inproceedings{ohERBenchEntityrelationshipBased2024a,
  title = {{{ERBench}}: {{An}} Entity-Relationship Based Automatically Verifiable Hallucination Benchmark for Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Oh, Jio and Kim, Soyeon and Seo, Junseok and Wang, Jindong and Xu, Ruochen and Xie, Xing and Whang, Steven Euijong},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {53064--53101},
  publisher = {Curran Associates, Inc.},
  join_key = {151}
}

@inproceedings{ouDialogBenchEvaluatingLLMs2024,
  title = {{{DialogBench}}: {{Evaluating LLMs}} as Human-like Dialogue Systems},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Ou, Jiao {and} Lu, Junda {and} Liu},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {6137--6170},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.341},
  abstract = {Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities by leveraging instruction tuning,which refreshes human impressions of dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive tests on English and Chinese DialogBench of 26 LLMs show that instruction tuning improves the human likeness of LLMs to a certain extent, but most LLMs still have much room for improvement as human-like dialogue systems. Interestingly, results also show that the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life.},
  join_key = {514}
}

@inproceedings{ouyangCliMedBenchLargescaleChinese2024a,
  title = {{{CliMedBench}}: A Large-Scale {{Chinese}} Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Ouyang, Zetian {and} Qiu, Yishuai {and} Wang},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {8428--8438},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.480},
  abstract = {With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research.},
  join_key = {451}
}

@inproceedings{panchalWhatSayWhen2024,
  title = {What to Say and When to Say It: {{Live}} Fitness Coaching as a Testbed for Situated Interaction},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Panchal, Sunny and Bhattacharyya, Apratim and Berger, Guillaume and Mercier, Antoine and B{\"o}hm, Cornelius and Dietrichkeit, Florian and Pourreza, Reza and Li, Xuanlin and Madan, Pulkit and Lee, Mingu and Todorovich, Mark and Bax, Ingo and Memisevic, Roland},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {75853--75882},
  publisher = {Curran Associates, Inc.},
  join_key = {229}
}

@inproceedings{panRewardsJustifyMeans2023,
  title = {Do the Rewards Justify the Means? {{Measuring}} Trade-Offs between Rewards and Ethical Behavior in the Machiavelli Benchmark},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  author = {Pan, Alexander and Chan, Jun Shern and Zou, Andy and Li, Nathaniel and Basart, Steven and Woodside, Thomas and Zhang, Hanlin and Emmons, Scott and Hendrycks, Dan},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  year = {2023-07-23/2023-07-29},
  series = {Proceedings of Machine Learning Research},
  volume = {202},
  pages = {26837--26867},
  publisher = {PMLR},
  abstract = {Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce Machiavelli, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.},
  join_key = {24}
}

@inproceedings{parcalabescuVALSETaskindependentBenchmark2022,
  title = {{{VALSE}}: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Parcalabescu, Letitia {and} Cafagna, Michele {and} Muradjan},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {8253--8280},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.567},
  abstract = {We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V\&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V\&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V\&L models from a linguistic perspective, complementing the canonical task-centred V\&L evaluations.},
  join_key = {256}
}

@inproceedings{parkOpenKoLLMLeaderboard2024,
  title = {Open {{Ko-LLM}} Leaderboard: {{Evaluating}} Large Language Models in {{Korean}} with {{Ko-H5}} Benchmark},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Park, Chanjun {and} Kim, Hyeonwoo {and} Kim},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {3220--3234},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.177},
  abstract = {This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.},
  join_key = {362}
}

@inproceedings{paruchuriWhatAreOdds2024,
  title = {What Are the Odds? {{Language}} Models Are Capable of Probabilistic Reasoning},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Paruchuri, Akshay {and} Garrison, Jake {and} Liao},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {11712--11733},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.654},
  abstract = {Language models (LM) are capable of remarkably complex linguistic tasks; however, numerical reasoning is an area in which they frequently struggle. An important but rarely evaluated form of reasoning is understanding probability distributions. In this paper, we focus on evaluating the probabilistic reasoning capabilities of LMs using idealized and real-world statistical distributions. We perform a systematic evaluation of state-of-the-art LMs on three tasks: estimating percentiles, drawing samples, and calculating probabilities. We evaluate three ways to provide context to LMs 1) anchoring examples from within a distribution or family of distributions, 2) real-world context, 3) summary statistics on which to base a Normal approximation. Models can make inferences about distributions, and can be further aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified. To conduct this work, we developed a comprehensive benchmark distribution dataset with associated question-answer pairs that we have released publicly.},
  join_key = {464}
}

@inproceedings{patelDARTevalComprehensiveDNA2024,
  title = {{{DART-eval}}: A Comprehensive {{DNA}} Language Model Evaluation Benchmark on Regulatory {{DNA}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Patel, Aman and Singhal, Arpita and Wang, Austin and Pampari, Anusri and Kasowski, Maya and Kundaje, Anshul},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {62024--62061},
  publisher = {Curran Associates, Inc.},
  join_key = {142}
}

@inproceedings{patelMultiLogiEvalEvaluatingMultistep2024,
  title = {Multi-{{LogiEval}}: {{Towards}} Evaluating Multi-Step Logical Reasoning Ability of Large Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Patel, Nisarg {and} Kulkarni, Mohith {and} Parmar},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {20856--20879},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1160},
  abstract = {As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types --- propositional, first-order, and non-monotonic consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs such as GPT-4, ChatGPT, Gemini-Pro, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of {\textasciitilde}68\% at depth-1 to {\textasciitilde}43\% at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs.},
  join_key = {491}
}

@inproceedings{pengCOPENProbingConceptual2022a,
  title = {{{COPEN}}: {{Probing}} Conceptual Knowledge in Pre-Trained Language Models},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Peng, Hao {and} Wang, Xiaozhi {and} Hu},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {5015--5035},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.335},
  abstract = {Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual knowledge Probing bENchmark. Extensive experiments on different sizes and types of PLMs show that existing PLMs systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are publicly released at https://github.com/THU-KEG/COPEN.},
  join_key = {265}
}

@inproceedings{pfisterSuperGLEBerGermanLanguage2024,
  title = {{{SuperGLEBer}}: {{German}} Language Understanding Evaluation Benchmark},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Pfister, Jan {and} Hotho, Andreas},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {7904--7923},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.438},
  abstract = {We assemble a broad Natural Language Understanding benchmark suite for the German language and consequently evaluate a wide array of existing German-capable models in order to create a better understanding of the current state of German LLMs. Our benchmark consists of 29 different tasks ranging over different types such as document classification, sequence tagging, sentence similarity, and question answering, on which we evaluate 10 different German-pretrained models, thereby charting the landscape of German LLMs. In our comprehensive evaluation we find that encoder models are a good choice for most tasks, but also that the largest encoder model does not necessarily perform best for all tasks. We make our benchmark suite and a leaderboard publically available at https://supergleber.professor-x.de and encourage the community to contribute new tasks and evaluate more models on it (https://github.com/LSX-UniWue/SuperGLEBer).},
  join_key = {516}
}

@inproceedings{piUOUOUncontextualizedUncommon2024a,
  title = {{{UOUO}}: {{Uncontextualized}} Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Pi, Xinyu {and} Wu, Mingyuan {and} Jiang},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {6432--6441},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.369},
  abstract = {Smaller-scale Vision-Language Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the ``Uncontextualized Uncommon Objects'' (UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs. Code and project details for UOUO can be found at https://zoezheng126.github.io/UOUO-Website/.},
  join_key = {443}
}

@inproceedings{pratoEpiKevalEvaluationLanguage2023a,
  title = {{{EpiK-eval}}: {{Evaluation}} for Language Models as Epistemic Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Prato, Gabriele {and} Huang, Jerry {and} Parthasarathi},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {9523--9557},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.593},
  abstract = {In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents---a crucial ability in numerous applications---remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from this study offer insights for developing more robust and reliable LLMs. Our code and benchmark are available at https://github.com/chandar-lab/EpiK-Eval},
  join_key = {326}
}

@inproceedings{pratoLargeLanguageModels2024,
  title = {Do Large Language Models Know How Much They Know?},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Prato, Gabriele {and} Huang, Jerry {and} Parthasarathi},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {6054--6070},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.348},
  abstract = {Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. Nevertheless, the rapid advancement in their deployment trails a comprehensive understanding of their internal mechanisms, as well as a delineation of their capabilities and limitations. A desired characteristic of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this attribute, we develop a benchmark that challenges these models to enumerate all information they possess on specific topics. This benchmark assesses whether the models recall excessive, insufficient, or the precise amount of required information, thereby indicating their awareness of how much they know about the given topic. Our findings reveal that the emergence of this property varies across different architectures and manifests at diverse rates. However, with sufficient scaling, all tested models are ultimately capable of performing this task. The insights gained from this research advance our understanding of LLMs, shedding light on their operational capabilities and contributing to the ongoing exploration of their intricate dynamics.},
  join_key = {441}
}

@inproceedings{pressCiteMECanLanguage2024a,
  title = {{{CiteME}}: {{Can}} Language Models Accurately Cite Scientific Claims?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Press, Ori and Hochlehnert, Andreas and Prabhu, Ameya and Udandarao, Vishaal and Press, Ofir and Bethge, Matthias},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {7847--7877},
  publisher = {Curran Associates, Inc.},
  join_key = {130}
}

@inproceedings{qiPreservingKnowledgeInvariance2023,
  title = {Preserving Knowledge Invariance: {{Rethinking}} Robustness Evaluation of Open Information Extraction},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Qi, Ji {and} Zhang, Chuchun {and} Wang},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {5876--5890},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.360},
  abstract = {The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F{$_1$} score. Our resources and code will be publicly available.},
  join_key = {314}
}

@inproceedings{ramamurthyReinforcementLearningNot2023,
  title = {Is Reinforcement Learning (Not) for Natural Language Processing: {{Benchmarks}}, Baselines, and Building Blocks for Natural Language Policy Optimization},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant{\'e} and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  year = {2023},
  join_key = {30}
}

@inproceedings{ramprasadAnalyzingLLMBehavior2024a,
  title = {Analyzing {{LLM}} Behavior in Dialogue Summarization: {{Unveiling}} Circumstantial Hallucination Trends},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Ramprasad, Sanjana {and} Ferracane, Elisa {and} Lipton},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {12549--12561},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.677},
  abstract = {Recent advancements in large language models (LLMs) have significantly advanced the capabilities of summarization systems.However, they continue to face a persistent challenge: hallucination. While prior work has extensively examined LLMs in news domains, evaluation of dialogue summarization has primarily focused on BART-based models, resulting in a notable gap in understanding LLM effectiveness.Our work seeks to address this gap by benchmarking LLMs for dialogue summarization faithfulness using human annotations,focusing on identifying and categorizing span-level inconsistencies.Specifically, we evaluate two prominent LLMs: GPT-4 and Alpaca-13B.Our evaluation reveals that LLMs often generate plausible, but not fully supported inferences based on conversation contextual cues, a trait absent in older models. As a result, we propose a refined taxonomy of errors, introducing a novel category termed ``Contextual Inference'' to address this aspect of LLM behavior. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors effectively. To address this, we introduce two prompt-based approaches for fine-grained error detection. Our methods outperform existing metrics, particularly in identifying the novel ``Contextual Inference'' error type.},
  join_key = {403}
}

@inproceedings{rayColaBenchmarkCompositional2023,
  title = {Cola: A Benchmark for Compositional Text-to-Image Retrieval},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ray, Arijit and Radenovic, Filip and Dubey, Abhimanyu and Plummer, Bryan and Krishna, Ranjay and Saenko, Kate},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {46433--46445},
  publisher = {Curran Associates, Inc.},
  join_key = {86}
}

@inproceedings{renBEACONBenchmarkComprehensive2024,
  title = {{{BEACON}}: {{Benchmark}} for Comprehensive {{RNA}} Tasks and Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ren, Yuchen and Chen, Zhiyuan and Qiao, Lifeng and Jing, Hongtai and Cai, Yuchen and Xu, Sheng and Ye, Peng and Ma, Xinzhu and Sun, Siqi and Yan, Hongliang and Yuan, Dong and Ouyang, Wanli and Liu, Xihui},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {92891--92921},
  publisher = {Curran Associates, Inc.},
  join_key = {120}
}

@inproceedings{renValueBenchComprehensivelyEvaluating2024,
  title = {{{ValueBench}}: {{Towards}} Comprehensively Evaluating Value Orientations and Understanding of Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Ren, Yuanyi {and} Ye, Haoran {and} Fang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {2015--2040},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.111},
  abstract = {Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks.},
  join_key = {356}
}

@inproceedings{ribeiroSTREETMULTITASKSTRUCTURED2023,
  title = {{{STREET}}: A {{MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK}}},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Ribeiro, Danilo Neves and Wang, Shen and Ma, Xiaofei and Zhu, Henghui and Dong, Rui and Kong, Deguang and Burger, Juliette and Ramos, Anjelica and {huang}, zhiheng and Wang, William Yang and Karypis, George and Xiang, Bing and Roth, Dan},
  year = {2023},
  join_key = {34}
}

@inproceedings{riemenschneiderExploringLargeLanguage2023,
  title = {Exploring Large Language Models for Classical Philology},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Riemenschneider, Frederick {and} Frank, Anette},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {15181--15199},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.846},
  abstract = {Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5`s decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology.},
  join_key = {295}
}

@inproceedings{romanouCRABAssessingStrength2023,
  title = {{{CRAB}}: {{Assessing}} the Strength of Causal Relationships between Real-World Events},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Romanou, Angelika {and} Montariol, Syrielle {and} Paul},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {15198--15216},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.940},
  abstract = {Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of causal relationships of events in narratives. In this work, we present CRAB, a new Causal Reasoning Assessment Benchmark designed to evaluate causal understanding of events in real-world narratives. CRAB contains fine-grained, contextual causality annotations for {\textasciitilde}2.7K pairs of real-world events that describe various newsworthy event timelines (e.g., the acquisition of Twitter by Elon Musk). Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task. Motivated by classical causal principles, we also analyze the causal structures of groups of events in CRAB, and find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains. We make our dataset and code available to the research community.},
  join_key = {341}
}

@inproceedings{romeroCVQACulturallydiverseMultilingual2024,
  title = {{{CVQA}}: {{Culturally-diverse}} Multilingual Visual Question Answering Benchmark},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Romero, David and Lyu, Chenyang and Wibowo, Haryo Akbarianto and Lynn, Teresa and Hamed, Injy and Kishore, Aditya Nanda and Mandal, Aishik and Dragonetti, Alina and Abzaliev, Artem and Tonja, Atnafu Lambebo and Balcha, Bontu Fufa and Whitehouse, Chenxi and Salamea, Christian and Velasco, Dan John and Adelani, David Ifeoluwa and Le Meur, David and {Villa-Cueva}, Emilio and Koto, Fajri and Farooqui, Fauzan and Belcavello, Frederico and Batnasan, Ganzorig and Vallejo, Gisela and Caulfield, Grainne and Ivetta, Guido and Song, Haiyue and Ademtew, Henok Biadglign and Maina, Hern{\'a}n and Lovenia, Holy and Azime, Israel Abebe and Cruz, Jan Christian Blaise and Gala, Jay and Geng, Jiahui and {Ortiz-Barajas}, Jesus-German and Baek, Jinheon and Dunstan, Jocelyn and Alemany, Laura Alonso and Nagasinghe, Kumaranage Ravindu Yasas and Benotti, Luciana and DHaro, Luis Fernando and Viridiano, Marcelo and {Estecha-Garitagoitia}, Marcos and Cabrera, Maria Camila Buitrago and {Rodr{\'{\i}}guez-Cantelar}, Mario and Jouitteau, M{\'e}lanie and Mihaylov, Mihail and Etori, Naome and Imam, Mohamed Fazli Mohamed and Adilazuarda, Muhammad Farid and Gochoo, Munkhjargal and Otgonbold, Munkh-Erdene and Niyomugisha, Olivier and Silva, Paula M{\'o}nica and Chitale, Pranjal and Dabre, Raj and Chevi, Rendi and Zhang, Ruochen and Diandaru, Ryandito and Cahyawijaya, Samuel and G{\'o}ngora, Santiago and Jeong, Soyeong and Purkayastha, Sukannya and Kuribayashi, Tatsuki and Clifford, Teresa and Jayakumar, Thanmay and Torrent, Tiago Timponi and Ehsan, Toqeer and Araujo, Vladimir and Kementchedjhieva, Yova and Burzo, Zara and Lim, Zheng Wei and Yong, Zheng Xin and Ignat, Oana and Nwatu, Joan and Mihalcea, Rada and Solorio, Thamar and Aji, Alham Fikri},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {11479--11505},
  publisher = {Curran Associates, Inc.},
  join_key = {141}
}

@inproceedings{royBenchCLAMPBenchmarkEvaluating2023,
  title = {{{BenchCLAMP}}: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Roy, Subhro and Thomson, Samuel and Chen, Tongfei and Shin, Richard and Pauls, Adam and Eisner, Jason and Van Durme, Benjamin},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {49814--49829},
  publisher = {Curran Associates, Inc.},
  join_key = {78}
}

@inproceedings{ryanRevisitingNonEnglishText2023,
  title = {Revisiting Non-{{English}} Text Simplification: A Unified Multilingual Benchmark},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Ryan, Michael J {and} Naous, Tarek {and} Xu},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {4898--4927},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.269},
  abstract = {Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.},
  join_key = {286}
}

@inproceedings{saad-falconBenchmarkingBuildingLongcontext2024,
  title = {Benchmarking and Building Long-Context Retrieval Models with {{LoCo}} and {{M2-BERT}}},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {{Saad-Falcon}, Jon and Fu, Daniel Y and Arora, Simran and Guha, Neel and Re, Christopher},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {42918--42946},
  publisher = {PMLR},
  abstract = {Retrieval pipelines are an integral component of many machine learning systems. However, they perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to finetune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scaling to documents up to 32K tokens long. We describe a pretraining data mixture which allows this encoder to process both short and long context sequences, and a finetuning approach that adapts this base model to retrieval with only single-sample batches. Finally, we validate the M2-BERT retrieval encoder on LoCoV1, finding that it outperforms competitive Transformer-based models by at least 22.2 points, despite containing 90{\texttimes} fewer parameters.},
  join_key = {8}
}

@inproceedings{sabourEmoBenchEvaluatingEmotional2024a,
  title = {{{EmoBench}}: {{Evaluating}} the Emotional Intelligence of Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Sabour, Sahand {and} Liu, Siyang {and} Zhang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {5986--6004},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.326},
  abstract = {Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.},
  join_key = {375}
}

@inproceedings{sadatMSciNLIDiverseBenchmark2024a,
  title = {{{MSciNLI}}: A Diverse Benchmark for Scientific Natural Language Inference},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Sadat, Mobashir {and} Caragea, Cornelia},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {1610--1629},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.90},
  abstract = {The task of scientific Natural Language Inference (NLI) involves predicting the semantic relation between two sentences extracted from research articles. This task was recently proposed along with a new dataset called SciNLI derived from papers published in the computational linguistics domain. In this paper, we aim to introduce diversity in the scientific NLI task and present MSciNLI, a dataset containing 132,320 sentence pairs extracted from five new scientific domains. The availability of multiple domains makes it possible to study domain shift for scientific NLI. We establish strong baselines on MSciNLI by fine-tuning Pre-trained Language Models (PLMs) and prompting Large Language Models (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21\% and 51.77\%, respectively, illustrating that MSciNLI is challenging for both types of models. Furthermore, we show that domain shift degrades the performance of scientific NLI models which demonstrates the diverse characteristics of different domains in our dataset. Finally, we use both scientific NLI datasets in an intermediate task transfer learning setting and show that they can improve the performance of downstream tasks in the scientific domain. We make our dataset and code available on Github.},
  join_key = {499}
}

@inproceedings{sahooIndiBiasBenchmarkDataset2024,
  title = {{{IndiBias}}: A Benchmark Dataset to Measure Social Biases in Language Models for {{Indian}} Context},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Sahoo, Nihar {and} Kulkarni, Pranamya {and} Ahmad},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {8786--8806},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.487},
  abstract = {The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India`s unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersectional biases along three intersectional dimensions. Our dataset contains 800 sentence pairs and 300 tuples for bias measurement across different demographics. The dataset is available in English and Hindi, providing a size comparable to existing benchmark datasets. Furthermore, using IndiBias we compare ten different language models on multiple bias measurement metrics. We observed that the language models exhibit more bias across a majority of the intersectional groups. All the scripts utilized and datasets created in this study are publicly available.},
  join_key = {518}
}

@inproceedings{salemiLaMPWhenLarge2024,
  title = {{{LaMP}}: {{When}} Large Language Models Meet Personalization},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Salemi, Alireza {and} Mysore, Sheshera {and} Bendersky},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {7370--7392},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.399},
  abstract = {This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark --- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.},
  join_key = {378}
}

@inproceedings{samdarshiConnectingDotsEvaluating2024,
  title = {Connecting the Dots: {{Evaluating}} Abstract Reasoning Capabilities of {{LLMs}} Using the {{New York Times}} Connections Word Game},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Samdarshi, Prisha {and} Mustafa, Mariam {and} Kulkarni},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {21219--21236},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1182},
  abstract = {The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect438 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice humanplayers. Our results show that even the best-performing LLM, Claude 3.5 Sonnet, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 18\% of the games. Novice and expert players perform better than Claude 3.5 Sonnet, with expert human players significantly outperforming it. We create a taxonomy of the knowledge types required to successfully cluster and categorize words in the Connections game. We find that while LLMs are decent at categorizing words based on semantic relations they struggle with other types of knowledge such as Encyclopedic Knowledge, Multiword Expressions or knowledge that combines both Word Form and Meaning. Our results establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.},
  join_key = {492}
}

@inproceedings{sanchetiAgentspecificDeonticModality2022,
  title = {Agent-Specific Deontic Modality Detection in Legal Language},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Sancheti, Abhilasha {and} Garimella, Aparna {and} Srinivasan},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {11563--11579},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.795},
  abstract = {Legal documents are typically long and written in legalese, which makes it particularly difficult for laypeople to understand their rights and duties. While natural language understanding technologies can be valuable in supporting such understanding in the legal domain, the limited availability of datasets annotated for deontic modalities in the legal domain, due to the cost of hiring experts and privacy issues, is a bottleneck. To this end, we introduce, LEXDEMOD, a corpus of English contracts annotatedwith deontic modality expressed with respect to a contracting party or agent along with the modal triggers. We benchmark this dataset on two tasks: (i) agent-specific multi-label deontic modality classification, and (ii) agent-specific deontic modality and trigger span detection using Transformer-based (Vaswani et al., 2017) language models. Transfer learning experiments show that the linguistic diversity of modal expressions in LEXDEMOD generalizes reasonably from lease to employment andrental agreements. A small case study indicates that a model trained on LEXDEMOD can detect red flags with high recall. We believe our work offers a new research direction for deontic modality detection in the legal domain.},
  join_key = {272}
}

@inproceedings{sanyalRobustLRDiagnosticBenchmark2022a,
  title = {{{RobustLR}}: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Sanyal, Soumya {and} Liao, Zeyi {and} Ren},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {9614--9631},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.653},
  abstract = {Transformers have been shown to be able to perform deductive reasoning on inputs containing rules and statements written in the English natural language. However, it is unclear if these models indeed follow rigorous logical reasoning to arrive at the prediction or rely on spurious correlation patterns in making decisions. A strong deductive reasoning model should consistently understand the semantics of different logical operators. To this end, we present RobustLR, a diagnostic benchmark that evaluates the robustness of language models to minimal logical edits in the inputs and different logical equivalence conditions. In our experiments with RoBERTa, T5, and GPT3 we show that the models trained on deductive reasoning datasets do not perform consistently on the RobustLR test set, thus showing that the models are not robust to our proposed logical perturbations. Further, we observe that the models find it especially hard to learn logical negation operators. Our results demonstrate the shortcomings of current language models in logical reasoning and call for the development of better inductive biases to teach the logical semantics to language models. All the datasets and code base have been made publicly available.},
  join_key = {268}
}

@inproceedings{saparinaAMBROSIABenchmarkParsing2024a,
  title = {{{AMBROSIA}}: A Benchmark for Parsing Ambiguous Questions into Database Queries},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Saparina, Irina and Lapata, Mirella},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {90600--90628},
  publisher = {Curran Associates, Inc.},
  join_key = {113}
}

@inproceedings{saxonWhoEvaluatesEvaluations2024,
  title = {Who Evaluates the Evaluations? {{Objectively}} Scoring Text-to-Image Prompt Coherence Metrics with {{T2IScoreScore}} ({{TS2}})},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Saxon, Michael and Jahara, Fatima and Khoshnoodi, Mahsa and Lu, Yujie and Sharma, Aditya and Wang, William Yang},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {85630--85657},
  publisher = {Curran Associates, Inc.},
  join_key = {232}
}

@inproceedings{schiappaRobustnessAnalysisVideolanguage2022,
  title = {Robustness Analysis of Video-Language Models against Visual and Language Perturbations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Schiappa, Madeline and Vyas, Shruti and Palangi, Hamid and Rawat, Yogesh and Vineet, Vibhav},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {34405--34420},
  publisher = {Curran Associates, Inc.},
  join_key = {71}
}

@inproceedings{schwettmannFINDFunctionDescription2023,
  title = {{{FIND}}: A Function Description Benchmark for Evaluating Interpretability Methods},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Schwettmann, Sarah and Shaham, Tamar and Materzynska, Joanna and Chowdhury, Neil and Li, Shuang and Andreas, Jacob and Bau, David and Torralba, Antonio},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {75688--75715},
  publisher = {Curran Associates, Inc.},
  join_key = {91}
}

@inproceedings{senelCoDA21EvaluatingLanguage2022,
  title = {{{CoDA21}}: {{Evaluating}} Language Understanding Capabilities of {{NLP}} Models with Context-Definition Alignment},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Senel, L{\"u}tfi Kerem {and} Schick, Timo {and} Schuetze},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {815--824},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-short.92},
  abstract = {Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.},
  join_key = {257}
}

@inproceedings{shahStackEvalBenchmarkingLlms2024a,
  title = {{{StackEval}}: {{Benchmarking}} Llms in Coding Assistance},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Shah, Nidhish and Genc, Zulkuf and Araci, Dogu},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {36976--36994},
  publisher = {Curran Associates, Inc.},
  join_key = {211}
}

@inproceedings{shahWhenFLUEMeets2022,
  title = {When {{FLUE}} Meets {{FLANG}}: {{Benchmarks}} and Large Pretrained Language Model for Financial Domain},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Shah, Raj {and} Chawla, Kunal {and} Eidnani},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {2322--2335},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.148},
  abstract = {Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We propose a novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective. Additionally, the evaluation benchmarks in the field have been limited. To this end, we contribute the Financial Language Understanding Evaluation (FLUE), an open-source comprehensive suite of benchmarks for the financial domain. These include new benchmarks across 5 NLP tasks in financial domain as well as common benchmarks used in the previous research. Experiments on these benchmarks suggest that our model outperforms those in prior literature on a variety of NLP tasks. Our models, code and benchmark data will be made publicly available on Github and Huggingface.},
  join_key = {261}
}

@inproceedings{shaoNYUCTFBench2024,
  title = {{{NYU CTF}} Bench: A Scalable Open-Source Benchmark Dataset for Evaluating Llms in Offensive Security},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Shao, Minghao and Jancheska, Sofija and Udeshi, Meet and {Dolan-Gavitt}, Brendan and Xi, Haoran and Milner, Kimberly and Chen, Boyuan and Yin, Max and Garg, Siddharth and Krishnamurthy, Prashanth and Khorrami, Farshad and Karri, Ramesh and Shafique, Muhammad},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {57472--57498},
  publisher = {Curran Associates, Inc.},
  join_key = {194}
}

@inproceedings{shavrinaRussianSuperGLUERussianLanguage2020,
  title = {{{RussianSuperGLUE}}: A {{Russian}} Language Understanding Evaluation Benchmark},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Shavrina, Tatiana {and} Fenogenova, Alena {and} Anton},
  editor = {Webber, Bonnie {and} Cohn, Trevor {and} He},
  year = {2020},
  month = nov,
  pages = {4717--4726},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.381},
  abstract = {In this paper, we introduce an advanced Russian general language understanding evaluation benchmark -- Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills - detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the Russian language. We also provide baselines, human level evaluation, open-source framework for evaluating models, and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art models independently of language.},
  join_key = {239}
}

@inproceedings{shenTaskBenchBenchmarkingLarge2024,
  title = {{{TaskBench}}: {{Benchmarking}} Large Language Models for Task Automation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {4540--4574},
  publisher = {Curran Associates, Inc.},
  join_key = {217}
}

@inproceedings{sheScoNeBenchmarkingNegation2023a,
  title = {{{ScoNe}}: {{Benchmarking}} Negation Reasoning in Language Models with Fine-Tuning and in-Context Learning},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {She, Jingyuan S. {and} Potts, Christopher {and} Bowman},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {1803--1821},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-short.154},
  abstract = {A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test the latest InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveals the model can correctly reason about negation, but struggles to do so on NLI examples outside of its core pretraining regime.},
  join_key = {299}
}

@inproceedings{shiLanguageModelsAre2023,
  title = {Language Models Are Multilingual Chain-of-Thought Reasoners},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and Das, Dipanjan and Wei, Jason},
  year = {2023},
  join_key = {31}
}

@inproceedings{shiLargeLanguageModels2023,
  title = {Large Language Models Can Be Easily Distracted by Irrelevant Context},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H. and Sch{\"a}rli, Nathanael and Zhou, Denny},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  year = {2023-07-23/2023-07-29},
  series = {Proceedings of Machine Learning Research},
  volume = {202},
  pages = {31210--31227},
  publisher = {PMLR},
  abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model prediction can be distracted by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of different prompting techniques for large language models, and find that the model is easily distracted by irrelevant information. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
  join_key = {23}
}

@inproceedings{shiriEmpiricalAnalysisSpatial2024,
  title = {An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Shiri, Fatemeh {and} Guo, Xiao-Yu {and} Far},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {21440--21455},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1195},
  abstract = {Large Multimodal Models (LMMs) have achieved strong performance across a range of vision and language tasks. However, their spatial reasoning capabilities are under-investigated. In this paper, we construct a novel VQA dataset, Spatial-MM, to comprehensively study LMMs' spatial understanding and reasoning capabilities. Our analyses on object-relationship and multi-hop reasoning reveal several important findings. Firstly, bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs' spatial reasoning. Secondly, LMMs struggle more with questions posed from the human perspective than the camera perspective about the image. Thirdly, chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations. Moreover, spatial reasoning steps are much less accurate than non-spatial ones across MLLMs. Lastly, our perturbation analysis on GQA-spatial reveals that LMMs are much stronger at basic object detection than complex spatial reasoning. We believe our new benchmark dataset and in-depth analyses can spark further research on LMMs spatial reasoning.},
  join_key = {493}
}

@inproceedings{shivagundeLargerProbesTell2023,
  title = {Larger Probes Tell a Different Story: {{Extending}} Psycholinguistic Datasets via in-Context Learning},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Shivagunde, Namrata {and} Lialin, Vladislav {and} Rumshisky},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {2094--2107},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.130},
  abstract = {Language model probing is often used to test specific capabilities of models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57\% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6\% of them during probing. The datasets and code are available on Github.},
  join_key = {306}
}

@inproceedings{singhIndicGenBenchMultilingualBenchmark2024,
  title = {{{IndicGenBench}}: A Multilingual Benchmark to Evaluate Generation Capabilities of {{LLMs}} on {{Indic}} Languages},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Singh, Harman {and} Gupta, Nitish {and} Bharadwaj},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {11047--11073},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.595},
  abstract = {As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench --- the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate stateof-the-art LLMs like GPT-3.5, GPT-4, PaLM2, and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench isavailable at www.github.com/google-researchdatasets/indic-gen-bench},
  join_key = {397}
}

@inproceedings{siREADINChineseMultitask2023,
  title = {{{READIN}}: A {{Chinese}} Multi-Task Benchmark with Realistic and Diverse Input Noises},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Si, Chenglei {and} Zhang, Zhengyan {and} Chen},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {8272--8285},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.460},
  abstract = {For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we construct READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input. We designed our annotation pipeline to maximize diversity, for example by instructing the annotators to use diverse input method editors (IMEs) for keyboard noises and recruiting speakers from diverse dialectical groups for speech noises. We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation. As the first large-scale attempt in creating a benchmark with noises geared towards user-generated inputs, we believe that READIN serves as an important complement to existing Chinese NLP benchmarks. The source code and dataset can be obtained from {$<$}a href="https://github.com/thunlp/READIN"{$>$}https://github.com/thunlp/READIN{$<$}/a{$>$}.},
  join_key = {288}
}

@inproceedings{sivasubramaniamSM3texttoquerySyntheticMultimodel2024a,
  title = {{{SM3-text-to-query}}: {{Synthetic}} Multi-Model Medical Text-to-Query Benchmark},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sivasubramaniam, Sithursan and {Osei-Akoto}, Cedric and Zhang, Yi and Stockinger, Kurt and F{\"u}rst, Jonathan},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {88627--88663},
  publisher = {Curran Associates, Inc.},
  join_key = {208}
}

@inproceedings{smitShouldWeBe2024,
  title = {Should We Be Going {{MAD}}? {{A}} Look at Multi-Agent Debate Strategies for {{LLMs}}},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Smit, Andries Petrus and Grinsztajn, Nathan and Duckworth, Paul and Barrett, Thomas D and Pretorius, Arnu},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {45883--45905},
  publisher = {PMLR},
  abstract = {Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter settings and difficult to optimize. We build on these results to offer insights into improving debating strategies, such as adjusting agent agreement levels, which can significantly enhance performance and even surpass all other non-debate protocols we evaluated. We provide an open-source repository to the community with several state-of-the-art protocols together with evaluation scripts to benchmark across popular research datasets.},
  join_key = {7}
}

@inproceedings{songMatSciNLPEvaluatingScientific2023,
  title = {{{MatSci-NLP}}: {{Evaluating}} Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Song, Yu {and} Miret, Santiago {and} Liu},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {3621--3639},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.201},
  abstract = {We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERT-based models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks. Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on \{pasted macro `BENCHMARK'\} and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning methods. The code and datasets are publicly available {$<$}a href="https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23"{$>$}https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23{$<$}/a{$>$}.},
  join_key = {283}
}

@inproceedings{songSLINGSinoLinguistic2022a,
  title = {{{SLING}}: {{Sino}} Linguistic Evaluation of Large Language Models},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Song, Yixiao {and} Krishna, Kalpesh {and} Bhatt},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {4606--4634},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.305},
  abstract = {To understand what kinds of linguistic knowledge are encoded by pretrained Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates the acceptability contrast of a specific syntactic or semantic phenomenon (e.g., The keys are lost vs. The keys is lost), and an LM should assign lower perplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang et al., 2021), which also contains Chinese minimal pairs and was created by translating the vocabulary of the English BLiMP dataset, the minimal pairs in SLING are derived primarily by applying syntactic and lexical transformations to naturally-occurring, linguist-annotated sentences from the Chinese Treebank 9.0, thus addressing severe issues in CLiMP`s data generation process. We test 18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and multi-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show that the average accuracy for LMs is far below human performance (69.7\% vs. 97.1\%), while BERT-base-zh achieves the highest accuracy (84.8\%) of all tested LMs, even much larger ones. Additionally, we find that most LMs have a strong gender and number (singular/plural) bias, and they perform better on local phenomena than hierarchical ones.},
  join_key = {263}
}

@inproceedings{sonMultitaskInferenceCan2024,
  title = {Multi-Task Inference: {{Can}} Large Language Models Follow Multiple Instructions at Once?},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Son, Guijin {and} Baek, SangWon {and} Nam},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {5606--5627},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.304},
  abstract = {Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle \emph{multiple} instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench (Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by {\texttimes}1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3\% and 12.4\% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this [link](https://anonymous.4open.science/r/MTI-Bench-6F01).},
  join_key = {373}
}

@inproceedings{spragueMuSRTestingLimits2024,
  title = {{{MuSR}}: {{Testing}} the Limits of Chain-of-Thought with Multistep Soft Reasoning},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Sprague, Zayne Rea and Ye, Xi and Bostrom, Kaj and Chaudhuri, Swarat and Durrett, Greg},
  year = {2024},
  join_key = {56}
}

@inproceedings{srinivasanCLiMBContinualLearning2022,
  title = {{{CLiMB}}: A Continual Learning Benchmark for Vision-and-Language Tasks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Srinivasan, Tejas and Chang, Ting-Yun and Pinto Alva, Leticia and Chochlakis, Georgios and Rostami, Mohammad and Thomason, Jesse},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {29440--29453},
  publisher = {Curran Associates, Inc.},
  join_key = {68}
}

@inproceedings{suActPlan1KBenchmarkingProcedural2024,
  title = {{{ActPlan-1K}}: {{Benchmarking}} the Procedural Planning Ability of Visual Language Models in Household Activities},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Su, Ying {and} Ling, Zhan {and} Shi},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {14953--14965},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.833},
  abstract = {Large language models(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model`s reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.},
  join_key = {472}
}

@inproceedings{subbiahSTORYSUMMEvaluatingFaithfulness2024,
  title = {{{STORYSUMM}}: {{Evaluating}} Faithfulness in Story Summarization},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Subbiah, Melanie {and} Ladhak, Faisal {and} Mishra},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {9988--10005},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.557},
  abstract = {Human evaluation has been the gold standard for checking faithfulness in abstractive summarization. However, with a challenging source domain like narrative, multiple annotators can agree a summary is faithful, while missing details that are obvious errors only once pointed out. We therefore introduce a new dataset, StorySumm, comprising LLM summaries of short stories with localized faithfulness labels and error explanations. This benchmark is for evaluation methods, testing whether a given method can detect challenging inconsistencies. Using this dataset, we first show that any one human annotation protocol is likely to miss inconsistencies, and we advocate for pursuing a range of methods when establishing ground truth for a summarization dataset. We finally test recent automatic metrics and find that none of them achieve more than 70\% balanced accuracy on this task, demonstrating that it is a challenging benchmark for future work in faithfulness evaluation.},
  join_key = {456}
}

@inproceedings{sukthankerHWGPTbenchHardwareawareArchitecture2024,
  title = {{{HW-GPT-bench}}: {{Hardware-aware}} Architecture Benchmark for Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sukthanker, Rhea Sanjay and Zela, Arber and Staffler, Benedikt and Klein, Aaron and Purucker, Lennart and Franke, J{\"o}rg K. H. and Hutter, Frank},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {60776--60834},
  publisher = {Curran Associates, Inc.},
  join_key = {161}
}

@inproceedings{suLivingMomentCan2024,
  title = {Living in the Moment: {{Can}} Large Language Models Grasp Co-Temporal Reasoning?},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Su, Zhaochen {and} Li, Juntao {and} Zhang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {13014--13033},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.703},
  abstract = {Temporal reasoning is fundamental for large language models (LLMs) to comprehend the world. Current temporal reasoning datasets are limited to questions about single or isolated events, falling short in mirroring the realistic temporal characteristics involving concurrent nature and intricate temporal interconnections. In this paper, we introduce CoTempQA, a comprehensive co-temporal Question Answering (QA) benchmark containing four co-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our extensive experiments reveal a significant gap between the performance of current LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced with Chain of Thought (CoT) methodologies, models consistently struggle with our task. In our preliminary exploration, we discovered that mathematical reasoning plays a significant role in handling co-temporal events and proposed a strategy to boost LLMs' co-temporal reasoning from a mathematical perspective. We hope that our CoTempQA datasets will encourage further advancements in improving the co-temporal reasoning capabilities of LLMs.},
  join_key = {407}
}

@inproceedings{sunBenchmarkingChineseCommonsense2024,
  title = {Benchmarking {{Chinese}} Commonsense Reasoning of {{LLMs}}: {{From Chinese-specifics}} to Reasoning-Memorization Correlations},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Sun, Jiaxing {and} Huang, Weiquan {and} Wu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {11205--11228},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.604},
  abstract = {We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM`s language orientation and the task`s domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM.},
  join_key = {398}
}

@inproceedings{sunFevalAsssessingFundamental2024,
  title = {F-Eval: {{Asssessing}} Fundamental Abilities with Refined Evaluation Methods},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Sun, Yu {and} Keyuchen, Keyuchen {and} Wang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {9348--9369},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.507},
  abstract = {Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs' fundamental abilities.},
  join_key = {388}
}

@inproceedings{sungCanLanguageModels2021a,
  title = {Can Language Models Be Biomedical Knowledge Bases?},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Sung, Mujeen {and} Lee, Jinhyuk {and} Yi},
  editor = {Moens, Marie-Francine {and} Huang, Xuanjing {and} Specia},
  year = {2021},
  month = nov,
  pages = {4723--4734},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.388},
  abstract = {Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51\% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.},
  join_key = {244}
}

@inproceedings{sunHeadtotailHowKnowledgeable2024a,
  title = {Head-to-Tail: {{How}} Knowledgeable Are Large Language Models ({{LLMs}})? {{A}}.{{K}}.{{A}}. {{Will LLMs}} Replace Knowledge Graphs?},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Sun, Kai {and} Xu, Yifan {and} Zha},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {311--325},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.18},
  abstract = {Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.},
  join_key = {496}
}

@inproceedings{sunInformalLanguageProcessing2024,
  title = {Toward Informal Language Processing: {{Knowledge}} of Slang in Large Language Models},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Sun, Zhewei {and} Hu, Qian {and} Gupta},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {1683--1701},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.94},
  abstract = {Recent advancement in large language models (LLMs) has offered a strong potential for natural language systems to process informal language. A representative form of informal language is slang, used commonly in daily conversations and online social media. To date, slang has not been comprehensively evaluated in LLMs due partly to the absence of a carefully designed and publicly accessible benchmark. Using movie subtitles, we construct a dataset that supports evaluation on a diverse set of tasks pertaining to automatic processing of slang. For both evaluation and finetuning, we show the effectiveness of our dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences. We also show how our dataset can be used to probe the output distributions of LLMs for interpretive insights. We find that while LLMs such as GPT-4 achieve good performance in a zero-shot setting, smaller BERT-like models finetuned on our dataset achieve comparable performance. Furthermore, we show that our dataset enables finetuning of LLMs such as GPT-3.5 that achieve substantially better performance than strong zero-shot baselines. Our work offers a comprehensive evaluation and a high-quality benchmark on English slang based on the OpenSubtitles corpus, serving both as a publicly accessible resource and a platform for applying tools for informal language processing.},
  join_key = {500}
}

@inproceedings{sunMeasuringEffectInfluential2023a,
  title = {Measuring the Effect of Influential Messages on Varying Personas},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Sun, Chenkai {and} Li, Jinning {and} Chan},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {554--562},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-short.48},
  abstract = {Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. We further evaluate the SOTA neural language models with our dataset. The empirical results suggest that the included persona attributes are helpful for the performance of all response dimensions. Our analysis shows that the best-performing models are capable of predicting responses that are consistent with the personas, and as a byproduct, the task formulation also enables many interesting applications in the analysis of social network groups and their opinions, such as the discovery of extreme opinion groups.},
  join_key = {298}
}

@inproceedings{sunRevealingPersonalityTraits2024,
  title = {Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Sun, Lei {and} Zhao, Jinming {and} Jin},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {19988--20002},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1115},
  abstract = {Personality recognition aims to identify the personality traits implied in user data such as dialogues and social media posts. Current research predominantly treats personality recognition as a classification task, failing to reveal the supporting evidence for the recognized personality. In this paper, we propose a novel task named Explainable Personality Recognition, aiming to reveal the reasoning process as supporting evidence of the personality trait. Inspired by personality theories, personality traits are made up of stable patterns of personality state, where the states are short-term characteristic patterns of thoughts, feelings, and behaviors in a concrete situation at a specific moment in time. We propose an explainable personality recognition framework called Chain-of-Personality-Evidence (CoPE), which involves a reasoning process from specific contexts to short-term personality states to long-term personality traits. Furthermore, based on the CoPE framework, we construct an explainable personality recognition dataset from dialogues, PersonalityEvd. We introduce two explainable personality state recognition and explainable personality trait recognition tasks, which require models to recognize the personality state and trait labels and their corresponding support evidence. Our extensive experiments based on Large Language Models on the two tasks show that revealing personality traits is very challenging and we present some insights for future research. We will release our dataset and source code to facilitate further studies in this direction.},
  join_key = {489}
}

@inproceedings{suTextttConflictBankBenchmarkEvaluating2024,
  title = {{\textbackslash}texttt\{\vphantom\}{{ConflictBank}}\vphantom\{\}: A Benchmark for Evaluating the Influence of Knowledge Conflicts in Llms},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Su, Zhaochen and Zhang, Jun and Qu, Xiaoye and Zhu, Tong and Li, Yanshu and Sun, Jiashuo and Li, Juntao and Zhang, Min and Cheng, Yu},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {103242--103268},
  publisher = {Curran Associates, Inc.},
  join_key = {110}
}

@inproceedings{taktashevaRuBLiMPRussianBenchmark2024,
  title = {{{RuBLiMP}}: {{Russian}} Benchmark of Linguistic Minimal Pairs},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Taktasheva, Ekaterina {and} Bazhukov, Maxim {and} Koncha},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {9268--9299},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.522},
  abstract = {Minimal pairs are a well-established approach to evaluating the grammatical knowledge of language models. However, existing resources for minimal pairs address a limited number of languages and lack diversity of language-specific grammatical phenomena. This paper introduces the Russian Benchmark of Linguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that differ in grammaticality and isolate a morphological, syntactic, or semantic phenomenon. In contrast to existing benchmarks of linguistic minimal pairs, RuBLiMP is created by applying linguistic perturbations to automatically annotated sentences from open text corpora and decontaminating test data. We describe the data collection protocol and present the results of evaluating 25 language models in various scenarios. We find that the widely used LMs for Russian are sensitive to morphological and agreement-oriented contrasts, but fall behind humans on phenomena requiring the understanding of structural relations, negation, transitivity, and tense. RuBLiMP, the codebase, and other materials are publicly available.},
  join_key = {455}
}

@inproceedings{tamkinTaskAmbiguityHumans2023,
  title = {Task Ambiguity in Humans and Language Models},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Tamkin, Alex and Handa, Kunal and Shrestha, Avash and Goodman, Noah},
  year = {2023},
  join_key = {35}
}

@inproceedings{tanBenchmarkingImprovingTemporal2023,
  title = {Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Tan, Qingyu {and} Ng, Hwee Tou {and} Bing},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {14820--14835},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.828},
  abstract = {Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.},
  join_key = {294}
}

@inproceedings{tanDevBenchMultimodalDevelopmental2024,
  title = {{{DevBench}}: {{A}} Multimodal Developmental Benchmark for Language Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Tan, Alvin W. M. and Yu, Sunny and Long, Bria and Anya, Wanjing and Murray, Tonya and Silverman, Rebecca D. and Yeatman, Jason D. and Frank, Michael C.},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {77445--77467},
  publisher = {Curran Associates, Inc.},
  join_key = {145}
}

@inproceedings{tangStrucbenchAreLarge2024,
  title = {Struc-Bench: {{Are}} Large Language Models Good at Generating Complex Structured Tabular Data?},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 2: {{Short}} Papers)},
  author = {Tang, Xiangru {and} Zong, Yiming {and} Phang},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {12--34},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-short.2},
  abstract = {Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs' proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions, coverage, formatting, reasoning, comprehension, pragmatics, and hallucination, highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.},
  join_key = {519}
}

@inproceedings{tangTofuEvalEvaluatingHallucinations2024,
  title = {{{TofuEval}}: {{Evaluating}} Hallucinations of {{LLMs}} on Topic-Focused Dialogue Summarization},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Tang, Liyan {and} Shalyminov, Igor {and} Wong},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {4455--4480},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.251},
  abstract = {Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence- level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model`s size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.},
  join_key = {508}
}

@inproceedings{tanzerBenchmarkLearningTranslate2024,
  title = {A Benchmark for Learning to Translate a New Language from One Grammar Book},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Tanzer, Garrett and Suzgun, Mirac and Visser, Eline and Jurafsky, Dan and {Melas-Kyriazi}, Luke},
  year = {2024},
  join_key = {38}
}

@inproceedings{tianDiagnosingFirstorderLogical2021,
  title = {Diagnosing the First-Order Logical Reasoning Ability through {{LogicNLI}}},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Tian, Jidong {and} Li, Yitian {and} Chen},
  editor = {Moens, Marie-Francine {and} Huang, Xuanjing {and} Specia},
  year = {2021},
  month = nov,
  pages = {3738--3747},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.303},
  abstract = {Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability.},
  join_key = {243}
}

@inproceedings{tianSciCodeResearchCoding2024,
  title = {{{SciCode}}: A Research Coding Benchmark Curated by Scientists},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Tian, Minyang and Gao, Luyu and Zhang, Shizhuo Dylan and Chen, Xinan and Fan, Cunwei and Guo, Xuefei and Haas, Roland and Ji, Pan and Krongchon, Kittithat and Li, Yao and Liu, Shengyan and Luo, Di and Ma, Yutao and Tong, Hao and Trinh, Kha and Tian, Chenyu and Wang, Zihan and Wu, Bohao and Xiong, Yanyu and Yin, Shengzhu and Zhu, Minhui and Lieret, Kilian and Lu, Yanxin and Liu, Genglin and Du, Yufeng and Tao, Tianhua and Press, Ofir and Callan, Jamie and Huerta, Eliu and Peng, Hao},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {30624--30650},
  publisher = {Curran Associates, Inc.},
  join_key = {204}
}

@inproceedings{toyerTensorTrustInterpretable2024,
  title = {Tensor Trust: {{Interpretable}} Prompt Injection Attacks from an Online Game},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Toyer, Sam and Watkins, Olivia and Mendes, Ethan Adrian and Svegliato, Justin and Bailey, Luke and Wang, Tiffany and Ong, Isaac and Elmaaroufi, Karim and Abbeel, Pieter and Darrell, Trevor and Ritter, Alan and Russell, Stuart},
  year = {2024},
  join_key = {62}
}

@inproceedings{trivediAppWorldControllableWorld2024,
  title = {{{AppWorld}}: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Trivedi, Harsh {and} Khot, Tushar {and} Hartmann},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {16022--16076},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.850},
  abstract = {Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of {\textasciitilde}100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only {\textasciitilde}49\% of our `normal' tasks and {\textasciitilde}30\% of `challenge' tasks, while other models solve at least 16\% fewer. This highlights the benchmark`s difficulty and AppWorld`s potential to push the frontiers of interactive coding agents.},
  join_key = {421}
}

@inproceedings{tsurutaSARSCoV2InteractionDataset2024,
  title = {A {{SARS-CoV-2}} Interaction Dataset and {{VHH}} Sequence Corpus for Antibody Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Tsuruta, Hirofumi and Yamazaki, Hiroyuki and Maeda, Ryota and Tamura, Ryotaro and Imura, Akihiro},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {116149--116171},
  publisher = {Curran Associates, Inc.},
  join_key = {116}
}

@inproceedings{tuCharacterEvalChineseBenchmark2024,
  title = {{{CharacterEval}}: A {{Chinese}} Benchmark for Role-Playing Conversational Agent Evaluation},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Tu, Quan {and} Fan, Shilong {and} Tian},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {11836--11850},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.638},
  abstract = {Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce \emph{CharacterEval}, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. \emph{CharacterEval} employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in \emph{CharacterEval}, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on \emph{CharacterEval} demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.},
  join_key = {401}
}

@inproceedings{tuWaterBenchHolisticEvaluation2024,
  title = {{{WaterBench}}: {{Towards}} Holistic Evaluation of Watermarks for Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Tu, Shangqing {and} Sun, Yuliang {and} Bai},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {1517--1542},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.83},
  abstract = {To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For benchmarking procedure, to ensure an apples-to-apples comparison, we first adjust each watermarking method`s hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For task selection, we diversify the input and output length to form a five-category taxonomy, covering 9 tasks. (3) For evaluation metric, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate 4 open-source watermarks on 2 LLMs under 2 watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github.com/THU-KEG/WaterBench.},
  join_key = {353}
}

@inproceedings{ushioGenerativeLanguageModels2022,
  title = {Generative Language Models for Paragraph-Level Question Generation},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Ushio, Asahi {and} Alva-Manchego, Fernando {and} Camacho-Collados},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {670--688},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.42},
  abstract = {Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting. It includes general-purpose datasets such as SQuAD for English, datasets from ten domains and two styles, as well as datasets in eight different languages. Using QG-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task. First, we propose robust QG baselines based on fine-tuning generative language models. Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating QG models. Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English.QG-Bench is released along with the fine-tuned models presented in the paper (https://github.com/asahi417/lm-question-generation), which are also available as a demo (https://autoqg.net/).},
  join_key = {258}
}

@inproceedings{valmeekamPlanBenchExtensibleBenchmark2023,
  title = {{{PlanBench}}: {{An}} Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {38975--38987},
  publisher = {Curran Associates, Inc.},
  join_key = {100}
}

@inproceedings{waghjaleECCOCanWe2024,
  title = {{{ECCO}}: {{Can}} We Improve Model-Generated Code Efficiency without Sacrificing Functional Correctness?},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Waghjale, Siddhant {and} Veerendranath, Vishruth {and} Wang},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {15362--15376},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.859},
  abstract = {Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code.},
  join_key = {477}
}

@inproceedings{wanFactualityTaxDiversityintervened2024,
  title = {The Factuality Tax of Diversity-Intervened Text-to-Image Generation: {{Benchmark}} and Fact-Augmented Intervention},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Wan, Yixin {and} Wu, Di {and} Wang},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {9082--9100},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.513},
  abstract = {Prompt-based ``diversity interventions'' are commonly adopted to improve the diversity of Text-to-Image (T2I) models depicting individuals with various racial or gender traits. However, will this strategy result in nonfactual demographic distribution, especially when generating real historical figures? In this work, we propose **DemOgraphic FActualIty Representation (DoFaiR)**, a benchmark to systematically quantify the trade-off between using diversity interventions and preserving demographic factuality in T2I models. DoFaiR consists of 756 meticulously fact-checked test instances to reveal the factuality tax of various diversity prompts through an automated evidence-supported evaluation pipeline. Experiments on DoFaiR unveil that diversity-oriented instructions increase the number of different gender and racial groups in DALLE-3`s generations at the cost of historically inaccurate demographic distributions. To resolve this issue, we propose **Fact-Augmented Intervention** (FAI), which instructs a Large Language Model (LLM) to reflect on verbalized or retrieved factual information about gender and racial compositions of generation subjects in history, and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI significantly improves the demographic factuality under diversity interventions while preserving diversity.},
  join_key = {454}
}

@inproceedings{wangAdaLEvalEvaluatingLongcontext2024,
  title = {Ada-{{LEval}}: {{Evaluating}} Long-Context {{LLMs}} with Length-Adaptable Benchmarks},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Wang, Chonghua {and} Duan, Haodong {and} Zhang},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {3712--3724},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.205},
  abstract = {Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.},
  join_key = {506}
}

@inproceedings{wangAppBenchPlanningMultiple2024,
  title = {{{AppBench}}: {{Planning}} of Multiple {{APIs}} from Various {{APPs}} for Complex User Instruction},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Wang, Hongru {and} Wang, Rui {and} Xue},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {15322--15336},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.856},
  abstract = {Large Language Models (LLMs) can interact with the real world by connecting with versatile external APIs, resulting in better problem-solving and task automation capabilities. Previous research primarily either focuses on APIs with limited arguments from a single source or overlooks the complex dependency relationship between different APIs. However, it is essential to utilize multiple APIs collaboratively from various sources, especially for complex user instructions. In this paper, we introduce MetaBench, the first benchmark to evaluate LLMs' ability to plan and execute multiple APIs from various sources in order to complete the user`s task. Specifically, we consider two significant challenges in multiple APIs: 1) graph structures: some APIs can be executed independently while others need to be executed one by one, resulting in graph-like execution order; and 2) permission constraints: which source is authorized to execute the API call. We have experimental results on 9 distinct LLMs; e.g., GPT-4o achieves only a 2.0\% success rate at the most complex instruction, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning and finetuning. Our code and data are publicly available at {$<$}a href="https://github.com/ruleGreen/AppBench"{$>$}https://github.com/ruleGreen/AppBench{$<$}/a{$>$}.},
  join_key = {476}
}

@inproceedings{wangCanLanguageModels2023,
  title = {Can Language Models Solve Graph Problems in Natural Language?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {30840--30861},
  publisher = {Curran Associates, Inc.},
  join_key = {82}
}

@inproceedings{wangCanLanguageModels2024,
  title = {Can Language Models Serve as Text-Based World Simulators?},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Wang, Ruoyao {and} Todd, Graham {and} Xiao},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {1--17},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-short.1},
  abstract = {Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM`s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.},
  join_key = {425}
}

@inproceedings{wangCMBComprehensiveMedical2024a,
  title = {{{CMB}}: A Comprehensive Medical Benchmark in {{Chinese}}},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Wang, Xidong {and} Chen, Guiming {and} Dingjie},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {6184--6205},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.343},
  abstract = {Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in \emph{contextual incongruities} to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB.},
  join_key = {515}
}

@inproceedings{wangDecodingTrustComprehensiveAssessment2023,
  title = {{{DecodingTrust}}: A Comprehensive Assessment of Trustworthiness in {{GPT}} Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and Truong, Sang and Arora, Simran and Mazeika, Mantas and Hendrycks, Dan and Lin, Zinan and Cheng, Yu and Koyejo, Sanmi and Song, Dawn and Li, Bo},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {31232--31339},
  publisher = {Curran Associates, Inc.},
  join_key = {87}
}

@inproceedings{wangGTABenchmarkGeneral2024,
  title = {{{GTA}}: A Benchmark for General Tool Agents},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Jize and Ma, Zerun and Li, Yining and Zhang, Songyang and Chen, Cailian and Chen, Kai and Le, Xinyi},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {75749--75790},
  publisher = {Curran Associates, Inc.},
  join_key = {159}
}

@inproceedings{wangIELMOpenInformation2022a,
  title = {{{IELM}}: {{An}} Open Information Extraction Benchmark for Pre-Trained Language Models},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Wang, Chenguang {and} Liu, Xiao {and} Song},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {8417--8437},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.576},
  abstract = {We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer ``fill-in-the-blank'' questions when given a pre-defined relation category. Instead of focusing on pre-defined relations, we create an OIE benchmark aiming to fully examine the open relational information present in the pre-trained LMs. We accomplish this by turning pre-trained LMs into zero-shot OIE systems. Surprisingly, pre-trained LMs are able to obtain competitive performance on both standard OIE datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets (TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For instance, the zero-shot pre-trained LMs outperform the F1 score of the state-of-the-art supervised OIE methods on our factual OIE datasets without needing to use any training sets.},
  join_key = {267}
}

@inproceedings{wangJourneyBenchChallengingOnestop2024,
  title = {{{JourneyBench}}: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Zhecan and Liu, Junzhang and Tang, Chia-Wei and Alomari, Hani and Sivakumar, Anushka and Sun, Rui and Li, Wenhao and Atabuzzaman, {\relax Md}. and Ayyubi, Hammad and You, Haoxuan and Ishmam, Alvi and Chang, Kai-Wei and Chang, Shih-Fu and Thomas, Chris},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {63110--63123},
  publisher = {Curran Associates, Inc.},
  join_key = {167}
}

@inproceedings{wangLargeLanguageModels2024,
  title = {Do Large Language Models Rank Fairly? {{An}} Empirical Study on the Fairness of {{LLMs}} as Rankers},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Wang, Yuan {and} Wu, Xuyang {and} Wu},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {5712--5724},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.319},
  abstract = {The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works such as RankGPT have demonstrated that the LLMs have better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.},
  join_key = {513}
}

@inproceedings{wangLeaveNoDocument2024,
  title = {Leave No Document behind: {{Benchmarking}} Long-Context {{LLMs}} with Extended Multi-Doc {{QA}}},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Wang, Minzheng {and} Chen, Longze {and} Cheng},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {5627--5646},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.322},
  abstract = {Long-context modeling capabilities of Large Language Models (LLMs) have garnered widespread attention, leading to the emergence of LLMs with ultra-context windows. Meanwhile, benchmarks for evaluating long-context language models are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong`s test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model`s long-context modeling capabilities.},
  join_key = {440}
}

@inproceedings{wangM4GTbenchEvaluationBenchmark2024,
  title = {{{M4GT-bench}}: {{Evaluation}} Benchmark for Black-Box Machine-Generated Text Detection},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Wang, Yuxia {and} Mansurov, Jonibek {and} Ivanov},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {3964--3992},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.218},
  abstract = {The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain and multi-generator corpus of MGTs --- M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github.com/mbzuai-nlp/M4GT-Bench.},
  join_key = {364}
}

@inproceedings{wangMAVENARGCompletingPuzzle2024,
  title = {{{MAVEN-ARG}}: {{Completing}} the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Wang, Xiaozhi {and} Peng, Hao {and} Guan},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {4072--4091},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.224},
  abstract = {Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and codes can be obtained from https://github.com/THU-KEG/MAVEN-Argument.},
  join_key = {365}
}

@inproceedings{wangMINTEvaluatingLLMs2024,
  title = {{{MINT}}: {{Evaluating LLMs}} in Multi-Turn Interaction with Tools and Language Feedback},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Wang, Xingyao and Wang, Zihan and Liu, Jiateng and Chen, Yangyi and Yuan, Lifan and Peng, Hao and Ji, Heng},
  year = {2024},
  join_key = {54}
}

@inproceedings{wangMMLUproMoreRobust2024,
  title = {{{MMLU-pro}}: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and Li, Tianle and Ku, Max and Wang, Kai and Zhuang, Alex and Fan, Rongqi and Yue, Xiang and Chen, Wenhu},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {95266--95290},
  publisher = {Curran Associates, Inc.},
  join_key = {185}
}

@inproceedings{wangMULFEMultilevelBenchmark2024,
  title = {{{MULFE}}: A Multi-Level Benchmark for Free Text Model Editing},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Wang, Chenhao {and} Cao, Pengfei {and} Jin},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {13570--13587},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.732},
  abstract = {Adjusting the outdated behaviors of large langugae models (LLMs) after deployment remains a significant challenge. It motivates the model editing research, which is however mainly explored in a restricted task form with triple-based edit requests. Recent works have initiated a transition to a more practical and unified editing task that takes free-form text as edit requests. However, there are gaps in nuanced benchmark designs and re-evaluation of existing methods. To bridge the gaps, we introduce a multi-level benchmark for free text model editing (MULFE). The benchmark categorizes probe queries into three levels of generalization, ranging from basic literal memory to deeper understanding and reasoning. Based on the benchmark, we conduct extensive experiments across various base models, edit sizes, and editing methods, including adaptations of mainstream locate-and-edit and hypernetwork methods. The results highlight the inconsistent behaviors of edited models on different generalization levels. Higher-level generalization remains a significant challenge. Based on the findings, we propose SIDE, a simple yet effective method based on in-context distillation to enhance the generalization performance. The benchmark dataset and evaluation scripts are publicly available at http://github.com/wchrepo/mulfe.},
  join_key = {409}
}

@inproceedings{wangNeedleMultimodalHaystack2024,
  title = {Needle in a Multimodal Haystack},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Weiyun and Zhang, Shuibo and Ren, Yiming and Duan, Yuchen and Li, Tiantong and Liu, Shuo and Hu, Mengkang and Chen, Zhe and Zhang, Kaipeng and Lu, Lewei and Zhu, Xizhou and Luo, Ping and Qiao, Yu and Dai, Jifeng and Shao, Wenqi and Wang, Wenhai},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {20540--20565},
  publisher = {Curran Associates, Inc.},
  join_key = {192}
}

@inproceedings{wangPandaLMAutomaticEvaluation2024,
  title = {{{PandaLM}}: {{An}} Automatic Evaluation Benchmark for {{LLM}} Instruction Tuning Optimization},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Wang, Yidong and Yu, Zhuohao and Yao, Wenjin and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and Ye, Wei and Zhang, Shikun and Zhang, Yue},
  year = {2024},
  join_key = {59}
}

@inproceedings{wangPictureWorthThousand2024,
  title = {Is a Picture Worth a Thousand Words? {{Delving}} into Spatial Reasoning for Vision Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Jiayu and Ming, Yifei and Shi, Zhenmei and Vineet, Vibhav and Wang, Xin and Li, Yixuan and Joshi, Neel},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {75392--75421},
  publisher = {Curran Associates, Inc.},
  join_key = {164}
}

@inproceedings{wangPretrainingLanguageModel2023,
  title = {On Pre-Training Language Model for Antibody},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Wang, Danqing and YE, Fei and Zhou, Hao},
  year = {2023},
  join_key = {33}
}

@inproceedings{wangSciBenchEvaluatingCollegelevel2024,
  title = {{{SciBench}}: {{Evaluating}} College-Level Scientific Problem-Solving Abilities of Large Language Models},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Wang, Xiaoxuan and Hu, Ziniu and Lu, Pan and Zhu, Yanqiao and Zhang, Jieyu and Subramaniam, Satyen and Loomba, Arjun R and Zhang, Shichang and Sun, Yizhou and Wang, Wei},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {50622--50649},
  publisher = {PMLR},
  abstract = {Most existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22},
  join_key = {6}
}

@inproceedings{wangUsercentricMultiintentBenchmark2024,
  title = {A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Wang, Jiayin {and} Mo, Fengran {and} Ma},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {3588--3612},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.210},
  abstract = {Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset, code, and process data are publicly available at https://github.com/Alice1998/URS.},
  join_key = {435}
}

@inproceedings{wattsPARIKSHALargescaleInvestigation2024,
  title = {{{PARIKSHA}}: A Large-Scale Investigation of Human-{{LLM}} Evaluator Agreement on Multilingual and Multi-Cultural Data},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Watts, Ishaan {and} Gumma, Varun {and} Yadavalli},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {7900--7932},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.451},
  abstract = {Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors -- the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.},
  join_key = {448}
}

@inproceedings{wenBenchmarkingComplexInstructionfollowing2024,
  title = {Benchmarking Complex Instruction-Following with Multiple Constraints Composition},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wen, Bosi and Ke, Pei and Gu, Xiaotao and Wu, Lindong and Huang, Hao and Zhou, Jinfeng and Li, Wenchuang and Hu, Binxin and Gao, Wendy and Xu, Jiaxin and Liu, Yiming and Tang, Jie and Wang, Hongning and Huang, Minlie},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {137610--137645},
  publisher = {Curran Associates, Inc.},
  join_key = {121}
}

@inproceedings{wuAKEWAssessingKnowledge2024,
  title = {{{AKEW}}: {{Assessing}} Knowledge Editing in the Wild},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Wu, Xiaobao {and} Pan, Liangming {and} Wang},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {15118--15133},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.843},
  abstract = {Knowledge editing injects knowledge updates into language models to keep them correct and up-to-date. However, its current evaluations deviate significantly from practice: their knowledge updates solely consist of structured facts derived from meticulously crafted datasets, instead of practical sources---unstructured texts like news articles, and they often overlook practical real-world knowledge updates. To address these issues, in this paper we propose AKEW (Assessing Knowledge Editing in the Wild), a new practical benchmark for knowledge editing. AKEW fully covers three editing settings of knowledge updates: structured facts, unstructured texts as facts, and extracted triplets. It further introduces new datasets featuring both counterfactual and real-world knowledge updates. Through extensive experiments, we demonstrate the considerable gap between state-of-the-art knowledge-editing methods and practical scenarios. Our analyses further highlight key insights to motivate future research for practical knowledge editing.},
  join_key = {474}
}

@inproceedings{wuClashEvalQuantifyingTugofwar2024a,
  title = {{{ClashEval}}: {{Quantifying}} the Tug-of-War between an {{LLM{\^a}}}{\texteuro}s Internal Prior and External Evidence},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wu, Kevin and Wu, Eric and Zou, James},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {33402--33422},
  publisher = {Curran Associates, Inc.},
  join_key = {131}
}

@inproceedings{wuDetectRLBenchmarkingLLMgenerated2024,
  title = {{{DetectRL}}: {{Benchmarking LLM-generated}} Text Detection in Real-World Scenarios},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wu, Junchao and Zhan, Runzhe and Wong, Derek F. and Yang, Shu and Yang, Xinyi and Yuan, Yulin and Chao, Lidia S.},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {100369--100401},
  publisher = {Curran Associates, Inc.},
  join_key = {144}
}

@inproceedings{wuEvaluatingAnalyzingRelationship2024a,
  title = {Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Wu, Mingrui and Ji, Jiayi and Huang, Oucheng and Li, Jiale and Wu, Yuhang and Sun, Xiaoshuai and Ji, Rongrong},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {53553--53570},
  publisher = {PMLR},
  abstract = {The issue of hallucinations is a prevalent concern in existing Large Vision-Language Models (LVLMs). Previous efforts have primarily focused on investigating object hallucinations, which can be easily alleviated by introducing object detectors. However, these efforts neglect hallucinations in inter-object relationships, which is essential for visual comprehension. In this work, we introduce R-Bench, a novel benchmark for evaluating Vision Relationship Hallucination. R-Bench features image-level questions that focus on the existence of relationships and instance-level questions that assess local visual comprehension. We identify three types of relationship co-occurrences that lead to hallucinations: relationship-relationship, subject-relationship, and relationship-object. The visual instruction tuning dataset's long-tail distribution significantly impacts LVLMs' understanding of visual relationships. Additionally, our analysis reveals that current LVLMs tend to overlook visual content, overly rely on the common sense knowledge of Large Language Models (LLMs), and struggle with spatial relationship reasoning based on contextual information.},
  join_key = {5}
}

@inproceedings{wuMedJourneyBenchmarkEvaluation2024a,
  title = {{{MedJourney}}: {{Benchmark}} and Evaluation of Large Language Models over Patient Clinical Journey},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wu, Xian and Zhao, Yutian and Zhang, Yunyan and Wu, Jiageng and Zhu, Zhihong and Zhang, Yingying and Ouyang, Yi and Zhang, Ziheng and Wang, Huimin and Lin, Zhenxi and Yang, Jie and Zhao, Shuang and Zheng, Yefeng},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {87621--87646},
  publisher = {Curran Associates, Inc.},
  join_key = {177}
}

@inproceedings{wuSmartPlayBenchmarkLLMs2024,
  title = {{{SmartPlay}} : A Benchmark for {{LLMs}} as Intelligent Agents},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Wu, Yue and Tang, Xuan and Mitchell, Tom and Li, Yuanzhi},
  year = {2024},
  join_key = {61}
}

@inproceedings{wuSTaRKBenchmarkingLLM2024a,
  title = {{{STaRK}}: {{Benchmarking LLM}} Retrieval on Textual and Relational Knowledge Bases},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wu, Shirley and Zhao, Shiyu and Yasunaga, Michihiro and Huang, Kexin and Cao, Kaidi and Huang, Qian and Ioannidis, Vassilis N. and Subbian, Karthik and Zou, James and Leskovec, Jure},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {127129--127153},
  publisher = {Curran Associates, Inc.},
  join_key = {212}
}

@inproceedings{wuStreamBenchBenchmarkingContinuous2024,
  title = {{{StreamBench}}: {{Towards}} Benchmarking Continuous Improvement of Language Agents},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wu, Cheng-Kuang and Tam, Zhi Rui and Lin, Chieh-Yen and Chen, Yun-Nung and Lee, Hung-yi},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {107039--107063},
  publisher = {Curran Associates, Inc.},
  join_key = {213}
}

@inproceedings{xiaCARESComprehensiveBenchmark2024a,
  title = {{{CARES}}: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Xia, Peng and Chen, Ze and Tian, Juanxi and Gong, Yangrui and Hou, Ruibo and Xu, Yue and Wu, Zhenbang and Fan, Zhiyuan and Zhou, Yiyang and Zhu, Kangyu and Zheng, Wenhao and Wang, Zhaoyang and Wang, Xiao and Zhang, Xuchao and Bansal, Chetan and Niethammer, Marc and Huang, Junzhou and Zhu, Hongtu and Li, Yun and Sun, Jimeng and Ge, Zongyuan and Li, Gang and Zou, James and Yao, Huaxiu},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {140334--140365},
  publisher = {Curran Associates, Inc.},
  join_key = {129}
}

@inproceedings{xiaFOFOBenchmarkEvaluate2024,
  title = {{{FOFO}}: A Benchmark to Evaluate {{LLMs}}' Format-Following Capability},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Xia, Congying {and} Xing, Chen {and} Du},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {680--699},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.40},
  abstract = {This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet under-examined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo`s role in guiding the selection of domain-specific AI agents. FoFo will be publicly released, contributing a critical tool for advancing LLM evaluation and application.},
  join_key = {348}
}

@inproceedings{xiangCAREMIChineseBenchmark2023a,
  title = {{{CARE-MI}}: {{Chinese}} Benchmark for Misinformation Evaluation in Maternity and Infant Care},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Xiang, Tong and Li, Liangzhi and Li, Wangyue and Bai, Mingbai and Wei, Lu and Wang, Bowen and Garcia, Noa},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {42358--42381},
  publisher = {Curran Associates, Inc.},
  join_key = {84}
}

@inproceedings{xiaSportQABenchmarkSports2024a,
  title = {{{SportQA}}: A Benchmark for Sports Understanding in Large Language Models},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Xia, Haotian {and} Yang, Zhengbang {and} Wang},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {5061--5081},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.283},
  abstract = {A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs. The dataset is available at https://github.com/haotianxia/SportQA},
  join_key = {511}
}

@inproceedings{xieFinBenHolisticFinancial2024,
  title = {{{FinBen}}: A Holistic Financial Benchmark for Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Xie, Qianqian and Han, Weiguang and Chen, Zhengyu and Xiang, Ruoyu and Zhang, Xiao and He, Yueru and Xiao, Mengxi and Li, Dong and Dai, Yongfu and Feng, Duanyu and Xu, Yijing and Kang, Haoqiang and Kuang, Ziyan and Yuan, Chenhan and Yang, Kailai and Luo, Zheheng and Zhang, Tianlin and Liu, Zhiwei and Xiong, Guojun and Deng, Zhiyang and Jiang, Yuechen and Yao, Zhiyuan and Li, Haohang and Yu, Yangyang and Hu, Gang and Huang, Jiajia and Liu, Xiao-Yang and {Lopez-Lira}, Alejandro and Wang, Benyou and Lai, Yanzhao and Wang, Hao and Peng, Min and Ananiadou, Sophia and Huang, Jimin},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {95716--95743},
  publisher = {Curran Associates, Inc.},
  join_key = {154}
}

@inproceedings{xieOSWorldBenchmarkingMultimodal2024,
  title = {{{OSWorld}}: {{Benchmarking}} Multimodal Agents for Open-Ended Tasks in Real Computer Environments},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Xie, Tianbao and Zhang, Danyang and Chen, Jixuan and Li, Xiaochuan and Zhao, Siheng and Cao, Ruisheng and Hua, Toh Jing and Cheng, Zhoujun and Shin, Dongchan and Lei, Fangyu and Liu, Yitao and Xu, Yiheng and Zhou, Shuyan and Savarese, Silvio and Xiong, Caiming and Zhong, Victor and Yu, Tao},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {52040--52094},
  publisher = {Curran Associates, Inc.},
  join_key = {197}
}

@inproceedings{xiePIXIUComprehensiveBenchmark2023,
  title = {{{PIXIU}}: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Xie, Qianqian and Han, Weiguang and Zhang, Xiao and Lai, Yanzhao and Peng, Min and {Lopez-Lira}, Alejandro and Huang, Jimin},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {33469--33484},
  publisher = {Curran Associates, Inc.},
  join_key = {99}
}

@inproceedings{xieTravelPlannerBenchmarkRealworld2024,
  title = {{{TravelPlanner}}: A Benchmark for Real-World Planning with Language Agents},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Xie, Jian and Zhang, Kai and Chen, Jiangjie and Zhu, Tinghui and Lou, Renze and Tian, Yuandong and Xiao, Yanghua and Su, Yu},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {54590--54613},
  publisher = {PMLR},
  abstract = {Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks---even GPT-4 only achieves a success rate of 0.6},
  join_key = {4}
}

@inproceedings{xieWhodunitBenchEvaluatingLarge2024,
  title = {{{WhodunitBench}}: {{Evaluating}} Large Multimodal Agents via Murder Mystery Games},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Xie, Junlin and Zhang, Ruifei and Chen, Zhihong and Wan, Xiang and Li, Guanbin},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {86655--86687},
  publisher = {Curran Associates, Inc.},
  join_key = {231}
}

@inproceedings{xiongTRIGOBenchmarkingFormal2023a,
  title = {{{TRIGO}}: {{Benchmarking}} Formal Mathematical Proof Reduction for Generative Language Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Xiong, Jing {and} Shen, Jianhao {and} Yuan},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {11594--11632},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.711},
  abstract = {Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks are mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proof but also evaluates a generative LM`s reasoning ability on formulas and capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from web, annotate the simplification process manually, and translate it into the ``Lean'' formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we also create three automatically generated training and testing datasets of varying difficulty and distributions. Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM`s including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data, and provide a new tool to study the generative LM`s ability on both formal and mathematical reasoning.},
  join_key = {330}
}

@inproceedings{xuBagTricksBenchmarking2024,
  title = {Bag of Tricks: {{Benchmarking}} of Jailbreak Attacks on Llms},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {XU, Zhao and LIU, Fan and LIU, Hao},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {32219--32250},
  publisher = {Curran Associates, Inc.},
  join_key = {119}
}

@inproceedings{xuDPOSuperiorPPO2024,
  title = {Is {{DPO}} Superior to {{PPO}} for {{LLM}} Alignment? {{A}} Comprehensive Study},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {54983--54998},
  publisher = {PMLR},
  abstract = {Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions.},
  join_key = {3}
}

@inproceedings{xuMAgICInvestigationLarge2024,
  title = {{{MAgIC}}: {{Investigation}} of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Xu, Lin {and} Hu, Zhiyuan {and} Zhou},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {7315--7332},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.416},
  abstract = {Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs' reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.We utilize two social deduction games alongside three game-theory scenarios to create diverse environments.Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37\%. Our data and code can be found here https://github.com/cathyxl/MAgIC.},
  join_key = {447}
}

@inproceedings{xuMultiInstructImprovingMultimodal2023,
  title = {{{MultiInstruct}}: {{Improving}} Multi-Modal Zero-Shot Learning via Instruction Tuning},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Xu, Zhiyang {and} Shen, Ying {and} Huang},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {11445--11465},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.641},
  abstract = {Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric -- Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.},
  join_key = {292}
}

@inproceedings{xuOpenToMComprehensiveBenchmark2024,
  title = {{{OpenToM}}: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Xu, Hainiu {and} Zhao, Runcong {and} Zhu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {8593--8623},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.466},
  abstract = {Neural Theory-of-Mind (N-ToM), machine`s ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.},
  join_key = {385}
}

@inproceedings{xuPEERComprehensiveMultitask2022,
  title = {{{PEER}}: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Xu, Minghao and Zhang, Zuobai and Lu, Jiarui and Zhu, Zhaocheng and Zhang, Yangtian and Chang, Ma and Liu, Runcheng and Tang, Jian},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {35156--35173},
  publisher = {Curran Associates, Inc.},
  join_key = {70}
}

@inproceedings{xuStresstestingLongcontextLanguage2024,
  title = {Stress-Testing Long-Context Language Models with Lifelong {{ICL}} and Task Haystack},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Xu, Xiaoyue and Ye, Qinyuan and Ren, Xiang},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {15801--15840},
  publisher = {Curran Associates, Inc.},
  join_key = {214}
}

@inproceedings{yanCodeScopeExecutionbasedMultilingual2024,
  title = {{{CodeScope}}: {{An}} Execution-Based Multilingual Multitask Multidimensional Benchmark for Evaluating {{LLMs}} on Code Understanding and Generation},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Yan, Weixiang {and} Liu, Haitian {and} Wang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {5511--5558},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.301},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce **CodeScope**, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers **43 programming languages** and **eight coding tasks**. It evaluates the coding performance of LLMs from three dimensions (perspectives): **length**, **difficulty**, and **efficiency**. To facilitate execution-based evaluations of code generation, we develop **MultiCodeEngine**, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope.},
  join_key = {372}
}

@inproceedings{yanComprehensiveStudyTextattributed2023,
  title = {A Comprehensive Study on Text-Attributed Graphs: {{Benchmarking}} and Rethinking},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yan, Hao and Li, Chaozhuo and Long, Ruosong and Yan, Chao and Zhao, Jianan and Zhuang, Wenwen and Yin, Jun and Zhang, Peiyan and Han, Weihao and Sun, Hao and Deng, Weiwei and Zhang, Qi and Sun, Lichao and Xie, Xing and Wang, Senzhang},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {17238--17264},
  publisher = {Curran Associates, Inc.},
  join_key = {76}
}

@inproceedings{yangCanLargeLanguage2024,
  title = {Can Large Language Models Always Solve Easy Problems If They Can Solve Harder Ones?},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Yang, Zhe {and} Zhang, Yichang {and} Liu},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {1531--1555},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.92},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2\% but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub.},
  join_key = {431}
}

@inproceedings{yangCRAGComprehensiveRAG2024a,
  title = {{{CRAG}} - Comprehensive {{RAG}} Benchmark},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yang, Xiao and Sun, Kai and Xin, Hao and Sun, Yushi and Bhalla, Nikita and Chen, Xiangsen and Choudhary, Sajal and Gui, Rongze Daniel and Jiang, Ziran Will and Jiang, Ziyu and Kong, Lingkun and Moran, Brian and Wang, Jiaqi and Xu, Yifan Ethan and Yan, An and Yang, Chenyu and Yuan, Eting and Zha, Hanwen and Tang, Nan and Chen, Lei and Scheffer, Nicolas and Liu, Yue and Shah, Nirav and Wanga, Rakesh and Kumar, Anuj and Yih, Wen-tau and Dong, Xin Luna},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {10470--10490},
  publisher = {Curran Associates, Inc.},
  join_key = {137}
}

@inproceedings{yangDataTalesBenchmarkRealworld2024,
  title = {{{DataTales}}: A Benchmark for Real-World Intelligent Data Narration},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Yang, Yajing {and} Liu, Qian {and} Kan},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {10764--10788},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.601},
  abstract = {We introduce DataTales, a novel benchmark designed to assess the proficiency of language models in data narration, a task crucial for transforming complex tabular data into accessible narratives. Existing benchmarks often fall short in capturing the requisite analytical complexity for practical applications. DataTales addresses this gap by offering 4.9k financial reports paired with corresponding market data, showcasing the demand for models to create clear narratives and analyze large datasets while understanding specialized terminology in the field. Our findings highlights the significant challenge that language models face in achieving the necessary precision and analytical depth for proficient data narration, suggesting promising avenues for future model development and evaluation methodologies.},
  join_key = {457}
}

@inproceedings{yangInterCodeStandardizingBenchmarking2023,
  title = {{{InterCode}}: {{Standardizing}} and Benchmarking Interactive Coding with Execution Feedback},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yang, John and Prabhakar, Akshara and Narasimhan, Karthik and Yao, Shunyu},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {23826--23854},
  publisher = {Curran Associates, Inc.},
  join_key = {92}
}

@inproceedings{yangLLMCBenchBenchmarkingLarge2024a,
  title = {{{LLMCBench}}: {{Benchmarking}} Large Language Model Compression for Efficient Deployment},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yang, Ge and He, Changyi and Guo, Jinyang and Wu, Jianyu and Ding, Yifu and Liu, Aishan and Qin, Haotong and Ji, Pengliang and Liu, Xianglong},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {87532--87544},
  publisher = {Curran Associates, Inc.},
  join_key = {173}
}

@inproceedings{yaoWebShopScalableRealworld2022a,
  title = {{{WebShop}}: {{Towards}} Scalable Real-World Web Interaction with Grounded Language Agents},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {20744--20757},
  publisher = {Curran Associates, Inc.},
  join_key = {74}
}

@inproceedings{yeAnaloBenchBenchmarkingIdentification2024,
  title = {{{AnaloBench}}: {{Benchmarking}} the Identification of Abstract and Long-Context Analogies},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Ye, Xiao {and} Wang, Andrew {and} Choi},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {13060--13082},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.725},
  abstract = {Humans regularly engage in analogical thinking, relating personal experiences to current situations (X is analogous to Y because of Z). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose AnaloBench, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We collect a set of 340 high quality, human written analogies for use in our benchmark, which constitutes the largest such collection to date. We then test a broad collection of models consisting of 12 open source and 3 proprietary in various sizes and architectures. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.},
  join_key = {467}
}

@inproceedings{yeBenchmarkingLlmsUncertainty2024,
  title = {Benchmarking Llms via Uncertainty Quantification},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ye, Fanghua and Yang, Mingming and Pang, Jianhui and Wang, Longyue and Wong, Derek F. and Yilmaz, Emine and Shi, Shuming and Tu, Zhaopeng},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {15356--15385},
  publisher = {Curran Associates, Inc.},
  join_key = {122}
}

@inproceedings{yeFedLLMbenchRealisticBenchmarks2024,
  title = {{{FedLLM-bench}}: {{Realistic}} Benchmarks for Federated Learning of Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ye, Rui and Ge, Rui and Zhu, Xinyu and Chai, Jingyi and Du, Yaxin and Liu, Yang and Wang, Yanfeng and Chen, Siheng},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {111106--111130},
  publisher = {Curran Associates, Inc.},
  join_key = {153}
}

@inproceedings{yeGlobeSummChallengingBenchmark2024,
  title = {{{GlobeSumm}}: A Challenging Benchmark towards Unifying Multi-Lingual, Cross-Lingual and Multi-Document News Summarization},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Ye, Yangfan {and} Feng, Xiachong {and} Feng},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {10803--10821},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.603},
  abstract = {News summarization in today`s global scene can be daunting with its flood of multilingual content and varied viewpoints from different sources. However, current studies often neglect such real-world scenarios as they tend to focus solely on either single-language or single-document tasks. To bridge this gap, we aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization into a novel task, i.e., MCMS, which encapsulates the real-world requirements all-in-one. Nevertheless, the lack of a benchmark inhibits researchers from adequately studying this invaluable problem. To tackle this, we have meticulously constructed the GLOBESUMM dataset by first collecting a wealth of multilingual news reports and restructuring them into event-centric format. Additionally, we introduce the method of protocol-guided prompting for high-quality and cost-effective reference annotation. In MCMS, we also highlight the challenge of conflicts between news reports, in addition to the issues of redundancies and omissions, further enhancing the complexity of GLOBESUMM. Through extensive experimental analysis, we validate the quality of our dataset and elucidate the inherent challenges of the task. We firmly believe that GLOBESUMM, given its challenging nature, will greatly contribute to the multilingual communities and the evaluation of LLMs.},
  join_key = {458}
}

@inproceedings{yeRoTBenchMultilevelBenchmark2024,
  title = {{{RoTBench}}: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Ye, Junjie {and} Wu, Yilong {and} Gao},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {313--333},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.19},
  abstract = {Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce *RoTBench*, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model`s resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.},
  join_key = {429}
}

@inproceedings{yinALCUNALargeLanguage2023a,
  title = {{{ALCUNA}}: {{Large}} Language Models Meet New Knowledge},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Yin, Xunjian {and} Huang, Baizhou {and} Wan},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {1397--1414},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.87},
  abstract = {With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models' capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs' abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model`s understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.},
  join_key = {303}
}

@inproceedings{yinGeoMLAMAGeodiverseCommonsense2022a,
  title = {{{GeoMLAMA}}: {{Geo-diverse}} Commonsense Probing on Multilingual Pre-Trained Language Models},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Yin, Da {and} Bansal, Hritik {and} Monajatipoor},
  editor = {Goldberg, Yoav {and} Kozareva, Zornitsa {and} Zhang},
  year = {2022},
  month = dec,
  pages = {2039--2055},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.132},
  abstract = {Recent work has shown that Pre-trained Language Models (PLMs) store the relational knowledge learned from data and utilize it for performing downstream tasks. However, commonsense knowledge across different regions may vary. For instance, the color of bridal dress is white in American weddings whereas it is red in Chinese weddings. In this paper, we introduce a benchmark dataset, Geo-diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for probing the diversity of the relational knowledge in multilingual PLMs. GeoMLAMA contains 3125 prompts in English, Chinese, Hindi, Persian, and Swahili, with a wide coverage of concepts shared by people from American, Chinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard multilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger multilingual PLMs variants do not necessarily store geo-diverse concepts better than its smaller variant; 2) multilingual PLMs are not intrinsically biased towards knowledge from the Western countries (the United States); 3) the native language of a country may not be the best language to probe its knowledge and 4) a language may better probe knowledge about a non-native country than its native country.},
  join_key = {260}
}

@inproceedings{yingMMTbenchComprehensiveMultimodal2024,
  title = {{{MMT-bench}}: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models towards Multitask {{AGI}}},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Ying, Kaining and Meng, Fanqing and Wang, Jin and Li, Zhiqian and Lin, Han and Yang, Yue and Zhang, Hao and Zhang, Wenbo and Lin, Yuqi and Liu, Shuo and Lei, Jiayi and Lu, Quanfeng and Chen, Runjian and Xu, Peng and Zhang, Renrui and Zhang, Haozhe and Gao, Peng and Wang, Yali and Qiao, Yu and Luo, Ping and Zhang, Kaipeng and Shao, Wenqi},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {57116--57198},
  publisher = {PMLR},
  abstract = {Large Vision-Language Models (LVLMs) show significant strides in general-propose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, and reasoning. MMT-Bench comprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering 32 core meta-tasks and 162 subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving 20 publicly available LVLMs such as the proprietary GeminiProVision model, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.},
  join_key = {2}
}

@inproceedings{yinLAMMLanguageassistedMultimodal2023,
  title = {{{LAMM}}: {{Language-assisted}} Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Huang, Xiaoshui and Wang, Zhiyong and Sheng, Lu and BAI, {\relax LEI} and Shao, Jing and Ouyang, Wanli},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {26650--26685},
  publisher = {Curran Associates, Inc.},
  join_key = {94}
}

@inproceedings{yinNaturalLanguageCode2023,
  title = {Natural Language to Code Generation in Interactive Data Science Notebooks},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Yin, Pengcheng {and} Li, Wen-Ding {and} Xiao},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {126--173},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.9},
  abstract = {Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at {$<$}a href="https://github.com/google-research/arcade-nl2code/"{$>$}https://github.com/google-research/arcade-nl2code/{$<$}/a{$>$}.},
  join_key = {279}
}

@inproceedings{yinSafeWorldGeodiverseSafety2024,
  title = {{{SafeWorld}}: {{Geo-diverse}} Safety Alignment},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yin, Da and Qiu, Haoyi and Huang, Kung-Hsiang and Chang, Kai-Wei and Peng, Nanyun},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {128734--128768},
  publisher = {Curran Associates, Inc.},
  join_key = {203}
}

@inproceedings{yoranAssistantBenchCanWeb2024,
  title = {{{AssistantBench}}: {{Can}} Web Agents Solve Realistic and Time-Consuming Tasks?},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Yoran, Ori {and} Amouyal, Samuel Joseph {and} Malaviya},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {8938--8968},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.505},
  abstract = {Language agents, built on top of language models (LMs), are systems that can interact with complex environments, such as the open web. In this work, we examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses. We introduce AssistantBench, a challenging new benchmark consisting of 214 realistic tasks that can be automatically evaluated, covering different scenarios and domains. We find that AssistantBench exposes the limitations of current systems, including language models and retrieval-augmented language models, as no model reaches an accuracy of more than 25 points. While closed-book LMs perform well in terms of accuracy, they exhibit low precision and tend to hallucinate facts. State-of-the-art web agents reach a score of near zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance. Moreover, we analyze failures of current systems and highlight that open web navigation remains a major challenge.},
  join_key = {453}
}

@inproceedings{youLLMevolveEvaluationLLM`s2024,
  title = {{{LLM-evolve}}: {{Evaluation}} for {{LLM}}`s Evolving Capability on Benchmarks},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {You, Jiaxuan {and} Liu, Mingjie {and} Prabhumoye},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {16937--16942},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.940},
  abstract = {The advancement of large language models (LLMs) has extended their use to dynamic and interactive real-world applications, where models engage continuously with their environment and potentially enhance their performance over time. Most existing LLM benchmarks evaluate LLMs on i.i.d. tasks, overlooking their ability to learn iteratively from past experiences. Our paper bridges this evaluation gap by proposing a novel framework, LLM-Evolve, which extends established benchmarks to sequential problem-solving settings. LLM-Evolve evaluates LLMs over multiple rounds, providing feedback after each round to build a demonstration memory that the models can query in future tasks. We applied LLM-Evolve to the MMLU, GSM8K, and AgentBench benchmarks, testing 8 state-of-the-art open-source and closed-source models. Results show that LLMs can achieve performance improvements of up to 17\% by learning from past interactions, with the quality of retrieval algorithms and feedback significantly influencing this capability. These insights advocate for more understanding and benchmarks for LLMs' performance in evolving interactive scenarios.},
  join_key = {482}
}

@inproceedings{yuALERTAdaptLanguage2023a,
  title = {{{ALERT}}: {{Adapt}} Language Models to Reasoning Tasks},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Yu, Ping {and} Wang, Tianlu {and} Golovneva},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {1055--1081},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.60},
  abstract = {Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context. To address this question, we introduce \{pasted macro `OUR'\}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. \{pasted macro `OUR'\}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using \{pasted macro `OUR'\}model we further investigate \emph{the role of finetuning}. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.},
  join_key = {280}
}

@inproceedings{yuanRevisitingOutofdistributionRobustness2023,
  title = {Revisiting Out-of-Distribution Robustness in {{NLP}}: {{Benchmarks}}, Analysis, and Llms Evaluations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yuan, Lifan and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Zou, FangYuan and Cheng, Xingyi and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {58478--58507},
  publisher = {Curran Associates, Inc.},
  join_key = {103}
}

@inproceedings{yuanUnlockingMarketsMultilingual2024,
  title = {Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Yuan, Yifei {and} Deng, Yang {and} S{\o}gaard},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {11154--11169},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.625},
  abstract = {Users post numerous product-related questions on e-commerce platforms, affecting their purchase decisions. Product-related question answering (PQA) entails utilizing product-related resources to provide precise responses to users. We propose a novel task of Multilingual Cross-market Product-based Question Answering (MCPQA) and define the task as providing answers to product-related questions in a main marketplace by utilizing information from another resource-rich auxiliary marketplace in a multilingual context. We introduce a large-scale dataset comprising over 7 million questions from 17 marketplaces across 11 languages. We then perform automatic translation on the Electronics category of our dataset, naming it as McMarket. We focus on two subtasks: review-based answer generation and product-related question ranking. For each subtask, we label a subset of McMarket using an LLM and further evaluate the quality of the annotations via human assessment. We then conduct experiments to benchmark our dataset, using models ranging from traditional lexical models to LLMs in both single-market and cross-market scenarios across McMarket and the corresponding LLM subset. Results show that incorporating cross-market information significantly enhances performance in both tasks.},
  join_key = {459}
}

@inproceedings{yuKoLACarefullyBenchmarking2024,
  title = {{{KoLA}}: {{Carefully}} Benchmarking World Knowledge of Large Language Models},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Yu, Jifan and Wang, Xiaozhi and Tu, Shangqing and Cao, Shulin and {Zhang-Li}, Daniel and Lv, Xin and Peng, Hao and Yao, Zijun and Zhang, Xiaohan and Li, Hanming and Li, Chunyang and Zhang, Zheyuan and Bai, Yushi and Liu, Yantao and Xin, Amy and Yun, Kaifeng and GONG, Linlu and Lin, Nianyi and Chen, Jianhui and Wu, Zhili and Qi, Yunjia and Li, Weikai and Guan, Yong and Zeng, Kaisheng and Qi, Ji and Jin, Hailong and Liu, Jinxin and Gu, Yu and Yao, Yuan and Ding, Ning and Hou, Lei and Liu, Zhiyuan and Bin, Xu and Tang, Jie and Li, Juanzi},
  year = {2024},
  join_key = {48}
}

@inproceedings{yuksekgonulWhenWhyVisionlanguage2023,
  title = {When and Why Vision-Language Models Behave like Bags-of-Words, and What to Do about It?},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},
  year = {2023},
  join_key = {36}
}

@inproceedings{yuMMvetEvaluatingLarge2024,
  title = {{{MM-vet}}: {{Evaluating}} Large Multimodal Models for Integrated Capabilities},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {57730--57754},
  publisher = {PMLR},
  abstract = {We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models.},
  join_key = {1}
}

@inproceedings{zambranochavesRaLEsBenchmarkRadiology2023a,
  title = {{{RaLEs}}: A Benchmark for Radiology Language Evaluations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zambrano Chaves, Juanma and Bhaskhar, Nandita and Attias, Maayane and Delbrouck, Jean-Benoit and Rubin, Daniel and Loening, Andreas and Langlotz, Curtis and Chaudhari, Akshay},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {74429--74454},
  publisher = {Curran Associates, Inc.},
  join_key = {101}
}

@inproceedings{zengEvaluatingLargeLanguage2024,
  title = {Evaluating Large Language Models at Evaluating Instruction Following},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Zeng, Zhiyuan and Yu, Jiatong and Gao, Tianyu and Meng, Yu and Goyal, Tanya and Chen, Danqi},
  year = {2024},
  join_key = {44}
}

@inproceedings{zengMRbenMetareasoningBenchmark2024,
  title = {{{MR-ben}}: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in Llms},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zeng, Zhongshen and Liu, Yinhong and Wan, Yingjia and Li, Jingyao and Chen, Pengguang and Dai, Jianbo and Yao, Yuxuan and Xu, Rongwu and Qi, Zehan and Zhao, Wanru and Shen, Linling and Lu, Jianqiao and Tan, Haochen and Chen, Yukang and Zhang, Hao and Shi, Zhan and Wang, Bailin and Guo, Zhijiang and Jia, Jiaya},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {119466--119546},
  publisher = {Curran Associates, Inc.},
  join_key = {188}
}

@inproceedings{zhangAnalyzingTemporalComplex2024,
  title = {Analyzing Temporal Complex Events with Large Language Models? {{A}} Benchmark towards Temporal, Long Context Understanding},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhang, Zhihan {and} Cao, Yixin {and} Ye},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {1588--1606},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.87},
  abstract = {The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events.We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.},
  join_key = {354}
}

@inproceedings{zhangBenchExtendingLong2024,
  title = {{$\infty$}{{Bench}}: {{Extending}} Long Context Evaluation beyond {{100K}} Tokens},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhang, Xinrong {and} Chen, Yingfa {and} Hu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {15262--15277},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.814},
  abstract = {Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose , the first LLM benchmark featuring an average data length surpassing 100K tokens. comprises synthetic and realistic tasks spanning diverse domains in English and Chinese. The tasks in are designed to require an understanding of long dependencies in contexts and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. Based on , we evaluate several state-of-the-art LLMs tailored for processing long contexts. The experimental results indicate that existing long-context LLMs still require significant advancements to process 100K+ contexts effectively. Furthermore, we present three intriguing analyses regarding the behavior of LLMs processing long context. Our code and data is released.},
  join_key = {416}
}

@inproceedings{zhangBenchmarkingDataScience2024,
  title = {Benchmarking Data Science Agents},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhang, Yuge {and} Jiang, Qiyang {and} XingyuHan},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {5677--5700},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.308},
  abstract = {In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.},
  join_key = {374}
}

@inproceedings{zhangCABComprehensiveAttention2023,
  title = {{{CAB}}: {{Comprehensive}} Attention Benchmarking on Long Sequence Modeling},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  author = {Zhang, Jun and Jiang, Shuyang and Feng, Jiangtao and Zheng, Lin and Kong, Lingpeng},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  year = {2023-07-23/2023-07-29},
  series = {Proceedings of Machine Learning Research},
  volume = {202},
  pages = {41194--41218},
  publisher = {PMLR},
  abstract = {Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.},
  join_key = {22}
}

@inproceedings{zhangCarefulExaminationLarge2024,
  title = {A Careful Examination of Large Language Model Performance on Grade School Arithmetic},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhang, Hugh and Da, Jeff and Lee, Dean and Robinson, Vaughn and Wu, Catherine and Song, Will and Zhao, Tiffany and Raja, Pranav and Zhuang, Charlotte and Slack, Dylan and Lyu, Qin and Hendryx, Sean and Kaplan, Russell and Lunati, Michele and Yue, Summer},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {46819--46836},
  publisher = {Curran Associates, Inc.},
  join_key = {111}
}

@inproceedings{zhangCLAMBERBenchmarkIdentifying2024a,
  title = {{{CLAMBER}}: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhang, Tong {and} Qin, Peixin {and} Deng},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {10746--10766},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.578},
  abstract = {Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct 12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs.Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge.In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs.},
  join_key = {394}
}

@inproceedings{zhangDTGBComprehensiveBenchmark2024,
  title = {{{DTGB}}: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhang, Jiasheng and Chen, Jialin and Yang, Menglin and Feng, Aosong and Liang, Shuang and Shao, Jie and Ying, Rex},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {91405--91429},
  publisher = {Curran Associates, Inc.},
  join_key = {146}
}

@inproceedings{zhangHumorAIMassive2024,
  title = {Humor in {{AI}}: {{Massive}} Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhang, Jifan and Jain, Lalit and Guo, Yang and Chen, Jiayi and Zhou, Kuan Lok and Suresh, Siddharth and Wagenmaker, Andrew and Sievert, Scott and Rogers, Timothy and Jamieson, Kevin and Mankoff, Robert and Nowak, Robert},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {125264--125286},
  publisher = {Curran Associates, Inc.},
  join_key = {160}
}

@inproceedings{zhangM3ExamMultilingualMultimodal2023a,
  title = {{{M3Exam}}: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhang, Wenxuan and Aljunied, Mahani and Gao, Chang and Chia, Yew Ken and Bing, Lidong},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {5484--5505},
  publisher = {Curran Associates, Inc.},
  join_key = {97}
}

@inproceedings{zhangMarathonRaceRealm2024,
  title = {Marathon: A Race through the Realm of Long Context with Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhang, Lei {and} Li, Yunshui {and} Liu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {5201--5217},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.284},
  abstract = {With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models' comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce Marathon, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs' capabilities in understanding and reasoning over extended contexts.},
  join_key = {370}
}

@inproceedings{zhangMELAMultilingualEvaluation2024,
  title = {{{MELA}}: {{Multilingual}} Evaluation of Linguistic Acceptability},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhang, Ziyin {and} Liu, Yikang {and} Huang},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {2658--2674},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.146},
  abstract = {In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability---MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language---Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks.},
  join_key = {358}
}

@inproceedings{zhangMIntRec20LargescaleBenchmark2024,
  title = {{{MIntRec2}}.0: A Large-Scale Benchmark Dataset for Multimodal Intent Recognition and out-of-Scope Detection in Conversations},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Zhang, Hanlei and Wang, Xin and Xu, Hua and Zhou, Qianrui and Gao, Kai and Su, Jianhua and Zhao, jinyue and Li, Wenrui and Chen, Yanting},
  year = {2024},
  join_key = {55}
}

@inproceedings{zhangMuCGECMultireferenceMultisource2022,
  title = {{{MuCGEC}}: A Multi-Reference Multi-Source Evaluation Dataset for {{Chinese}} Grammatical Error Correction},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Zhang, Yue {and} Li, Zhenghua {and} Bao},
  editor = {Carpuat, Marine {and} de Marneffe, Marie-Catherine {and} Meza Ruiz},
  year = {2022},
  month = jul,
  pages = {3118--3130},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.227},
  abstract = {This paper presents MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each sentence is corrected by three annotators, and their corrections are carefully reviewed by a senior annotator, resulting in 2.3 references per sentence. We conduct experiments with two mainstream CGEC models, i.e., the sequence-to-sequence model and the sequence-to-edit model, both enhanced with large pretrained language models, achieving competitive benchmark performance on previous and our datasets. We also discuss CGEC evaluation methodologies, including the effect of multiple references and using a char-based metric. Our annotation guidelines, data, and code are available at {$<$}a href="https://github.com/HillZhang1999/MuCGEC"{$>$}https://github.com/HillZhang1999/MuCGEC{$<$}/a{$>$}.},
  join_key = {274}
}

@inproceedings{zhangMultilingualLargeLanguage2023,
  title = {Multilingual Large Language Models Are Not (yet) Code-Switchers},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhang, Ruochen {and} Cahyawijaya, Samuel {and} Cruz},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {12567--12582},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.774},
  abstract = {Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current ``multilingualism' in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.},
  join_key = {333}
}

@inproceedings{zhangMultimodalSelfinstructSynthetic2024,
  title = {Multimodal Self-Instruct: {{Synthetic}} Abstract Image and Visual Reasoning Instruction Using Language Model},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhang, Wenqi {and} Cheng, Zhenglin {and} He},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {19228--19252},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1072},
  abstract = {Although most current large multimodal models (LMMs) can already understand photos of natural scenes and portraits, their understanding of abstract images, e.g., charts, maps, or layouts, and visual reasoning capabilities remains quite rudimentary. They often struggle with simple daily tasks, such as reading time from a clock, understanding a flowchart, or planning a route using a road map. In light of this, we design a multi-modal self-instruct, utilizing large language models and their code capabilities to synthesize massive abstract images and visual reasoning instructions across daily scenarios. Our strategy effortlessly creates a multimodal benchmark with 11,193 instructions for eight visual scenarios: charts, tables, simulated maps, dashboards, flowcharts, relation graphs, floor plans, and visual puzzles. \textbf{This benchmark, constructed with simple lines and geometric elements, exposes the shortcomings of most advanced LMMs} like GPT-4V and Llava in abstract image understanding, spatial relations reasoning, and visual element induction. Besides, to verify the quality of our synthetic data, we fine-tune an LMM using 62,476 synthetic chart, table and road map instructions. The results demonstrate improved chart understanding and map navigation performance, and also demonstrate potential benefits for other visual reasoning tasks.},
  join_key = {487}
}

@inproceedings{zhangMultiTrustComprehensiveBenchmark2024,
  title = {{{MultiTrust}}: A Comprehensive Benchmark towards Trustworthy Multimodal Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhang, Yichi and Huang, Yao and Sun, Yitong and Liu, Chang and Zhao, Zhe and Fang, Zhengwei and Wang, Yifan and Chen, Huanran and Yang, Xiao and Wei, Xingxing and Su, Hang and Dong, Yinpeng and Zhu, Jun},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {49279--49383},
  publisher = {Curran Associates, Inc.},
  join_key = {190}
}

@inproceedings{zhangRevisitingZerothorderOptimization2024,
  title = {Revisiting Zeroth-Order Optimization for Memory-Efficient {{LLM}} Fine-Tuning: A Benchmark},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D. and Yin, Wotao and Hong, Mingyi and Wang, Zhangyang and Liu, Sijia and Chen, Tianlong},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  year = {2024-07-21/2024-07-27},
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {59173--59190},
  publisher = {PMLR},
  abstract = {In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow in size, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by (Malladi et al., 2023). Unlike traditional ZO-SGD methods, ouwork expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families, three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments will be made public.},
  join_key = {0}
}

@inproceedings{zhangSafetyBenchEvaluatingSafety2024,
  title = {{{SafetyBench}}: {{Evaluating}} the Safety of Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhang, Zhexin {and} Lei, Leqi {and} Wu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {15537--15553},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.830},
  abstract = {With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at https://github.com/thu-coai/SafetyBench. Submission entrance and leaderboard are available at https://llmbench.ai/safety.},
  join_key = {417}
}

@inproceedings{zhangSelenePioneeringAutomated2024,
  title = {Selene: {{Pioneering}} Automated Proof in Software Verification},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhang, Lichen {and} Lu, Shuai {and} Duan},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {1776--1789},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.98},
  abstract = {Ensuring correctness is a pivotal aspect of software engineering. Among the various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce Selene in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level operating system microkernel, seL4. Selene provides a comprehensive framework for end-to-end proof generation and a lightweight verification environment. Our experimental results with advanced large language models (LLMs), such as GPT-3.5-turbo and GPT-4, highlight the capabilities of LLMs in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by Selene can be mitigated in future research endeavors.},
  join_key = {355}
}

@inproceedings{zhangSkippedBeatStudy2023,
  title = {The Skipped Beat: A Study of Sociopragmatic Understanding in {{LLMs}} for 64 Languages},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhang, Chiyu {and} Doan, Khai {and} Liao},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {2630--2662},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.160},
  abstract = {Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately represented in any of the existing benchmarks. To address this gap, we present SPARROW, an extensive multilingual benchmark specifically designed for SM understanding. SPARROW comprises 169 datasets covering 13 task types across six primary categories (e.g., anti-social language detection, emotion recognition). SPARROW datasets encompass 64 different languages originating from 12 language families representing 16 writing scripts. We evaluate the performance of various multilingual pretrained language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT) on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score. Our benchmark is available at: https://github.com/UBC-NLP/SPARROW},
  join_key = {309}
}

@inproceedings{zhangTaskMeAnything2024,
  title = {Task Me Anything},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhang, Jieyu and Huang, Weikai and Ma, Zixian and Michel, Oscar and He, Dong and Gupta, Tanmay and Ma, Wei-Chiu and Farhadi, Ali and Kembhavi, Aniruddha and Krishna, Ranjay},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {19965--19974},
  publisher = {Curran Associates, Inc.},
  join_key = {218}
}

@inproceedings{zhangToolBeHonestMultilevelHallucination2024,
  title = {{{ToolBeHonest}}: A Multi-Level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhang, Yuxiang {and} Chen, Jing {and} Wang},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {11388--11422},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.637},
  abstract = {Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLM`s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.},
  join_key = {460}
}

@inproceedings{zhangUnveilingTapestryConsistency2024,
  title = {Unveiling the Tapestry of Consistency in Large Vision-Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhang, Yuan and Xiao, Fei and Huang, Tao and Fan, Chun-Kai and Dong, Hongyuan and Li, Jiawen and Wang, Jiacong and Cheng, Kuan and Zhang, Shanghang and Guo, Haoyuan},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {118632--118653},
  publisher = {Curran Associates, Inc.},
  join_key = {222}
}

@inproceedings{zhangXSemPLRCrosslingualSemantic2023,
  title = {{{XSemPLR}}: {{Cross-lingual}} Semantic Parsing in Multiple Natural Languages and Meaning Representations},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhang, Yusen {and} Wang, Jun {and} Wang},
  editor = {Rogers, Anna {and} Boyd-Graber, Jordan {and} Okazaki},
  year = {2023},
  month = jul,
  pages = {15918--15947},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.887},
  abstract = {Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at {$<$}a href="https://github.com/psunlpgroup/XSemPLR"{$>$}https://github.com/psunlpgroup/XSemPLR{$<$}/a{$>$}.},
  join_key = {296}
}

@inproceedings{zhaoCould`veAskedThat2024a,
  title = {I Could`ve Asked That: {{Reformulating}} Unanswerable Questions},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhao, Wenting {and} Gao, Ge {and} Cardie},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {4207--4220},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.242},
  abstract = {When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26\% and 12\% of the time, respectively. Error analysis shows that 62\% of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark and the code to reproduce the experiments.},
  join_key = {437}
}

@inproceedings{zhaoDocMathevalEvaluatingMath2024,
  title = {{{DocMath-eval}}: {{Evaluating}} Math Reasoning Capabilities of {{LLMs}} in Understanding Long and Specialized Documents},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhao, Yilun {and} Long, Yitao {and} Liu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {16103--16120},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.852},
  abstract = {Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables. We conduct an extensive evaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting methods, aiming to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that even the current best-performing system (i.e., GPT-4o) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe that DocMath-Eval can serve as a valuable benchmark for evaluating LLMs' capabilities in solving challenging numerical reasoning problems within expert domains.},
  join_key = {423}
}

@inproceedings{zhaoFinanceMATHKnowledgeintensiveMath2024,
  title = {{{FinanceMATH}}: {{Knowledge-intensive}} Math Reasoning in Finance Domains},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zhao, Yilun {and} Liu, Hongjun {and} Long},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {12841--12858},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.693},
  abstract = {We introduce FinanceMath, a novel benchmark designed to evaluate LLMs' capabilities in solving knowledge-intensive math reasoning problems. Compared to prior works, this study features three core advancements. First, FinanceMath includes 1,200 problems with a hybrid of textual and tabular content. These problems require college-level knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment. We also construct a finance-domain knowledge bank and investigate various knowledge integration strategies. Finally, we evaluate a wide spectrum of 44 LLMs with both Chain-of-Thought and Program-of-Thought prompting methods. Our experimental results reveal that the current best-performing system (i.e., GPT-4o) achieves only 60.9\% accuracy using CoT prompting, leaving substantial room for improvement. Moreover, while augmenting LLMs with external knowledge can improve model performance (e.g., from 47.5\% to 54.5\% for Gemini-1.5-Pro), their accuracy remains significantly lower than the estimated human expert performance of 92\%. We believe that FinanceMath can advance future research in the area of domain-specific knowledge retrieval and integration, particularly within the context of solving reasoning-intensive tasks.},
  join_key = {405}
}

@inproceedings{zhaoFinDVerExplainableClaim2024a,
  title = {{{FinDVer}}: {{Explainable}} Claim Verification over Long and Hybrid-Content Financial Documents},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhao, Yilun {and} Long, Yitao {and} Jiang},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {14739--14752},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.818},
  abstract = {We introduce FinDVer, a comprehensive benchmark specifically designed to evaluate the explainable claim verification capabilities of LLMs in the context of understanding and analyzing long, hybrid-content financial documents. FinDVer contains 4,000 expert-annotated examples across four subsets, each focusing on a type of scenario that frequently arises in real-world financial domains. We assess a broad spectrum of 25 LLMs under long-context and RAG settings. Our results show that even the current best-performing system (i.e., GPT-4o) significantly lags behind human experts. Our detailed findings and insights highlight the strengths and limitations of existing LLMs in this new task. We believe FinDVer can serve as a valuable benchmark for evaluating LLM capabilities in claim verification over complex, expert-domain documents.},
  join_key = {471}
}

@inproceedings{zhaoORCHIDChineseDebate2023,
  title = {{{ORCHID}}: A {{Chinese}} Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhao, Xiutian {and} Wang, Ke {and} Peng},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {9358--9375},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.582},
  abstract = {Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative dialogues. However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages. To address this language resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first Chinese dataset for benchmarking target-independent stance detection and debate summarization. Our dataset consists of 1,218 real-world debates that were conducted in Chinese on 476 unique topics, containing 2,436 stance-specific summaries and 14,133 fully annotated utterances. Besides providing a versatile testbed for future research, we also conduct an empirical study on the dataset and propose an integrated task. The results show the challenging nature of the dataset and suggest a potential of incorporating stance detection in summarization for argumentative dialogue.},
  join_key = {325}
}

@inproceedings{zhaoQTSummQueryfocusedSummarization2023,
  title = {{{QTSumm}}: {{Query-focused}} Summarization over Tabular Data},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhao, Yilun {and} Qi, Zhenting {and} Nan},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {1157--1172},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.74},
  abstract = {People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users' information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.},
  join_key = {302}
}

@inproceedings{zhengJudgingLLMasajudgeMTbench2023,
  title = {Judging {{LLM-as-a-judge}} with {{MT-bench}} and Chatbot Arena},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {46595--46623},
  publisher = {Curran Associates, Inc.},
  join_key = {93}
}

@inproceedings{zhengLMSYSchat1MLargescaleRealworld2024,
  title = {{{LMSYS-chat-1M}}: A Large-Scale Real-World {{LLM}} Conversation Dataset},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric and Gonzalez, Joseph E. and Stoica, Ion and Zhang, Hao},
  year = {2024},
  join_key = {50}
}

@inproceedings{zhengNEOBENCHEvaluatingRobustness2024a,
  title = {{{NEO-BENCH}}: {{Evaluating}} Robustness of Large Language Models with Neologisms},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Zheng, Jonathan {and} Ritter, Alan {and} Xu},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {13885--13906},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.749},
  abstract = {The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.},
  join_key = {411}
}

@inproceedings{zhongMQuAKEAssessingKnowledge2023,
  title = {{{MQuAKE}}: {{Assessing}} Knowledge Editing in Language Models via Multi-Hop Questions},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhong, Zexuan {and} Wu, Zhengxuan {and} Manning},
  editor = {Bouamor, Houda {and} Pino, Juan {and} Bali},
  year = {2023},
  month = dec,
  pages = {15686--15702},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.971},
  abstract = {The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model`s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.},
  join_key = {342}
}

@inproceedings{zhouHAZARDChallengeEmbodied2024,
  title = {{{HAZARD}} Challenge: {{Embodied}} Decision Making in Dynamically Changing Environments},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Zhou, Qinhong and Chen, Sunli and Wang, Yisong and Xu, Haozhe and Du, Weihua and Zhang, Hongxin and Du, Yilun and Tenenbaum, Joshua B. and Gan, Chuang},
  year = {2024},
  join_key = {46}
}

@inproceedings{zhouRICAEvaluatingRobust2021,
  title = {{{RICA}}: {{Evaluating}} Robust Inference Capabilities Based on Commonsense Axioms},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhou, Pei {and} Khanna, Rahul {and} Lee},
  editor = {Moens, Marie-Francine {and} Huang, Xuanjing {and} Specia},
  year = {2021},
  month = nov,
  pages = {7560--7579},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.598},
  abstract = {Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.},
  join_key = {246}
}

@inproceedings{zhouVLUEMultitaskMultidimension2022,
  title = {{{VLUE}}: A Multi-Task Multi-Dimension Benchmark for Evaluating Vision-Language Pre-Training},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  author = {Zhou, Wangchunshu and Zeng, Yan and Diao, Shizhe and Zhang, Xinsong},
  editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  year = {2022-07-17/2022-07-23},
  series = {Proceedings of Machine Learning Research},
  volume = {162},
  pages = {27395--27411},
  publisher = {PMLR},
  abstract = {Recent advances in vision-language pre-training (VLP) have demonstrated impressive performance in a range of vision-language (VL) tasks. However, there exist several challenges for measuring the community's progress in building general multi-modal intelligence. First, most of the downstream VL datasets are annotated using raw images that are already seen during pre-training, which may result in an overestimation of current VLP models' generalization ability. Second, recent VLP work mainly focuses on absolute performance but overlooks the efficiency-performance trade-off, which is also an important indicator for measuring progress. To this end, we introduce the Vision-Language Understanding Evaluation (VLUE) benchmark, a multi-task multi-dimension benchmark for evaluating the generalization capabilities and the efficiency-performance trade-off (``Pareto SOTA'') of VLP models. We demonstrate that there is a sizable generalization gap for all VLP models when testing on out-of-distribution test sets annotated on images from a more diverse distribution that spreads across cultures. Moreover, we find that measuring the efficiency-performance trade-off of VLP models leads to complementary insights for several design choices of VLP. We release the VLUE benchmark to promote research on building vision-language models that generalize well to images unseen during pre-training and are practical in terms of efficiency-performance trade-off.},
  join_key = {26}
}

@inproceedings{zhouWebArenaRealisticWeb2024,
  title = {{{WebArena}}: A Realistic Web Environment for Building Autonomous Agents},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Zhou, Shuyan and Xu, Frank F. and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou, Tianyue and Bisk, Yonatan and Fried, Daniel and Alon, Uri and Neubig, Graham},
  year = {2024},
  join_key = {65}
}

@inproceedings{zhuangToolQADatasetLLM2023,
  title = {{{ToolQA}}: A Dataset for {{LLM}} Question Answering with External Tools},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {50117--50143},
  publisher = {Curran Associates, Inc.},
  join_key = {105}
}

@inproceedings{zhuAreLargeLanguage2024,
  title = {Are Large Language Models Good Statisticians?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhu, Yizhang and Du, Shiyin and Li, Boyan and Luo, Yuyu and Tang, Nan},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {62697--62731},
  publisher = {Curran Associates, Inc.},
  join_key = {114}
}

@inproceedings{zhuFanOutQAMultihopMultidocument2024,
  title = {{{FanOutQA}}: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Zhu, Andrew {and} Hwang, Alyssa {and} Dugan},
  editor = {Ku, Lun-Wei {and} Martins, Andre {and} Srikumar},
  year = {2024},
  month = aug,
  pages = {18--37},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-short.2},
  abstract = {One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset, along with open-source tools to run models to encourage evaluation.},
  join_key = {426}
}

@inproceedings{ziemsMoralIntegrityCorpus2022,
  title = {The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Ziems, Caleb {and} Yu, Jane {and} Wang},
  editor = {Muresan, Smaranda {and} Nakov, Preslav {and} Villavicencio},
  year = {2022},
  month = may,
  pages = {3755--3773},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.261},
  abstract = {Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user`s trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot`s reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models' implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see {$<$}a href="https://github.com/GT-SALT/mic"{$>$}https://github.com/GT-SALT/mic{$<$}/a{$>$}},
  join_key = {251}
}

@inproceedings{zoharLOVMLanguageonlyVision2023a,
  title = {{{LOVM}}: {{Language-only}} Vision Model Selection},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zohar, Orr and Huang, Shih-Cheng and Wang, Kuan-Chieh and Yeung, Serena},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {33120--33132},
  publisher = {Curran Associates, Inc.},
  join_key = {96}
}

@inproceedings{zouLargeScaleSearch2022,
  title = {A Large Scale Search Dataset for Unbiased Learning to Rank},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zou, Lixin and Mao, Haitao and Chu, Xiaokai and Tang, Jiliang and Ye, Wenwen and Wang, Shuaiqiang and Yin, Dawei},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {1127--1139},
  publisher = {Curran Associates, Inc.},
  join_key = {66}
}

@inproceedings{zouVGBenchEvaluatingLarge2024,
  title = {{{VGBench}}: {{Evaluating}} Large Language Models on Vector Graphics Understanding and Generation},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  author = {Zou, Bocheng {and} Cai, Mu {and} Zhang},
  editor = {{Al-Onaizan}, Yaser {and} Bansal, Mohit {and} Chen},
  year = {2024},
  month = nov,
  pages = {3647--3659},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.213},
  abstract = {In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons, sketches and scientific figures. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs and (f) comparison with VLMs on rasterized representations. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced.},
  join_key = {436}
}

@inproceedings{zuoPatentEvalUnderstandingErrors2024,
  title = {{{PatentEval}}: {{Understanding}} Errors in Patent Generation},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies (Volume 1: {{Long}} Papers)},
  author = {Zuo, You {and} Gerdes, Kim {and} Clergerie},
  editor = {Duh, Kevin {and} Gomez, Helena {and} Bethard},
  year = {2024},
  month = jun,
  pages = {2687--2710},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-long.147},
  abstract = {In this work, we introduce a comprehensive error typology specifically designed for evaluating two distinct tasks in machine-generated patent texts: claims-to-abstract generation, and the generation of the next claim given previous ones. We have also developed a benchmark, PatentEval, for systematically assessing language models in this context. Our study includes a comparative analysis, annotated by humans, of various models. These range from those specifically adapted during training for tasks within the patent domain to the latest general-purpose large language models (LLMs). Furthermore, we explored and evaluated some metrics to approximate human judgments in patent text evaluation, analyzing the extent to which these metrics align with expert assessments. These approaches provide valuable insights into the capabilities and limitations of current language models in the specialized field of patent text generation.},
  join_key = {503}
}
