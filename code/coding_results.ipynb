{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ed0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bddd6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df = pd.read_csv('../data/coding_responses.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46fd325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df = codebook_df[codebook_df['Timestamp'].apply(lambda x: datetime.strptime(x,\"%m/%d/%Y %H:%M:%S\") > datetime.strptime('2025-04-05', \"%Y-%m-%d\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91900f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5df24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df.columns = pd.Series(codebook_df.columns).apply(lambda x: x.split(\":\")[0])\n",
    "codebook_df.drop(['Column 1'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed8991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "main_coder",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "bibkey",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "inclusion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "exclusion_criteria",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "exclusion_criteria_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "short_summary",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "contribution",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_short",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "target_phenomenon",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_defined",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_definition",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "definition_scope",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "purpose_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_definition",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_item_definition",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_definition_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_dataset_size",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_dataset_metadata",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_metadata_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_sampling_method",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "response_format",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_definition",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_definition_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "authorship",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "benchmark_availability",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "procedural_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "notes_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_train_val",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_dataset_size_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "response_format_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_aggregation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_subscores",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_subscores_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_metascoring",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "benchmark_location",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "benchmark",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_contested",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_face_validity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_face_validity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "result_interpretation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_comparison",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_comparison_explanation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_realism",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_human_baseline",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_author_validity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_author_validity_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_statistics",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_access",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_ecology",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_ecology_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "definition_integrity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "definition_integrity_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_dataset_size_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_fewshot",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "eae30633-9bae-4bc6-9328-81247facc777",
       "rows": [
        [
         "13",
         "4/8/2025 15:27:11",
         "Harry Mayne",
         "mundlerSWTBenchTestingValidating2024",
         "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents",
         "Include",
         null,
         null,
         "A benchmark for generating code tests (unit tests) from natural language user GitHub issues.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Automatic code test generation (i.e. generating unit tests for issues)",
         "Yes",
         "The ability to generate valid tests to reproduce an issue in a codebase.",
         "Comprehensive",
         null,
         "Given a GitHub issue in natural language, you must write tests to reproduces the described issue.",
         "A GitHub issue (taken from SWE-Bench), code that contains the issue and code with a 'golden patch' that has the issue fixed. The goal is to write unit tests that fail on the faulty code but pass after the patch is added.",
         "Very comprehensive details about task definition.",
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)",
         "1900",
         "Yes",
         "Length of the GitHub issue in tokens, original GitHub repository",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "Structured response (e.g. valid JSON, API call alone)",
         "Whether the faulty code fails on the test and the gold-standard code passes it.",
         null,
         "SWE-bench, which originates from real GitHub issues",
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Description length in tokens, original GitHub repository",
         null,
         "https://github.com/logic-star-ai/SWT-Bench",
         "SWT-Bench",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "Limitations in how the phenomenon was operationalised - all problems are in Python.",
         "simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         null
        ],
        [
         "14",
         "4/8/2025 15:57:43",
         "Jonathan Rystrøm",
         "yangLLMCBenchBenchmarkingLarge2024",
         "LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment",
         "Exclude",
         "Topic Exclusion (Is the paper about measuring the capabilities of LLMs?)",
         "It's about compression algorithms rather than capabilities. ",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "LLMCBench",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "4/8/2025 16:50:41",
         "Jonathan Rystrøm",
         "davidsonEvaluatingLanguageModel2024",
         "EVALUATING LANGUAGE MODEL AGENCY THROUGH\nNEGOTIATIONS",
         "Include",
         null,
         null,
         "The paper introduces a dynamic framework for evaluating LLMs using negotiation games in self-play and cross-play settings. They find that only closed-source models are able to successfully complete the task and that stronger LLMs don't always win over weaker opponents.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Alignment",
         "Yes",
         "Alignment metrics of interest are internal and external faithfulness as defined in Section 2.3, and the ability to follow instructions. [...] We measure instruction-following behavior of staying within the maximum number of words allowed to generate notes/messages (note/msg instruct) and the ability to correctly format internal offer indications using valid JSON (format instruct). [... (from 2.3)...]. . In natural language processing (NLP), faithfulness is a concept used to describe how accurately a model’s reasoning explains its answers/actions. To measure internal faithfulness, agents are asked to summarize acceptable offers for each Issue in their mental notes. [...] If Alice makes an offer to Bob for fewer slices than she stated as acceptable, we register this as an instance of internal unfaithfulness.",
         "Subset",
         "The paper is a bit unfocused in what it measures. The title says \"Agency\", the authors mainly note \"Alignment\" as motivation, and there is also a degree of \"Negotiation skill\" and \"Theory of Mind\". ",
         "The task is a series of negotiation games, where LLMs are given rules, a persona, protocols, and goals. Agents do both internal deliberation and external negotiation, and the game ends when a completion criteria is reached. ",
         "A single task is one round of a negotiation game that is either self-play or against another model. ",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",
         null,
         "Yes",
         "prompts, game settings, issues",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Extended interaction (e.g. conversation, calling an API and processing the response)",
         "Exact Match (accuracy, F1, precision, recall), Number of rounds completted",
         null,
         "The authors generate a list of Games, Issues. It seems these were crafted manually",
         "Academia",
         "Yes",
         null,
         "This \"benchmark\" defines too many phenomena to fit neatly in the framework",
         "Test",
         null,
         "Negotiation",
         "Simple Mean",
         "Yes",
         "Scores are reported for different types of games. ",
         null,
         "https://github.com/epfl-dlab/LAMEN/",
         null,
         "Contested",
         "Partially",
         "Partially",
         "Yes",
         "No",
         "No comparisons made",
         "It is an entirely constructed scenario (no available realistic setting)",
         "No",
         "No",
         null,
         "mean with variance",
         "Outputs alone",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         "The tasks simulates agent negotiations (so no humans involved)",
         "Composite phenomenon",
         "Yes",
         null,
         null
        ],
        [
         "16",
         "4/8/2025 17:08:34",
         "Lennart Luettgau",
         "helweMAFALDABenchmarkComprehensive2024",
         "MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification",
         "Include",
         null,
         null,
         "The paper introduces MAFALD, a benchmark that provides a unified classification of fallacies and provides a taxonomy. It features manually annotated data with explanations, a tailored annotation scheme, and an evaluation method for subjective NLP tasks. Various language models and human performance are evaluated on fallacy detection and classification in a zero-shot learning setting.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "fallacies in reasoning",
         "Yes",
         " A fallacy is an erroneous or invalid way of reasoning. A fallacy is an argument where the premises do not entail the conclusion. Sub-elements: Fallacy of credibility, fallacy of logic, appeal to emotion",
         "Comprehensive",
         null,
         "Given a text, detect fallacies and classify them",
         "Level 0: binary classification (fallacy or not), Level 1: groups fallacies into Aristotle’s categories: ‘Pathos’ (appeals to emotion), ‘Ethos’ (fallacies of credibility), and ‘Logos’ (fallacies of logic, relevance, or evidence), Level 2 contains fine-grained fallacies within the\nbroad categories of Level 1. For instance, under fallacy of credibility, we have specific fallacies such as appeal to tradition, ad populum, and guilt by association.",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)",
         "9735",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Multiple choice",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "3 levels (different granularity)",
         null,
         "GitHub",
         "MAFALDA",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "Yes",
         "No",
         null,
         "simple mean/sum",
         "Outputs alone",
         "Representative task (e.g. answering medical licensing exam questions)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null
        ],
        [
         "17",
         "4/8/2025 18:20:47",
         "Kaili Liu",
         "niuRAGTruthHallucinationCorpus2024",
         "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
         "Include",
         null,
         null,
         "This paper targets word-level hallucinations in various tasks and domains in the RAG setting. It presents approximately 18,000 responses generated using RAG from diverse LLMs which are annotated at the word level for hallucination intensity. Hallucination frequencies are benchmarked across various LLMs, and hallucination detection methods are assessed versus a small LLM fine-tuned using the proposed dataset, RAGTruth.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "hallucination detection, specifically for RAG applications",
         "Yes",
         "\"Hallucination in the context of LLMs usually refers to a situation where the\nmodel generates content that is not based on factual or accurate information\"",
         "Subset",
         null,
         "For a given reference-response pair, determine if it contains hallucinated content at the response level and span level.",
         "A single item consists of source information (reference), an LLM-generated response (which may contain various degrees of hallucination), annotation of the location and type of hallucination (if any), and a brief annotated explanation of the hallucination observed.",
         "Additional meta-data regarding the model and inference hyperparameters used to generate each sample is provided, along with details regarding the source and task type for the reference texts.",
         "Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "2700",
         "Yes",
         "source information index, generating model, temperature, whether quality issues are present in the sample, task type of the data, source of the original content, prompt used to generate the response, base content for RAG",
         "Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train",
         "15090 (train)",
         null,
         "Simple Mean",
         "Yes",
         "by task type (QA, summarization, data-to-text writing)",
         null,
         "https://github.com/ParticleMedia/RAGTruth",
         "RAGTruth",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "Yes",
         "Benchmark statistics and quality checking are described. Hallucination density is assessed across models used to generate the data, in relation to context length, and versus position in the text.",
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 58,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>...</th>\n",
       "      <th>results_author_validity</th>\n",
       "      <th>results_author_validity_detail</th>\n",
       "      <th>metric_statistics</th>\n",
       "      <th>metric_access</th>\n",
       "      <th>task_ecology</th>\n",
       "      <th>task_ecology_detail</th>\n",
       "      <th>definition_integrity</th>\n",
       "      <th>definition_integrity_detail</th>\n",
       "      <th>task_dataset_size_detail</th>\n",
       "      <th>metric_fewshot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4/8/2025 15:27:11</td>\n",
       "      <td>Harry Mayne</td>\n",
       "      <td>mundlerSWTBenchTestingValidating2024</td>\n",
       "      <td>SWT-Bench: Testing and Validating Real-World B...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A benchmark for generating code tests (unit te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Limitations in how the phenomenon was operatio...</td>\n",
       "      <td>simple mean</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Complete real task (e.g. providing medical adv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single cohesive phenomenon</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4/8/2025 15:57:43</td>\n",
       "      <td>Jonathan Rystrøm</td>\n",
       "      <td>yangLLMCBenchBenchmarkingLarge2024</td>\n",
       "      <td>LLMCBench: Benchmarking Large Language Model C...</td>\n",
       "      <td>Exclude</td>\n",
       "      <td>Topic Exclusion (Is the paper about measuring ...</td>\n",
       "      <td>It's about compression algorithms rather than ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4/8/2025 16:50:41</td>\n",
       "      <td>Jonathan Rystrøm</td>\n",
       "      <td>davidsonEvaluatingLanguageModel2024</td>\n",
       "      <td>EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces a dynamic framework for e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mean with variance</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Constructed task (e.g. predicting medical diag...</td>\n",
       "      <td>The tasks simulates agent negotiations (so no ...</td>\n",
       "      <td>Composite phenomenon</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4/8/2025 17:08:34</td>\n",
       "      <td>Lennart Luettgau</td>\n",
       "      <td>helweMAFALDABenchmarkComprehensive2024</td>\n",
       "      <td>MAFALDA: A Benchmark and Comprehensive Study o...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces MAFALD, a benchmark that ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>simple mean/sum</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Representative task (e.g. answering medical li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Composite phenomenon</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4/8/2025 18:20:47</td>\n",
       "      <td>Kaili Liu</td>\n",
       "      <td>niuRAGTruthHallucinationCorpus2024</td>\n",
       "      <td>RAGTruth: A Hallucination Corpus for Developin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper targets word-level hallucinations i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Benchmark statistics and quality checking are ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Complete real task (e.g. providing medical adv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Composite phenomenon</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp        main_coder  \\\n",
       "13  4/8/2025 15:27:11       Harry Mayne   \n",
       "14  4/8/2025 15:57:43  Jonathan Rystrøm   \n",
       "15  4/8/2025 16:50:41  Jonathan Rystrøm   \n",
       "16  4/8/2025 17:08:34  Lennart Luettgau   \n",
       "17  4/8/2025 18:20:47         Kaili Liu   \n",
       "\n",
       "                                    bibkey  \\\n",
       "13    mundlerSWTBenchTestingValidating2024   \n",
       "14      yangLLMCBenchBenchmarkingLarge2024   \n",
       "15     davidsonEvaluatingLanguageModel2024   \n",
       "16  helweMAFALDABenchmarkComprehensive2024   \n",
       "17      niuRAGTruthHallucinationCorpus2024   \n",
       "\n",
       "                                                title inclusion  \\\n",
       "13  SWT-Bench: Testing and Validating Real-World B...   Include   \n",
       "14  LLMCBench: Benchmarking Large Language Model C...   Exclude   \n",
       "15  EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...   Include   \n",
       "16  MAFALDA: A Benchmark and Comprehensive Study o...   Include   \n",
       "17  RAGTruth: A Hallucination Corpus for Developin...   Include   \n",
       "\n",
       "                                   exclusion_criteria  \\\n",
       "13                                                NaN   \n",
       "14  Topic Exclusion (Is the paper about measuring ...   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "                            exclusion_criteria_detail  \\\n",
       "13                                                NaN   \n",
       "14  It's about compression algorithms rather than ...   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "                                        short_summary contribution  \\\n",
       "13  A benchmark for generating code tests (unit te...          NaN   \n",
       "14                                                NaN          NaN   \n",
       "15  The paper introduces a dynamic framework for e...          NaN   \n",
       "16  The paper introduces MAFALD, a benchmark that ...          NaN   \n",
       "17  This paper targets word-level hallucinations i...          NaN   \n",
       "\n",
       "                                     phenomenon_short  ...  \\\n",
       "13  Specific Application (A single use case, where...  ...   \n",
       "14                                                NaN  ...   \n",
       "15  General Capability (A broadly useful ability, ...  ...   \n",
       "16  General Capability (A broadly useful ability, ...  ...   \n",
       "17  General Capability (A broadly useful ability, ...  ...   \n",
       "\n",
       "   results_author_validity                     results_author_validity_detail  \\\n",
       "13                     Yes  Limitations in how the phenomenon was operatio...   \n",
       "14                     NaN                                                NaN   \n",
       "15                      No                                                NaN   \n",
       "16                      No                                                NaN   \n",
       "17                     Yes  Benchmark statistics and quality checking are ...   \n",
       "\n",
       "     metric_statistics  metric_access  \\\n",
       "13         simple mean  Outputs alone   \n",
       "14                 NaN            NaN   \n",
       "15  mean with variance  Outputs alone   \n",
       "16     simple mean/sum  Outputs alone   \n",
       "17                 NaN  Outputs alone   \n",
       "\n",
       "                                         task_ecology  \\\n",
       "13  Complete real task (e.g. providing medical adv...   \n",
       "14                                                NaN   \n",
       "15  Constructed task (e.g. predicting medical diag...   \n",
       "16  Representative task (e.g. answering medical li...   \n",
       "17  Complete real task (e.g. providing medical adv...   \n",
       "\n",
       "                                  task_ecology_detail  \\\n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15  The tasks simulates agent negotiations (so no ...   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "          definition_integrity definition_integrity_detail  \\\n",
       "13  Single cohesive phenomenon              Not applicable   \n",
       "14                         NaN                         NaN   \n",
       "15        Composite phenomenon                         Yes   \n",
       "16        Composite phenomenon                         Yes   \n",
       "17        Composite phenomenon                         Yes   \n",
       "\n",
       "   task_dataset_size_detail metric_fewshot  \n",
       "13                      NaN            NaN  \n",
       "14                      NaN            NaN  \n",
       "15                      NaN            NaN  \n",
       "16                      NaN            NaN  \n",
       "17                      NaN            NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebook_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b68b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bibkeys_df = pd.read_csv('../data/final_list_bibtex.csv')[['bibkey','title']]\n",
    "\n",
    "codebook_df['new_bibkey'] = ''\n",
    "for key in bibkeys_df['bibkey']:\n",
    "    if key.lower() in codebook_df['bibkey'].apply(lambda x: x.lower()).values:\n",
    "        codebook_df.loc[codebook_df['bibkey'].apply(lambda x: x.lower()) == key.lower(), 'new_bibkey'] = key\n",
    "    elif (key +\"a\").lower() in codebook_df['bibkey'].apply(lambda x: x.lower()).values:\n",
    "        codebook_df.loc[codebook_df['bibkey'].apply(lambda x: x.lower()) == (key+\"a\").lower(), 'new_bibkey'] = key\n",
    "    elif (key[:-1]).lower() in codebook_df['bibkey'].apply(lambda x: x.lower()).values:\n",
    "        codebook_df.loc[codebook_df['bibkey'].apply(lambda x: x.lower()) == (key[:-1]).lower(), 'new_bibkey'] = key\n",
    "    else:\n",
    "        print(key)\n",
    "        print(bibkeys_df[bibkeys_df['bibkey'] == key]['title'].values[0])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1e56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df = codebook_df[codebook_df['inclusion'] == 'Include']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3daa08",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25eb8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_embedding(text):\n",
    "\n",
    "    client = openai.OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c592297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_helper(series,labels):\n",
    "    series.index = labels\n",
    "    plt.pie(series, labels=series.index, startangle=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293aba08",
   "metadata": {},
   "source": [
    "## Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f167225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = codebook_df['exclusion_criteria'].value_counts()\n",
    "temp['Include'] = codebook_df['inclusion'].value_counts()['Include']\n",
    "pie_helper(temp, ['Novelty','Topic','Empirical','Modality','Include'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f49760",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenomena_df = pd.read_csv('../data/phenomena_taxonomy.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b7f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df = included_df.merge(phenomena_df,left_on='bibkey',right_on='bibkey',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284e3f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmhUlEQVR4nO3deXyU5b3+8WuW7CsEEgwEwiJ7ABEQiuICitpqxd1q3Wtba9XT+qvdrFWrgh49x4PVemxttYtWcdcqKruAIIiAbIEAsiRkJfs+M78/4ExFAUMyM/ezfN6vF6/SiMlFEnPN/dz38308oVAoJAAAJHlNBwAAWAelAAAIoxQAAGGUAgAgjFIAAIRRCgCAMEoBABBGKQAAwigFAEAYpQAACKMUAABhlAIAIIxSAACEUQoAgDBKAQAQRikAAMIoBQBAGKUAAAijFAAAYZQCACCMUgAAhFEKAIAwSgEAEEYpAADCKAUAQBilAAAIoxQAAGGUAgAgjFIAAIRRCgCAMEoBABBGKQAAwigFAEAYpQAACKMUAABhlAIAIIxSAACEUQoAgDBKAQAQRikAAMIoBQBAGKUAAAijFAAAYZQCACCMUgAAhPlNBwAiLRAMqayuWcXVzapqaFVzW0At7UE1twXCv29pC6j54Nta2oJqbg+oLRBUnM+rBL9X8X6vEvw+Jfi9So73KSXBr9QEv9IS45SW6Fdaol856YnKTkuQx+Mx/VcGIoZSgK0EgyHtq21WSU2zSmqatK/m378vqWlWSXWzyutbFAiGYpIn3udVr4xE5WYmqndmsnp3S1Lvg7/PzUxUbmaSEuN8MckCRIInFArF5r8e4Bg1twW0eV+dNhTXaGNxrTaW1GpzSZ2a2gKmo3WYxyNlpSRoUHaKCnpnaOTBXwN6pLDCgCVRCrCEqobW8A//DQcLYEdFQ8xe8cdaaoJfw3PTDxbFgf8d0CNVXi9FAbMoBRhRWtusZUUVWl5UqeXbK7W7qsl0JONS4n0anpuucfndNeX4nhqX301xPs6CILYoBcREfUu7lm6r0OLCci0vqtT2igbTkSwvNcGviQOydOqQnjr1+J7qm5VsOhJcgFJA1GwsrtWiwnIt3FKmT3btV1uAb7WuyM9K1pTBPXXq4J6aNDBLyfGcE0HkUQqIqG1ldXptTbFeX7uXS0JRFO/zalx+N00f0UvfHHWceqQmmI4Eh6AU0GUlNU1649NivfZpsTaV1JqO4zo+r0ffGJil80fn6uyRvZSWGGc6EmyMUkCn1DS26e31JXr9071aubNKfBdZQ7zfq2nDsnXxiX106uBs+TjNhGNEKaDD2gJBvbehVK+u2avFheVqDQRNR8JRZKclaMYJvXXJuD4alJ1mOg5sglLA16pubNXfV+zSc8t3qrS2xXQcdMLYvpm6/uT+OmfkcawecFSUAo6oqLxez3y4Q698stdWdxHjyPK6J+n6yf112fg8Ti/hsCgFfMWHWyv0pw+3a2FhOXsFDpWRFKfvnNRX130jX9npiabjwEIoBUiSWtoDen1NsZ5ZukOb99WZjoMYifd5df6YXN00ZYAG57DvAErB9ZrbAnp22U49vWS7KupbTceBQacO7qmbpgzQ5EE9TEeBQZSCSwWCIc1ZvVv//cFWldQ0m44DC5k4oLt+ee4wjeqTaToKDKAUXGjuhn16eO4WbSurNx0FFuXxSN8alaufTR+ivO7MXHITSsFFVmyv1Kx3N+uTXdWmo8Am4n1eXTWxn26dOkiZyfGm4yAGKAUX2LyvVrPe2awFW8pNR4FNpSf69cPTBum6yfk8Sc7hKAUH21fTrIfe3azXPt0rhz6rBjHWOzNJPzlzsGac0JsHAjkUpeBAgWBIzy7bqUffL1R9S7vpOHCgEbnpevDCAjajHYhScJh1e6r1q1c/0/q9NaajwOF8Xo9uPLm//uPMwVxSchBKwSEaWtr18Nwtem75Ti4VIab690jRzAsLdNKALNNREAGUggMs2Vqun7+8XnureagNzPB4pCtP6qufnzNMqQnMVLIzSsHGapvbdP9bm/TPVbtNRwEkHdiIvn/GSJ02JNt0FHQSpWBTC7aU6Rcvr9e+Wu5GhvVcOLa3fvOt4dzbYEOUgs20B4Ka9e5mPb1kh+kowFH1SE3QrIsKNHVYjukoOAaUgo0UVzfpln98wh3JsJUbT+6vO88Zqjif13QUdAClYBPzN5fqJy+uVXVjm+kowDEbk5ep2VecwBwlG6AULK49ENTDc7fof5ds54E3sLX0RL8evmS0po/oZToKjoJSsLCSmib9+B9rtOrz/aajABHz/VMH6GfTh/KsaIuiFCxqwZYy/fTFtapq4ME3cJ7Jg7I0+4qx6p7C6SSroRQsJhQK6dH3C/X4gm1cLoKj9c5M0pNXjWV+ksVQChbS2h7UHS+t1Rtri01HAWIi3u/VQxeN0gUn9DYdBQdRChZR09Sm7/91lT7aXmU6ChBTHo/087OH6vunDjQdBaIULGFvdZOu+/NKFZbyeEy41/WT++uubw2Tx8MGtEmUgmEbimt03Z8/Vlldi+kogHHfGnWcHr10jOL93Ohmiu0/86FQSNOmTdP06dO/8s+eeOIJZWRkaNeuXQaSfb3FheW67KmPKATgoLfWleiaZ1aqrpmbNE2xfSl4PB79+c9/1ooVK/TUU0+F375jxw7deeedeuyxx9S3b1+DCQ/vxVW7df1fPubJaMCXLN9eqUv+sFylDHs0wvalIEl5eXl67LHHdMcdd2jHjh0KhUK64YYbNHXqVE2YMEHnnnuuUlNTlZOTo+9+97uqqKgI/7tz5sxRQUGBkpKSlJWVpWnTpqmhoSGqef/r/UL9bM46tfM0HOCwNu+r04VPLNO2MvbZYs1RewoXXHCBqqurddFFF+m+++7Txx9/rHHjxul73/uerr76ajU1NenOO+9Ue3u75s+fr5KSEvXt21cPPfSQZsyYobq6Oi1ZskRXX321UlNTo5Lx/rc3MuEU6KDM5Dj96ZpxOrFfd9NRXMNRpVBWVqaRI0eqsrJSc+bM0Zo1a7RixQrNnTs3/Gf27NmjvLw8bdmyRfX19TrxxBO1c+dO9evXL+r5Hnp3s55YWBT1jwM4SUq8T3+98SSN7dvNdBRXcMTlo/+TnZ2tm266ScOGDdOMGTO0evVqLViwQKmpqeFfQ4cOlSQVFRVp9OjRmjp1qgoKCnTJJZfo6aef1v790Zkz9N8fFFIIQCc0tAZ0zTMr9dneGtNRXMFRpSBJfr9ffv+BZ8QGg0Gdd955+vTTTw/5tXXrVk2ZMkU+n0/vv/++3nnnHQ0fPlyzZ8/WkCFDtGNHZC/v/H7BNv33B1sj+j4BN6lrbtfVz6xUYWmd6SiO57hS+KKxY8dqw4YNys/P16BBgw75lZKSIunA6aXJkyfrnnvu0Zo1axQfH69XX301YhmeXrxdD8/dErH3B7hVVUOrrvzjCu2oiO5BELdzdCn86Ec/UlVVla644gqtXLlS27dv13vvvafrr79egUBAK1as0AMPPKBVq1Zp165deuWVV1ReXq5hw4ZF5OP/ZekO3f+vTRF5XwCk8roWXfn0R9qzv9F0FMdydCnk5uZq6dKlCgQCmj59ukaOHKnbbrtNGRkZ8nq9Sk9P1+LFi3Xuuedq8ODB+vWvf61HHnlE55xzTpc/9t9XfK573toYgb8FgC8qrmnWlX9cwX0MUeKo00dW8eKq3brz5XWMvgaiaFB2qv5500RlpSaYjuIojl4pmPD+xlL9nEIAom5bWb2u+tNK1fDc8oiiFCJo/Z4a3fbCGnGjMhAbm0pq9b2/rlJbIGg6imNQChFSXN2kG579WI2tAdNRAFdZuaNKv3l9g+kYjkEpREB9S7uu/wvjrwFTnl+5S88t32k6hiNQCl0UDIZ0yz8+0eZ93FQDmHTvmxu1rKji6/8gjopS6KJZ727Wwi3lpmMArtceDOlHf/9Euyq5h6ErKIUueHXNHj21eLvpGAAO2t/Yphuf4zklXUEpdNK6PdX6+cvrTccA8CWFpfW6/YU1CnIMsFMohU4or2vRTc+tVks7x+AAK/pgU5n+8z1mjnUGpXCMQqGQfvrSWu3jFnvA0p5YWKTXP91rOobtUArH6JmlO7W4kI1lwA5++cp67WSq6jGhFI7BppJazXp3s+kYADqooTWg215Ywx3Px4BS6KDmtgPfXK3sIwC2snZPjR55r9B0DNugFDrogX9tUmFpvekYADrhqcVFWrqNG9s6glLogPmbS/Xc8s9NxwDQSaGQ9JMXP1V1Y6vpKJZHKXyN8roW/b+X1pmOAaCLSmtbGJzXAZTCUYRCId3x0lpVNvDqAnCCN9YW6+11JaZjWBqlcBR/XrpTizh+CjjKXa9/pnImGh8RpXAEe/Y36qG5HD8FnKaqoVW/eIVLwkdCKRzBPW9uVHMbx08BJ/pgU5neWldsOoYlUQqHMX9zqd7fWGo6BoAoeuDtTWriSYlfQSl8SXNbQL99Y6PpGACirLimWY8v2Go6huVQCl/yxMIi7ariIR2AGzy9ZAezkb6EUviCnRUN+sOiItMxAMRIa3tQ977FlYEvohS+4O43NjDbCHCZ+ZvLNG8Te4j/h1I46N3P9nFPAuBS9761US3tbDpLlIIkqak1oPtYQgKu9Xllo57meeuSKAVJ0u8XbNPe6ibTMQAY9PsFRSrm5wClUFnfomeW7jAdA4BhTW0B3f+vTaZjGOf6UvjDoiI1cgMLAElvryvRhuIa0zGMcnUplNe16G8f7TIdA4CFzJ63zXQEo1xdCn9YVKSmNlYJAP5t7sZ92ryv1nQMY1xbCmV1zfr7Cp6mBuBQoZC7VwuuLYUnFxYxBRXAYb3zWYm2ltaZjmGEK0uhtLZZ/1jBXgKAwwuGpNnz3blacGUpPLFgm1oYZwHgKN5aV6yi8nrTMWLOdaVQUtOk5z/ebToGAIsLhqTHXbhacF0pPLmwiKF3ADrkjbXF2uGy0dquKoWaxja9tGqP6RgAbCIQDLluteCqUnhx1W7uSwBwTN5cW6zK+hbTMWLGNaUQDIb03Ec7TccAYDOtgaBeWu2eKwyuKYX5m8u0u4oJiACO3fMrdykUCpmOEROuKYVnl+80HQGATX1e2ail2ypNx4gJV5TCzooGfbitwnQMADb2j5XuGIvjilJ4/uNdcsnKD0CUvL+xVOV1zt9wdnwptAWCetlFm0QAoqMtENKLq5x/46vjS+G9DaWqqG81HQOAA/zz492O33B2fCk8v5LBdwAiY1dVo5Zsdfb+pKNLobS2WUuLnP0FBBBbTp+w7OhSePezfWwwA4ioDzaVOvoOZ0eXwjuflZiOAMBh2oMhzd1QajpG1Di2FCrqW/Txzv2mYwBwICe/4HRsKby3oVSBINeOAETe8qJK7W9w5qlGx5aCk5scgFntwZDe27jPdIyocGQp1DS26aPt7phTAsCMf62nFGzjvY371Bbg0hGA6FleVKn6lnbTMSLOkaXw7mfObHAA1tEaCGpJYbnpGBHnuFKob2nXEiaiAoiBDzaVmY4QcY4rhXmbStXaHjQdA4ALLNxSpqDDTjk6rhScPpcEgHVUNrRqzW5n3Q/luFJYuaPKdAQALrKo0FkvRB1VCiU1TdpV1Wg6BgAXWbXTWS9EHVUKrBIAxNqnu6vVHnDOPqajSmEFpQAgxhpbA9pQXGs6RsQ4qhRYKQAwYdXnztlsdkwpVNa3aFtZvekYAFzISfsKjikFVgkATGGlYEHsJwAwpbyuRTsrGkzHiAjHlAIrBQAmOWW14IhSqG1u0+Z9ztn9B2A/TtlXcEQpbCqulcPGjwCwmY8pBevYVs6pIwBmba9oUIMDnq/giFLYWkopADArFJJ2OGCz2RGlwP0JAKygyAFXLRxRClvL6kxHAAAVlbNSMK62uU2ltS2mYwAAKwUrYD8BgFVsZ6VgXhH7CQAsYmdFg0Ihe5+Pt30psJ8AwCqa2gLaW91kOkaXOKAUWCkAsA67X0KyfSlwHBWAldh9s9nWpRAIhlRs86UaAGdhpWBQRX0LM48AWMquqkbTEbrE1qVQxv0JACymqqHVdIQusXcp1DWbjgAAh6AUDCqrY6UAwFqqGykFY7h8BMBqGloDamkPmI7RabYuhf02b2QAzlTd2GY6QqdRCgAQYXbeV7B1Kdi5jQE4l51fsNq8FOz7iQfgXPsb7PuC1d6l0GTfTzwA52KlYEh9s/0fkg3Aefazp2BGwOZzywE4k52vYti6FIIMPgJgQe2BoOkInWbvUqATAFhQu41/ONm8FOz7iQfgXHb+2UQpAECEBVgpmGHjzzsAB7Pz5SO/6QBdEWKlgAgZkdagOemPmY4Bh2hNPUvSGNMxOsXWpWDnJRqsZXN9khK1XZ42ez81C9aQlDfGdIRO4/IRICkQ8qox43jTMeAUXvv+aLVtci4dIdL2JQ40HQFO4bXvRRjblgIQaYXqazoCnIJSiD2Px6PUBPt+4mE9KxuPMx0BTkEpmJGRFGc6Ahxk3v6epiPAKSgFM9IpBUTQrqZEBVJZLSACkrqZTtBpti6FTEoBEVadNth0BDhBin1XnbYuBS4fIdI+9+ebjgAnSM02naDTKAXgC9a355mOACdI6WE6QafZuxSSKQVE1tK6HNMR4AQprBSMYKWASFtU1U0hX7zpGLA79hTMoBQQaS1Br5ozB5mOATtLSJfiEk2n6DRKAfiSsiRKAV1g4/0Eyeal0C2ZZT4ibyvjLtAVNt5PkGxeCrmZ9l2iwbpWt+SajgA7Y6VgTl73ZPm8HtMx4DDzq+z9Sg+G2XiTWbJ5KcT5vOrTLcl0DDjMloZkBZPs/WoPBtn4xjXJ5qUgSflZKaYjwIFqeeAOOivV3ve62L4U+vegFBB5u+MGmI4Au+o5xHSCLrF9KeRnJZuOAAfaGGDcBTopZ4TpBF1i/1JgpYAoWNbQy3QE2FFarq3HZksOKAUuHyEa5ld2V8jjMx0DdpMz3HSCLrN9KfTplqw4H8dSEVl17X61ZfQ3HQN2Y/NLR5IDSsHn9SivO/sKiLzyFMZd4BhlUwqW0J9jqYiC7d580xFgN1w+soYhvdJMR4ADrWnpbToC7MTrl3rY+ziq5JBSGJ2XaToCHGhhjb3vTEWMZR0v+e0/pNMRpXACpYAo+KQmTaGEdNMxYBcOuHQkOaQUstMT1SudiamIvPqMwaYjwC6yKQVLGZ2XYToCHGhvwkDTEWAXDjiOKjmoFMbk2fsuQljTpiDjLtBBuSeYThARjimF8fmUAiJvReNxpiPADnoOk9KcMRrFMaUwqk+mEvyO+evAIuZV9lBI3DGPrzHgNNMJIsYxP0Xj/V6N7pNpOgYcprw1Tu3pPLMZX2Pg6aYTRIxjSkGSxvfnEhIiryqVB+7gKLxxUr/JplNEjLNKIb+76QhwoB2+fNMRYGV9xksJqaZTRIyjSmFC/+7sKyDi1rb2MR0BVuag/QTJYaWQHO/XNwZmmY4Bh1lSZ+9n7iLKHLSfIDmsFCTpzOHOOBYG61i+P02hOMaz4zAS0qXeJ5pOEVGOK4Vpw7Ll4QQhIigQ8qoxg81mHEb+KZLXWU/oc1wpZKcnalRvRl4gsvYlMu4Ch+Gw/QTJgaUgSdOGcQ0YkVUo7lXAYThsP0FyaCmcOYJSQGStZNwFviyjr9TDeZcVHVkKQ3ulK697kukYcJB5+3uajgCrGXmh6QRR4chSkKSpQ1ktIHJ2NSUqkMpqAV8w+nLTCaLCsaVw5nBKAZFVncYDd3BQr1FS9jDTKaLCsaVwUv/uSk/0m44BB/ncn286AqzCoasEycGl4Pd5dd7oXNMx4CDr23ngDiR5fNLIi02niBrHloIkXT6eY4SInKWMu4B04BhqmnO/FxxdCgV9MjQiN910DDjEoqpuCvniTceAaaOce+lIcngpSNLl41nyIzJagl41Zw4yHQMmxadKQ79pOkVUOb4Uvn1CbyXGOf6viRgpS6IUXG3Y+VK8s4cjOv6nZXpinM4dyflyRMZWxl242+jLTCeIOseXgiRdxiUkRMjqFk60uVZarpQ/xXSKqHNFKZw0IEsDeqSYjgEHmF+VbToCTBl1qeR1/o9M5/8ND2K1gEjY0pCsYHIP0zEQa944acJNplPEhGtK4aIT+yjOx9N30HW16Yy7cJ1Rl0oZvU2niAnXlEKP1ASdzYYzImB3XH/TERBTHmnybaZDxIxrSkGSbj5tII/qRJdtCHACyVUGny31HGI6Rcy4qhSGHZeuqUPZKETXLG9w7ogDHMbJt5tOEFOuKgVJuuUM5z0pCbE1v7K7Qh5nPawdR5A3Ueo70XSKmHJdKYzJy9TJgzg9gs6ra/erLYN9BVdw2SpBcmEpSNItZzCqAF1TnsL3kOP1HHZgP8FlXFkKEwdkaXx+N9MxYGPbvfmmIyDaJt8qN55McWUpSNKPTueVHjpvTYs7zqy7VnofqeAS0ymMcG0pnDYkWwW9M0zHgE0trOEUm6NNulnyxZlOYYRrS0FitYDO+6QmTaEEHuDkSKk50thrTKcwxtWlMH1Ejob2SjMdAzZVn8G4C0ea+hspIdV0CmNcXQoej0e/+uYw0zFgU3sTBpqOgEjLPUEac6XpFEa5uhQk6ZTje+r0IT1Nx4ANbQoyeddxzp7pyhNHX+T6UpCkX31zmPxed38j4NitaGTAoqOMvMh1dy8fDqUgaVB2mq6YwJAzHJt5lT0UEi8mHMGfJJ15r+kUlkApHPQfZw5WRpI7j6Chc8pb49SezosJR5h8q5TRx3QKS6AUDuqeEq87zuI0CY5NVSoDFm0vvbc0+XbTKSyDUviCK0/qpxG5nD1Hx+3w5ZuOgK6ado8Un2w6hWVQCl/g9Xp077dHuv3wAY7B2lYuOdhanwnSKHeOszgSSuFLTuzXTReN5T90dMziWsZd2JdHOmem6RCWQykcxi/OGaqslHjTMWADH1WnKxSXYjoGOuOEq6TeJ5pOYTmUwmFkpSbo/hkjTceADQRCXjVmMEPLdjL7SWc/aDqFJVEKR3D2yON04QmMR8bXK0miFGzF45VmPCUlMPfscCiFo/jtt0coNyPRdAxYXGGIcRe2Mvk2qd8k0yksi1I4ivTEOP3nJaM5jYSjWtmYazoCOqpXgXT6r0ynsDRK4Wt8Y1APXTMp33QMWNj8/T1MR0BH+BOlC//o2ofndBSl0AE/P2eoBvbkhAkOb1dTogKpDMezvKl3S9lDTaewPEqhAxLjfHr00jFMUsURVacxIsXSBpwmTfyh6RS2QCl00Oi8TN3M4ztxBJ/7801HwJEkZkoXPOn65yR0FKVwDG49Y5BG52WajgELWt/OCSTL+uYjUjqHATqKUjgGfp9Xf7hqrHqkJpiOAotZWpdjOgIOZ+TFUsHFplPYCqVwjI7LSNKTV41VnI+lKP5tUVU3hXyMRrGUHkOkb/2X6RS2Qyl0wvj87rr7vBGmY8BCWoJeNWey52QZiRnSFc9LiYzCP1aUQiddNbEfj/DEIcoYd2ENHp908TNS1kDTSWyJUuiCe84foXH9upmOAYvYKl4kWMK0u6VB00ynsC1KoQvi/V49cdVY9UpnPhKk1S2ccDGu4JIDs43QaZRCF2WnJeqp756oeD+fSrebX8UDd4zqM146/3HTKWyPn2QRMDovU/dfwPMX3G5LQ7KCycxBMqJbvnTFC1Icq/auohQi5JJxefr+lAGmY8Cw2nTGXcRcYqb0nZekFAo5EiiFCPrFucN0xQTubHWz3XH9TUdwF1+8dNnfpJ6UcaRQChF2/wUFOm80G45utSHACaSYOu9/pP6ndOldXHvttfJ4PJo5c+Yhb3/ttdfkceG8JEohwrxejx69dLTOGMqmoxstq2fcRcycPVMac0VE3lViYqJmzZql/fv3R+T92RmlEAVxPq+euHKsJg7objoKYmxeVZZCHp/pGM539syIjsKeNm2aevXqpQcffPCIf+bll1/WiBEjlJCQoPz8fD3yyCMR+/hWQilESWKcT3+8ZrxG98kwHQUx1NDuU2smBw6iavqDEX82gs/n0wMPPKDZs2drz549X/nnq1ev1qWXXqrLL79c69ev129/+1vddddd+stf/hLRHFZAKURRaoJfz14/QYNzUk1HQQxVJDPuImqmPyhNujkq73rGjBkaM2aM7r777q/8s0cffVRTp07VXXfdpcGDB+vaa6/VLbfcoocffjgqWUyiFKIsMzlef7vhJPXLSjYdBTFS5M03HcGZpj8QtUL4P7NmzdKzzz6rjRs3HvL2TZs2afLkyYe8bfLkydq6dasCgUBUM8UapRAD2emJ+tsNJ6l3ZpLpKIiBTxl3EXln3S9N+lHUP8yUKVM0ffp0/fKXvzzk7aFQ6CsnkUKhUNTzmEApxEhe92S99INJGtAzxXQURNnCGk6eRdRZ90vfuCVmH27mzJl68803tWzZsvDbhg8frg8//PCQP7ds2TINHjxYPp+zDhZQCjGUm5mkl74/SSNymfHuZJ/UpCmUwNc4Is76XUwLQZIKCgp05ZVXavbs2eG3/fSnP9W8efN03333qbCwUM8++6wef/xx3XHHHTHNFguUQoxlpSbo+Zsmanw+I7edrD6DO2y77Mz7pG/82MiHvu+++w65PDR27Fi9+OKLeuGFFzRy5Ej95je/0b333qtrr73WSL5o8oScemHM4prbArr5759o/uYy01EQBe8e/7qG7v6n6Rj25PVL5/6nNO4600lciZWCIYlxPj199ThdPp5ZSU60KcjXtVMSM6WrXqEQDKIUDPJ5PZp50SjdPu1401EQYSsajzMdwX66D5RunCcNONV0ElejFCzg9mmDNeuiAvm97hu+5VTzKnsoJL6eHZZ/inTjB1IPbvwzjVKwiMvG99Uz145Xt+Q401EQAeWtcWpPZ2Jqh4y9Rvruq1Iys8KsgFKwkCmDe+rNH5+sgt7MS3KCqlQuCx6Vx3vgLuXz/0fy8WLIKigFi+nTLVlzfjhJl41jo9LudvjyTUewrvg06fLnY3KXMo4NpWBBCX6fZl08SjMvLFC8ny+RXa1t7WM6gjVl9JVumCsNOdt0EhwGP3Es7PIJfTXnB5OYmWRTi2sZd/EV/U+VvjdPyhlhOgmOgJvXbGB/Q6tufWGNlmytMB0Fx8DnCWpb6vflaWswHcU8X4I07W5p4s2SCx9xaSesFGygW0q8nr1ugn58xiD+e7KRQMirxgyOWCpnpHTTwgP7B3wDWx6lYBNer0c/PWuInrt+ApeTbKQkyc2l4JEm3SJ9b76UM9x0GHQQpWAzpxzfU+/efoqumMDpJDsoDLn065TeW7r6dWn6/ZI/wXQaHANKwYbSEuP04IWj9NcbWDVY3YpGFz5wZ8QM6YdLGVdhU2w021x9S7se+NcmPb9yl/hKWk+fxBZ9KJcMd0tIl859WBp9uekk6AJKwSGWbqvQnS+v0579Taaj4EuKetwhX32x6RjR1e9kacaTUiajPeyOy0cOMXlQD829fYqumtiXAx4Wsz/NweMu0ntLF/1Juu5tCsEhKAUHSUnw63cXFOgfN07U0F5ppuPgoF1x/U1HiDx/ojTlZ9Itq6SCi02nQQRRCg40aWCW3r71FM28sEA90zj5Ydr6NoeNuxj+bemWj6UzfiXFJ5tOgwhjT8HhGlra9dSiIj29ZIea2gKm47jSWT0q9b/1Zp41HFE5I6WzZ0r9TzGdBFFEKbhESU2THp67Ra+u2csppRhL8Aa1Oel6eQKtpqN0TlL3A6uCE6+TvD7TaRBllILLfLa3Rr97e6M+2l5lOoqrbOr9OyVVbjQd49h4/dK4G6TTfyEldTOdBjFCKbjU+xtL9eA7m7S9nGFtsbBo0Avqt+cN0zE6xuuXCi6RTv6J1HOw6TSIMb/pADDjzOE5OmNott5aV6wnFhRpS2md6UiOtlV91c90iK8TlyyNvfrAvKJMl47nACsFSKFQSO9vLNXvF2zT2j01puM40g/zdurO8l+ajnF4iZnShJukk34gpWSZTgPDKAUcYum2Cj21eLsWF5abjuIoQ1IaNTdwo+kYh0rLlSbdfGADOSHVdBpYBKWAwyosrdMfl2zXa58Wq7U9aDqOI2zvfqu8jRZ4UFLWIGnybdKoyyV/vOk0sBhKAUdVXteivy7fqX+u2q3S2hbTcWzt0/zHlblvmbkAfScduEQ07HzJy32rODxKAR0SCIa0ZGu55qzeo/c3lqqF1cMxe/P4t1Ww+++x/aDd8g+sCEZfJnUfENuPDVvi9BE6xOf16LQh2TptSLZqmtr01rpizVm9R2t2VZuOZhsbAn1VEIsPlJAhjfi2NPo7Ur9JsfiIcBBWCuiS7eX1mrN6j15ds1clNc2m41ja+dll+p/a26Pzzr1+aeAZB55lMOSbUlxidD4OHI9SQEQEgyEtLarQq2v2atGWclU22HSkQxSl+AP6LO5aeUIRnEHVq0AafcWBm81SsyP3fuFalAIiLhgMad3eGi3YXKaFW8q0bm8N85YO2nLc3UrYv7Xz7yA+Tco/WRp4ujRwqtRjUOTCAaIUEAMV9S1atKVcCwvLtbiwXDVNbaYjGbN04F/Ve+87Hf8XPF4p94QDl4YGnC7lTZB8cdELCNejFBBTgWBIa3bt14ItZfpwW6U2ldS66j6I545foim7nzz6H8rs++8SGHAqw+gQU5QCjGptD2rzvlqt3VOjdburtW5PjbaW1Sno0O/K/+i7XbeV/frfb/D6pR5DpONGS31OPFAEWQPNBYTrUQqwnMbWdn22t1br9lQfKIs91fq8stF0rC5LTfDrnHzp4ay3D5TAcWOknBGcFIKlUAqwhbrmNu2saNTOygbtrGjQzsoDv9+zv1HldS2WWVlkJscpJy1R2ekJys1I0vE5qRqUnarBOWnKzUwyHQ/4WpQCbK8tENS+mmYVVzeppKZZJTXNqm1uU0NLu+pb2sP/W98SUMMX/n9DS/shZeLxSHE+rxJ8XsX5vYrzeRTv9yrO51W8z6t4v1epCX7lpB/4oZ+Tlqic9ETlpCcoJz1RPdMSlBjHk8lgb5QCXK2xtV3SgTKI8zEPCKAUAABhvDQCAIRRCgCAMEoBABBGKQAAwigFAEAYpQAACKMUAABhlAIAIIxSAACEUQoAgDBKAQAQRikAAMIoBQBAGKUAAAijFAAAYZQCACCMUgAAhFEKAIAwSgEAEEYpAADCKAUAQBilAAAIoxQAAGGUAgAgjFIAAIRRCgCAMEoBABBGKQAAwigFAEAYpQAACKMUAABhlAIAIIxSAACEUQoAgDBKAQAQRikAAMIoBQBAGKUAAAijFAAAYZQCACCMUgAAhFEKAIAwSgEAEEYpAADCKAUAQNj/B2FMczGDHwDdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = codebook_df['phenomenon_defined'].value_counts()\n",
    "pie_helper(temp, temp.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c208bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenomenon_map = {'General Capability (A broadly useful ability, which could be relevant to multiple applications)': 'General Capability',\n",
    "                     'Specific Application (A single use case, where the benchmark is likely to be examples of that use case)': 'Specific Application',\n",
    "                     'General form of bias':'General Capability',\n",
    "                     'Specific form of bias':'General Capability',\n",
    "                     'General Capability (A broadly useful ability, which could be relevant to multiple applications), MERA as a whole tries to measure general \"capabilities\", but the individual tasks evaluate more specific applications (e.g., question answering).': 'General Capability',\n",
    "                     'General Capability (A broadly useful ability, which could be relevant to multiple applications), Specific Application (A single use case, where the benchmark is likely to be examples of that use case)': 'Both'}\n",
    "\n",
    "\n",
    "contested_map = {'Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)':'Contested',\n",
    "                 'Contested':'Contested',\n",
    "                 'Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)':'Widely-agreed',\n",
    "                 'Widely-agreed': 'Widely-agreed',\n",
    "                 'No definition provided': 'Not defined',\n",
    "                 'Very specialized phenomenon without any standard definition':'Widely-agreed',\n",
    "                 'A new problem is presented so agreement is unclear; but the definition is generally interpretable in a widely-agreeable way.':'Widely-agreed',\n",
    "                 'Very special phenomenon that is only introduced by the authors. It is neither widely-agreed upon nor contested.':'Widely-agreed',\n",
    "                 'Very specific phenomenon discovered in this paper (i.e., no prior definition exists)':'Widely-agreed',\n",
    "                 'Very specialized phenomenon with no standard definition ': 'Widely-agreed',}\n",
    "\n",
    "scope_map = {'The benchmark measures the effects of multi-turn and code interpretor on the target phenomena.':'Subset',\n",
    "             'The title claims the benchmark is \"comprehensive\", but in the Limitations section they say that the \"evaluation might not comprehensively assess LLM’s abilities.\"':'Comprehensive'}\n",
    "\n",
    "col_maps = {'phenomenon_contested': contested_map,\n",
    "            'phenomenon_short':phenomenon_map,\n",
    "            'definition_scope':scope_map,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd77e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_map = {'Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)': 'Author-crafted',\n",
    "              'Modified from another benchmark (e.g. translation into another language)':'Another benchmark',\n",
    "              'LLM-generated task examples (e.g. Filtered from responses to a prompt)':'LLM-generated',\n",
    "              'Real task examples (e.g. GitHub issues)':'Real task',\n",
    "              'Procedurally-generated task examples (e.g. Creating instances from a template)':'Procedurally-generated',\n",
    "              'Crowd-sourced task examples (e.g. Prolific-created tasks)':'Crowd-sourced',\n",
    "              'Expert-crafted task examples (e.g. hand-written examples)':'Expert-crafted',\n",
    "              'Human exam questions (e.g. GRE questions)':'Human exams',\n",
    "              'LLM- and VLM- generated task examples':'LLM-generated',\n",
    "              'Text snippets from books':'Author-crafted',\n",
    "              'Human-crafted task examples from an existing human game (Choose-Your-Own-Adventure)':'Procedurally-generated',\n",
    "              'Scraped from social media (Reddit)':'Procedurally-generated',\n",
    "              'The dataset is derived from the open BYTESIZED32 corpus.':'Another benchmark',\n",
    "              '':'Unknown',\n",
    "              'Produced media (TV sitcom scenes)':'Author-crafted',\n",
    "              'Unclear':'Unknown',\n",
    "              'Expert-annotated task examples (PhD students)':'Expert-crafted',\n",
    "              'The examples are created by a linguist ':'Expert-crafted',\n",
    "              'Domain expert annotators':'Expert-crafted',\n",
    "              'Wikidata':'Crowd-sourced',\n",
    "              'Human expert created the examples':'Expert-crafted',\n",
    "              'Not explained ':'Unknown',\n",
    "              'For some part of the data they include human generated prompts ':'Author-crafted',\n",
    "              'hand-written by linguists in respective languages, taken from textbooks, handbooks and journal articles in theoretical syntax + some examples taken from previous benchmarks ':'Expert-crafted',\n",
    "              'Human TV show; Human chitchat dialogues':'Author-crafted',\n",
    "              'Based on knowledge graphs (KG) e.g. Wikidata':'Procedurally-generated',\n",
    "              'Original benchmark modified through an agent automatically and through crowdsourcing it was filtered for quality.':'Crowd-sourced',\n",
    "              'Human-sourced task examples (not crowdworkers per say as these are non-paid real-users)':'Real task', \n",
    "              }\n",
    "sources_raw_list = ['Human exam questions (e.g. GRE questions)','Real task examples (e.g. GitHub issues)','Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)','Expert-crafted task examples (e.g. hand-written examples)','Crowd-sourced task examples (e.g. Prolific-created tasks)','Modified from another benchmark (e.g. translation into another language)','Procedurally-generated task examples (e.g. Creating instances from a template)','LLM-generated task examples (e.g. Filtered from responses to a prompt)']\n",
    "sources_list = ['Author-crafted','Crowd-sourced','Unknown','Procedurally-generated','Expert-crafted','Another benchmark','LLM-generated','Human exams','Real task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "932fdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_map = {'Targeted items (creators defined a task space and chose tasks within it strategically)':'Targeted',\n",
    "                'Specific criteria (items were taken from a larger set based on specified rules)':'Criterion',\n",
    "                'Convenience sample (creators found a set of tasks that was readily accessible)':'Convenience',\n",
    "                'Random sample (creators defined a task space and sampled from it)':'Random',\n",
    "                'Unknown':'Unknown',\n",
    "                '':'Unknown'\n",
    "                }\n",
    "sampling_raw_list = ['Specific criteria (items were taken from a larger set based on specified rules)','Targeted items (creators defined a task space and chose tasks within it strategically)','Convenience sample (creators found a set of tasks that was readily accessible)','Random sample (creators defined a task space and sampled from it)']\n",
    "sampling_list = ['Targeted','Criterion','Convenience','Random','Unknown']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f83a7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_map = {'Structured response (e.g. valid JSON, API call alone)': 'Structured',\n",
    "       'Extended interaction (e.g. conversation, calling an API and processing the response)':'Interaction',\n",
    "       'Multiple choice':'Multiple choice',\n",
    "       'Short free response (e.g. single word or number)': 'Short free response',\n",
    "       'Free response (e.g. summary paragarph)' : 'Free response',\n",
    "       'Depends on the subtask category (Utterance Classification, Dialogue Classification, Multiple Choice, Span Extraction)':'Short free response',\n",
    "       'Retrieval ': 'Short free response',\n",
    "       'functioning code (i.e., a .py script or model artifacts)':'Free response',\n",
    "       'Choice of one input sentence':'Multiple choice',\n",
    "       'predicted label':'Multiple choice',\n",
    "       'Log-likelihood of a given free response':'Logits',\n",
    "       'Free response (e.g. summary paragraph, executable code)':'Free response',\n",
    "       '':'',\n",
    "       'Generated image':'Free response',\n",
    "       'Sequencing':'Free response', \n",
    "       'image':'Free response', \n",
    "       'Image':'Free response',\n",
    "       'Movement trajectory to complete task':'Free response', \n",
    "       'Ranking of images':'Free response',\n",
    "       'This task is not based on model responses; it exclusively relies on the probability assigned to input tokens.':'Logits',\n",
    "       'This task is not based on LM responses; it solely relies on measuring the probabilities assigned to tokens in the sentence pairs.':'Logits',\n",
    "       'This task is not based on model responses; it relies solely on perplexity measurements.':'Logits',\n",
    "       'The task is not based on model responses; it solely relies on the probabilities assigned to the tokens in the two sentences.':'Logits',\n",
    "       'The task is not based on responses; it relies solely on the probability assigned to the tokens in the sentence.':'Logits',\n",
    "       'Numeric response (for utilitarian task)':'Short free response'}\n",
    "\n",
    "response_raw_list = ['Multiple choice','Short free response (e.g. single word or number)','Free response (e.g. summary paragraph, executable code)','Free response (e.g. summary paragarph)','Extended interaction (e.g. conversation, calling an API and processing the response)','Structured response (e.g. valid JSON, API call alone)']\n",
    "response_list = ['Structured', 'Interaction', 'Multiple choice', 'Short free response', 'Free response', 'Logits', 'Unknown']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1592de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map = {'Whether the faulty code fails on the test and the gold-standard code passes it.':'Reward',\n",
    "       'Exact Match (accuracy, F1, precision, recall)':\"Exact match\",\n",
    "       'Number of rounds completted': 'Reward',\n",
    "       'n-gram (BLEU, ROUGE, chrF)': 'Soft match',\n",
    "       'Distribution (perplexity, calibration, correlation)':'Distribution',\n",
    "       'LLM post-processing (extracting answers, reformatting for automated scoring)': 'LLM post-processing',\n",
    "       'reward is computed based on the final product chosen by the agent, compared against known attributes, options, and price of the target product.':'Reward',\n",
    "       'The paper defines a reward score':'Reward',\n",
    "       'LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)':'LLM-as-a-Judge',\n",
    "       'Win rate':'Reward',\n",
    "       'Human ratings (text quality, preference, NOT manual scoring of other metrics)':'Human ratings',\n",
    "       'Matthews Correlation Coefficient (MCC, Matthews), which is a measure of similarity between binary distributions taking values from -1 to 1 and always yielding 0 for any two uncorrelated distributions, regardless of class imbalance.':'Correlation',\n",
    "       'P-Score (Prompting Score) and H-Score (Heuristical Score)':'LLM-as-a-Judge',\n",
    "       'The paper introduces 2 new metrics for language confusion. Line-level pass rate (LPR) and Word-level pass rate (WPR).':'Exact match',\n",
    "       'They also report two metrics to compare an LLM’s predictive confidence in slang usages relative to their literal counterparts.':'Correlation',\n",
    "       'Matthews Correlation Coefficient (MCC), and AUC (Area Under the ROC Curve)':'Correlation',\n",
    "       'instruction following rate':'Exact match',\n",
    "       'Human accuracy evaluation':'Human ratings',\n",
    "       'cosine similarity, log generation probability':'Distribution',\n",
    "       'pass rate':'Exact match',\n",
    "       'Normalized score relative to GPT-3.5-Turbo-16K performance':'',\n",
    "       'Spearman’s ρ, L/5 precision, RMSE':'Correlation',\n",
    "       'Score improvement of script':'Reward',\n",
    "       'Accuracy when the generated function is executed.':'Reward',\n",
    "       'Pear./Spear. Corr , Avg. Precision':'Correlation',\n",
    "       'exact match, MCC (Matthews Correlation Coefficient)':'Exact match',\n",
    "       'Also consider unit tests for some questions.':'Reward',\n",
    "       'runtime percentile of the LLM-generated code on the runtime distribution supported by corresponding solutions (the Leetcode solutions)':'Reward',\n",
    "       'Krippendorff’s α':'Correlation',\n",
    "       'Memorization score':'Exact match',\n",
    "       'Generated proof verified by an independent prover system.':'Reward',\n",
    "       'Reasoning Graph Accuracy and Reasoning Graph Similarity based on graph edit distance and textual similarity e.g. BLEURT.':'Soft match',\n",
    "       'Meteor':'Soft match',\n",
    "       'Explanation Completeness P/R/F1, Explanation Logical Consistency %':'Exact match',\n",
    "       'Unordered/Ordered BERT-F1 using DeBERTa-based BERTScore':'LLM-as-a-Judge',\n",
    "       'Binary F1 for Evidence IDs':'Exact match',\n",
    "       'Landmark Coverage Rate (LCR(%)) for route-planning':'Exact match',\n",
    "       'recall@1':'Exact match',\n",
    "       'Macro-F1 for the multi-class certainty prediction':'Exact match',\n",
    "       'Top L Precision, Top-k ACC, R^2, AUC, MCRMSE, Spearmann core':'Correlation',\n",
    "       'Execute the code and evaluate exact match of table vs ground truth table.':'Exact match',\n",
    "       'Functional correctness checks. Evaluated by (1) producing a dependency graph from the code (2) using an IaC policy engine to check whether the instruction specification are in the program.':'Reward',\n",
    "       'Code \"Speedup\" and \"Memory Reduction\" versus reference solutions.':'Reward',\n",
    "       'They use an ambiguity classifier as well from previous work':'LLM-as-a-Judge',\n",
    "       'partial credit':'Soft match',\n",
    "       'Mean IoU (Intersection over Union)':'Soft match',\n",
    "       'The code is executed and results are verified against ground truth results':'Reward',\n",
    "       'checkpoint coverage':'Exact match',\n",
    "       'Custom metrics: multi-modal gain, multi-modal leakage':'Exact match',\n",
    "       'For dialogue assessment, they introduce four metrics: average turn (AT), average dialogue refinement (ADR), average turn refinement (ATR), and refinement ratio (RR).':'Reward',\n",
    "       'Execution time and memory usage efficiency; unit test correctness':'Reward',\n",
    "       '3D IoU-based Average Precision':'Soft match', \n",
    "       'Unit test cases':'Reward',\n",
    "       'Soft Accuracy for counting task':'Soft match',\n",
    "       \"Correlation (Matthew's correlation, Pearson's r)\":'Correlation',\n",
    "       \"Execution-based evaluation. e.g. run the agent's code and see if it matches the ground-truth results. Plus different rubrics for each task.\":'Reward',\n",
    "       'Execution-based / functional correctness. Pass unit tests.':'Reward',\n",
    "       'Execution-Based Evaluation (unit tests)':'Reward',\n",
    "       'Diversity@k: that measures the cultural diversity among the retrieved images, helping to identify models’ bias towards specific countries or regions.':'Distribution',\n",
    "       'Execution-based evaluation (unit tests)':'Reward',\n",
    "       'Execution-based metrics.':'Reward', \n",
    "       'Elo ratings, Win rate':'Reward', \n",
    "       'VQAScore':'Exact match',\n",
    "       'Intersection over Union (IoU)':'Soft match',\n",
    "       'A non-defined \"jailbreak success rate\". likely LLM-as-a-Judge but unclear.':'Reward',\n",
    "       'Execution-based evaluation scripts':'Reward',\n",
    "       '- Contains: A less restrictive option is to consider a response correct if the prediction contains the true class name after preprocessing - ClipMatch: matching the prediction and label using cosine similarity in a vector embedding space':'Soft match',\n",
    "       'Mean Absolute Error':'Soft match',\n",
    "       'Mel-Cepstral Distortion is a measure of audio quality for TTS. A custom index is defined to balance all the evaluation metrics. ':'',\n",
    "       'MCD, MSD, PSNR, SSIM':'Soft match', \n",
    "       'BERTScore, GLEU':'Soft match',\n",
    "       'Execution-based (unit tests)':'Reward',\n",
    "       'Generation Metric and Generation Quality Drop are never explicitly defined in the paper. ':'LLM-as-a-Judge',\n",
    "       '(school) grade':'Soft match',\n",
    "       'Custom reward functions (e.g. must_include, eval_vqa, eval_fuzzy_image_match)':'Exact match',\n",
    "       'Execution-based scoring.':'Reward',\n",
    "       'Execution-based evaluation. Fairly comprehensive.':'Reward',\n",
    "       'Execution-based evaluation':'Reward',\n",
    "       'execution-based verification, file-based comparison, information-based validation':'Reward',\n",
    "       'edit-f1':'Soft match',\n",
    "       \"must_include', 'fuzzy_match', and programmatic checks which don't fit standard categories\":'Soft match',\n",
    "       'Resolution task: accuracy gap. Retrieval bias: Bias@K, Skew@K, NDKL.':'Distribution',\n",
    "       'They also report accuracy drop between cases where the image supports the correct answer choice and the cases where it supports one of the incorrect answer choices.':'',\n",
    "       'BiasScore: percentage of demographic groups in a dataset for which the LM continuations are more negative (e.g., toxic) than the average percentage of negative generations across demographic groups':'Distribution',\n",
    "       'Toxicity Score, Entailment Score':'LLM-as-a-Judge', \n",
    "       'Ko-H5 score':'Unknown',\n",
    "       \"The primary metrics (GP, SR, SPL, PWSR) measure the task performer's navigation success when guided by the helper. Helper effectiveness is inferred from these outcomes, with SPL and PWSR combining success and path efficiency.\":'Reward',\n",
    "       'For multiple correct multiple choice questions, if there were 4 correct answers and the taker selects 3, they score 0.75. If they selected 4 correct and an additional 5th incorrect, they score 0. This is to mimic actual IEE exam scoring. ':'Soft match',\n",
    "       'rescued value rate, averaged rescue step, averaged damaged rate':'Reward',\n",
    "       'Consistency Score, Relative Consistency Score':'Distribution',\n",
    "       'Many chemistry specific metrics, such as molecule validity, Fingerprint Tanimoto Similarity etc.':'Soft match',\n",
    "       'Bias Score: percentage of examples for which the likelihood of the more stereotypical sentence is higher than the likelihood of the less stereotypical sentence.':'Distribution',\n",
    "       'Win Rate, Population Block Ratio (PBR), Resource Utilisation Ratio (RUR), Average Population Utilization (APU), Technology Rate (TR)':'Reward',\n",
    "       'Post-processing with heuristics':'Soft match',\n",
    "       'Bias Percentage: percentage of sentence pairs for which the more stereotypical sentence has a higher probability than the less stereotypical sentence.':'Distribution',\n",
    "       'SoFa Score: variance in normalized log perplexity across grouped sentences (i.e., sentences with the same stereotype and different identities)':'Distribution',\n",
    "       'TrueScore, win rates, reward (game specific)':'Reward',\n",
    "       'Percentage of items (i.e., sentence pairs) for which an LM assigns a higher (psuedo-)likelihood to the stereotyping sentence over the less stereotyping sentence':'Distribution',\n",
    "       'BertScore, kwPrec':'Soft match',\n",
    "       'Semantic Similarity, BARTScore, Char-level edit distance':'Soft match',\n",
    "       'Execution Accuracy (EX) and Valid Efficiency Score (VES)':'Reward',\n",
    "       'Jaccard Index between model predictions and human-labeled associations':'Human ratings',\n",
    "       'Output probability change of attribute':'',\n",
    "       'Mean of the output logits':'Distribution',\n",
    "       'Factual Diversity Divergence (quantifies the divergence in the level of demographic diversity in model generations compared with the factual ground truth)':'Distribution',\n",
    "       'Define MR-Score, a weighted combination of matthews correlation, and two variants of accuracy':'Correlation',\n",
    "       'FactScore (Min et al., 2023), a method that evaluates the factuality of generated text by decomposing both the reference and hypothesis into atomic facts; MMRelevance':'Exact match',\n",
    "       'Mean entailment':'Distribution',\n",
    "       'A key metric is: Score = # harms committed by agent / # harms committed by random baseline (aka a normalised ratio relative to random baseline of 1000 random trajectories)':'Reward',\n",
    "       'Define 2 new metrics, RND and OCC which handle intricacies of the mutli-turn evaluation':'Reward',\n",
    "       'IOU':'Soft match',\n",
    "       'Reward in the environment':'Reward',}\n",
    "\n",
    "metric_raw_list = ['Exact Match (accuracy, F1, precision, recall)','n-gram (BLEU, ROUGE, chrF)','Human ratings (text quality, preference, NOT manual scoring of other metrics)','LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)','LLM post-processing (extracting answers, reformatting for automated scoring)','Distribution (perplexity, calibration, correlation)',\"Correlation (Matthew's correlation, Pearson's r)\"]\n",
    "metric_list = ['Exact match', 'Human ratings', 'LLM-as-a-Judge', 'LLM post-processing', 'Distribution', 'Correlation', 'Reward', 'Soft match', 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddf6b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "contested_map = {'Widely-agreed':'Widely-agreed',\n",
    "                 'Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)':'Widely-agreed',\n",
    "                 'Contested':'Contested',\n",
    "                 'Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)':'Contested',\n",
    "                 'No definition provided':'No definition',\n",
    "                 'A new problem is presented so agreement is unclear; but the definition is generally interpretable in a widely-agreeable way.':'Widely-agreed',\n",
    "                 'Very special phenomenon that is only introduced by the authors. It is neither widely-agreed upon nor contested.':'Widely-agreed',\n",
    "                 'Very specific phenomenon discovered in this paper (i.e., no prior definition exists)':'Widely-agreed',\n",
    "                 'Very specialized phenomenon with no standard definition ':'Widely-agreed',\n",
    "                 'Very specialized phenomenon without any standard definition':'Widely-agreed'\n",
    "                 }\n",
    "contested_raw_list = ['Widely-agreed','Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)','Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)','No definition provided'] \n",
    "contested_list = list(set(contested_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81b8f3",
   "metadata": {},
   "source": [
    "#### Face Validity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77434782",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_validity_map = {'Yes':'Yes',\n",
    "                     'Partially':'Partially',\n",
    "                     \"They follow the lead of popular knowledge and reasoning benchmarks, so it's hard to say here. \":'Partially',\n",
    "                     '':'',\n",
    "                    'It is evaluating temporal misaglignment through the specific lens of factual information on Wikipedia.':'Partially',\n",
    "                    'Too vaguely defined phenomenon':'No',\n",
    "                    'No':'No',\n",
    "                    'but fairly poor task definition.':'Partially',\n",
    "                    'Somewhat. Certain tasks in the benchmark align well with how real-world analysts evaluate cyber threat intelligence, suggesting some face validity. However, other tasks focus more on knowledge retrieval, which may not reflect the full nature of cyber threat intelligence, where knowledge retrieval, understanding, reasoning, and application are all important. These aspects are tested separately, so the benchmark doesn’t provide a full picture of end-to-end evaluation.':'Partially',\n",
    "                    \"There is no specified phenomenon besides the models' ability to answer open-ended questions.\":'No',\n",
    "                    'Highly simplified version of the phenomena':'Partially',\n",
    "                    'Tricky to say since the paper does not provide a principled definition of the target phenomenon. It just talks of general \"capabilities,\" as well as the ten skills mentioned above. As for the ten skills, face validity varies -- for some (e.g., mathematics) it seems higher than for others (e.g., ethics).':'Partially',\n",
    "                    'yes, but only a small subset':'Partially',\n",
    "                    'Too broad to tell.':'No',\n",
    "                    'It really depends on the phenomena and task':'Partially',\n",
    "                    'Only partly':'Partially',\n",
    "                    'Not sure about this. Compared to other similar benchmarks, yes. In general, probably not. ':'Partially',\n",
    "                    \"rima facie reason to believe that perplexity on factual completions is a valid metric for benchmarking a language model's ability to adapt to changing knowledge over time (the target phenomenon of temporal misalignment). But the task format is very synthetic.\":'Yes',\n",
    "                    'Mixed. Keywords/n-grams are a limited way of assessing performance.':'Partially',\n",
    "                    'The Vera and Grammar models may be well-established and commonly used in compositionality or linguistic tasks, but it is not apparent in the paper. No justification is provided for the use of Vera and Grammar.':'Partially',\n",
    "                    'Mixed. For the execution-based tasks, yes, but for the code summarisation tasks they use BLEU/CodeBLEU':'Partially',\n",
    "                    'It works since most LMs do not output toxic content all the time, but this does not make it a metric that is suitable for bias measurement in principle.':'Partially',\n",
    "                    'High validity for detecting the presence of stereotypes, but low validity for measuring the absence of stereotypes. The authors acknowledge this distinction.':'Partially',\n",
    "                    'High values of the metric indicate presence of bias, but low values do not mean that a model is unbiased. The authors do not acknowledge that.':'Partially',\n",
    "                    \"Maybe - it relies on the GPT-4 evaluator being able to assess the correctness of the answer relative to human key points. If the GPT-4-as-a-judge lacked nuanced scientific understanding, it may fail to evaluate another LLM's response against the key points (e.g., requires capabilities for classic entailment,contradiction task). Howver, they do show high correlation empirically between GPT-4-as-a-judge and human evaluators. \":'Partially',\n",
    "                    'Maybe - they only keep very non-ambiguous examples but this only covers a subset of human values which can have disagreements and be ambigous in some settings':'Partially',\n",
    "                    'Depends on the game':'Partially',\n",
    "                    'Computing the mean of the logits does not seem mathematically sound, but the general approach of examining the output probabilities is valid.':'No',\n",
    "                    'The metric is new and not very well motivated':'No',\n",
    "                    'Maybe - good on ecological validity but a very small and specific set of 200 prompts':'Partially',\n",
    "                    \"Probablistic reasoning is a wide ranging and difficult to estimate phenomenon, and whilst these tasks do measure a subset of this phenomenon they don't come close to measuring everything.\":'Partially',\n",
    "                    \"Maybe: You could imagine that GPT-4 is of lower capability than the model being evaluated which would mean it couldn't necessarily judge what a good or correct answer is.\":'Partially',\n",
    "                    'The task is too unclear to know':'No',\n",
    "                    'It seems unlikely that so broad a concept could be measured well, but this is a good effort to cast a wide net.':'Partially',\n",
    "                    'Very limited scope':'Partially',\n",
    "                    'It measures the ability to solve STEM multiple choice questions, but not as the authors claim \"expert level intelligence across a diverse range of tasks\". ':'No',\n",
    "                    'Whilst relevant for this task,  it is debatable whether Theory of Mind can be boiled down to yes/no classifcation tasks. Ie therapists getting an idea for how their patient feels.':'Partially',   }\n",
    "face_validity_raw_list = ['Yes','No']\n",
    "face_validity_list = list(set(face_validity_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81558203",
   "metadata": {},
   "source": [
    "#### Realism Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69a7d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "realism_map = {'The benchmark is itself realistic':'Realistic',\n",
    " 'It is an entirely constructed scenario (no available realistic setting)':'Not possible',\n",
    " 'No':'No comparison made', 'Yes':'Comparison made',\n",
    " 'They do a partial study with actual human feedback on the benchmark tasks.':'Comparison made',\n",
    " 'Given that the benchmark is trying to measure general capabilities, it is unclear how a more realistic setting would look like.':'Not possible',\n",
    " '':'No',\n",
    " 'No - but you could say the commonsense morality task is scraped from social media so has some realism':'Realistic'}\n",
    "realism_raw_list = ['Yes','No','The benchmark is itself realistic']\n",
    "realism_list = list(set(realism_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaff4f4",
   "metadata": {},
   "source": [
    "#### Author Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9d04b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_map = {'Yes':'Yes', 'No':'No',\n",
    " 'for short-answer questions, there is a human evaluation, which to some extent can represent the validity of the questions':'Yes',\n",
    " '':'', 'Somewhat':'Yes', 'Somehwat':'Yes',\n",
    " 'The papers justify the improvement of the task design displayed in their benchmark, but not the choice of the task itself.':'Yes',\n",
    " 'Partially; addressed their own limitations.':'Yes', 'indirectly address it':'Yes',\n",
    " 'They indirectly address it':'Yes', 'Indirectly address it':'Yes', 'implicitly':'Yes',\n",
    " 'They compare scores on benchmark to human judgment':'Yes',\n",
    " 'Partial pre-analysis.':'Yes', 'A bit (but not a strong Yes)':'Yes',\n",
    " 'They acknowledge the lateral thinking is hard to measure: \"In this paper, we seek to explore and elicit the lateral thinking ability of LLMs. However, accurately evaluating this capability poses significant challenges due to the complexity of measuring creative thinking [29 , 19 ] and the difficulty of obtaining relevant data. The generation of novel ideas is inherently non-trivial, even for humans [13 , 14 ]. Considering these challenges, we propose the exploration of lateral thinking in LLMs by situation puzzles as a primary research tool\"':'Yes'}\n",
    "author_raw_list = ['Yes','No']\n",
    "author_list = list(set(author_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd1324",
   "metadata": {},
   "source": [
    "#### Ecology Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01c5092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecology_map = {'Complete real task (e.g. providing medical advice to real people interactively)':'Complete',\n",
    " \"Constructed task (e.g. predicting medical diagnoses from clinicians' notes)\":'Constructed',\n",
    " 'Representative task (e.g. answering medical licensing exam questions)':'Representative',\n",
    " 'Partial real task (e.g. answering medical questions collected from real people)':'Partial',\n",
    " '':'', 'Low ecology':'Constructed',\n",
    " 'Low ecology, humans wouldn’t usually ask LLMs to do these tasks.':'Constructed',\n",
    " 'Artificial task':'Constructed',\n",
    " 'Proxy task - tries to get at real-world scenarios of agents via fictional adventures':'Representative'}\n",
    "\n",
    "ecology_raw_list = ['Complete real task (e.g. providing medical advice to real people interactively)','Partial real task (e.g. answering medical questions collected from real people)','Representative task (e.g. answering medical licensing exam questions)',\"Constructed task (e.g. predicting medical diagnoses from clinicians' notes)\"]\n",
    "ecology_list = list(set(ecology_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b872f9",
   "metadata": {},
   "source": [
    "#### Metrics Agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f964e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_agg_map = {\n",
    " 'simple mean': ['Mean'],\n",
    " 'simple mean/sum': ['Mean'],\n",
    " 'Simple mean': ['Mean'],\n",
    " 'Mean': ['Mean'],\n",
    " 'Mean, ': ['Mean'],\n",
    " 'Simple mean and standard deviation': ['Mean','Std'],\n",
    " 'mean and variance': ['Mean','Std'],\n",
    " 'simple mean and std': ['Mean','Std'],\n",
    " 'simple mean, std': ['Mean','Std'],\n",
    " 'mean and variance, t-tests': ['Mean','Std','Tests'],\n",
    " 'mean': ['Mean'],\n",
    " 'Simple Mean': ['Mean'],\n",
    " 'Averages and win/tie percentages in human eval; no advanced statistics reported\\n': ['Mean'],\n",
    " 'Means, standard deviations, and Spearman/Pearson correlations with expert rankings.\\n': ['Mean','Std','Other'],\n",
    " 'simple mean, statistical tests': ['Mean','Tests'],\n",
    " 'Flesch Kincaid, Rouge-L, Kendalls, Spearmans\\n' :['Other'],\n",
    " 'simple mean, standard error of the mean': ['Mean'],\n",
    " 'Pearson correlation, RMSE, differences to student baselines\\n': ['Other'],\n",
    " 'Simple mean, average':['Mean'],\n",
    " 'Average':['Mean'],\n",
    " 'mean with variance':['Mean','Std'],\n",
    "'The authors carry out some error analysis: \"We argue that we are measuring a lower bound for what LMs know. To further understand the shortcomings of the current method, we conduct an error analysis of the errors in precision on all datasets. We choose BERTLARGE for the study. We sample 100 documents from the Wikidata-OIE dataset, and manually check the reasons for the errors\"':['Other'],\n",
    "\"Simple means for performance metrics; agreement percentages and Cohen's Kappa for annotation reliability.\":['Mean','Other'],\n",
    "'simple mean, Anova for p-values, Tukey-HSD':['Mean','Tests'],\n",
    "'The authors report average task score and success rate across trials. They also include standard deviation/error bars in some result plots (e.g. Figure 4), mainly to show the variation across multiple runs.':['Mean','Std'],\n",
    "'mean of weighted-F1 scores':['Mean'],\n",
    "\"Mean, and they they show a delta (for change in aggregate sources across all tasks). It is unclear if this is a range or a standard deviation. I think it's a range.\":['Mean'],\n",
    "'The authors use a weighted mean in calculating an approximate human performance threshold but not for model performance. They take a weighted average of the annual medal thresholds for ‘Advanced’ problems. ':['Mean'],\n",
    "'simple mean and STD':['Mean','Std'],\n",
    "'Simple means and macro-averaging (mean across tasks, which is identical here because each task has same # of instances)':['Mean'],\n",
    "'macro-accuracy':['Mean'], \n",
    "'Simple mean (no variance or standard reported)':['Mean'],\n",
    "'mean and standard deviation':['Mean','Std'],\n",
    "'Simple mean/sum; % improvement between contexts':['Mean','Other'],\n",
    "'simple mean and standard deviation ':['Mean','Std'],\n",
    "'accuracy, F1, standard deviation':['Mean','Std','Other'],\n",
    "'retrieval rate R@K metric':['Other'],\n",
    "'For each tuple, the F1 is computed, then across a clique the minimum is computed and aggregated across the dataset as mean.':['Mean'],\n",
    "'Simple mean: F1 scores and accuracy. MSE. nDCG and MRR. Perplexity':['Mean','Other'],\n",
    "'Mean and standard deviation':['Mean','Std'], \n",
    "'simple mean (as percentage)':['Mean'],\n",
    "'Simple average of perplexity for different snapshots of the wikipedia data.':['Mean'],\n",
    "'Aggregated scores (no additional stats)':['Mean'], \n",
    "'mean over 8 runs. ':['Mean'],\n",
    "'Simple summary stats. ':['Mean'], \n",
    "'Success rate':['Mean'],\n",
    "'simple mean, for tasks with more than one metric (like Pearson and Spearman correlation for sentiment regression), scores are averaged to get a single task score':['Mean'],\n",
    "'Min, max, average':['Mean','Other'],\n",
    "'Weighted Precision, Recall, F1 scores, and macro-F1 scores for binary and multi-class classification. Hamming loss is also reported for multi-class classification. ':['Mean','Other'],\n",
    "'Visual semantic tasks were measured with representational similarity analysis (RSA), while the other tasks were measured with a novel metric: softmax-optimized Kullback-Leibler divergence':['Other'],\n",
    "'simple mean,  inter-annotator agreement with WAWA and the Dawid-Skene method for vote aggregation.  delta-scores to measure performance differences between models under different dataset filtering conditions':['Mean','Other'],\n",
    "'Mean, standard deviation.':['Mean','Std'],\n",
    "'Simple mean/average scores (MSQA Correctness Score C, MSNN Accuracy) are used to aggregate results. Different models or settings are compared directly based on these mean scores presented in tables.':['Mean'],\n",
    "'Simple mean/average of Hit@k, Recall@k, and MRR over the test sets.':['Mean'],\n",
    "'Accuracy, MRR, Precision, Recall, F1, BLEU, ROUGE-L, Keyword Recall, Mean Likert scores.':['Mean','Other'],\n",
    "'BLEU-4, ROUGE-L, MRR, Precision@3. Mean scores are reported, sometimes with standard deviation (e.g., for text lengths in Table 2 ).':['Mean','Std'],\n",
    "'Precision, Recall, F1 score, Exact Match (EM)':['Mean','Other'],\n",
    "'Macro F1 score, Exact Match (EM)':['Mean'], \n",
    "' simple mean/sum':['Mean'],\n",
    "'simple mean, mean and std, averaging across multiple metrics':['Mean','Std'],\n",
    "'simple mean/sum, t-tests':['Mean','Tests'], \n",
    "'simple mean/sum, GLMs':['Mean','Tests'],\n",
    "'Accuracy is reported for classification in both open and closed-world settings. Fine-tuned accuracy and linear probing accuracy are reported in a closed-world setting, while 1NN-genus probing accuracy is reported in an open-world setting. AMI is reported for zero-shot transfer learning, and in multimodal retrieval learning, micro and macro top-1 accuracy is reported. ':['Mean','Other'],\n",
    "'The metrics are averaged and normalized against human performance':['Mean'],\n",
    "'BLEU-4, Rouge-1, BERTScore, Keyword Insertion Rates (KWD), Sentence Length Regulation Compliance Rates (REG), Pearson and Spearman Correlation for Human Evaluation':['Other'],\n",
    "' Macro F1 score, per-class F1 score':['Mean'], \n",
    "'Unknown':['Unknown'],\n",
    "'Answer Accuracy (Exact Match %), Reasoning Graph Accuracy (%), Reasoning Graph Similarity (%).':['Mean'],\n",
    "'Factuality is calculated with Named Entity Recognition (NER) empowered accuracy, described in the paper. Style is measured with BLEU. Insightfulness is measured by human assessments based on impact (breadth of claim), and significance (magnitude of changes) on a 5 point Likert scale, and the average of the human review is reported. ':['Mean'],\n",
    "'F1, EM, R1, R2, MET':['Mean'],\n",
    "'Accuracy (%). Kappa score used for error analysis inter-rater reliability.':['Mean','Other'],\n",
    "'Simple mean and variance on accuracy are used to assess the overall and best pick comparisons for cartoons, and expectation adjusted distinct N-grams (EAD) and Sentence-BERT embedding cosine similarity (SBERT) are used to assess caption diversity. ':['Mean','Std','Other'],\n",
    "'Precision, Recall, F1 score, Completeness (P/R/F1), Logical Consistency (%).':['Mean'],\n",
    "'BERT-F1, ROUGE-L F1, Human Judgement Proportions (%), Pearson Correlation (r) for metric validation.':['Mean'],\n",
    "'Accuracy, F1 score, BERTScore F1, Average score (1-5 scale).':['Mean'],\n",
    "'simple mean/sum, percentage point improvements':['Mean'],\n",
    "'F1 score, AllCorrect (Exact Match), Accuracy, Macro F1':['Mean'],\n",
    "'Precision, Recall, F1, ROUGE-1, ROUGE-2, ROUGE-L, Human evaluation win/tie/lose rates (%).':['Mean'],\n",
    "'Accuracy (%), F1 Score (%)':['Mean'],\n",
    "'Accuracy (%), Standard Deviation, Error Rate (%)':['Mean','Std'],\n",
    "'Reports average scores for commonsense Vera score gap and Grammar score gap. The paper also reports the pairwise better ratio between SugarCrepe and ARO+CREPE. ':['Mean'],\n",
    "'Mean and std':['Mean','Std'], 'Mean, variance':['Mean','Std'], 'simple average ':['Mean'],\n",
    "'mean and standard dev':['Mean','Std'],\n",
    "'No statistical methods used. just simple mean and differences in means.':['Mean'],\n",
    "'simple mean and for rating-based evaluations they measure \"hedging rate\"':['Mean'],\n",
    "'simple mean to aggregate performance over scenarios and roles':['Mean'],\n",
    "'simple mean/sum, plus comparisons to scores from the base LLMs comprising the multi-modal models (called \"multi-modal gain\" and \"multi-modal leakage\" statistics)':['Mean'],\n",
    "'Mean, worst and best out of 11':['Mean'],\n",
    "'simple mean. no inferential statistics (even though the LLM-as-judge have fairly low accuracy).':['Mean'],\n",
    "'mean with \"error bars from 3 runs at temperature 0.2\" (unsure if this is a standard error or just the range in scores) (17)':['Mean','Std'],\n",
    "'Exact Match (EM), F1 Score (%)':['Mean'], 'Mean and standard deviation\\n':['Mean'],\n",
    "'Accuracy (%)':['Mean'], 'Accuracy (%), BLEU-4, ROUGE-L':['Mean'],\n",
    "'Simple mean to aggregate automatic scores, Pearson and Spearman correlation between human and automatic ratings\\u200b, and Krippendorff’s Alpha inter-rater agreement for human ratings.':['Mean','Other'],\n",
    "'mean/sum, where problem correct means all subproblems must be correct':['Mean'],\n",
    "'simple means to report F1 scores and ROUGE metrics':['Mean'], 'Mean,':['Mean'],\n",
    "'simple mean + std':['Mean','Std'], 'Mean, standard errors':['Mean','Std'],\n",
    "'Mean, standard deviation':['Mean','Std'],\n",
    "'Mean, Spearman/Pearson Correlations\\n(For completeness, they also report the standard Pearson but also mention that Pearson is not the ideal metric due to the curve-of-best-fit not appearing linear.)':['Mean','Other'],\n",
    "'mean,':['Mean'], 'Means, comparisons with percentage point gaps.':['Mean'],\n",
    "'Wilcoxon tests, variance analysis':['Other'],\n",
    "'Means and percentage differences':['Mean'],\n",
    "'simple mean/sum, correlation between overall rankings and general capabilities based on MMBench':['Mean','Other'],\n",
    "'Mean, percentage':['Mean'],\n",
    "'simple mean, std, relative performance changes':['Mean','Std'],\n",
    "'Simple Means, the percentage of questions answered correctly.':['Mean'],\n",
    "'simple mean, weighted and unweighted clustering scores, frequency counts, Fleiss Kappa':['Mean','Other'],\n",
    "'Experiments are repeated 5 times but resulting information onf uncertainty are not reported.':['Unknown'],\n",
    "'Mean error, scatter plots, attention heatmaps':['Mean'],\n",
    "'Micro-averaged entity-level F1 score reported as means across 3 runs with standard deviations. Simple means used for comparing approaches across different noise types.':['Mean','Std'],\n",
    "'Reporting accuracy, precision, recall, and F1 scores, both as macro averages across all categories and separately for offensive and non-offensive classes.':['Mean'],\n",
    "'Image retrieval error, effect score bias':['Mean'],\n",
    "'F1 scores (micro-averaged and macro-averaged) as the primary statistical method.':['Mean'],\n",
    "'Simple mean/accuracy':['Mean'],\n",
    "'Just simple mean, with occasional reporting of variance or distribution plots.':['Mean','Std'],\n",
    "'Simple proportion, human/model comparisons':['Mean'],\n",
    "\"Simple mean scores for each metric. For correlation analysis between automatic metrics and human judgments: Spearman's rank correlation coefficient.\":['Mean','Other'],\n",
    "'simple mean and variance':['Mean','Std'], \n",
    "'Percentage, comparison':['Mean'],\n",
    "'Simple means for the main metrics':['Mean'],\n",
    "'Means, standard deviations, weighted Fleiss‑k':['Mean','Std'],\n",
    "'simple mean, 95% confidence interval, percentage point of performance gains over baselines':['Mean','Tests'],\n",
    "'Simple mean/sum, custom normalized aggregate metric':['Mean','Other'],\n",
    "'Mean with 95% Confidence Interval ':['Mean','Tests'], 'Simple mean accuracy':['Mean'],\n",
    "'Simple mean + standard deviations':['Mean','Std'],\n",
    "'Primary metrics used are F1 scores, accuracy, recall, and precision. Partially, micro and macro averages are reported.':['Mean'],\n",
    "'Mean and Standard deviation':['Mean','Std'], 'simple mean with percentage point':['Mean'],\n",
    "'Simple mean to aggregate across different settings. For each detector, they report AUROC and F1 Score values for each specific condition and the average across those conditions. ':['Mean'],\n",
    "'simple mean ':['Mean'],\n",
    "'Simple mean, char-based F0.5 scores for overall performance, along with precision and recall.':['Mean'],\n",
    "'mean + standard deviatin, significance test. proportion':['Mean','Std'],\n",
    "'Stratified human agreement evaluation on LLM-graded items; comparisons to BLEU/F1 for scoring validity.\\n':['Mean'],\n",
    "'Accuracy, Fleiss’ k for human agreement\\n':['Mean','Other'],\n",
    "'Mean scores across different data splits and standard deviation for low-resource settings':['Mean','Std'],\n",
    "\"Automatic Evaluation Metrics (SARI; BLEU); Measure inter-annotator agreement using Krippendorff's alpha\":['Mean'],\n",
    "'Micro-Average, Worst-Average':['Mean'],\n",
    "\"Win and draw rates from pairwise comparisons. For automated metrics and human judgment evaluation: Kendall's Tau \":['Mean','Other'],\n",
    "'Mean, error bars on figures in appendix.':['Mean','Std'],\n",
    "'Dataset score is calculated as a macro-average of the per-language score.':['Mean'],\n",
    "'Simple means, McNemar test, Minimum detectable effect (MDE)':['Mean','Other'],\n",
    "'Scores are normalised relative to human performance.':['Other'],\n",
    "'simple mean, weighted mean':['Mean'],\n",
    "\"mean, standard deviation, entropy calculations, z-scores, p-values, bootstrapping, Le Cam's lemma, multiplicative damping factors\":['Mean','Std','Tests','Other'],\n",
    "'True Positive Rate, True Negative Rate, Generation Metric and Generation Quality Drop':['Mean'],\n",
    "'Binomial mixed effects regression models':['Tests'],\n",
    "'Mean, ANOVA, post-hoc pairwise tests':['Mean','Tests'],\n",
    "'Means, standard deviations, comparisons':['Mean','Std'],\n",
    "'Descriptive accuracy only\\n':['Mean'],\n",
    "'Binomial standard error reported\\n':['Tests'],\n",
    "'Inter-rater agreement (Krippendorff’s alpha), statistical comparisons\\n':['Other'],\n",
    "'Basic comparisons across models; no significance tests\\n':['Mean'],\n",
    "'Simple mean success rates across tasks and subsets. There is no formal hypothesis testing or statistical significance tests.':['Mean'],\n",
    "'Mean, error bars on some plots':['Mean','Std'],\n",
    "'Simple mean and standard error of the mean (for plots like accuracy vs. steps)':['Mean','Std'],\n",
    "'simple mean across samples, then the relative decline in accuracy due to injection is computed.':['Mean'],\n",
    "'Simple mean/sum (Success Rate %)':['Mean'],\n",
    "'Mean and standard error, with bootstrap confidence intervals':['Mean','Std'],\n",
    "'Simple mean (for direct assessment scores), Elo rating via Maximum Likelihood Estimation (MLE) (for pairwise comparisons), Fleiss’ Kappa and Percentage Agreement (for inter-annotator agreement and human-LLM agreement), and Kendall’s Tau (for human-LLM leaderboard rank correlation).':['Mean','Other'],\n",
    "'simple mean\\xa0and\\xa0standard error':['Mean','Std'],\n",
    "\"Simple mean/sum, correlation (Pearson's r)\":['Mean','Other'],\n",
    "'simple mean, correlation':['Mean','Other'], 'Simple means with error bars':['Mean','Std'],\n",
    "'simple mean, correlation, calibration':['Mean','Other'], 'Unkown ':['Unknown'],\n",
    "'simple mean, optionally the negative scoring as above':['Mean'],\n",
    "'Simple mean, standard deviation':['Mean','Std'],\n",
    "'Simple sum/mean - raw percentages':['Mean'],\n",
    "'Mean, standard deviation, Spearman correlation for difficulty alignment (IRT vs human/GPT4)':['Mean','Std'],\n",
    "'Mean and pairwise agreement figures;':['Mean','Other'],\n",
    "'Simple mean scores, simple sum scores, inter‑annotator agreement percentages.':['Mean'],\n",
    "'Mean accuracy is used to aggregate model performance across tasks and conditions':['Mean'],\n",
    "'They report simple percentage means, and compute Spearman’s\\xa0ρ and Pearson’s\\xa0r between rule‑ and LLM‑based scores to show consistency; and give a 98\\xa0% human‑vs‑ChatGPT agreement figure (percent agreement, not kappa).':['Mean','Other'],\n",
    "'Mean, recall, F1 with standard error comparisons':['Mean','Std'],\n",
    "'Mean and standard error; calibration (ECE) and AUROC for confidence analyses.':['Mean','Std'],\n",
    "'Mean; McNemar x^2 test for mining':['Mean','Tests'],\n",
    "'simple mean, significance clusters':['Mean'],\n",
    "'Mean and breakdown by category':['Mean'],\n",
    "'simple mean + standard errors, standard deviations':['Mean','Std'],\n",
    "'The models are primarily ordered by rank.':['Mean'],\n",
    "'In the main table, they only reported the weighted mean. In the appendix, they say \"Where plotted, the error bars show what the standard error would be if estimated using bootstrapping. To estimate the standard error of our reported model scores, we analytically estimate the standard error.\"':['Mean','Std'],\n",
    "'Weighted mean based on test samples.':['Mean'],\n",
    "'Simple mean across questions for each category. They also compute correlation metrics between human and GPT-4 evaluations: Pearson, Spearman, and Kendall-τ coefficients to validate GPT-4 as a reliable evaluator.':['Mean','Other'],\n",
    "'Simple mean over examples (accuracy)':['Mean'],\n",
    "'The paper reports simple means for calculating the overall accuracy as a macro average across all five dimensions':['Mean'],\n",
    "'mostly mean - variance only reported for best model. ':['Mean','Std'],\n",
    "'simple mean. ':['Mean'],\n",
    "'Mean, and human–automatic correlation (Cohen’s Kappa coefficient) for validation.':['Mean','Other'],\n",
    "'simple mean, standard deviation':['Mean','Std'],\n",
    "'Simple mean, and for annotation agreement Cohen’s Kappa coefficient was used.':['Mean','Other'],\n",
    "'Simple mean is used for aggregation.':['Mean'],\n",
    "'Mean; authors additionally report t‑tests for MGCT effect differences and classification accuracy for the detector.':['Mean','Tests'],\n",
    "'‑ Per‑metric means & confidence via single runs\\n\\n‑ Spearman correlation (response/evidence vs position)\\n\\n‑ Cohen’s\\xa0κ for human IAA (0.74‑0.77)':['Mean','Other'],\n",
    "'Mean, sample‑level Pearson\\xa0r, system‑level Pearson\\xa0r, pairwise win‑rate % (for agreement studies).':['Mean','Other'],\n",
    "'Mean, F1, balanced accuracy; 95% CIs via bootstrap.':['Mean','Std'],\n",
    "'Mean, Precision/Recall/F1, Balanced Accuracy; inter‑annotator agreement (Cohen’s\\xa0κ / raw %).':['Mean','Other'],\n",
    "'Simple mean, Spearman correlation (with p‑value <\\xa00.05)':['Mean','Other'],\n",
    "'The paper uses simple means for the primary evaluation metric. For each task, they report the percentage of correct predictions. For the overall score, they take a simple average across the five ethical categories. They also test whether models can distinguish ambiguous scenarios from clear-cut scenarios by using predictive uncertainty estimates (Area Under the Receiver Operating Characteristic curve).':['Mean'],\n",
    "'mean, std':['Mean','Std'],\n",
    "'Simple mean ± 95% confidence interval':['Mean','Std'],\n",
    "'simple mean/sum, mean and variance for accuracy and BLEU':['Mean','Std'],\n",
    "'Mean and variance, standard deviations':['Mean','Std'],\n",
    "'Simple mean/sum': ['Mean'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec6c5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_map = {}\n",
    "stats_raw_list = []\n",
    "stats_list = list(set(stats_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23365023",
   "metadata": {},
   "source": [
    "### Applying Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4125bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_maps = {'task_source': {'rename_map':source_map,'raw_names': sources_raw_list,'final_names': sources_list},\n",
    "                'dataset_sampling_method':{'rename_map': sampling_map, 'raw_names': sampling_raw_list, 'final_names': sampling_list},\n",
    "                'response_format':{'rename_map':response_map, 'raw_names':response_raw_list, 'final_names':response_list},\n",
    "                'metric_definition': {'rename_map':metrics_map, 'raw_names':metric_raw_list, 'final_names':metric_list},\n",
    "                'phenomenon_contested':{'rename_map':contested_map,'raw_names':contested_raw_list,'final_names':contested_list},\n",
    "                'task_face_validity':{'rename_map':face_validity_map,'raw_names':face_validity_raw_list,'final_names':face_validity_list},\n",
    "                'metric_face_validity':{'rename_map':face_validity_map,'raw_names':face_validity_raw_list,'final_names':face_validity_list},\n",
    "                'results_realism':{'rename_map':realism_map,'raw_names':realism_raw_list,'final_names':realism_list},\n",
    "                'results_author_validity':{'rename_map':author_map,'raw_names':author_raw_list,'final_names':author_list},\n",
    "                'task_ecology':{'rename_map':ecology_map,'raw_names':ecology_raw_list,'final_names':ecology_list},\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcef3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_str(list_str,split_list):\n",
    "    if isinstance(list_str,str):\n",
    "        for split in split_list:\n",
    "            if split in list_str:\n",
    "                list_str = list_str.replace(split+',',split+'///,')\n",
    "        list_str = list_str.split('///, ')\n",
    "        return list_str\n",
    "    else:\n",
    "        if pd.isna(list_str):\n",
    "            return ['']\n",
    "        return [list_str]\n",
    "    \n",
    "def match_occurences(items:list, count:list, header:str):\n",
    "    return {header+': '+key:key in items for key in count}\n",
    "\n",
    "\n",
    "def expand_columns(df, col_name,rename_map,raw_names,final_names):\n",
    "    try:\n",
    "        df[col_name + '_clean'] = df[col_name].apply(lambda x: [rename_map[y] for y in split_list_str(x,raw_names)])\n",
    "        return pd.concat([df,df[col_name].apply(lambda x: [rename_map[y] for y in split_list_str(x,raw_names)]).apply(lambda x: match_occurences(x,final_names,col_name)).apply(pd.Series)],axis=1)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} in column {col_name}\")\n",
    "        print(df[col_name].apply(lambda x: [y for y in split_list_str(x,raw_names)]).explode().unique())\n",
    "        raise e\n",
    "        return included_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8497ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in split_maps.keys():\n",
    "    included_df = expand_columns(included_df, col, **split_maps[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64c8c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df['metric_statistics_clean'] = included_df['metric_statistics'].apply(lambda x: metrics_agg_map[x] if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "154da4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#included_df.to_csv('../data/clean_codebook.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ba204d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_ecology: Representative\n",
      "task_ecology: Constructed\n",
      "task_ecology: Complete\n",
      "task_ecology: Partial\n",
      "task_ecology: \n",
      "results_author_validity: \n",
      "results_author_validity: No\n",
      "results_author_validity: Yes\n",
      "results_realism: Comparison made\n",
      "results_realism: No\n",
      "results_realism: Not possible\n",
      "results_realism: Realistic\n",
      "results_realism: No comparison made\n",
      "metric_face_validity: Partially\n",
      "metric_face_validity: No\n",
      "metric_face_validity: \n",
      "metric_face_validity: Yes\n",
      "task_face_validity: Partially\n",
      "task_face_validity: No\n",
      "task_face_validity: \n",
      "task_face_validity: Yes\n",
      "phenomenon_contested: No definition\n",
      "phenomenon_contested: Widely-agreed\n",
      "phenomenon_contested: Contested\n",
      "metric_definition: Unknown\n",
      "metric_definition: Soft match\n",
      "metric_definition: Reward\n",
      "metric_definition: Correlation\n",
      "metric_definition: Distribution\n",
      "metric_definition: LLM post-processing\n",
      "metric_definition: LLM-as-a-Judge\n",
      "metric_definition: Human ratings\n",
      "metric_definition: Exact match\n",
      "response_format: Unknown\n",
      "response_format: Logits\n",
      "response_format: Free response\n",
      "response_format: Short free response\n",
      "response_format: Multiple choice\n",
      "response_format: Interaction\n",
      "response_format: Structured\n",
      "dataset_sampling_method: Unknown\n",
      "dataset_sampling_method: Random\n",
      "dataset_sampling_method: Convenience\n",
      "dataset_sampling_method: Criterion\n",
      "dataset_sampling_method: Targeted\n",
      "task_dataset_metadata\n",
      "task_source: Real task\n",
      "task_source: Human exams\n",
      "task_source: LLM-generated\n",
      "task_source: Another benchmark\n",
      "task_source: Expert-crafted\n",
      "task_source: Procedurally-generated\n",
      "task_source: Unknown\n",
      "task_source: Crowd-sourced\n",
      "task_source: Author-crafted\n",
      "definition_scope\n",
      "definition_integrity\n",
      "phenomenon_contested\n",
      "phenomenon_defined\n",
      "phenomenon_short\n"
     ]
    }
   ],
   "source": [
    "categorized_columns = ['phenomenon_short','phenomenon_defined','phenomenon_contested','definition_integrity','definition_scope',\n",
    "'task_source: Author-crafted','task_source: Crowd-sourced','task_source: Unknown','task_source: Procedurally-generated','task_source: Expert-crafted','task_source: Another benchmark','task_source: LLM-generated','task_source: Human exams','task_source: Real task',\n",
    "'task_dataset_metadata','dataset_sampling_method: Targeted', 'dataset_sampling_method: Criterion',\n",
    "       'dataset_sampling_method: Convenience', 'dataset_sampling_method: Random','dataset_sampling_method: Unknown','response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown', 'metric_definition: Exact match',\n",
    "       'metric_definition: Human ratings', 'metric_definition: LLM-as-a-Judge',\n",
    "       'metric_definition: LLM post-processing',\n",
    "       'metric_definition: Distribution', 'metric_definition: Correlation',\n",
    "       'metric_definition: Reward', 'metric_definition: Soft match',\n",
    "       'metric_definition: Unknown',\n",
    "       'phenomenon_contested: Contested',\n",
    "'phenomenon_contested: Widely-agreed',\n",
    "'phenomenon_contested: No definition',\n",
    "'task_face_validity: Yes',\n",
    "'task_face_validity: ', \n",
    "'task_face_validity: No',\n",
    "'task_face_validity: Partially',\n",
    "'metric_face_validity: Yes',\n",
    "'metric_face_validity: ',\n",
    "'metric_face_validity: No',\n",
    "'metric_face_validity: Partially',\n",
    "'results_realism: No comparison made',\n",
    "'results_realism: Realistic',\n",
    "'results_realism: Not possible',\n",
    "'results_realism: No',\n",
    "'results_realism: Comparison made',\n",
    "'results_author_validity: Yes',\n",
    "'results_author_validity: No',\n",
    "'results_author_validity: ',\n",
    "'task_ecology: ',\n",
    "'task_ecology: Partial',\n",
    "'task_ecology: Complete',\n",
    "'task_ecology: Constructed',\n",
    "'task_ecology: Representative',\n",
    "]\n",
    "\n",
    "for col in categorized_columns.__reversed__():\n",
    "    temp = included_df[col]\n",
    "    if col in col_maps:\n",
    "        temp = temp.apply(lambda x: col_maps[col][x] if col_maps[col].get(x) else x)\n",
    "    temp = temp.value_counts()\n",
    "    temp = temp[temp > 0]\n",
    "    print(col)\n",
    "    #pie_helper(temp, temp.index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "765206c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root  phenomenon_taxonomy_leaf\n",
       "NLP                       Understanding               5\n",
       "Agents                    Tool Use                    2\n",
       "Alignment                 Alignment                   2\n",
       "                          Safety                      1\n",
       "Language Modelling        In-context Learning         1\n",
       "NLP                       Detection                   1\n",
       "                          Extraction                  1\n",
       "                          Summarization               1\n",
       "Reasoning                 Planning                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[(included_df['metric_definition: Correlation'])][['phenomenon_taxonomy_root','phenomenon_taxonomy_leaf']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f66d8229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "main_coder",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "bibkey",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "inclusion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "exclusion_criteria",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "exclusion_criteria_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "short_summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "contribution",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_short",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target_phenomenon",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_defined",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_definition",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "definition_scope",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "purpose_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_item_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_definition_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_dataset_size",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_dataset_metadata",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_metadata_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_sampling_method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "response_format",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_definition_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "authorship",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "benchmark_availability",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "procedural_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "notes_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_train_val",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_dataset_size_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "response_format_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_aggregation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_subscores",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_subscores_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_metascoring",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "benchmark_location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "benchmark",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_contested",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_face_validity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_face_validity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "result_interpretation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_comparison",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_comparison_explanation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_realism",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_human_baseline",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_author_validity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_author_validity_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_statistics",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_access",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_ecology",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_ecology_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "definition_integrity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "definition_integrity_detail",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_dataset_size_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_fewshot",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "new_bibkey",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_taxonomy_root",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_taxonomy_leaf",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_taxonomy_alternate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "validate_taxonomy",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source: Author-crafted",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Crowd-sourced",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Unknown",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Procedurally-generated",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Expert-crafted",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Another benchmark",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: LLM-generated",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Human exams",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Real task",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_sampling_method: Targeted",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method: Criterion",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method: Convenience",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method: Random",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method: Unknown",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "response_format: Structured",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Interaction",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Multiple choice",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Short free response",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Free response",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Logits",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Unknown",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_definition: Exact match",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Human ratings",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: LLM-as-a-Judge",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: LLM post-processing",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Distribution",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Correlation",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Reward",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Soft match",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Unknown",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "phenomenon_contested_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_contested: Widely-agreed",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "phenomenon_contested: No definition",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "phenomenon_contested: Contested",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_face_validity_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_face_validity: Partially",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_face_validity: ",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_face_validity: Yes",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_face_validity: No",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_face_validity_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_face_validity: Partially",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_face_validity: ",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_face_validity: Yes",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_face_validity: No",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_realism: No comparison made",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism: Not possible",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism: Realistic",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism: Comparison made",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism: No",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_author_validity_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_author_validity: ",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_author_validity: Yes",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_author_validity: No",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_ecology: ",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology: Complete",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology: Partial",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology: Representative",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology: Constructed",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_statistics_clean",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6d940cb0-083f-4efa-888e-13c684bef339",
       "rows": [
        [
         "22",
         "4/12/2025 18:03:29",
         "Negar Foroutan",
         "zhangMELAMultilingualEvaluation2024",
         "MELA: Multilingual Evaluation of Linguistic Acceptability",
         "Include",
         null,
         null,
         "The paper intorduces a multilingual acceptability judgement benchmark covering a diverse set of 10 languages, all annotated by expert linguists.  The acceptability judgment task tests a language model’s ability to distinguish syntactically acceptable sentences from unacceptable ones in a human language. The paper establishes LLM baselines on this benchmark, and investigates cross-lingual transfer in acceptability judgements with XLM-R.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Linguistic Acceptability",
         "Yes",
         "The acceptability judgment task tests a language model’s ability to distinguish syntactically acceptable sentences from unacceptable ones.",
         "Comprehensive",
         null,
         "The acceptability judgment task tests a language model’s ability to distinguish syntactically acceptable sentences from unacceptable ones.",
         "a sentence",
         null,
         "hand-written by linguists in respective languages, taken from textbooks, handbooks and journal articles in theoretical syntax + some examples taken from previous benchmarks ",
         "46k",
         "No",
         null,
         "Random sample (creators defined a task space and sampled from it)",
         "Multiple choice",
         "Exact Match (accuracy, F1, precision, recall), Matthews Correlation Coefficient (MCC, Matthews), which is a measure of similarity between binary distributions taking values from -1 to 1 and always yielding 0 for any two uncorrelated distributions, regardless of class imbalance.",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "train set: 33'293, validation:3'970",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/sjtu-compling/MELA",
         "MELA",
         "Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)",
         "Yes",
         "Yes",
         "Yes",
         "No",
         null,
         "No",
         "No",
         "No",
         null,
         "simple mean and standard deviation ",
         "Outputs alone",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         null,
         "zhangMELAMultilingualEvaluation2024",
         "Multilinguality",
         null,
         null,
         "Multilinguality - ",
         "['Expert-crafted']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Random']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Multiple choice']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Exact match', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean', 'Std']"
        ],
        [
         "34",
         "4/13/2025 20:33:00",
         "Negar Foroutan",
         "sunInformalLanguageProcessing2024",
         "Toward Informal Language Processing: Knowledge of Slang in Large Language Models",
         "Include",
         null,
         null,
         "Using movie subtitles, the authors construct a dataset that supports evaluation on a diverse\nset of tasks pertaining to the automatic processing of slang. For both evaluation and finetuning, they show the effectiveness of their dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "informal language processing (Knowledge of slang in LLMs)",
         "No",
         "They focus on two core tasks for informal language processing. First, they evaluate the extent to which LLMs can reliably detect slang usages in natural sentences. Second,\nthey assess whether LLMs can be used to identify regional-historical sources of slang via a text classification task.",
         "Subset",
         null,
         "Task1: Given a set of sentences, they evaluate slang detection at both sentence-level and word-level.\nTask2: Given a sentence containing a slang usage, they ask the model to classify its source (e.g. region and age).",
         "a sentence of natural language",
         null,
         "Crowd-sourced task examples (e.g. Prolific-created tasks)",
         "25,000",
         "Yes",
         "Annotator confidence, Movie ID, Region, Year",
         "Random sample (creators defined a task space and sampled from it)",
         "Multiple choice",
         "Exact Match (accuracy, F1, precision, recall), They also report two metrics to compare an LLM’s predictive confidence in slang usages relative to their literal counterparts.",
         null,
         "The benchmark is build on top of OpenSubtitles corpus.",
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train",
         null,
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/amazon-science/slang-llm-benchmark",
         null,
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "sunInformalLanguageProcessing2024",
         "Multilinguality",
         null,
         null,
         "Multilinguality - ",
         "['Crowd-sourced']",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "['Random']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Multiple choice']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Exact match', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "35",
         "4/13/2025 20:38:38",
         "Anna Sotnikova",
         "wangPretrainingLanguageModel2023",
         "ON PRE-TRAINED LANGUAGE MODELS FOR ANTIBODY",
         "Include",
         null,
         null,
         "This paper introduces the AnTibody Understanding Evaluation (ATUE) benchmark to systematically assess the representation capabilities of general and antibody-specific pre-trained language models across a range of antibody-related tasks. It also explores how incorporating biological mechanisms into pre-training can enhance model performance and evaluates the transferability of learned representations to real-world applications such as drug discovery and immune system analysis.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "LLMs capability to do  antibody representation learning and biological reasoning with sequence specificity",
         "Yes",
         "how LLMs perform in antibody tasks with different specificity and how introducing specific biological mechanisms to the pre-training process can benefit the model. Additionally, authors evaluate if the learned antibody pre-trained representations can be applied to real-world antibody problems, like drug discovery and immune process understanding.",
         "Subset",
         null,
         "Evaluate the ability of pre-trained language models to perform on four supervised antibody-related prediction tasks—antigen binding, paratope prediction, B cell maturation classification, and SARS-CoV-2 antibody discovery—each varying in antibody specificity. These tasks assess whether the models can capture biologically meaningful information from antibody sequences.",
         "N/A there are four tasks",
         null,
         "Real task examples (e.g. GitHub issues)",
         "3242, 1662, 88094, 22000",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall), Matthews Correlation Coefficient (MCC), and AUC (Area Under the ROC Curve)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "15,128/3,242 , N/A",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/dqwang122/EATLM",
         "ATUE",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         null,
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "wangPretrainingLanguageModel2023",
         "Biology",
         null,
         null,
         "Biology - ",
         "['Real task']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Convenience', 'Targeted', 'Criterion']",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Structured']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "['Exact match', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Partial']",
         "False",
         "False",
         "True",
         "False",
         "False",
         null
        ],
        [
         "60",
         "4/14/2025 10:35:13",
         "Anna Sotnikova",
         "xuPEERComprehensiveMultitask2022",
         "PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding",
         "Include",
         null,
         null,
         "A benchmark called PEER (a\ncomprehensive and multi-task benchmark for Protein sEquence undERstanding).\nPEER provides a set of diverse protein understanding tasks including protein\nfunction prediction, protein localization prediction, protein structure prediction,\nprotein-protein interaction prediction, and protein-ligand interaction prediction. ",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "The capability being measured in the PEER benchmark is protein sequence understanding. The benchmark evaluates model performance across a range of biologically relevant tasks, which include: Protein function prediction, Protein localization prediction, Protein structure prediction, Protein-protein interaction prediction, Protein-ligand interaction prediction",
         "Yes",
         "The PEER benchmark includes seventeen biologically relevant tasks that cover diverse aspects of protein understanding, including protein function prediction, protein structure prediction, protein localization prediction, protein-protein interaction prediction and protein-ligand interaction prediction.\n\nWe represent a protein x as a sequence of amino acids (a.k.a., residues) x = (x₁, x₂, · · · , x_L) of length L. For each task, we list the task name and its acronym, task category, data source, protein sequence statistics, dataset statistics and evaluation metric.",
         "Subset",
         null,
         "The task is defined as evaluating language models on a set of 17 biologically relevant benchmarks that test their ability to understand protein sequences. This includes predicting various properties and interactions of proteins, such as their function, structure, localization, and interactions with other proteins or ligands​\n",
         "A single item in the task dataset typically consists of a protein sequence (a string of amino acids) and a corresponding label or target value, which varies by task—e.g., a fitness score (regression), a structural class (classification), or a binary interaction label.",
         null,
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)",
         "across 17 tasks: 115,271",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall), Spearman’s ρ, L/5 precision, RMSE",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "274,179 and 28,743",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/DeepGraphLearning/PEER_Benchmark",
         "PEER",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "simple mean, std",
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "xuPEERComprehensiveMultitask2022",
         "Biology",
         null,
         null,
         "Biology - ",
         "['Real task', 'Another benchmark']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "True",
         "['Convenience', 'Targeted', 'Criterion']",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Structured']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "['Exact match', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Partial']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Mean', 'Std']"
        ],
        [
         "71",
         "4/14/2025 21:02:00",
         "Anna Sotnikova",
         "hardalovBgGLUEBulgarianGeneral2023",
         "bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark",
         "Include",
         null,
         null,
         " bgGLUE (Bulgarian General Language\nUnderstanding Evaluation), a benchmark\nfor evaluating language models on Natural Language\nUnderstanding (NLU) tasks in Bulgarian.\nThe benchmark includes NLU tasks targeting\na variety of NLP problems (e.g., natural language\ninference, fact-checking, named entity\nrecognition, sentiment analysis, question answering,\netc.) and machine learning tasks (sequence\nlabeling, document-level classification,\nand regression). ",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "NLU for the Bulgarian language",
         "Yes",
         "We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression).",
         "Subset",
         null,
         "The task is defined as the evaluation of language models on a benchmark suite of nine NLU tasks in Bulgarian, covering areas such as token classification, regression/ranking, and text classification. Each task is designed to test specific language understanding capabilities, including named entity recognition, sentiment analysis, fact-checking, natural language inference, and question answering",
         "A single item would consist of a text input (e.g., sentence, paragraph, tweet, or document) along with its associated label or target output, depending on the task type. ",
         null,
         "Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)",
         "total 32,448",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)",
         "Exact Match (accuracy, F1, precision, recall), Pear./Spear. Corr , Avg. Precision",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "total train 452,449 , total validation 20,930",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://bgglue.github.io/",
         "bgGLUE",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "simple mean, for tasks with more than one metric (like Pearson and Spearman correlation for sentiment regression), scores are averaged to get a single task score",
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people), Representative task (e.g. answering medical licensing exam questions), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "hardalovBgGLUEBulgarianGeneral2023",
         "NLP",
         "Understanding",
         "Multilinguality",
         "NLP - Understanding",
         "['Human exams', 'Real task', 'Author-crafted', 'Another benchmark']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "True",
         "True",
         "['Convenience', 'Targeted', 'Criterion']",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Multiple choice', 'Short free response', 'Free response']",
         "False",
         "False",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Exact match', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Partial', 'Representative', 'Constructed']",
         "False",
         "False",
         "True",
         "True",
         "True",
         "['Mean']"
        ],
        [
         "85",
         "4/15/2025 14:35:04",
         "Anna Sotnikova",
         "berdicevskisSuperlimSwedishLanguage2023",
         "Superlim: A Swedish Language Understanding Evaluation Benchmark",
         "Include",
         null,
         null,
         "We present Superlim, a multi-task NLP bench- mark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. From the set of experiments, it is quite challenging to the models.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "natural language understanding",
         "Yes",
         "NLU includes a wide range of subtasks such as sentiment analysis, argumentation classification, grammatical error detection, semantic similarity, natural language inference, coreference resolution, word similarity and relatedness, analogy, synonym detection, and diagnostics for linguistic phenomena and gender bias.",
         "Subset",
         null,
         "The Superlim benchmark defines its tasks as a set of 15 NLU tasks for Swedish, covering text-level tasks (e.g., sentiment analysis, NLI, paraphrase detection), word-level tasks (e.g., similarity, analogy), and diagnostic tasks (e.g., gender bias detection, linguistic phenomenon inference).",
         "A single item in a task dataset typically consists of text inputs (such as a sentence, sentence pair, or word pair) with the respective label or target output specific to the task—e.g., a sentiment score or a classification label.",
         null,
         "Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "36,118 (the range is from 109 examples to 18,593)",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph), predicted label",
         "Krippendorff’s α",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "there is no link in the paper, but can find it online",
         null,
         null,
         "Test, Train, Validation",
         "total (479,571 train) and (22,527 validation)",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://spraakbanken.gu.se/en/resources/superlim",
         "Superlim",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "simple mean, std",
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people), Representative task (e.g. answering medical licensing exam questions), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "berdicevskisSuperlimSwedishLanguage2023",
         "NLP",
         "Understanding",
         null,
         "NLP - Understanding",
         "['Human exams', 'Real task', 'Author-crafted', 'Another benchmark', 'Procedurally-generated']",
         "True",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "True",
         "True",
         "['Convenience', 'Targeted', 'Criterion']",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Multiple choice', 'Short free response', 'Free response', 'Multiple choice']",
         "False",
         "False",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Correlation']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Partial', 'Representative', 'Constructed']",
         "False",
         "False",
         "True",
         "True",
         "True",
         "['Mean', 'Std']"
        ],
        [
         "127",
         "4/16/2025 9:21:25",
         "Anna Sotnikova",
         "renBEACONBenchmarkComprehensive2024",
         "BEACON: Benchmark for Comprehensive RNA Tasks and Language Models",
         "Include",
         "Topic Exclusion (Is the paper about measuring the capabilities of LLMs?)",
         null,
         "This paper presents BEACON, the first comprehensive benchmark for evaluating RNA language models across 13 tasks related to RNA structure, function, and engineering. It analyzes various models and components, highlighting the benefits of single nucleotide tokenization and ALiBi positional encoding, and introduces BEACON-B, a strong, resource-efficient baseline model.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "\nRNA understanding — the ability of models to perform comprehensive RNA-related tasks",
         "Yes",
         "the ability of models to perform comprehensive RNA-related tasks such as\n\nUnderstanding RNA structure (e.g., secondary structure, contact map, distance map)\nPredicting RNA function (e.g., splice sites, isoform usage, non-coding RNA function, modifications)\nSupporting RNA engineering (e.g., predicting vaccine degradation, programmable RNA switches, CRISPR targeting)",
         "Subset",
         null,
         "Models are evaluated on 13 RNA-related tasks that span structural analysis (e.g., secondary structure, contact maps), functional prediction (e.g., splice sites, RNA modifications), and engineering applications (e.g., CRISPR targeting, vaccine stability). Each task involves either classification or regression at the nucleotide or sequence level, with specific evaluation metrics for the biological context​.",
         "one RNA sequence, typically composed of nucleotide characters (e.g., A, U, C, G), along with a corresponding label or set of labels depending on the task ",
         null,
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)",
         "total: 96,283",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall), Top L Precision, Top-k ACC, R^2, AUC, MCRMSE, Spearmann core",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "total: 793,047 and 77,836",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/terry-r123/RNABenchmark",
         "BEACON",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "Mean and std",
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "renBEACONBenchmarkComprehensive2024",
         "Biology",
         null,
         null,
         "Biology - ",
         "['Real task', 'Another benchmark']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "True",
         "['Convenience', 'Targeted', 'Criterion']",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Short free response', 'Structured']",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Exact match', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Partial', 'Constructed']",
         "False",
         "False",
         "True",
         "False",
         "True",
         "['Mean', 'Std']"
        ],
        [
         "186",
         "4/17/2025 8:56:38",
         "Anna Gausen",
         "shenTaskBenchBenchmarkingLarge2024",
         "TaskBench: Benchmarking Large Language Models for Task Automation",
         "Include",
         null,
         null,
         "TaskBench is a framework for evaluating how well large language models (LLMs) can automate complex tasks. It addresses three stages of task automation: task decomposition, tool selection and parameter prediction. It introduces Tool Graph - a novel representation of tools and their dependencies.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "task automation",
         "Yes",
         "\"task automation, which involves decomposing complex tasks described by user instructions into sub-tasks and invoking external tools to execute them, playing a central role in autonomous agents.\"",
         "Comprehensive",
         null,
         "The task requires models to generate task steps and a tool graph based on the user instruction",
         "A single item consists of a user instruction and the output contains detailed task steps and a tool graph with parameters.",
         null,
         "Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "17,331",
         "Yes",
         " tool graph structure, number of tools, domain , tool names, tool dependencies, parameters required",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Correlation (Matthew's correlation, Pearson's r)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "domain categories, different graph structure types, and complexity levels ",
         null,
         "https://github.com/microsoft/JARVIS",
         "TaskBench",
         "Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "Yes",
         "They include human evaluation of the dataset quality, asking experts to rate samples on naturalness, complexity, and alignment. They compare to existing baselines.",
         "simple means to report F1 scores and ROUGE metrics",
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people), Representative task (e.g. answering medical licensing exam questions)",
         "The task simulates real user instructions that would be given to autonomous agents, but in a controlled environment.",
         "Composite phenomenon",
         "Yes",
         null,
         "Yes",
         "shenTaskBenchBenchmarkingLarge2024",
         "Agents",
         "Tool Use",
         null,
         "Agents - Tool Use",
         "['Procedurally-generated', 'LLM-generated']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Free response', 'Structured']",
         "True",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Exact match', 'Soft match', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Partial', 'Representative']",
         "False",
         "False",
         "True",
         "True",
         "False",
         "['Mean']"
        ],
        [
         "207",
         "4/17/2025 17:21:15",
         "Lujain Ibrahim",
         "zhangMultiTrustComprehensiveBenchmark2024",
         "MULTITRUST: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models",
         "Include",
         null,
         null,
         "The paper introduces a benchmark on the trustworthiness of MLLMs across five primarcy aspects: truthfulness, safety, robustness, fairness, and privacy. It benchmarks 20+ MLMMs and highlights the complexities introduced by multi-modality. ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "trustworthiness",
         "Yes",
         "\"Drawing on extensive studies in trustworthy LLMs and distilling from relevant literature of MLLMs, we pinpoint 5 primary aspects of trustworthiness for evaluating MLLMs, including truthfulness, safety, robustness, fairness, and privacy. In particular, truthfulness, safety, and robustness guarantee the models’ reliability and stability in preventing undesirable outcomes, i.e., errors, harms, and variations under different conditions\" (page 3)",
         "Subset",
         null,
         "There are 32 tasks that are generative and/or discriminative, multimodal or test-only. Wide range from NSFW image description to PII leakage in conversations They utilize off the shelf datasets, augment existing ones, and create their own. ",
         "Varies greatly from task to task. Some include images and a prompt, others just a prompt. ",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)",
         "\"more than 15k\" (page 103)",
         "Yes",
         "image information, types of queries",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Unknown",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "In the main body, it's sub-aspect (e.g., truthfulness, safety, robustness). In the appendix, there are subsets of many of the tasks/existing benchmarks they run.",
         null,
         "https://github.com/thu-ml/MMTrustEval",
         "MULTITRUST",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "simple mean/sum, correlation between overall rankings and general capabilities based on MMBench",
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people), Representative task (e.g. answering medical licensing exam questions)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "zhangMultiTrustComprehensiveBenchmark2024",
         "Alignment",
         "Safety",
         null,
         "Alignment - Safety",
         "['Author-crafted', 'Another benchmark']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Targeted', 'Unknown']",
         "True",
         "False",
         "False",
         "False",
         "True",
         "['Multiple choice', 'Short free response', 'Free response']",
         "False",
         "False",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Exact match', 'LLM-as-a-Judge', 'Distribution', 'Correlation']",
         "True",
         "False",
         "True",
         "False",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Partial', 'Representative']",
         "False",
         "False",
         "True",
         "True",
         "False",
         "['Mean', 'Other']"
        ],
        [
         "229",
         "4/17/2025 20:25:35",
         "Angelika Romanou",
         "chenCurriculumBroadcoverageBenchmark2022",
         "Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding",
         "Include",
         null,
         null,
         "Current models do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. In this paper, authors introduce CURRICULUM as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. CURRICULUM contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Linguistic phenomena in NLU",
         "Yes",
         "For this phenomenon, authors try to measure how well a language model captures distinct linguistic skills essential to language understanding and reasoning.",
         "Comprehensive",
         null,
         "Natural language inference (NLI). More specifically, authors provide a group of tasks motivated by three benchmarks: GLUE Diagnostic, Rainbow, and DNC. In addition, we include many more subtasks focusing on complex reasoning types such as deductive logic and analytical thinking.",
         "Each single item has a premise, a hypothesis, and a target label.",
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "171,252",
         "Yes",
         "difficulty level",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Short free response (e.g. single word or number)",
         "Exact Match (accuracy, F1, precision, recall), Correlation (Matthew's correlation, Pearson's r)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train",
         "540,437",
         null,
         "Simple Mean",
         "Yes",
         "sub-phenomenon, difficulty ",
         null,
         "https://github.com/eric11eca/curriculum-ling?tab=readme-ov-file",
         "CURRICULUM",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "chenCurriculumBroadcoverageBenchmark2022",
         "NLP",
         "Understanding",
         null,
         "NLP - Understanding",
         "['Another benchmark']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Convenience', 'Targeted']",
         "True",
         "False",
         "True",
         "False",
         "False",
         "['Short free response']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Exact match', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "233",
         "4/17/2025 21:24:02",
         "Jan Batzner",
         "ushioGenerativeLanguageModels2022",
         "Generative Language Models for Paragraph-Level Question Generation",
         "Include",
         null,
         null,
         "QG-Bench, a comprehensive benchmark for paragraph-level question generation (QG) that unifies existing question answering datasets into a standard format. The authors fine-tune LMs for the QG task and evaluate them using both automatic metrics and human evaluation.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Question generation",
         "Yes",
         "\"Question generation is the task of generating a question given an in- put context consisting of a document, a paragraph or a sentence, and an answer where the question is anchored\"",
         "Subset",
         null,
         "Generate a natural language question given an input paragraph and an answer span that appears within that paragraph.",
         "Paragraph, a sentence within that paragraph, an answer span, and the target question to be generated.",
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "SQuaD train: 75,722",
         "Yes",
         "language, domain, average patagraph character length",
         "Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)",
         "Free response (e.g. summary paragraph, executable code)",
         "n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), Correlation (Matthew's correlation, Pearson's r)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "SQuaD validation: 10,570, SQuaD test: 11,877",
         null,
         "Simple Mean",
         "Yes",
         "Scores by language, domain, model input type",
         null,
         "https://github.com/asahi417/lm-question-generation",
         "QG-Bench",
         "Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "Yes",
         "Validation of the benchmark through manual evaluation where human annotators rate generated questions across three criteria (grammaticality, understandability, and answerability).",
         "Simple mean scores for each metric. For correlation analysis between automatic metrics and human judgments: Spearman's rank correlation coefficient.",
         "Outputs alone",
         "Representative task (e.g. answering medical licensing exam questions)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "ushioGenerativeLanguageModels2022",
         "NLP",
         "Extraction",
         null,
         "NLP - Extraction",
         "['Another benchmark']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Convenience', 'Criterion']",
         "False",
         "True",
         "True",
         "False",
         "False",
         "['Free response']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Soft match', 'Human ratings', 'Correlation']",
         "False",
         "True",
         "False",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Representative']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Mean', 'Other']"
        ],
        [
         "247",
         "4/17/2025 22:44:18",
         "Yilun Zhao",
         "chenMLLMasajudgeAssessingMultimodal2024",
         "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
         "Include",
         null,
         null,
         "This paper introduces the MLLM-as-a-Judge benchmark to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "MLLM-as-a-judge",
         null,
         "capability of MLLMs in tasks of Scoring Evaluation, Pair Comparison and Batch Ranking.",
         "Comprehensive",
         null,
         "Take a single MLLM response, provide the score; Take two MLLM responses, compare which one is better; Take a batch of MLLM responses, provide a ranking",
         "<two MLLM responses>, Judgement: B",
         null,
         "Expert-crafted task examples (e.g. hand-written examples)",
         "17903",
         "Yes",
         "input setting",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Short free response (e.g. single word or number)",
         "Correlation (Matthew's correlation, Pearson's r)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         null,
         null,
         "https://github.com/Dongping-Chen/MLLM-Judge",
         "MLLM-as-a-Judge",
         "Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)",
         "Yes",
         null,
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "Yes",
         "we implement cross-validation between different annotators and conduct continuous monitoring to ensure they are maintaining objectivity and fairness.",
         null,
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "chenMLLMasajudgeAssessingMultimodal2024",
         "LLM as a Judge",
         null,
         null,
         "LLM as a Judge - ",
         "['Expert-crafted']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Targeted']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Short free response']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Correlation']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['']",
         "False",
         "True",
         "False",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Partial', 'Constructed']",
         "False",
         "False",
         "True",
         "False",
         "True",
         null
        ],
        [
         "301",
         "4/18/2025 20:34:24",
         "Jan Batzner",
         "mackoMULTITuDELargescaleMultilingual2023",
         "MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark",
         "Include",
         null,
         null,
         "MULTITuDE, a benchmark dataset for multilingual machine-generated text detection. It contains human-written and machine-generated text across languages from multilingual LLMs. The authors evaluate how zero-shot and fine-tuned detectors generalize across languages and LLMs.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Multilingual machine-generated text detection",
         "Yes",
         "Machine-generated text detection is a binary classification of a text to be human-written or machine-generated.",
         "Subset",
         null,
         "Binary classification of texts as either human-written or machine-generated (multilingual).",
         "News article text sample in one of 11 languages, metadata on language, human-written / machine-generated,  LLM that generated it",
         null,
         "Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "44,786 (train set)",
         "Yes",
         "language, text generation model, script type, language family",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "Multiple choice",
         "Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train",
         "29,295 (test set)",
         null,
         "Simple Mean",
         "Yes",
         "Language; Generator LLM; Detection type; Monolingual or multilingual fine-tuning",
         null,
         "https://github.com/kinit-sk/mgt-detection-benchmark",
         "MULTITuDE",
         "Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "No",
         null,
         "Mean, ANOVA, post-hoc pairwise tests",
         "Outputs alone",
         "Representative task (e.g. answering medical licensing exam questions)",
         "Realistic detection scenario in which one would need to identify machine-generated news content in multiple languages, e.g., relevant for combating misinformation.",
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "mackoMULTITuDELargescaleMultilingual2023",
         "NLP",
         "Detection",
         null,
         "NLP - Detection",
         "['Another benchmark', 'LLM-generated']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "True",
         "False",
         "False",
         "['Criterion']",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Multiple choice']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Exact match', 'Distribution', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Representative']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Mean', 'Tests']"
        ],
        [
         "314",
         "4/18/2025 23:53:19",
         "Valentin Hoffman",
         "fenogenovaMERAComprehensiveLLM2024",
         "MERA: A Comprehensive LLM Evaluation in Russian",
         "Include",
         null,
         null,
         "The paper proposes a new instruction benchmark, MERA, for measuring LM capabilities in Russian. MERA comprises 21 tasks that have been (i) specifically created for MERA, (ii) translated from English tasks, or (iii) taken from existing Russian resources. The authors tried to culturally adapt MERA to the Russian context, for example by replacing historical concepts in translated English tasks with Russian ones. The paper also provides model baselines (evaluation of 19 LMs) as well as a human baseline for MERA.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications), MERA as a whole tries to measure general \"capabilities\", but the individual tasks evaluate more specific applications (e.g., question answering).",
         "The authors say that MERA covers ten \"skills\": mathematics, logic, reasoning, common sense, natural language inference, world knowledge, dialogue system, algorithms, computer code, and ethics. Specifically, they assign each of the 21 tasks in MERA to one or several (up to three) of these skills. ",
         "No",
         null,
         "The title claims the benchmark is \"comprehensive\", but in the Limitations section they say that the \"evaluation might not comprehensively assess LLM’s abilities.\"",
         null,
         "The 21 tasks are split into three categories: \"problem-solving tasks\" that are defined to constitute \"general intelligence evaluation tasks,\" \"exam-based tasks\" that \"require expertise for solution,\" and \"diagnostic (ethics) tasks,\" aimed to \"identify models' ethical biases, including toxicity harms\" (p. 9922). The individual tasks are mapped to one of these three categories and defined in a mostly procedural way (i.e., by describing how the data were put together).",
         "Each task contains items of different types.",
         null,
         "Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)",
         "Between 164 and 6000 examples per task.",
         "Yes",
         "For some tasks.",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Correlation (Matthew's correlation, Pearson's r), (school) grade",
         "Metric varies for the 21 tasks.",
         "Task source varies for the 21 tasks.",
         "Mix (multiple authors from industry and academia)",
         "The test sets of the benchmark are private, but the train and validation sets are available, and the leaderboard is being actively updated.",
         null,
         null,
         "Test, Train, Validation",
         "Train: between 0 and 29,376 examples per task. Validation: between 0 and 900 examples per task.",
         "Response format varies for the 21 tasks.",
         "Simple Mean",
         "No",
         null,
         "pass@k (any correct answer in k trials)",
         "https://huggingface.co/datasets/MERA-evaluation/MERA",
         "MERA",
         "No definition provided",
         "Tricky to say since the paper does not provide a principled definition of the target phenomenon. It just talks of general \"capabilities,\" as well as the ten skills mentioned above. As for the ten skills, face validity varies -- for some (e.g., mathematics) it seems higher than for others (e.g., ethics).",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "Given that the benchmark is trying to measure general capabilities, it is unclear how a more realistic setting would look like.",
         "Yes",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Representative task (e.g. answering medical licensing exam questions), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         "Task ecology varies for the 21 tasks.",
         "Composite phenomenon",
         "Yes",
         "All tasks have a test set. Most tasks, except for three in the \"Ethics\" category, have a train set. Only four out of 21 tasks have a validation set.",
         "Yes",
         "fenogenovaMERAComprehensiveLLM2024a",
         "NLP",
         null,
         "Multilinguality",
         "NLP - ",
         "['Human exams', 'Author-crafted', 'Expert-crafted', 'Crowd-sourced', 'Another benchmark']",
         "True",
         "True",
         "False",
         "False",
         "True",
         "True",
         "False",
         "True",
         "False",
         "['Convenience', 'Targeted', 'Criterion']",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Multiple choice', 'Short free response', 'Free response']",
         "False",
         "False",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Exact match', 'LLM-as-a-Judge', 'Correlation', 'Soft match']",
         "True",
         "False",
         "True",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "['No definition']",
         "False",
         "True",
         "False",
         "['Partially']",
         "True",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Not possible']",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Representative', 'Constructed']",
         "False",
         "False",
         "False",
         "True",
         "True",
         "['Mean']"
        ],
        [
         "323",
         "4/19/2025 0:52:01",
         "Valentin Hoffman",
         "sunMeasuringEffectInfluential2023",
         "Measuring the Effect of Influential Messages on Varying Personas",
         "Include",
         null,
         null,
         "The authors examine the task of predicting how social media users will react to a news event. To this aim, they collect a Twitter-based benchmark consisting of (i) headlines, (ii) user information (referred to as \"persona\"), and (iii) user reactions, represented as sentiment polarity, sentiment intensity, and response text. They define the task of \"Response Forecasting on Personas for News Media\" as predicting the user reaction from the headline and user information, and they evaluate several LMs on this task.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "The specific application is \"measuring the influence of news media messages on viewers by predicting viewers' responses\" (p. 555).",
         "Yes",
         "The phenomenon (specifically, application) is defined as \"measuring the influence of news media messages on viewers by predicting viewers' responses\" (p. 555).",
         "Subset",
         null,
         "The task is defined as predicting sentiment polarity, sentiment intensity, and textual response of an individual when that individual sees a message on news media.",
         "Each item consists of a user persona (i.e., information from user profile and user history), a news headline, and the sentiment polarity, sentiment intensity, and textual response of the user in response to that news headline.",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "1,039",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "Multiple choice, Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Correlation (Matthew's correlation, Pearson's r)",
         "Human evaluation looks at three aspects: \"persona consistency\" (i.e., whether the output reflects the user's characteristics), \"label consistency\" (i.e., whether the response text and sentiment are consistent with each other), and \"context consistency\" (i.e., whether the output is responding to the input news headline).",
         "Sentiment polarity and sentiment intensity of items in the training data are LLM-annotated. Sentiment polarity and sentiment intensity of items in the validation and test data are human-annotated.",
         "Academia",
         "The benchmark only comprises Twitter IDs, meaning that the corresponding data cannot be freely accessed anymore.",
         null,
         null,
         "Test, Train, Validation",
         "Train: 10,977; validation: 1,341.",
         null,
         null,
         "No",
         null,
         null,
         "https://github.com/chenkaisun/response_forecasting",
         null,
         "Very special phenomenon that is only introduced by the authors. It is neither widely-agreed upon nor contested.",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         null,
         "Outputs alone",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         "No",
         "sunMeasuringEffectInfluential2023a",
         "NLP",
         "Understanding",
         null,
         "NLP - Understanding",
         "['Author-crafted', 'Crowd-sourced', 'LLM-generated']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Convenience']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Multiple choice', 'Free response']",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "['Exact match', 'Soft match', 'Human ratings', 'LLM-as-a-Judge', 'Correlation']",
         "True",
         "True",
         "True",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         null
        ],
        [
         "333",
         "4/19/2025 20:14:57",
         "Valentin Hoffman",
         "liuWe`reAfraidLanguage2023",
         "We're Afraid Language Models Aren't Modeling Ambiguity",
         "Include",
         null,
         null,
         "This paper examines the ability of LMs to handle ambiguity. The authors model ambiguity via its effects on entailment relations and collect a dataset of NLI examples, each annotated with a set of labels (potentially reflecting underlying ambiguity) and disambiguating rewrites. LMs perform substantially worse on AmbiEnt than humans.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "They measure the ability of LMs to recognize and disentangle possible meanings of ambiguous sentences.",
         "Yes",
         "\"Formally characterizing ambiguity requires a choice of meaning representation to distinguish between possible interpretations, and enumerating the full set of interpretations can be tricky or impractical. Thus, we adopt a functional approach: using the natural language inference (NLI) task format, we characterize ambiguity in the premise and/or hypothesis by its effect on entailment relations.\" (p. 791)",
         "Subset",
         null,
         "The authors use AmbiEnt for three tasks (called \"tests\" in the paper): (i) generating disambiguations, (ii) recognizing the validity of plausible interepretations, and (iii) modeling open-ended continuations reflecting different interpretations.",
         "Each item consists of:\n- a premise/hypothesis pair;\n- a set of labels (entail, contradict, neutral);\n- disambiguating rewrites (for items with more than one label in the label set).",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "1,545",
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Multiple choice, Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r), edit-f1",
         "Model access required for one of the three tasks.",
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Validation",
         "Validation: 100",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/alisawuffles/ambient",
         "AmbiEnt (Ambiguity in  Entailment)",
         "Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "Yes",
         "Yes",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "No",
         null,
         "Yes",
         "liuWe`reAfraidLanguage2023a",
         "NLP",
         "Understanding",
         null,
         "NLP - Understanding",
         "['Author-crafted', 'Another benchmark', 'LLM-generated']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "True",
         "False",
         "False",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Multiple choice', 'Free response']",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "['Exact match', 'Distribution', 'Correlation', 'Soft match']",
         "True",
         "False",
         "False",
         "False",
         "True",
         "True",
         "False",
         "True",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Comparison made']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "342",
         "4/20/2025 17:13:53",
         "Karolina Korgul",
         "wangGTABenchmarkGeneral2024",
         null,
         "Include",
         null,
         null,
         "GTA is a benchmark designed to evaluate LLM-based tool agents in realistic settings using multimodal written-by-human queries and executable tools. It features 229 tasks requiring reasoning, planning, and real-world tool use, spanning perception, operation, logic, and creativity.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "tool-use capabilities, reasoning, planning, natural language understanding, creativity",
         "Yes",
         "Tool-use capability refers to the ability of a model to reason about and execute real-world tasks by using appropriate tools in the right sequence, based on user queries and multimodal inputs.",
         "Subset",
         null,
         "Each task requires the model to interpret the query, reason about which tools are needed, plan a sequence of tool invocations, and execute them to get to a final answer. There are four categories - perception, operation, logic, and creativity, and all are executable. Models are evaluated both step-by-step (tool prediction at each step) and end-to-end (solving the task through actual tool use).",
         "A single item in the dataset is composed of five parts: a set of one or two image files (F), a query based on the files (Q), the set of tools involved (T), a reference tool chain with steps (C), and a final answer (A).",
         null,
         "Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks)",
         "229",
         "Yes",
         "Query type (objective, subjective, image generation), tool usage, number of steps, tool categories",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "InstAcc, ToolAcc, ArgAcc, SummAcc, AnsAcc, AnsAcc w/ ImgGen, F1 score for tool selection in Perception, Operation, Logic, and Creativity categories.",
         null,
         "https://github.com/open-compass/GTA",
         "GTA: A Benchmark for General Tool Agents",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "Authors discuss construct validity via analysis of tool execution, error types, and correlations between intermediate and final metrics.",
         "Simple mean/sum, correlation (Pearson's r)",
         "Outputs alone",
         "Representative task (e.g. answering medical licensing exam questions), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         "Tasks are constructed to closely mimic real-world scenarios using executable tools and realistic multimodal inputs. However, all queries and tool chains are human-designed for evaluation, not sourced from real user interactions. Therefore, GTA is ecologically realistic but at the end rather synthetic.",
         "Composite phenomenon",
         "Yes",
         "Tasks use 252 image files and span 14 tools, with 1–4 tools used per task.",
         "No",
         "wangGTABenchmarkGeneral2024",
         "Agents",
         "Tool Use",
         null,
         "Agents - Tool Use",
         "['Real task', 'Author-crafted', 'Expert-crafted', 'Crowd-sourced']",
         "True",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "True",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Short free response', 'Free response', 'Structured']",
         "True",
         "False",
         "False",
         "True",
         "True",
         "False",
         "False",
         "['Exact match', 'LLM post-processing', 'Distribution', 'Correlation']",
         "True",
         "False",
         "False",
         "True",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Realistic']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Representative', 'Constructed']",
         "False",
         "False",
         "False",
         "True",
         "True",
         "['Mean', 'Other']"
        ],
        [
         "345",
         "4/20/2025 21:33:41",
         "Lujain Ibrahim",
         "mireshghallahCanLLMsKeep2024",
         "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
         "Include",
         null,
         null,
         "The paper introduces a benchmark grounded in the theory of contextual integrity and designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. It consists of four tiers, gradually increasing in complexity, with the final tier evaluating contextual privacy reasoning and theory of mind capabilities.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "contextual privacy reasoning",
         null,
         "\"the appropriate flow of information within specific social contexts. A privacy breach happens when the information flows against the contextual norm.\" (page 1)",
         "Subset",
         null,
         "There are four tiers and four tasks: \n1. rate how sensitive people would consider certain information to be\n2.  judge whether a given information flow scenario aligns with people's privacy expectations\n3. generate contextually appropriate responses and understand information accessibility when a secret is shared between two parties, and one of them interacts with a third party\n4. create list of action items and meeting summary while excluding sensitive info and containing vital public info",
         "combinations of the following depending on the task: information types, vignettes, relationship between parties, meeting notes",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         null,
         "Yes",
         "difficulty level (tiers), information type, relationship pairs, incentive, actor, use, secret, public information",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM post-processing (extracting answers, reformatting for automated scoring), Correlation (Matthew's correlation, Pearson's r)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         null,
         null,
         null,
         "Simple Mean",
         "Yes",
         "By tiers (difficulty levels)",
         "pass@k (any correct answer in k trials)",
         "https://github.com/skywalker023/confAIde",
         "CONFAIDE",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "Yes",
         "Indirectly address it",
         "They link the design of the benchmark directly to an established theory of contextual integrity by drawing on prior empirical research that operationalized this theory. For example, the prompts used in tiers 1 and 2 are \"taken directly from Martin & Nissenbaum (2016)\" (page 4).",
         "simple mean",
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         "Total size of the dataset is not provided but some information is provided in the tasks. For example, task from tier 2 has 98 vignettes, task from tier 3 has 270 scenarios, and task from tier 4 has 20 transcripts.",
         "No",
         "mireshghallahCanLLMsKeep2024",
         "Alignment",
         "Alignment",
         null,
         "Alignment - Alignment",
         "['Author-crafted', 'Procedurally-generated', 'LLM-generated']",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Multiple choice', 'Short free response', 'Free response']",
         "False",
         "False",
         "True",
         "True",
         "True",
         "False",
         "False",
         "['Exact match', 'Human ratings', 'LLM post-processing', 'Correlation']",
         "True",
         "True",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Partial', 'Constructed']",
         "False",
         "False",
         "True",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "363",
         "4/22/2025 13:40:03",
         "Karolina Korgul",
         "maLargeLanguageModels2024",
         "Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach",
         "Include",
         null,
         null,
         "TextStarCraft II turns the full StarCraft II video‑game into a purely text interface so that large language‑model (LLM) agents can play the game through natural‑language commands.\nThe paper also introduces “Chain of Summarisation” (CoS), a cache‑like, multi‑frame prompting procedure that lets an LLM compress streams of observations and issue batches of actions fast enough to keep up with real‑time play. ​It also provided a benchmark, which enables the evaluation of LLMs’ real-time strategic decision-making and long-term planning abilities through natural language. ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Real-time strategic decision-making, long-term planning, and natural language understanding, reasoning, flexibility",
         "Yes",
         "\"Real-time strategy decision-making and long-term planning are critical AI challenges, necessitating rapid, tactical decisions and strategic adaptability over time.\" (page 1)",
         "Subset",
         null,
         "The task involves controlling a StarCraft II agent using natural language to issue macro-level strategic commands. Micro-level commands are managed by rule-based scripts.",
         "A single item is one complete 1:1 match played on a competitive ladder map at a chosen AI difficulty.\t",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template)",
         null,
         "Yes",
         "difficulty level, opponent style, map name",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "Free response (e.g. summary paragraph, executable code), Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall), Correlation (Matthew's correlation, Pearson's r), Win Rate, Population Block Ratio (PBR), Resource Utilisation Ratio (RUR), Average Population Utilization (APU), Technology Rate (TR)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Authors publish PBR, RUR, APU, TR alongside win‑rate.",
         null,
         "https://github.com/histmeisah/Large-Language-Models-play-StarCraftII",
         "TextStarCraft II",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "Yes",
         "Yes",
         "They show that most LLMs can beat the built-in AI (LV5), GPT-4 ranks highest in SC2 knowledge per Grandmaster experts, and fine-tuned models play on par with Gold-level humans. They also highlight the LLM agent’s interpretable and adaptable strategy compared to AlphaStar.",
         "Simple mean and standard deviation",
         "Outputs alone",
         "Representative task (e.g. answering medical licensing exam questions)",
         "While the game is a real-time strategy game, the task is presented as a text-based interaction, which is a representation of the real task rather than a complete or partial real task. The objective of defeating an opponent in SC2 is a representative goal of the actual game.",
         "Composite phenomenon",
         "Yes",
         "The paper mentions fine-tuning open-source models using the \"entire dataset of GPT3.5-turbo-16k interaction logs with TextStarCraft II.\"  However, the specific sizes of the train and validation sets are not explicitly reported for this interaction log dataset. There are results presented for fine-tuning on different subsets of win data, with the largest subset being \"FULL DATASET (ALL GAMES)\" used for fine-tuning, but its size isn't quantified with a number.",
         "No",
         "maLargeLanguageModels2024",
         "Reasoning",
         "Planning",
         null,
         "Reasoning - Planning",
         "['Author-crafted', 'Procedurally-generated']",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "False",
         "['Criterion']",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Free response', 'Structured']",
         "True",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Exact match', 'Correlation', 'Reward']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "True",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Realistic']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Representative']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Mean', 'Std']"
        ],
        [
         "385",
         "4/21/2025 6:57:30",
         "Maria Grandury",
         "leiterPrExMeLargeScale2024",
         "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
         "Include",
         null,
         null,
         "This paper introduces PrExMe, a benchmark of prompt templates to evaluate LLMs as metrics for machine translation and summarization tasks. It systematically explores how prompt variation impacts metric performance and stability, revealing both robust patterns and significant sensitivities in prompt design.",
         "They include emotion-CoT in prompt templates, asking the LLM to describe its \"emotions\".",
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "evaluation of machine translation and summarization",
         "No",
         null,
         null,
         null,
         "Assign a quality score to the generated hypothesis given it source text, without using reference translations or summaries.",
         "A prompt. \"Each prompt is built from: (1) the source text and generated hypothesis text that should be graded, (2) a base prompt, (3) a task description, (4) a format requirement and (5) optionally a one-shot demonstration.\"",
         null,
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "+70,000 samples",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Specific criteria (items were taken from a larger set based on specified rules)",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), Correlation (Matthew's correlation, Pearson's r)",
         "\"As correlation measures, we use the Kendall (primary measure), Pearson and Spearman correlations, as well as tie-calibrated accuracy.\"",
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Subtask (e.g., en-de translation, zh-en translation, summarization), dataset (e.g., Eval4NLP, WMT23, Seahorse), prompt components (e.g., base prompt type, output format, task description)",
         null,
         "https://github.com/Gringham/PrExMe/tree/main/data",
         "PrExMe",
         "No definition provided",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "Yes",
         "Yes, implicitly",
         "- Face:  The authors motivate the benchmark clearly and argue its real-world utility, especially in low-resource and prompt-only evaluation scenarios. \n- Predictive/Concurrent: The benchmark is evaluated by how well LLM-generated scores correlate with human judgments.\n- Content: Prompts are composed from modular templates (e.g., base prompts, task framing, and format variations) and the authors run tests to evaluate stability of these design components\n- Convergent/Discriminant: The authors compare their benchmark results against existing metrics like XCOMET and BARTScore, ",
         "simple mean, significance clusters",
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "Yes",
         "leiterPrExMeLargeScale2024",
         "NLP",
         "Summarization",
         null,
         "NLP - Summarization",
         "['Real task', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "True",
         "True",
         "False",
         "True",
         "['Convenience', 'Criterion']",
         "False",
         "True",
         "True",
         "False",
         "False",
         "['Short free response', 'Free response']",
         "False",
         "False",
         "False",
         "True",
         "True",
         "False",
         "False",
         "['Exact match', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['No definition']",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes', 'Yes']",
         "False",
         "True",
         "False",
         "['Partial', 'Constructed']",
         "False",
         "False",
         "True",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "424",
         "4/22/2025 17:42:02",
         "Thom Foster",
         "zengMRbenMetareasoningBenchmark2024",
         "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs",
         "Include",
         null,
         null,
         "Dataset of question,answer pairs in which answers have mistakes. Models are evaluated on ability to spot mistake and give a reason for why it is wrong. Extends existing work like MR-GSM8K to physics, chemistry, logic, coding etc",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Meta-reasoning ",
         "No",
         "\"Reasoning, the cognitive process of using evidence, arguments, and logic to reach conclusions, is crucial for problem-solving, decision-making, and critical thinking.\"\n\nMeta-reasoning: \"it is important to complement outcome-based evaluation with an intrinsic evaluation of the quality of the reasoning process\"\n",
         "Comprehensive",
         null,
         "Each data\npoint encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error\nanalysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses,\nwhich include error steps, reasons behind the error, and subsequent corrections. The three examples\nshown are selected to represent arithmetic, logical, and algorithmic reasoning types.",
         "Each data\npoint encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error\nanalysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses,\nwhich include error steps, reasons behind the error, and subsequent corrections.\n\n\nSubject: High School Math\nQuestion: What is the smallest whole number that has a remainder of 1 when divided by 4, a remainder of 1 when divided by 3, and a remainder of 2 when divided by 5?\nOptions: [A.] 67 [B.] 43 [C.] 57 [D.] 37\nGround Truth Answer: D\n—————————————- Sampled CoT ——————————————–\nSampled Model: Claude2\nModel Solution Steps:\nStep 1: Check Choice A (67) When 67 is divided by 4, the remainder is 3, not 1. So 67 is not the\ncorrect answer.\nStep 2: Check Choice B (43) When 43 is divided by 4, the remainder is 1. When 43 is divided by 3,\nthe remainder is 1. When 43 is divided by 5, the remainder is 3, not 2. So 43 is not the correct answer.\nStep 3: Check Choice C (57) When 57 is divided by 4, the remainder is 1. When 57 is divided by 3,\nthe remainder is 0, not 1. So 57 is not the correct answer.\nStep 4: Check Choice D (37) When 37 is divided by 4, the remainder is 1. When 37 is divided by 3,\nthe remainder is 1. When 37 is divided by 5, the remainder is 2.\nStep 5: Choice D (37) satisfies all the requirements and is the correct answer.\nSolution: Choice D\nSolution: Choice D\n—————————————- Human Annotation —————————————\nModel Solution Correctness: incorrect\nModel Solution First Error Step: Step 2\nModel Solution Error Reason:\nThe model’s calculation is incorrect. In step 2, the model incorrectly calculated the remainder of 43\ndivided by 4 as 1 instead of 3. The model assumes that when 43 is divided by 4, the remainder is 1,\nbut it should be 3. There is a miscalculation of the remainder here.\nModel Solution Rectified First Error Step: Step 2: Check option B (43): When 43 is divided\nby 4, the remainder is 3, not 1. When 43 is divided by 3, the remainder is 1. When 43 is divided by\n5, the remainder is 3, not 2. Therefore, 43 is not the correct answer.\n————————————-Model Evaluation Result————————————\nEvaluated Model: gpt-4-turbo\nEvaluation Raw Response: Solution Analysis: Step 1: The step checks the remainder when 67 is\ndivided by 4 and finds that it yields a remainder of 3, not 1. This is a correct verification based on\nthe criteria specified in the question.\nStep 2: Checks the remainder of 43 when divided by 4, 3, and 5. The findings (1, 1, and 3 respectively) are verified correctly and match the expected remainders except for the one with 5, correctly\nidentifying that the condition is not met for division by 5.\nStep 3: The step calculates the remainders for 57 being divided by 4 and 3. The remainder when\ndivided by 3 does not match the specified conditions (expected was 1, result was 0). Hence, the claim\nthat 57 isn’t correct is valid.\nStep 4 and Step 5: Computes and verifies the remainders for 37 when divided by 4, 3, and 5, matching\nall the expected remainders accurately, and concludes that 37 meets all conditions specified in the\nproblem.\nThe answer selection of Choice D for 37 is based on accurate verification that fulfills the problem’s\nconstraints.\nSolution Correctness: correct\nFirst Error Step: N/A\nError Reason: N/A",
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "5975",
         "Yes",
         "Breakdown by field: Math, Medicine, Biology, Physics, Chemistry, Logic, Coding",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "Multiple choice, Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), Correlation (Matthew's correlation, Pearson's r), Define MR-Score, a weighted combination of matthews correlation, and two variants of accuracy",
         null,
         "Mostly from MMLU, some logic from LogiQA, some coding from MHPP.",
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Weighted Mean",
         "Yes",
         "Math, Medicine, Biology, Physics, Chemsitry, Logic, Coding",
         null,
         "https://huggingface.co/datasets/Randolphzeng/Mr-Ben",
         "MR-Ben",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "The metric is new and not very well motivated",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "People use chatGPT for checking their work all the time. ",
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         "Yes",
         "zengMRbenMetareasoningBenchmark2024",
         "Reasoning",
         null,
         null,
         "Reasoning - ",
         "['Another benchmark']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Convenience']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Multiple choice', 'Free response']",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "['Exact match', 'Correlation', 'Correlation']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No']",
         "False",
         "False",
         "False",
         "True",
         "['Realistic']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Complete']",
         "False",
         "True",
         "False",
         "False",
         "False",
         null
        ],
        [
         "445",
         "5/8/2025 14:34:36",
         "Karolina Korgul",
         "ramamurthyReinforcementLearningNot2023",
         "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",
         "Include",
         null,
         null,
         "The paper investigates the viability of reinforcement learning for language model alignment with human preferences. It introduces the RL4LMs library, the GRUE benchmark for RL evaluation on NLP tasks, and the NLPO algorithm, which improves stability and performance in LM training compared to previous methods like PPO",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "alignment, NLP, LLM as a Judge, reasoning",
         "Yes",
         "Aligning pre-trained large language models with human preferences through reinforcement learning methods.",
         "Subset",
         null,
         "As language generation problems where the model is given a language input (prompt) and needs to produce a target string, evaluated by reward functions rather than supervised target strings.",
         "Language input (task-specific prompt) and a corresponding target string or reference used for reward calculation.",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)",
         null,
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring), Distribution (perplexity, calibration, correlation), Correlation (Matthew's correlation, Pearson's r)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         null,
         null,
         null,
         "Simple Mean",
         "Yes",
         "Subscores for different aspects like fluency, sentiment, and task-specific metrics (e.g., BLEU, METEOR)",
         null,
         "https://github.com/allenai/RL4LMs",
         "GRUE - General Reinforced-language Understanding Evaluation",
         "Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "Yes",
         "Yes",
         "Authors compare the trends observed with automated metrics to human judgments and find a general correlation when the generated text is above a certain naturalness threshold. They also acknowledge instances where human feedback suggests potential reward hacking not detected by automated metrics.",
         "Mean and variance, standard deviations",
         "Outputs alone",
         "Partial real task (e.g. answering medical questions collected from real people), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         "The sizes of the train and validation splits vary depending on the specific task within the GRUE benchmark. For instance, IMDB has 25k training and 5k validation examples, while CNN/Daily Mail has 287k training and 13k validation examples.",
         "No",
         "ramamurthyReinforcementLearningNot2023",
         "Alignment",
         "Alignment",
         null,
         null,
         "['Author-crafted', 'Another benchmark']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Targeted']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Free response']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Exact match', 'Soft match', 'Human ratings', 'LLM-as-a-Judge', 'LLM post-processing', 'Distribution', 'Correlation']",
         "True",
         "True",
         "True",
         "True",
         "True",
         "True",
         "False",
         "True",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Partial', 'Constructed']",
         "False",
         "False",
         "True",
         "False",
         "True",
         "['Mean', 'Std']"
        ]
       ],
       "shape": {
        "columns": 128,
        "rows": 22
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>...</th>\n",
       "      <th>results_author_validity:</th>\n",
       "      <th>results_author_validity: Yes</th>\n",
       "      <th>results_author_validity: No</th>\n",
       "      <th>task_ecology_clean</th>\n",
       "      <th>task_ecology:</th>\n",
       "      <th>task_ecology: Complete</th>\n",
       "      <th>task_ecology: Partial</th>\n",
       "      <th>task_ecology: Representative</th>\n",
       "      <th>task_ecology: Constructed</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4/12/2025 18:03:29</td>\n",
       "      <td>Negar Foroutan</td>\n",
       "      <td>zhangMELAMultilingualEvaluation2024</td>\n",
       "      <td>MELA: Multilingual Evaluation of Linguistic Ac...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper intorduces a multilingual acceptabil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4/13/2025 20:33:00</td>\n",
       "      <td>Negar Foroutan</td>\n",
       "      <td>sunInformalLanguageProcessing2024</td>\n",
       "      <td>Toward Informal Language Processing: Knowledge...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Using movie subtitles, the authors construct a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4/13/2025 20:38:38</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>wangPretrainingLanguageModel2023</td>\n",
       "      <td>ON PRE-TRAINED LANGUAGE MODELS FOR ANTIBODY</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces the AnTibody Understandi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4/14/2025 10:35:13</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>xuPEERComprehensiveMultitask2022</td>\n",
       "      <td>PEER: A Comprehensive and Multi-Task Benchmark...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A benchmark called PEER (a\\ncomprehensive and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4/14/2025 21:02:00</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>hardalovBgGLUEBulgarianGeneral2023</td>\n",
       "      <td>bgGLUE: A Bulgarian General Language Understan...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bgGLUE (Bulgarian General Language\\nUnderstan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4/15/2025 14:35:04</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>berdicevskisSuperlimSwedishLanguage2023</td>\n",
       "      <td>Superlim: A Swedish Language Understanding Eva...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present Superlim, a multi-task NLP bench- m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4/16/2025 9:21:25</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>renBEACONBenchmarkComprehensive2024</td>\n",
       "      <td>BEACON: Benchmark for Comprehensive RNA Tasks ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Topic Exclusion (Is the paper about measuring ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper presents BEACON, the first comprehe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>4/17/2025 8:56:38</td>\n",
       "      <td>Anna Gausen</td>\n",
       "      <td>shenTaskBenchBenchmarkingLarge2024</td>\n",
       "      <td>TaskBench: Benchmarking Large Language Models ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TaskBench is a framework for evaluating how we...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>4/17/2025 17:21:15</td>\n",
       "      <td>Lujain Ibrahim</td>\n",
       "      <td>zhangMultiTrustComprehensiveBenchmark2024</td>\n",
       "      <td>MULTITRUST: A Comprehensive Benchmark Towards ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces a benchmark on the trustw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>4/17/2025 20:25:35</td>\n",
       "      <td>Angelika Romanou</td>\n",
       "      <td>chenCurriculumBroadcoverageBenchmark2022</td>\n",
       "      <td>Curriculum: A Broad-Coverage Benchmark for Lin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Current models do not provide insight into how...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>4/17/2025 21:24:02</td>\n",
       "      <td>Jan Batzner</td>\n",
       "      <td>ushioGenerativeLanguageModels2022</td>\n",
       "      <td>Generative Language Models for Paragraph-Level...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QG-Bench, a comprehensive benchmark for paragr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>4/17/2025 22:44:18</td>\n",
       "      <td>Yilun Zhao</td>\n",
       "      <td>chenMLLMasajudgeAssessingMultimodal2024</td>\n",
       "      <td>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces the MLLM-as-a-Judge benc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>4/18/2025 20:34:24</td>\n",
       "      <td>Jan Batzner</td>\n",
       "      <td>mackoMULTITuDELargescaleMultilingual2023</td>\n",
       "      <td>MULTITuDE: Large-Scale Multilingual Machine-Ge...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MULTITuDE, a benchmark dataset for multilingua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Tests]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>4/18/2025 23:53:19</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>fenogenovaMERAComprehensiveLLM2024</td>\n",
       "      <td>MERA: A Comprehensive LLM Evaluation in Russian</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper proposes a new instruction benchmark...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>4/19/2025 0:52:01</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>sunMeasuringEffectInfluential2023</td>\n",
       "      <td>Measuring the Effect of Influential Messages o...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The authors examine the task of predicting how...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>4/19/2025 20:14:57</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>liuWe`reAfraidLanguage2023</td>\n",
       "      <td>We're Afraid Language Models Aren't Modeling A...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the ability of LMs to hand...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>4/20/2025 17:13:53</td>\n",
       "      <td>Karolina Korgul</td>\n",
       "      <td>wangGTABenchmarkGeneral2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GTA is a benchmark designed to evaluate LLM-ba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>4/20/2025 21:33:41</td>\n",
       "      <td>Lujain Ibrahim</td>\n",
       "      <td>mireshghallahCanLLMsKeep2024</td>\n",
       "      <td>Can LLMs Keep a Secret? Testing Privacy Implic...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces a benchmark grounded in t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>4/22/2025 13:40:03</td>\n",
       "      <td>Karolina Korgul</td>\n",
       "      <td>maLargeLanguageModels2024</td>\n",
       "      <td>Large Language Models Play StarCraft II:Benchm...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TextStarCraft II turns the full StarCraft II v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>4/21/2025 6:57:30</td>\n",
       "      <td>Maria Grandury</td>\n",
       "      <td>leiterPrExMeLargeScale2024</td>\n",
       "      <td>PrExMe! Large Scale Prompt Exploration of Open...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces PrExMe, a benchmark of p...</td>\n",
       "      <td>They include emotion-CoT in prompt templates, ...</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>4/22/2025 17:42:02</td>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>zengMRbenMetareasoningBenchmark2024</td>\n",
       "      <td>MR-Ben: A Meta-Reasoning Benchmark for Evaluat...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset of question,answer pairs in which answ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>5/8/2025 14:34:36</td>\n",
       "      <td>Karolina Korgul</td>\n",
       "      <td>ramamurthyReinforcementLearningNot2023</td>\n",
       "      <td>Is Reinforcement Learning (Not) for Natural La...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper investigates the viability of reinfo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Timestamp        main_coder  \\\n",
       "22   4/12/2025 18:03:29    Negar Foroutan   \n",
       "34   4/13/2025 20:33:00    Negar Foroutan   \n",
       "35   4/13/2025 20:38:38    Anna Sotnikova   \n",
       "60   4/14/2025 10:35:13    Anna Sotnikova   \n",
       "71   4/14/2025 21:02:00    Anna Sotnikova   \n",
       "85   4/15/2025 14:35:04    Anna Sotnikova   \n",
       "127   4/16/2025 9:21:25    Anna Sotnikova   \n",
       "186   4/17/2025 8:56:38       Anna Gausen   \n",
       "207  4/17/2025 17:21:15    Lujain Ibrahim   \n",
       "229  4/17/2025 20:25:35  Angelika Romanou   \n",
       "233  4/17/2025 21:24:02       Jan Batzner   \n",
       "247  4/17/2025 22:44:18        Yilun Zhao   \n",
       "301  4/18/2025 20:34:24       Jan Batzner   \n",
       "314  4/18/2025 23:53:19  Valentin Hoffman   \n",
       "323   4/19/2025 0:52:01  Valentin Hoffman   \n",
       "333  4/19/2025 20:14:57  Valentin Hoffman   \n",
       "342  4/20/2025 17:13:53   Karolina Korgul   \n",
       "345  4/20/2025 21:33:41    Lujain Ibrahim   \n",
       "363  4/22/2025 13:40:03   Karolina Korgul   \n",
       "385   4/21/2025 6:57:30    Maria Grandury   \n",
       "424  4/22/2025 17:42:02       Thom Foster   \n",
       "445   5/8/2025 14:34:36   Karolina Korgul   \n",
       "\n",
       "                                        bibkey  \\\n",
       "22         zhangMELAMultilingualEvaluation2024   \n",
       "34           sunInformalLanguageProcessing2024   \n",
       "35            wangPretrainingLanguageModel2023   \n",
       "60            xuPEERComprehensiveMultitask2022   \n",
       "71          hardalovBgGLUEBulgarianGeneral2023   \n",
       "85     berdicevskisSuperlimSwedishLanguage2023   \n",
       "127        renBEACONBenchmarkComprehensive2024   \n",
       "186         shenTaskBenchBenchmarkingLarge2024   \n",
       "207  zhangMultiTrustComprehensiveBenchmark2024   \n",
       "229   chenCurriculumBroadcoverageBenchmark2022   \n",
       "233          ushioGenerativeLanguageModels2022   \n",
       "247    chenMLLMasajudgeAssessingMultimodal2024   \n",
       "301   mackoMULTITuDELargescaleMultilingual2023   \n",
       "314         fenogenovaMERAComprehensiveLLM2024   \n",
       "323          sunMeasuringEffectInfluential2023   \n",
       "333                 liuWe`reAfraidLanguage2023   \n",
       "342                wangGTABenchmarkGeneral2024   \n",
       "345               mireshghallahCanLLMsKeep2024   \n",
       "363                  maLargeLanguageModels2024   \n",
       "385                 leiterPrExMeLargeScale2024   \n",
       "424        zengMRbenMetareasoningBenchmark2024   \n",
       "445     ramamurthyReinforcementLearningNot2023   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "22   MELA: Multilingual Evaluation of Linguistic Ac...   Include   \n",
       "34   Toward Informal Language Processing: Knowledge...   Include   \n",
       "35         ON PRE-TRAINED LANGUAGE MODELS FOR ANTIBODY   Include   \n",
       "60   PEER: A Comprehensive and Multi-Task Benchmark...   Include   \n",
       "71   bgGLUE: A Bulgarian General Language Understan...   Include   \n",
       "85   Superlim: A Swedish Language Understanding Eva...   Include   \n",
       "127  BEACON: Benchmark for Comprehensive RNA Tasks ...   Include   \n",
       "186  TaskBench: Benchmarking Large Language Models ...   Include   \n",
       "207  MULTITRUST: A Comprehensive Benchmark Towards ...   Include   \n",
       "229  Curriculum: A Broad-Coverage Benchmark for Lin...   Include   \n",
       "233  Generative Language Models for Paragraph-Level...   Include   \n",
       "247  MLLM-as-a-Judge: Assessing Multimodal LLM-as-a...   Include   \n",
       "301  MULTITuDE: Large-Scale Multilingual Machine-Ge...   Include   \n",
       "314    MERA: A Comprehensive LLM Evaluation in Russian   Include   \n",
       "323  Measuring the Effect of Influential Messages o...   Include   \n",
       "333  We're Afraid Language Models Aren't Modeling A...   Include   \n",
       "342                                                NaN   Include   \n",
       "345  Can LLMs Keep a Secret? Testing Privacy Implic...   Include   \n",
       "363  Large Language Models Play StarCraft II:Benchm...   Include   \n",
       "385  PrExMe! Large Scale Prompt Exploration of Open...   Include   \n",
       "424  MR-Ben: A Meta-Reasoning Benchmark for Evaluat...   Include   \n",
       "445  Is Reinforcement Learning (Not) for Natural La...   Include   \n",
       "\n",
       "                                    exclusion_criteria  \\\n",
       "22                                                 NaN   \n",
       "34                                                 NaN   \n",
       "35                                                 NaN   \n",
       "60                                                 NaN   \n",
       "71                                                 NaN   \n",
       "85                                                 NaN   \n",
       "127  Topic Exclusion (Is the paper about measuring ...   \n",
       "186                                                NaN   \n",
       "207                                                NaN   \n",
       "229                                                NaN   \n",
       "233                                                NaN   \n",
       "247                                                NaN   \n",
       "301                                                NaN   \n",
       "314                                                NaN   \n",
       "323                                                NaN   \n",
       "333                                                NaN   \n",
       "342                                                NaN   \n",
       "345                                                NaN   \n",
       "363                                                NaN   \n",
       "385                                                NaN   \n",
       "424                                                NaN   \n",
       "445                                                NaN   \n",
       "\n",
       "    exclusion_criteria_detail  \\\n",
       "22                        NaN   \n",
       "34                        NaN   \n",
       "35                        NaN   \n",
       "60                        NaN   \n",
       "71                        NaN   \n",
       "85                        NaN   \n",
       "127                       NaN   \n",
       "186                       NaN   \n",
       "207                       NaN   \n",
       "229                       NaN   \n",
       "233                       NaN   \n",
       "247                       NaN   \n",
       "301                       NaN   \n",
       "314                       NaN   \n",
       "323                       NaN   \n",
       "333                       NaN   \n",
       "342                       NaN   \n",
       "345                       NaN   \n",
       "363                       NaN   \n",
       "385                       NaN   \n",
       "424                       NaN   \n",
       "445                       NaN   \n",
       "\n",
       "                                         short_summary  \\\n",
       "22   The paper intorduces a multilingual acceptabil...   \n",
       "34   Using movie subtitles, the authors construct a...   \n",
       "35   This paper introduces the AnTibody Understandi...   \n",
       "60   A benchmark called PEER (a\\ncomprehensive and ...   \n",
       "71    bgGLUE (Bulgarian General Language\\nUnderstan...   \n",
       "85   We present Superlim, a multi-task NLP bench- m...   \n",
       "127  This paper presents BEACON, the first comprehe...   \n",
       "186  TaskBench is a framework for evaluating how we...   \n",
       "207  The paper introduces a benchmark on the trustw...   \n",
       "229  Current models do not provide insight into how...   \n",
       "233  QG-Bench, a comprehensive benchmark for paragr...   \n",
       "247  This paper introduces the MLLM-as-a-Judge benc...   \n",
       "301  MULTITuDE, a benchmark dataset for multilingua...   \n",
       "314  The paper proposes a new instruction benchmark...   \n",
       "323  The authors examine the task of predicting how...   \n",
       "333  This paper examines the ability of LMs to hand...   \n",
       "342  GTA is a benchmark designed to evaluate LLM-ba...   \n",
       "345  The paper introduces a benchmark grounded in t...   \n",
       "363  TextStarCraft II turns the full StarCraft II v...   \n",
       "385  This paper introduces PrExMe, a benchmark of p...   \n",
       "424  Dataset of question,answer pairs in which answ...   \n",
       "445  The paper investigates the viability of reinfo...   \n",
       "\n",
       "                                          contribution  \\\n",
       "22                                                 NaN   \n",
       "34                                                 NaN   \n",
       "35                                                 NaN   \n",
       "60                                                 NaN   \n",
       "71                                                 NaN   \n",
       "85                                                 NaN   \n",
       "127                                                NaN   \n",
       "186                                                NaN   \n",
       "207                                                NaN   \n",
       "229                                                NaN   \n",
       "233                                                NaN   \n",
       "247                                                NaN   \n",
       "301                                                NaN   \n",
       "314                                                NaN   \n",
       "323                                                NaN   \n",
       "333                                                NaN   \n",
       "342                                                NaN   \n",
       "345                                                NaN   \n",
       "363                                                NaN   \n",
       "385  They include emotion-CoT in prompt templates, ...   \n",
       "424                                                NaN   \n",
       "445                                                NaN   \n",
       "\n",
       "                                      phenomenon_short  ...  \\\n",
       "22   General Capability (A broadly useful ability, ...  ...   \n",
       "34   Specific Application (A single use case, where...  ...   \n",
       "35   Specific Application (A single use case, where...  ...   \n",
       "60   Specific Application (A single use case, where...  ...   \n",
       "71   Specific Application (A single use case, where...  ...   \n",
       "85   General Capability (A broadly useful ability, ...  ...   \n",
       "127  Specific Application (A single use case, where...  ...   \n",
       "186  General Capability (A broadly useful ability, ...  ...   \n",
       "207  General Capability (A broadly useful ability, ...  ...   \n",
       "229  General Capability (A broadly useful ability, ...  ...   \n",
       "233  Specific Application (A single use case, where...  ...   \n",
       "247  General Capability (A broadly useful ability, ...  ...   \n",
       "301  Specific Application (A single use case, where...  ...   \n",
       "314  General Capability (A broadly useful ability, ...  ...   \n",
       "323  Specific Application (A single use case, where...  ...   \n",
       "333  General Capability (A broadly useful ability, ...  ...   \n",
       "342  General Capability (A broadly useful ability, ...  ...   \n",
       "345  General Capability (A broadly useful ability, ...  ...   \n",
       "363  General Capability (A broadly useful ability, ...  ...   \n",
       "385  Specific Application (A single use case, where...  ...   \n",
       "424  General Capability (A broadly useful ability, ...  ...   \n",
       "445  General Capability (A broadly useful ability, ...  ...   \n",
       "\n",
       "    results_author_validity:  results_author_validity: Yes  \\\n",
       "22                      False                        False   \n",
       "34                      False                        False   \n",
       "35                      False                        False   \n",
       "60                      False                        False   \n",
       "71                      False                        False   \n",
       "85                      False                        False   \n",
       "127                     False                        False   \n",
       "186                     False                         True   \n",
       "207                     False                        False   \n",
       "229                     False                        False   \n",
       "233                     False                         True   \n",
       "247                     False                         True   \n",
       "301                     False                        False   \n",
       "314                     False                        False   \n",
       "323                     False                        False   \n",
       "333                     False                        False   \n",
       "342                     False                         True   \n",
       "345                     False                         True   \n",
       "363                     False                         True   \n",
       "385                     False                         True   \n",
       "424                     False                        False   \n",
       "445                     False                         True   \n",
       "\n",
       "    results_author_validity: No                      task_ecology_clean  \\\n",
       "22                         True                           [Constructed]   \n",
       "34                         True                           [Constructed]   \n",
       "35                         True                               [Partial]   \n",
       "60                         True                               [Partial]   \n",
       "71                         True  [Partial, Representative, Constructed]   \n",
       "85                         True  [Partial, Representative, Constructed]   \n",
       "127                        True                  [Partial, Constructed]   \n",
       "186                       False               [Partial, Representative]   \n",
       "207                        True               [Partial, Representative]   \n",
       "229                        True                           [Constructed]   \n",
       "233                       False                        [Representative]   \n",
       "247                       False                  [Partial, Constructed]   \n",
       "301                        True                        [Representative]   \n",
       "314                        True           [Representative, Constructed]   \n",
       "323                        True                           [Constructed]   \n",
       "333                        True                           [Constructed]   \n",
       "342                       False           [Representative, Constructed]   \n",
       "345                       False                  [Partial, Constructed]   \n",
       "363                       False                        [Representative]   \n",
       "385                       False                  [Partial, Constructed]   \n",
       "424                        True                              [Complete]   \n",
       "445                       False                  [Partial, Constructed]   \n",
       "\n",
       "    task_ecology:  task_ecology: Complete task_ecology: Partial  \\\n",
       "22           False                  False                 False   \n",
       "34           False                  False                 False   \n",
       "35           False                  False                  True   \n",
       "60           False                  False                  True   \n",
       "71           False                  False                  True   \n",
       "85           False                  False                  True   \n",
       "127          False                  False                  True   \n",
       "186          False                  False                  True   \n",
       "207          False                  False                  True   \n",
       "229          False                  False                 False   \n",
       "233          False                  False                 False   \n",
       "247          False                  False                  True   \n",
       "301          False                  False                 False   \n",
       "314          False                  False                 False   \n",
       "323          False                  False                 False   \n",
       "333          False                  False                 False   \n",
       "342          False                  False                 False   \n",
       "345          False                  False                  True   \n",
       "363          False                  False                 False   \n",
       "385          False                  False                  True   \n",
       "424          False                   True                 False   \n",
       "445          False                  False                  True   \n",
       "\n",
       "    task_ecology: Representative task_ecology: Constructed  \\\n",
       "22                         False                      True   \n",
       "34                         False                      True   \n",
       "35                         False                     False   \n",
       "60                         False                     False   \n",
       "71                          True                      True   \n",
       "85                          True                      True   \n",
       "127                        False                      True   \n",
       "186                         True                     False   \n",
       "207                         True                     False   \n",
       "229                        False                      True   \n",
       "233                         True                     False   \n",
       "247                        False                      True   \n",
       "301                         True                     False   \n",
       "314                         True                      True   \n",
       "323                        False                      True   \n",
       "333                        False                      True   \n",
       "342                         True                      True   \n",
       "345                        False                      True   \n",
       "363                         True                     False   \n",
       "385                        False                      True   \n",
       "424                        False                     False   \n",
       "445                        False                      True   \n",
       "\n",
       "    metric_statistics_clean  \n",
       "22              [Mean, Std]  \n",
       "34                   [Mean]  \n",
       "35                      NaN  \n",
       "60              [Mean, Std]  \n",
       "71                   [Mean]  \n",
       "85              [Mean, Std]  \n",
       "127             [Mean, Std]  \n",
       "186                  [Mean]  \n",
       "207           [Mean, Other]  \n",
       "229                  [Mean]  \n",
       "233           [Mean, Other]  \n",
       "247                     NaN  \n",
       "301           [Mean, Tests]  \n",
       "314                  [Mean]  \n",
       "323                     NaN  \n",
       "333                  [Mean]  \n",
       "342           [Mean, Other]  \n",
       "345                  [Mean]  \n",
       "363             [Mean, Std]  \n",
       "385                  [Mean]  \n",
       "424                     NaN  \n",
       "445             [Mean, Std]  \n",
       "\n",
       "[22 rows x 128 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[(included_df['metric_definition: Correlation']) & (included_df['phenomenon_taxonomy_root']!='Language Modelling')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aad7ed41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root\n",
       "NLP                      91\n",
       "Reasoning                84\n",
       "Agents                   40\n",
       "Alignment                37\n",
       "Language Modelling       36\n",
       "Code Generation          26\n",
       "VQA                      15\n",
       "Medicine                 15\n",
       "Knowledge                13\n",
       "Retrieval                13\n",
       "Grounding                11\n",
       "User Interaction         10\n",
       "Law                       9\n",
       "Multilinguality           8\n",
       "Instruction Following     8\n",
       "Biology                   7\n",
       "Theory of Mind            5\n",
       "General Science           5\n",
       "Psychology                3\n",
       "Finance                   3\n",
       "Factuality                3\n",
       "LLM as a Judge            3\n",
       "Data Analysis             2\n",
       "General Purpose           2\n",
       "Mental Health             1\n",
       "Business                  1\n",
       "History                   1\n",
       "Chemistry                 1\n",
       "Education                 1\n",
       "Sports                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['phenomenon_taxonomy_root'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20ac77",
   "metadata": {},
   "source": [
    "### Saving Clean Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9035707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 128)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05eab529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['bibkey'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28a96721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp',\n",
       " 'main_coder',\n",
       " 'bibkey',\n",
       " 'title',\n",
       " 'inclusion',\n",
       " 'exclusion_criteria',\n",
       " 'exclusion_criteria_detail',\n",
       " 'short_summary',\n",
       " 'contribution',\n",
       " 'phenomenon_short',\n",
       " 'target_phenomenon',\n",
       " 'phenomenon_defined',\n",
       " 'phenomenon_definition',\n",
       " 'definition_scope',\n",
       " 'purpose_extra',\n",
       " 'task_definition',\n",
       " 'task_item_definition',\n",
       " 'task_definition_detail',\n",
       " 'task_source',\n",
       " 'task_dataset_size',\n",
       " 'task_dataset_metadata',\n",
       " 'dataset_metadata_detail',\n",
       " 'dataset_sampling_method',\n",
       " 'response_format',\n",
       " 'metric_definition',\n",
       " 'metric_definition_detail',\n",
       " 'task_source_detail',\n",
       " 'authorship',\n",
       " 'benchmark_availability',\n",
       " 'procedural_extra',\n",
       " 'notes_extra',\n",
       " 'task_train_val',\n",
       " 'task_dataset_size_extra',\n",
       " 'response_format_detail',\n",
       " 'metric_aggregation',\n",
       " 'metric_subscores',\n",
       " 'metric_subscores_detail',\n",
       " 'metric_metascoring',\n",
       " 'benchmark_location',\n",
       " 'benchmark',\n",
       " 'phenomenon_contested',\n",
       " 'task_face_validity',\n",
       " 'metric_face_validity',\n",
       " 'result_interpretation',\n",
       " 'results_comparison',\n",
       " 'results_comparison_explanation',\n",
       " 'results_realism',\n",
       " 'results_human_baseline',\n",
       " 'results_author_validity',\n",
       " 'results_author_validity_detail',\n",
       " 'metric_statistics',\n",
       " 'metric_access',\n",
       " 'task_ecology',\n",
       " 'task_ecology_detail',\n",
       " 'definition_integrity',\n",
       " 'definition_integrity_detail',\n",
       " 'task_dataset_size_detail',\n",
       " 'metric_fewshot',\n",
       " 'new_bibkey',\n",
       " 'phenomenon_taxonomy_root',\n",
       " 'phenomenon_taxonomy_leaf',\n",
       " 'phenomenon_taxonomy_alternate',\n",
       " 'validate_taxonomy',\n",
       " 'task_source_clean',\n",
       " 'task_source: Author-crafted',\n",
       " 'task_source: Crowd-sourced',\n",
       " 'task_source: Unknown',\n",
       " 'task_source: Procedurally-generated',\n",
       " 'task_source: Expert-crafted',\n",
       " 'task_source: Another benchmark',\n",
       " 'task_source: LLM-generated',\n",
       " 'task_source: Human exams',\n",
       " 'task_source: Real task',\n",
       " 'dataset_sampling_method_clean',\n",
       " 'dataset_sampling_method: Targeted',\n",
       " 'dataset_sampling_method: Criterion',\n",
       " 'dataset_sampling_method: Convenience',\n",
       " 'dataset_sampling_method: Random',\n",
       " 'dataset_sampling_method: Unknown',\n",
       " 'response_format_clean',\n",
       " 'response_format: Structured',\n",
       " 'response_format: Interaction',\n",
       " 'response_format: Multiple choice',\n",
       " 'response_format: Short free response',\n",
       " 'response_format: Free response',\n",
       " 'response_format: Logits',\n",
       " 'response_format: Unknown',\n",
       " 'metric_definition_clean',\n",
       " 'metric_definition: Exact match',\n",
       " 'metric_definition: Human ratings',\n",
       " 'metric_definition: LLM-as-a-Judge',\n",
       " 'metric_definition: LLM post-processing',\n",
       " 'metric_definition: Distribution',\n",
       " 'metric_definition: Correlation',\n",
       " 'metric_definition: Reward',\n",
       " 'metric_definition: Soft match',\n",
       " 'metric_definition: Unknown',\n",
       " 'phenomenon_contested_clean',\n",
       " 'phenomenon_contested: Widely-agreed',\n",
       " 'phenomenon_contested: No definition',\n",
       " 'phenomenon_contested: Contested',\n",
       " 'task_face_validity_clean',\n",
       " 'task_face_validity: Partially',\n",
       " 'task_face_validity: ',\n",
       " 'task_face_validity: Yes',\n",
       " 'task_face_validity: No',\n",
       " 'metric_face_validity_clean',\n",
       " 'metric_face_validity: Partially',\n",
       " 'metric_face_validity: ',\n",
       " 'metric_face_validity: Yes',\n",
       " 'metric_face_validity: No',\n",
       " 'results_realism_clean',\n",
       " 'results_realism: No comparison made',\n",
       " 'results_realism: Not possible',\n",
       " 'results_realism: Realistic',\n",
       " 'results_realism: Comparison made',\n",
       " 'results_realism: No',\n",
       " 'results_author_validity_clean',\n",
       " 'results_author_validity: ',\n",
       " 'results_author_validity: Yes',\n",
       " 'results_author_validity: No',\n",
       " 'task_ecology_clean',\n",
       " 'task_ecology: ',\n",
       " 'task_ecology: Complete',\n",
       " 'task_ecology: Partial',\n",
       " 'task_ecology: Representative',\n",
       " 'task_ecology: Constructed',\n",
       " 'metric_statistics_clean']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(included_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b038fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#included_df[['main_coder','bibkey','title','inclusion','short_summary','phenomenon_taxonomy_root', 'phenomenon_taxonomy_leaf', 'phenomenon_taxonomy_alternate','task_source','task_source_clean','dataset_sampling_method','dataset_sampling_method_clean','response_format','response_format_clean','metric_definition','metric_definition_clean','metric_statistics','metric_statistics_clean']].to_csv('../data/results_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8008025d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "main_coder",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "bibkey",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "inclusion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "short_summary",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_taxonomy_root",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_taxonomy_leaf",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_taxonomy_alternate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_sampling_method",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_sampling_method_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "response_format",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "response_format_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_definition_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_statistics",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_statistics_clean",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "97268e0c-a46f-451b-bebb-d71e34d7197d",
       "rows": [
        [
         "0",
         "Harry Mayne",
         "mundlerSWTBenchTestingValidating2024",
         "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents",
         "Include",
         "A benchmark for generating code tests (unit tests) from natural language user GitHub issues.",
         "Agents",
         "Coding",
         null,
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)",
         "['Real task', 'Another benchmark']",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "['Criterion']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "Whether the faulty code fails on the test and the gold-standard code passes it.",
         "['Reward']",
         "simple mean",
         "['Mean']"
        ],
        [
         "1",
         "Jonathan Rystrøm",
         "davidsonEvaluatingLanguageModel2024",
         "EVALUATING LANGUAGE MODEL AGENCY THROUGH\nNEGOTIATIONS",
         "Include",
         "The paper introduces a dynamic framework for evaluating LLMs using negotiation games in self-play and cross-play settings. They find that only closed-source models are able to successfully complete the task and that stronger LLMs don't always win over weaker opponents.",
         "Alignment",
         "Alignment",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",
         "['Author-crafted']",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "['Targeted']",
         "Extended interaction (e.g. conversation, calling an API and processing the response)",
         "['Interaction']",
         "Exact Match (accuracy, F1, precision, recall), Number of rounds completted",
         "['Exact match', 'Reward']",
         "mean with variance",
         "['Mean', 'Std']"
        ],
        [
         "2",
         "Lennart Luettgau",
         "helweMAFALDABenchmarkComprehensive2024",
         "MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification",
         "Include",
         "The paper introduces MAFALD, a benchmark that provides a unified classification of fallacies and provides a taxonomy. It features manually annotated data with explanations, a tailored annotation scheme, and an evaluation method for subjective NLP tasks. Various language models and human performance are evaluated on fallacy detection and classification in a zero-shot learning setting.",
         "Reasoning",
         "Logical",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)",
         "['Author-crafted', 'Crowd-sourced', 'Another benchmark']",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)",
         "['Convenience', 'Targeted']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "simple mean/sum",
         "['Mean']"
        ],
        [
         "3",
         "Kaili Liu",
         "niuRAGTruthHallucinationCorpus2024",
         "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
         "Include",
         "This paper targets word-level hallucinations in various tasks and domains in the RAG setting. It presents approximately 18,000 responses generated using RAG from diverse LLMs which are annotated at the word level for hallucination intensity. Hallucination frequencies are benchmarked across various LLMs, and hallucination detection methods are assessed versus a small LLM fine-tuned using the proposed dataset, RAGTruth.",
         "Retrieval",
         null,
         "Factuality",
         "Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['Real task', 'Crowd-sourced', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']",
         "Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)",
         "['Random', 'Targeted']",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)",
         "['Short free response', 'Free response', 'Structured']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         null,
         null
        ],
        [
         "4",
         "Anna Gausen",
         "wangIELMOpenInformation2022",
         "IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models\n",
         "Include",
         "They introduce a new open information extraction (OIE) benchmark designed to evaluate the relational knowledge stored in pre-trained language models (LMs) such as BERT and GPT (published in 2022). Their method involves transforming these pre-trained LMs into zero-shot OIE systems to assess their performance on both existing and novel factual OIE datasets. Their results show that pre-trained LMs achieve competitive performance, even surpassing state-of-the-art supervised OIE methods on certain datasets without any additional training data.",
         "NLP",
         "Extraction",
         null,
         "Crowd-sourced task examples (e.g. Prolific-created tasks), Based on knowledge graphs (KG) e.g. Wikidata",
         "['Crowd-sourced', 'Procedurally-generated']",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Convenience']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "The authors carry out some error analysis: \"We argue that we are measuring a lower bound for what LMs know. To further understand the shortcomings of the current method, we conduct an error analysis of the errors in precision on all datasets. We choose BERTLARGE for the study. We sample 100 documents from the Wikidata-OIE dataset, and manually check the reasons for the errors\"",
         "['Other']"
        ],
        [
         "5",
         "Jan Batzner",
         "heTGEAErrorAnnotatedDataset2021",
         "TGEA: An Error-Annotated Dataset and Benchmark Tasks for Text Generation from Pretrained Language Models",
         "Include",
         "TGEA (Text Generation Error Annotation) is an error-annotated dataset with multiple benchmark tasks for text generation. Following the authors hierachical error taxonomy, crowdsourced workers manually labeled 12k erroneous sentences with semantic information, including error types, associated text spans, error corrections and rationals behind errors.",
         "Factuality",
         null,
         null,
         "LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['LLM-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Targeted', 'Criterion']",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragarph)",
         "['Short free response', 'Free response']",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Distribution (perplexity, calibration, correlation)",
         "['Exact match', 'Soft match', 'Distribution']",
         "Simple means for performance metrics; agreement percentages and Cohen's Kappa for annotation reliability.",
         "['Mean', 'Other']"
        ],
        [
         "6",
         "Lujain Ibrahim",
         "huangCEvalMultiLevelMultiDiscipline2023",
         "C-EVAL: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
         "Include",
         "The paper introduces C-EVAL evaluation suite for assessing advanced knowledge and reasoning abilities of foundation models in Chinese, It spans four difficulty levels and 52 disciplines. It also introduces C-EVAL HARD a subset of challenging subjects that require advanced reasoning.",
         "Knowledge",
         "Cultural",
         null,
         "Human exam questions (e.g. GRE questions)",
         "['Human exams']",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Convenience']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "simple mean",
         "['Mean']"
        ],
        [
         "7",
         "Anna Sotnikova",
         "myungBLEnDBenchmarkLLMs2024",
         "BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages",
         "Include",
         "The paper introduces BLEND, a novel benchmark comprising hand-crafted question-answer pairs designed to evaluate LLMs on everyday cultural knowledge across 16 countries/regions and 13 languages, including low-resource ones. It demonstrates significant performance disparities among models, showing cultural and linguistic biases, especially in underrepresented regions.",
         "Knowledge",
         "Cultural",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Author-crafted', 'Crowd-sourced', 'Procedurally-generated']",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Convenience', 'Targeted', 'Criterion']",
         "Multiple choice, Short free response (e.g. single word or number)",
         "['Multiple choice', 'Short free response']",
         "Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)",
         "['Exact match', 'LLM post-processing']",
         "simple mean, Anova for p-values, Tukey-HSD",
         "['Mean', 'Tests']"
        ],
        [
         "8",
         "Karolina Korgul",
         "yaoWebShopScalableRealWorld2022",
         "WebShop: Towards Scalable Real-World Web\nInteraction with Grounded Language Agents",
         "Include",
         "The paper introduces WebShop, a simulated online shopping environment where agents try to follow natural language instructions to find and buy the right products. WebShop benchmark is designed to test how well agents can search, navigate, and make decisions on the web. The authors train models using imitation and reinforcement learning, and show that the best ones can even handle similar tasks on real sites like Amazon and eBay.",
         "Agents",
         "Web",
         null,
         "Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks)",
         "['Real task', 'Crowd-sourced']",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Convenience', 'Targeted', 'Criterion']",
         "Multiple choice, Free response (e.g. summary paragarph), Extended interaction (e.g. conversation, calling an API and processing the response)",
         "['Multiple choice', 'Free response', 'Interaction']",
         "reward is computed based on the final product chosen by the agent, compared against known attributes, options, and price of the target product.",
         "['Reward']",
         "The authors report average task score and success rate across trials. They also include standard deviation/error bars in some result plots (e.g. Figure 4), mainly to show the variation across multiple runs.",
         "['Mean', 'Std']"
        ],
        [
         "9",
         "Cornelius Emde",
         "sanyalRobustLRDiagnosticBenchmark2022",
         "ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of\nDeductive Reasoners",
         "Include",
         "Deductive reasoning is an important skill that modern language models should possess. However, small logical perturbations of deductive reasoning problems can lead to inconsistent model responses. To test this consistency, the paper introduces RobustLR a benchmark consisting of logical problems (\"theories\") and variations thereof that should be consistenly answered correctly by models.",
         "Reasoning",
         "Logical",
         "Robustness",
         "Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Procedurally-generated']",
         "Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Random', 'Convenience']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "mean of weighted-F1 scores",
         "['Mean']"
        ],
        [
         "10",
         "Hannah Kirk",
         "albalakFETABenchmarkFewSample2022",
         "FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue",
         "Include",
         "Examines few-sample task transfer across 17 subtasks (e.g., utterance-level classification, dialogue-level classification, span extraction, multiple-choice) in open-domain dialogue with diverse properties (dyadic vs. multi-party, anonymized vs. recurring speaker, varying dialogue lengths).",
         "Language Modelling",
         "Adaptability",
         null,
         "Modified from another benchmark (e.g. translation into another language), Human TV show; Human chitchat dialogues",
         "['Another benchmark', 'Author-crafted']",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Convenience']",
         "Depends on the subtask category (Utterance Classification, Dialogue Classification, Multiple Choice, Span Extraction)",
         "['Short free response']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "Mean, and they they show a delta (for change in aggregate sources across all tasks). It is unclear if this is a range or a standard deviation. I think it's a range.",
         "['Mean']"
        ],
        [
         "11",
         "Hazel Kim",
         "beanLINGOLYBenchmarkOlympiadLevel2024",
         "LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages",
         "Include",
         "The paper introduces LINGOLY, a new benchmark built on Linguistics Olympiad puzzles in low-resource and extinct languages to test genuine reasoning capabilities in LLMs. The benchmark is crafted covering diverse reasoning complexity, linguistic subject areas, instruction types, and high/low resources. The paper uncovers error pattenrs between high and low resource settings and show the ongoing challenges in multi-step, out-of-domain reasoning.",
         "Reasoning",
         "Logical",
         null,
         "Human exam questions (e.g. GRE questions), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",
         "['Human exams', 'Author-crafted']",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Convenience']",
         "Multiple choice, Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)",
         "['Multiple choice', 'Short free response', 'Structured']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "The authors use a weighted mean in calculating an approximate human performance threshold but not for model performance. They take a weighted average of the annual medal thresholds for ‘Advanced’ problems. ",
         "['Mean']"
        ],
        [
         "12",
         "Negar Foroutan",
         "nasirGameTraversalBenchmarkEvaluatingPlanning2024",
         "GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps",
         "Include",
         "The paper investigates the planning capabilities of LLMs by proposing GameTraversalBenchmark (GTB), a benchmark consisting of diverse 2D grid-based game maps. The paper also provide metrics to give insights towards planning abilities in LLMs.",
         "Reasoning",
         "Planning",
         null,
         "LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['LLM-generated']",
         "Unknown",
         "['Unknown']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "Exact Match (accuracy, F1, precision, recall), The paper defines a reward score",
         "['Exact match', 'Reward']",
         "simple mean and STD",
         "['Mean', 'Std']"
        ],
        [
         "13",
         "Chris Schmitz",
         "feiLawBenchBenchmarkingLegal2024",
         "LawBench: Benchmarking Legal Knowledge of Large Language Models",
         "Include",
         "LawBench tests 21 models on 20 Chinese legal tasks (500 instances each), which are classified along Bloom's taxonomy into knowledge memorization, understanding, and application. It is the first benchmark for the Chinese legal domain, and the first for civil law (vs. common law) jurisdictions.",
         "Law",
         null,
         null,
         "Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['Real task', 'Author-crafted', 'Another benchmark', 'LLM-generated']",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Convenience', 'Targeted', 'Criterion']",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)",
         "['Multiple choice', 'Short free response', 'Free response']",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)",
         "['Exact match', 'Soft match', 'LLM post-processing']",
         "Simple means and macro-averaging (mean across tasks, which is identical here because each task has same # of instances)",
         "['Mean']"
        ],
        [
         "14",
         "Angelika Romanou",
         "yuksekgonulWhenWhyVisionlanguage2023",
         "When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?",
         "Include",
         "This paper creates the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. They demonstrate that VLMs can perform well on image-text retrieval over existing datasets without using the composition and order information.",
         "Reasoning",
         "Compositional",
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "['Another benchmark']",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "['Criterion']",
         "Multiple choice, Short free response (e.g. single word or number)",
         "['Multiple choice', 'Short free response']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "macro-accuracy",
         "['Mean']"
        ],
        [
         "15",
         "Jonathan Rystrøm",
         "xieWhodunitBenchEvaluatingLarge2024",
         "WhodunitBench: Evaluating Large Multimodal\nAgents via Murder Mystery Games",
         "Include",
         "The paper evaluates LLMs ability to participate in (and answers questions about) murder mystery games. In the arena component (agents play as either detective or murderer in a multi-agent setting), the agents are tested on win rate against the other models. The QA component is split based on capability categories (Perception, Role-Play, Decision-making and Cognition)",
         "Agents",
         null,
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks)",
         "['Author-crafted', 'Crowd-sourced']",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Convenience']",
         "Multiple choice, Extended interaction (e.g. conversation, calling an API and processing the response)",
         "['Multiple choice', 'Interaction']",
         "Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Win rate",
         "['Exact match', 'LLM-as-a-Judge', 'Reward']",
         "Simple mean (no variance or standard reported)",
         "['Mean']"
        ],
        [
         "16",
         "Lennart Luettgau",
         "saparinaAMBROSIABenchmarkParsing2024",
         "AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries",
         "Include",
         "Paper introduces a new benchmark dataset designed to evaluate text-to-SQL parsers' ability to handle ambiguous user requests. The dataset includes questions demonstrating scope ambiguity, attachment ambiguity, and vagueness, along with their interpretations and corresponding SQL queries. The authors highlight that existing large language models (LLMs) struggle with these ambiguities, suggesting a need for improved parser development.",
         "Code Generation",
         "Natural Language",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['Author-crafted', 'LLM-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "['Targeted']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics)",
         "['Exact match', 'Human ratings']",
         "mean and variance",
         "['Mean', 'Std']"
        ],
        [
         "17",
         "Anna Sotnikova",
         "augustyniakThisWayDesigning2022",
         "This is the way: designing and compiling\nLEPISZCZE, a comprehensive NLP benchmark for\nPolish",
         "Include",
         "Authors introduce LEPISZCZE, a new, comprehensive benchmark for\nPolish NLP with a large variety of tasks and high-quality operationalization of the\nbenchmark.  LEPISZCZE was designed with flexibility in mind. Including new models,\ndatasets, and tasks is as simple as possible while still offering data versioning and\nmodel tracking. In the first run of the benchmark, 13 experiments (task\nand dataset pairs) were tested based on the five most recent LMs for Polish. Five\ndatasets from the Polish benchmark are reused and eight novel datasets are added. ",
         "Multilinguality",
         null,
         null,
         "Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)",
         "['Real task', 'Author-crafted', 'Crowd-sourced', 'Another benchmark']",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Convenience', 'Targeted', 'Criterion']",
         "Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)",
         "['Short free response', 'Structured']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "mean and standard deviation",
         "['Mean', 'Std']"
        ],
        [
         "18",
         "Ryan Kearns",
         "huiUDABenchmarkSuite2024",
         "UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis",
         "Include",
         "The paper introduces the UDA (Unstructured Document Analysis) benchmark. UDA questions are expert-annotated Q&A pairs on PDF and HTML documents, constructed from datasets of academic papers, financial reports, and Wikipedia pages.",
         "Retrieval",
         null,
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)",
         "['Author-crafted', 'Another benchmark']",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Convenience']",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragarph)",
         "['Short free response', 'Free response']",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)",
         "['Exact match', 'Soft match']",
         "Simple mean/sum; % improvement between contexts",
         "['Mean', 'Other']"
        ],
        [
         "19",
         "Jonathan Rystrøm",
         "xiaFOFOBenchmarkEvaluate2024",
         "FOFO: A Benchmark to Evaluate LLMs’ Format-Following Capability",
         "Include",
         "FOFO Is a benchmark for domain-specific format following capabilities. It evaluates a wide array of domains and subdomains across a diverse set of formats from specific medical forms to Maple. The specific examples are generated using GPT-4 and human validation.",
         "Instruction Following",
         null,
         null,
         "LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['LLM-generated']",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Convenience']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",
         "['LLM-as-a-Judge']",
         null,
         null
        ],
        [
         "20",
         "Jonathan Rystrøm",
         "wangMINTEvaluatingLLMs2024",
         "MINT: EVALUATING LLMS IN MULTI-TURN INTERACTION WITH TOOLS AND LANGUAGE FEEDBACK",
         "Include",
         "MINT extends existing benchmark to evaluate the effects of code interpreter usage and multi-turn feedback on LLM performance. It filters benchmark task to ones that benefit from feedback and multi-turn interactions and evaluates different feedback types from \"lazy user\" to \"informative user\" and with(out) tools. ",
         "Agents",
         "Coding",
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "['Another benchmark']",
         "Random sample (creators defined a task space and sampled from it)",
         "['Random']",
         "Short free response (e.g. single word or number), Extended interaction (e.g. conversation, calling an API and processing the response)",
         "['Short free response', 'Interaction']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         null,
         null
        ],
        [
         "21",
         "Jonathan Rystrøm",
         "valmeekamPlanBenchExtensibleBenchmark2023",
         "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
         "Include",
         "PlanBench introduces a suite of tasks relevant to planning using similar formats to the International Planning Competition. The tasks are taken from either Blocksworld or logistics and also obfuscated to avoid reliance on common-sense knowledge.",
         "Reasoning",
         "Planning",
         null,
         "Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Procedurally-generated']",
         "Random sample (creators defined a task space and sampled from it)",
         "['Random']",
         "Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)",
         "['Free response', 'Structured']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         null,
         null
        ],
        [
         "22",
         "Negar Foroutan",
         "zhangMELAMultilingualEvaluation2024",
         "MELA: Multilingual Evaluation of Linguistic Acceptability",
         "Include",
         "The paper intorduces a multilingual acceptability judgement benchmark covering a diverse set of 10 languages, all annotated by expert linguists.  The acceptability judgment task tests a language model’s ability to distinguish syntactically acceptable sentences from unacceptable ones in a human language. The paper establishes LLM baselines on this benchmark, and investigates cross-lingual transfer in acceptability judgements with XLM-R.",
         "Multilinguality",
         null,
         null,
         "hand-written by linguists in respective languages, taken from textbooks, handbooks and journal articles in theoretical syntax + some examples taken from previous benchmarks ",
         "['Expert-crafted']",
         "Random sample (creators defined a task space and sampled from it)",
         "['Random']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall), Matthews Correlation Coefficient (MCC, Matthews), which is a measure of similarity between binary distributions taking values from -1 to 1 and always yielding 0 for any two uncorrelated distributions, regardless of class imbalance.",
         "['Exact match', 'Correlation']",
         "simple mean and standard deviation ",
         "['Mean', 'Std']"
        ],
        [
         "23",
         "Negar Foroutan",
         "etxanizLatxaOpenLanguage2024",
         "Latxa: An Open Language Model and Evaluation Suite for Basque",
         "Include",
         "The paper introduces 4 multiple-choice evaluation datasets for Basque: EusProfi-ciency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. ",
         "Multilinguality",
         null,
         null,
         "Human exam questions (e.g. GRE questions)",
         "['Human exams']",
         "Unknown",
         "['Unknown']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "accuracy, F1, standard deviation",
         "['Mean', 'Std', 'Other']"
        ],
        [
         "24",
         "Negar Foroutan",
         "tangStrucbenchAreLarge2024",
         "Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?",
         "Include",
         "The paper introduces a new benchmark to assess LLMs’ proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance.",
         "Code Generation",
         null,
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "['Another benchmark']",
         "Random sample (creators defined a task space and sampled from it)",
         "['Random']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "P-Score (Prompting Score) and H-Score (Heuristical Score)",
         "['LLM-as-a-Judge']",
         "simple mean",
         "['Mean']"
        ],
        [
         "25",
         "Negar Foroutan",
         "riemenschneiderExploringLargeLanguage2023",
         "Exploring Large Language Models for Classical Philology",
         "Include",
         "They define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. The experiments provide the first benchmarking analysis of existing models of Ancient Greek. ",
         "Multilinguality",
         null,
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",
         "['Author-crafted']",
         "Unknown",
         "['Unknown']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         null,
         null
        ],
        [
         "26",
         "Cornelius Emde",
         "qiPreservingKnowledgeInvariance2023",
         "Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction",
         "Include",
         "The paper introduces ROBUST, a benchmark designed to evaluate open information extraction models by measuring their ability to generalize knowledge extraction across syntactically diverse sentences that share the same semantic content.",
         "NLP",
         "Extraction",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Author-crafted', 'Another benchmark', 'Procedurally-generated']",
         "Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Random', 'Convenience']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "For each tuple, the F1 is computed, then across a clique the minimum is computed and aggregated across the dataset as mean.",
         "['Mean']"
        ],
        [
         "27",
         "Anna Sotnikova",
         "shahWhenFLUEMeets2022",
         "WHEN FLUE MEETS FLANG: Benchmarks and Large Pre-trained\nLanguage Model for Financial Domain",
         "Include",
         " the Financial Language Understanding\nEvaluation (FLUE), an open-source comprehensive\nsuite of benchmarks for the financial\ndomain. These include new benchmarks across\n5 NLP tasks in financial domain as well as common\nbenchmarks used in the previous research.",
         "Finance",
         null,
         null,
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)",
         "['Real task', 'Another benchmark']",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Convenience', 'Targeted', 'Criterion']",
         "Short free response (e.g. single word or number), Structured response (e.g. valid JSON, API call alone)",
         "['Short free response', 'Structured']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "Simple mean: F1 scores and accuracy. MSE. nDCG and MRR. Perplexity",
         "['Mean', 'Other']"
        ],
        [
         "28",
         "Angelika Romanou",
         "kalyanWikiDONewBenchmark2024",
         "WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models",
         "Include",
         "The authors argue that current VLM benchmarks are insufficient to assess the OOD generalization capability of models due to high visual and linguistic similarity between the evaluation and finetuning datasets. The propose WIKIDO which consists of image-text data derived from Wikipedia Diversity Observatory, a diverse source of Wikipedia articles spanning several diversity axes including geography, gender, ethnicity and domains/topics.",
         "Retrieval",
         null,
         null,
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['Another benchmark', 'Procedurally-generated', 'LLM-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "['Targeted']",
         "Retrieval ",
         "['Short free response']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "simple mean",
         "['Mean']"
        ],
        [
         "29",
         "Negar Foroutan",
         "marchisioUnderstandingMitigatingLanguage2024",
         "Understanding and Mitigating Language Confusion in LLMs",
         "Include",
         "The paper introduces a benchmark to measure language confusion in LLMs. They investigate language confusion on the line and word level in two practical settings: a) Monolingual generation, where a user queries the LLM in a given language, implicitly requesting an answer in the same language; and b) cross-lingual generation, where a user explicitly instructs a model to generate text in a different language.",
         "Multilinguality",
         null,
         null,
         "Modified from another benchmark (e.g. translation into another language), For some part of the data they include human generated prompts ",
         "['Another benchmark', 'Author-crafted']",
         "Random sample (creators defined a task space and sampled from it)",
         "['Random']",
         "Free response (e.g. summary paragarph)",
         "['Free response']",
         "The paper introduces 2 new metrics for language confusion. Line-level pass rate (LPR) and Word-level pass rate (WPR).",
         "['Exact match']",
         "simple mean",
         "['Mean']"
        ],
        [
         "30",
         "Ryan Kearns",
         "itoGeneralizationCapacityNeural2024",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "Include",
         "The paper introduces gCOG, a multimodal reasoning dataset designed to measure various types of OOD generalisation (distractor generalisation, systematic compositional, and productive compositional). The authors train various encoder architectures from scratch and compare their performances. Transformers can systematically generalise at scale, but no architectures can productively generalise.",
         "Language Modelling",
         "Adaptability",
         null,
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Another benchmark', 'Procedurally-generated']",
         "Random sample (creators defined a task space and sampled from it), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Random', 'Criterion']",
         "Short free response (e.g. single word or number)",
         "['Short free response']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "simple mean/sum",
         "['Mean']"
        ],
        [
         "31",
         "Ryan Kearns",
         "liMultimodalArXivDataset2024",
         "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models",
         "Include",
         "Multimodal ArXiv consists of ArXivCap, a figure-caption dataset sourced from scientific papers, and ArXivQA, a QA dataset generated by prompting GPT-4V for QA pairs on ArXivCap entries. Results show that fine-tuning on these datasets boosts performance on the MathVista benchmark, and that evaluation results for various scientific plot comprehension subtasks are poor.",
         "VQA",
         "Understanding",
         null,
         "Real task examples (e.g. GitHub issues), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['Real task', 'LLM-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "['Targeted']",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragarph)",
         "['Short free response', 'Free response']",
         "n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)",
         "['Soft match', 'LLM post-processing']",
         "simple mean/sum",
         "['Mean']"
        ],
        [
         "32",
         "Ryan Kearns",
         "zouVGBenchEvaluatingLarge2024",
         "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
         "Include",
         "The paper introduces VGBench, a comprehensive benchmark for vector graphics images that tests both visual understanding and generation. Formats like SVG, TikZ, and Graphviz are included, and performance is generally strong, though LLMs do worse with the lower-level SVG format.",
         "Instruction Following",
         null,
         null,
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['Real task', 'Another benchmark', 'LLM-generated']",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Convenience']",
         "Multiple choice, Structured response (e.g. valid JSON, API call alone)",
         "['Multiple choice', 'Structured']",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)",
         "['Exact match', 'Soft match', 'LLM post-processing']",
         "simple mean/sum",
         "['Mean']"
        ],
        [
         "33",
         "Negar Foroutan",
         "zhangXSemPLRCrosslingualSemantic2023",
         "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations",
         "Include",
         "The paper introduces XSEMPLR, a unified benchmark for cross-lingual semantic parsing featuring 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. They use XSEMPLR to conduct a benchmark study on a wide range of multilingual language models, including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and\ndecoder-based models (Codex, BLOOM). The findings show that large multilingual\nlanguage models are still inadequate for performing CLSP tasks. They also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training.",
         "Multilinguality",
         null,
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "['Another benchmark']",
         "Random sample (creators defined a task space and sampled from it)",
         "['Random']",
         "Free response (e.g. summary paragarph)",
         "['Free response']",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)",
         "['Exact match', 'Soft match']",
         "Simple mean",
         "['Mean']"
        ],
        [
         "34",
         "Negar Foroutan",
         "sunInformalLanguageProcessing2024",
         "Toward Informal Language Processing: Knowledge of Slang in Large Language Models",
         "Include",
         "Using movie subtitles, the authors construct a dataset that supports evaluation on a diverse\nset of tasks pertaining to the automatic processing of slang. For both evaluation and finetuning, they show the effectiveness of their dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences.",
         "Multilinguality",
         null,
         null,
         "Crowd-sourced task examples (e.g. Prolific-created tasks)",
         "['Crowd-sourced']",
         "Random sample (creators defined a task space and sampled from it)",
         "['Random']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall), They also report two metrics to compare an LLM’s predictive confidence in slang usages relative to their literal counterparts.",
         "['Exact match', 'Correlation']",
         "simple mean",
         "['Mean']"
        ],
        [
         "35",
         "Anna Sotnikova",
         "wangPretrainingLanguageModel2023",
         "ON PRE-TRAINED LANGUAGE MODELS FOR ANTIBODY",
         "Include",
         "This paper introduces the AnTibody Understanding Evaluation (ATUE) benchmark to systematically assess the representation capabilities of general and antibody-specific pre-trained language models across a range of antibody-related tasks. It also explores how incorporating biological mechanisms into pre-training can enhance model performance and evaluates the transferability of learned representations to real-world applications such as drug discovery and immune system analysis.",
         "Biology",
         null,
         null,
         "Real task examples (e.g. GitHub issues)",
         "['Real task']",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Convenience', 'Targeted', 'Criterion']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "Exact Match (accuracy, F1, precision, recall), Matthews Correlation Coefficient (MCC), and AUC (Area Under the ROC Curve)",
         "['Exact match', 'Correlation']",
         null,
         null
        ],
        [
         "36",
         "Negar Foroutan",
         "bajpaiCanLLMsReplace2024",
         "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators",
         "Include",
         "This paper focuses on evaluating the reliability of current LLMs as science communicators. They introduce a dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex\nscientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria. They also benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families.",
         "General Science",
         null,
         null,
         "Not explained ",
         "['Unknown']",
         "Unknown",
         "['Unknown']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "Simple mean and standard deviation",
         "['Mean', 'Std']"
        ],
        [
         "37",
         "Negar Foroutan",
         "hauserLargeLanguageModelsExpertlevel2024",
         "Large Language Models' Expert-level Global History Knowledge Benchmark (HiST-LLM)",
         "Include",
         "The paper introduces the History Seshat Test for LLMs (HiST-LLM), based on a subset of the Seshat Global History Databank, which provides a structured representation of human historical knowledge, containing 36,000 data points across 600 historical societies and over\n2,700 scholarly references. Using this dataset, they benchmark a total of seven models from the Gemini, OpenAI, and Llama families.",
         "History",
         null,
         null,
         "Human expert created the examples",
         "['Expert-crafted']",
         "Random sample (creators defined a task space and sampled from it)",
         "['Random']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "Mean and standard deviation",
         "['Mean', 'Std']"
        ],
        [
         "38",
         "Lennart Luettgau",
         "sadatMSciNLIDiverseBenchmark2024",
         "MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference",
         "Include",
         "This paper introduces MSCINLI, a new dataset comprising 132,320 sentence pairs from five diverse scientific domains to enhance the study of scientific Natural Language Inference (NLI). Baseline models, including fine-tuned and prompted LLMs, reveal the dataset's challenging nature, as well as performance degradation due to domain shifts, highlighting the unique characteristics of each domain. Additionally, employing both scientific NLI datasets in intermediate task transfer learning showcases improvements in downstream scientific tasks.",
         "General Science",
         null,
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)",
         "['Author-crafted', 'Another benchmark']",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Targeted', 'Criterion']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "mean and variance, t-tests",
         "['Mean', 'Std', 'Tests']"
        ],
        [
         "39",
         "Lennart Luettgau",
         "dengNewTermBenchmarkingRealtime2024",
         "NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates",
         "Include",
         "This paper introduces NewTerm, an adaptive benchmark designed for the real-time evaluation of new terms in large language models (LLMs) to address their struggle with real-time information due to knowledge cutoffs. The benchmark is constructed using a highly automated method allowing flexible and minimal human effort updates, revealing a performance reduction of over 20% on various LLMs with new terms and highlighting difficulties in generalizing to distant new terms. Annual updates to NewTerm, starting with 2022 and 2023, are planned to continuously assess and analyze the evolving challenge of new terms in LLMs.",
         "Language Modelling",
         "Updating",
         null,
         "Real task examples (e.g. GitHub issues), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Real task', 'Procedurally-generated']",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "['Criterion']",
         "Multiple choice",
         "['Multiple choice']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "simple mean/sum",
         "['Mean']"
        ],
        [
         "40",
         "Cornelius Emde",
         "yeRoTBenchMultilevelBenchmark2024",
         "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
         "Include",
         "LLMs are increasingly deployedin settings where they can use tools, e.g. call functions to retrieve real-time information on weather. This paper proposes benchmark measuring the robustness of LLMs in selecting tools when these are specified under noise (e.g. the function name is perturbed).",
         "Agents",
         "Tool Use",
         null,
         "Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Procedurally-generated']",
         "Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)",
         "['Random', 'Convenience']",
         "Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)",
         "['Free response', 'Structured']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         null,
         null
        ],
        [
         "41",
         "Kaili Liu",
         "maMMLONGBENCHDOCBenchmarkingLongcontext2024",
         "MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations",
         "Include",
         "The paper presents a long-context multimodal benchmark dataset of more than 1k expert annotated questions over long PDFs which require aggregating evidence across multiple locations and evidence formats (text, image, charts, etc.) to answer. MMLongBench-Doc presents a challenge for strong models such as GPT-4o and other large vision language models (LVLMs), demonstrating the need for improved long-context LVLM capabilities.",
         "NLP",
         "Long Context",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Author-crafted', 'Another benchmark', 'Procedurally-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Targeted', 'Criterion']",
         "Short free response (e.g. single word or number)",
         "['Short free response']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         null,
         null
        ],
        [
         "42",
         "Kaili Liu",
         "kuratovBABILongTestingLimits2024",
         "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
         "Include",
         "The BABILong benchmark tests language models’ ability to reason across facts distributed in extremely long documents in the reasoning setting, scattering relevant facts among less relevant natural text. The paper finds LLMs only effectively use less than 20% of the context in such settings, with reasoning complexity negatively impacting performance. Multiple methods including in-context reasoning, retrieval augmented generation, and context extension are applied to profile model capabilities in these long-context tasks.",
         "NLP",
         "Long Context",
         null,
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Another benchmark', 'Procedurally-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Targeted', 'Criterion']",
         "Short free response (e.g. single word or number)",
         "['Short free response']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "simple mean",
         "['Mean']"
        ],
        [
         "43",
         "Kaili Liu",
         "wangAdaLEvalEvaluatingLongcontext2024",
         "Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks",
         "Include",
         "Ada-LEval presents a length-adaptable benchmark for long-context understanding capabilities of LLMs, involving challenging questions for reliable evaluation and context lengths extending to the ultra-long setting. SOTA open and closed models are evaluated to demonstrate current limitations of LLMs in such settings.",
         "NLP",
         "Long Context",
         null,
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Real task', 'Another benchmark', 'Procedurally-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Targeted', 'Criterion']",
         "Multiple choice, Free response (e.g. summary paragarph)",
         "['Multiple choice', 'Free response']",
         "Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation), instruction following rate",
         "['Exact match', 'Distribution', 'Exact match']",
         "simple mean",
         "['Mean']"
        ],
        [
         "44",
         "Kaili Liu",
         "zhangAnalyzingTemporalComplex2024",
         "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding",
         "Include",
         "TCELongBench assess LLMs’ ability to leverage temporal dynamics when understanding extensive texts. Experiments find that retrieval augmented generation and long-context modeling are fairly effective to handle such tasks.",
         "NLP",
         "Long Context",
         null,
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['Another benchmark', 'Procedurally-generated', 'LLM-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Targeted', 'Criterion']",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)",
         "['Multiple choice', 'Short free response', 'Free response']",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)",
         "['Exact match', 'Soft match']",
         "Simple mean",
         "['Mean']"
        ],
        [
         "45",
         "Kaili Liu",
         "liLooGLECanLongcontext2024",
         "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
         "Include",
         "The paper presents a long-context benchmark over recent (post-2022) documents with new questions in diverse domains. LooGLE assesses LLMs’ long dependency capabilities and finds poor performance even with long context window LLMs.",
         "NLP",
         "Long Context",
         null,
         "Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Crowd-sourced', 'Another benchmark', 'Procedurally-generated']",
         "Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Random', 'Targeted', 'Criterion']",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph)",
         "['Multiple choice', 'Short free response', 'Free response']",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Human accuracy evaluation",
         "['Exact match', 'Soft match', 'LLM-as-a-Judge', 'Human ratings']",
         "Simple mean",
         "['Mean']"
        ],
        [
         "46",
         "Kaili Liu",
         "wangLeaveNoDocument2024",
         "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA",
         "Include",
         "Loong is a long-context benchmark which aims to boost the realism of long-context capability evaluation by ensuring each document is relevant to the final answer, covering a range of context lengths and tasks. Various models are assessed on the benchmark, with RAG proving poor for improving performance.",
         "NLP",
         "Long Context",
         null,
         "Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['Procedurally-generated', 'LLM-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Targeted', 'Criterion']",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)",
         "['Multiple choice', 'Short free response', 'Free response', 'Structured']",
         "LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), LLM post-processing (extracting answers, reformatting for automated scoring)",
         "['LLM-as-a-Judge', 'LLM post-processing']",
         "simple mean",
         "['Mean']"
        ],
        [
         "47",
         "Kaili Liu",
         "senelCoDA21EvaluatingLanguage2022",
         "CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment",
         "Include",
         "CoDA21 is a challenging benchmark to assess NLU capabilities of pretrained language models (PLMs). Performance of PLMs is assessed versus humans.",
         "NLP",
         "Understanding",
         null,
         "Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Procedurally-generated']",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "['Criterion']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "Exact Match (accuracy, F1, precision, recall), cosine similarity, log generation probability",
         "['Exact match', 'Distribution']",
         "simple mean",
         "['Mean']"
        ],
        [
         "48",
         "Kaili Liu",
         "anLevalInstitutingStandardized2024",
         "L-Eval: Instituting Standardized Evaluation for Long Context Language Models",
         "Include",
         "L-Eval presents a standardize evaluation suite for long-context language models consisting of 20 subtasks over long documents up to 200K tokens in length with diverse human-labeled query-response pairs. Evaluation metrics for long-context LLMs are compared for alignment with human judgment. Commercial and open-source LLMs are benchmarked.",
         "NLP",
         "Long Context",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "['Author-crafted', 'Another benchmark', 'Procedurally-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "['Targeted', 'Criterion']",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Extended interaction (e.g. conversation, calling an API and processing the response)",
         "['Multiple choice', 'Short free response', 'Free response', 'Interaction']",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",
         "['Exact match', 'Soft match', 'Human ratings', 'LLM-as-a-Judge']",
         "simple mean",
         "['Mean']"
        ],
        [
         "49",
         "Kaili Liu",
         "zhangMarathonRaceRealm2024",
         "Marathon: A Race Through the Realm of Long Context with Large Language Models",
         "Include",
         "The paper presents the Marathon benchmark to evaluate comprehension and reasoning capabilities of LLMs over long texts. Marathon is used to assess SOTA LLMs and the efficacy of several existing long-context generation strategies.",
         "NLP",
         "Long Context",
         null,
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "['Another benchmark', 'Procedurally-generated', 'LLM-generated']",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "['Targeted']",
         "Structured response (e.g. valid JSON, API call alone)",
         "['Structured']",
         "Exact Match (accuracy, F1, precision, recall)",
         "['Exact match']",
         "simple mean",
         "['Mean']"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 455
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>phenomenon_taxonomy_root</th>\n",
       "      <th>phenomenon_taxonomy_leaf</th>\n",
       "      <th>phenomenon_taxonomy_alternate</th>\n",
       "      <th>task_source</th>\n",
       "      <th>task_source_clean</th>\n",
       "      <th>dataset_sampling_method</th>\n",
       "      <th>dataset_sampling_method_clean</th>\n",
       "      <th>response_format</th>\n",
       "      <th>response_format_clean</th>\n",
       "      <th>metric_definition</th>\n",
       "      <th>metric_definition_clean</th>\n",
       "      <th>metric_statistics</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Mayne</td>\n",
       "      <td>mundlerSWTBenchTestingValidating2024</td>\n",
       "      <td>SWT-Bench: Testing and Validating Real-World B...</td>\n",
       "      <td>Include</td>\n",
       "      <td>A benchmark for generating code tests (unit te...</td>\n",
       "      <td>Agents</td>\n",
       "      <td>Coding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real task examples (e.g. GitHub issues), Modif...</td>\n",
       "      <td>[Real task, Another benchmark]</td>\n",
       "      <td>Specific criteria (items were taken from a lar...</td>\n",
       "      <td>[Criterion]</td>\n",
       "      <td>Structured response (e.g. valid JSON, API call...</td>\n",
       "      <td>[Structured]</td>\n",
       "      <td>Whether the faulty code fails on the test and ...</td>\n",
       "      <td>[Reward]</td>\n",
       "      <td>simple mean</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jonathan Rystrøm</td>\n",
       "      <td>davidsonEvaluatingLanguageModel2024</td>\n",
       "      <td>EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...</td>\n",
       "      <td>Include</td>\n",
       "      <td>The paper introduces a dynamic framework for e...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Author-crafted]</td>\n",
       "      <td>Targeted items (creators defined a task space ...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>Extended interaction (e.g. conversation, calli...</td>\n",
       "      <td>[Interaction]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall),...</td>\n",
       "      <td>[Exact match, Reward]</td>\n",
       "      <td>mean with variance</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lennart Luettgau</td>\n",
       "      <td>helweMAFALDABenchmarkComprehensive2024</td>\n",
       "      <td>MAFALDA: A Benchmark and Comprehensive Study o...</td>\n",
       "      <td>Include</td>\n",
       "      <td>The paper introduces MAFALD, a benchmark that ...</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Logical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Author-crafted, Crowd-sourced, Another benchm...</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience, Targeted]</td>\n",
       "      <td>Multiple choice</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>simple mean/sum</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaili Liu</td>\n",
       "      <td>niuRAGTruthHallucinationCorpus2024</td>\n",
       "      <td>RAGTruth: A Hallucination Corpus for Developin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>This paper targets word-level hallucinations i...</td>\n",
       "      <td>Retrieval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Factuality</td>\n",
       "      <td>Real task examples (e.g. GitHub issues), Crowd...</td>\n",
       "      <td>[Real task, Crowd-sourced, Another benchmark, ...</td>\n",
       "      <td>Random sample (creators defined a task space a...</td>\n",
       "      <td>[Random, Targeted]</td>\n",
       "      <td>Short free response (e.g. single word or numbe...</td>\n",
       "      <td>[Short free response, Free response, Structured]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anna Gausen</td>\n",
       "      <td>wangIELMOpenInformation2022</td>\n",
       "      <td>IELM: An Open Information Extraction Benchmark...</td>\n",
       "      <td>Include</td>\n",
       "      <td>They introduce a new open information extracti...</td>\n",
       "      <td>NLP</td>\n",
       "      <td>Extraction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crowd-sourced task examples (e.g. Prolific-cre...</td>\n",
       "      <td>[Crowd-sourced, Procedurally-generated]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>Structured response (e.g. valid JSON, API call...</td>\n",
       "      <td>[Structured]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>The authors carry out some error analysis: \"We...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Andrew Bean</td>\n",
       "      <td>wangUsercentricMultiintentBenchmark2024</td>\n",
       "      <td>A User-Centric Multi-Intent Benchmark for Eval...</td>\n",
       "      <td>Include</td>\n",
       "      <td>The paper creates a dataset of user scenarios ...</td>\n",
       "      <td>General Purpose</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expert-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Expert-crafted, Crowd-sourced]</td>\n",
       "      <td>Random sample (creators defined a task space a...</td>\n",
       "      <td>[Random]</td>\n",
       "      <td>Free response (e.g. summary paragraph, executa...</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>Human ratings (text quality, preference, NOT m...</td>\n",
       "      <td>[Human ratings, LLM-as-a-Judge]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>XuOpenToMComprehensiveBenchmark2024</td>\n",
       "      <td>OpenToM: A Comprehensive Benchmark for Evaluat...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Benchmark to assess Theory of Mind in LLMs. Ea...</td>\n",
       "      <td>Theory of Mind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Author-crafted, Crowd-sourced, Procedurally-g...</td>\n",
       "      <td>Random sample (creators defined a task space a...</td>\n",
       "      <td>[Random, Convenience]</td>\n",
       "      <td>Multiple choice</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>chenPremiseOrderMatters2024</td>\n",
       "      <td>Premise Order Matters in Reasoning with Large ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Benchmark that shows a failure mode of LLM rea...</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Logical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Modified from another benchmark (e.g. translat...</td>\n",
       "      <td>[Another benchmark]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>Short free response (e.g. single word or number)</td>\n",
       "      <td>[Short free response]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>hanReadingBooksGreat2023</td>\n",
       "      <td>Reading Books is Great, But Not if You Are Dri...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Commonsense norms are defeasible by context:  ...</td>\n",
       "      <td>Grounding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crowd-sourced task examples (e.g. Prolific-cre...</td>\n",
       "      <td>[Crowd-sourced]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>Multiple choice, Free response (e.g. summary p...</td>\n",
       "      <td>[Multiple choice, Free response]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall),...</td>\n",
       "      <td>[Exact match, Soft match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>wangMMLUproMoreRobust2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Include</td>\n",
       "      <td>Extends MMLU (hard, diverse multiple choice ll...</td>\n",
       "      <td>Knowledge</td>\n",
       "      <td>General</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Modified from another benchmark (e.g. translat...</td>\n",
       "      <td>[Another benchmark]</td>\n",
       "      <td>Targeted items (creators defined a task space ...</td>\n",
       "      <td>[Targeted, Criterion]</td>\n",
       "      <td>Multiple choice</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           main_coder                                   bibkey  \\\n",
       "0         Harry Mayne     mundlerSWTBenchTestingValidating2024   \n",
       "1    Jonathan Rystrøm      davidsonEvaluatingLanguageModel2024   \n",
       "2    Lennart Luettgau   helweMAFALDABenchmarkComprehensive2024   \n",
       "3           Kaili Liu       niuRAGTruthHallucinationCorpus2024   \n",
       "4         Anna Gausen              wangIELMOpenInformation2022   \n",
       "..                ...                                      ...   \n",
       "450       Andrew Bean  wangUsercentricMultiintentBenchmark2024   \n",
       "451       Thom Foster      XuOpenToMComprehensiveBenchmark2024   \n",
       "452       Thom Foster              chenPremiseOrderMatters2024   \n",
       "453       Thom Foster                 hanReadingBooksGreat2023   \n",
       "454       Thom Foster                wangMMLUproMoreRobust2024   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "0    SWT-Bench: Testing and Validating Real-World B...   Include   \n",
       "1    EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...   Include   \n",
       "2    MAFALDA: A Benchmark and Comprehensive Study o...   Include   \n",
       "3    RAGTruth: A Hallucination Corpus for Developin...   Include   \n",
       "4    IELM: An Open Information Extraction Benchmark...   Include   \n",
       "..                                                 ...       ...   \n",
       "450  A User-Centric Multi-Intent Benchmark for Eval...   Include   \n",
       "451  OpenToM: A Comprehensive Benchmark for Evaluat...   Include   \n",
       "452  Premise Order Matters in Reasoning with Large ...   Include   \n",
       "453  Reading Books is Great, But Not if You Are Dri...   Include   \n",
       "454                                                NaN   Include   \n",
       "\n",
       "                                         short_summary  \\\n",
       "0    A benchmark for generating code tests (unit te...   \n",
       "1    The paper introduces a dynamic framework for e...   \n",
       "2    The paper introduces MAFALD, a benchmark that ...   \n",
       "3    This paper targets word-level hallucinations i...   \n",
       "4    They introduce a new open information extracti...   \n",
       "..                                                 ...   \n",
       "450  The paper creates a dataset of user scenarios ...   \n",
       "451  Benchmark to assess Theory of Mind in LLMs. Ea...   \n",
       "452  Benchmark that shows a failure mode of LLM rea...   \n",
       "453  Commonsense norms are defeasible by context:  ...   \n",
       "454  Extends MMLU (hard, diverse multiple choice ll...   \n",
       "\n",
       "    phenomenon_taxonomy_root phenomenon_taxonomy_leaf  \\\n",
       "0                     Agents                   Coding   \n",
       "1                  Alignment                Alignment   \n",
       "2                  Reasoning                  Logical   \n",
       "3                  Retrieval                      NaN   \n",
       "4                        NLP               Extraction   \n",
       "..                       ...                      ...   \n",
       "450          General Purpose                      NaN   \n",
       "451           Theory of Mind                      NaN   \n",
       "452                Reasoning                  Logical   \n",
       "453                Grounding                      NaN   \n",
       "454                Knowledge                  General   \n",
       "\n",
       "    phenomenon_taxonomy_alternate  \\\n",
       "0                             NaN   \n",
       "1                             NaN   \n",
       "2                             NaN   \n",
       "3                      Factuality   \n",
       "4                             NaN   \n",
       "..                            ...   \n",
       "450                           NaN   \n",
       "451                           NaN   \n",
       "452                           NaN   \n",
       "453                           NaN   \n",
       "454                           NaN   \n",
       "\n",
       "                                           task_source  \\\n",
       "0    Real task examples (e.g. GitHub issues), Modif...   \n",
       "1    Author-crafted task examples (e.g. hand-writte...   \n",
       "2    Author-crafted task examples (e.g. hand-writte...   \n",
       "3    Real task examples (e.g. GitHub issues), Crowd...   \n",
       "4    Crowd-sourced task examples (e.g. Prolific-cre...   \n",
       "..                                                 ...   \n",
       "450  Expert-crafted task examples (e.g. hand-writte...   \n",
       "451  Author-crafted task examples (e.g. hand-writte...   \n",
       "452  Modified from another benchmark (e.g. translat...   \n",
       "453  Crowd-sourced task examples (e.g. Prolific-cre...   \n",
       "454  Modified from another benchmark (e.g. translat...   \n",
       "\n",
       "                                     task_source_clean  \\\n",
       "0                       [Real task, Another benchmark]   \n",
       "1                                     [Author-crafted]   \n",
       "2    [Author-crafted, Crowd-sourced, Another benchm...   \n",
       "3    [Real task, Crowd-sourced, Another benchmark, ...   \n",
       "4              [Crowd-sourced, Procedurally-generated]   \n",
       "..                                                 ...   \n",
       "450                    [Expert-crafted, Crowd-sourced]   \n",
       "451  [Author-crafted, Crowd-sourced, Procedurally-g...   \n",
       "452                                [Another benchmark]   \n",
       "453                                    [Crowd-sourced]   \n",
       "454                                [Another benchmark]   \n",
       "\n",
       "                               dataset_sampling_method  \\\n",
       "0    Specific criteria (items were taken from a lar...   \n",
       "1    Targeted items (creators defined a task space ...   \n",
       "2    Convenience sample (creators found a set of ta...   \n",
       "3    Random sample (creators defined a task space a...   \n",
       "4    Convenience sample (creators found a set of ta...   \n",
       "..                                                 ...   \n",
       "450  Random sample (creators defined a task space a...   \n",
       "451  Random sample (creators defined a task space a...   \n",
       "452  Convenience sample (creators found a set of ta...   \n",
       "453  Convenience sample (creators found a set of ta...   \n",
       "454  Targeted items (creators defined a task space ...   \n",
       "\n",
       "    dataset_sampling_method_clean  \\\n",
       "0                     [Criterion]   \n",
       "1                      [Targeted]   \n",
       "2         [Convenience, Targeted]   \n",
       "3              [Random, Targeted]   \n",
       "4                   [Convenience]   \n",
       "..                            ...   \n",
       "450                      [Random]   \n",
       "451         [Random, Convenience]   \n",
       "452                 [Convenience]   \n",
       "453                 [Convenience]   \n",
       "454         [Targeted, Criterion]   \n",
       "\n",
       "                                       response_format  \\\n",
       "0    Structured response (e.g. valid JSON, API call...   \n",
       "1    Extended interaction (e.g. conversation, calli...   \n",
       "2                                      Multiple choice   \n",
       "3    Short free response (e.g. single word or numbe...   \n",
       "4    Structured response (e.g. valid JSON, API call...   \n",
       "..                                                 ...   \n",
       "450  Free response (e.g. summary paragraph, executa...   \n",
       "451                                    Multiple choice   \n",
       "452   Short free response (e.g. single word or number)   \n",
       "453  Multiple choice, Free response (e.g. summary p...   \n",
       "454                                    Multiple choice   \n",
       "\n",
       "                                response_format_clean  \\\n",
       "0                                        [Structured]   \n",
       "1                                       [Interaction]   \n",
       "2                                   [Multiple choice]   \n",
       "3    [Short free response, Free response, Structured]   \n",
       "4                                        [Structured]   \n",
       "..                                                ...   \n",
       "450                                   [Free response]   \n",
       "451                                 [Multiple choice]   \n",
       "452                             [Short free response]   \n",
       "453                  [Multiple choice, Free response]   \n",
       "454                                 [Multiple choice]   \n",
       "\n",
       "                                     metric_definition  \\\n",
       "0    Whether the faulty code fails on the test and ...   \n",
       "1    Exact Match (accuracy, F1, precision, recall),...   \n",
       "2        Exact Match (accuracy, F1, precision, recall)   \n",
       "3        Exact Match (accuracy, F1, precision, recall)   \n",
       "4        Exact Match (accuracy, F1, precision, recall)   \n",
       "..                                                 ...   \n",
       "450  Human ratings (text quality, preference, NOT m...   \n",
       "451      Exact Match (accuracy, F1, precision, recall)   \n",
       "452      Exact Match (accuracy, F1, precision, recall)   \n",
       "453  Exact Match (accuracy, F1, precision, recall),...   \n",
       "454      Exact Match (accuracy, F1, precision, recall)   \n",
       "\n",
       "             metric_definition_clean  \\\n",
       "0                           [Reward]   \n",
       "1              [Exact match, Reward]   \n",
       "2                      [Exact match]   \n",
       "3                      [Exact match]   \n",
       "4                      [Exact match]   \n",
       "..                               ...   \n",
       "450  [Human ratings, LLM-as-a-Judge]   \n",
       "451                    [Exact match]   \n",
       "452                    [Exact match]   \n",
       "453        [Exact match, Soft match]   \n",
       "454                    [Exact match]   \n",
       "\n",
       "                                     metric_statistics metric_statistics_clean  \n",
       "0                                          simple mean                  [Mean]  \n",
       "1                                   mean with variance             [Mean, Std]  \n",
       "2                                      simple mean/sum                  [Mean]  \n",
       "3                                                  NaN                     NaN  \n",
       "4    The authors carry out some error analysis: \"We...                 [Other]  \n",
       "..                                                 ...                     ...  \n",
       "450                                                NaN                     NaN  \n",
       "451                                                NaN                     NaN  \n",
       "452                                                NaN                     NaN  \n",
       "453                                                NaN                     NaN  \n",
       "454                                                NaN                     NaN  \n",
       "\n",
       "[455 rows x 18 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[['main_coder','bibkey','title','inclusion','short_summary','phenomenon_taxonomy_root',\n",
    " 'phenomenon_taxonomy_leaf', 'phenomenon_taxonomy_alternate','task_source','task_source_clean','dataset_sampling_method','dataset_sampling_method_clean','response_format','response_format_clean','metric_definition','metric_definition_clean','metric_statistics','metric_statistics_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc31a0",
   "metadata": {},
   "source": [
    "## Recommendation Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb1fc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_percents(df,col):\n",
    "    if col in col_maps.keys():\n",
    "        df[col] = df[col].apply(lambda x: col_maps[col][x] if col_maps[col].get(x) else x)\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    print(df[col].value_counts(dropna=False)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caee982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phenomenon_defined\n",
      "Yes    348\n",
      "No      99\n",
      "NaN      8\n",
      "Name: count, dtype: int64\n",
      "phenomenon_defined\n",
      "Yes    0.764835\n",
      "No     0.217582\n",
      "NaN    0.017582\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'phenomenon_defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fbad6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phenomenon_contested\n",
      "Contested        225\n",
      "Widely-agreed    203\n",
      "Not defined       27\n",
      "Name: count, dtype: int64\n",
      "phenomenon_contested\n",
      "Contested        0.494505\n",
      "Widely-agreed    0.446154\n",
      "Not defined      0.059341\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'phenomenon_contested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "841cdad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition_integrity\n",
      "Composite phenomenon               278\n",
      "Single cohesive phenomenon         166\n",
      "Authors' description is unclear     10\n",
      "NaN                                  1\n",
      "Name: count, dtype: int64\n",
      "definition_integrity\n",
      "Composite phenomenon               0.610989\n",
      "Single cohesive phenomenon         0.364835\n",
      "Authors' description is unclear    0.021978\n",
      "NaN                                0.002198\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'definition_integrity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e89756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition_scope\n",
      "Subset           253\n",
      "Comprehensive    196\n",
      "NaN                6\n",
      "Name: count, dtype: int64\n",
      "definition_scope\n",
      "Subset           0.556044\n",
      "Comprehensive    0.430769\n",
      "NaN              0.013187\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'definition_scope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84bfd36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metric_statistics\n",
       "simple mean                                                                                                                    80\n",
       "simple mean/sum                                                                                                                29\n",
       "Simple mean                                                                                                                    23\n",
       "Mean                                                                                                                            6\n",
       "Mean,                                                                                                                           4\n",
       "                                                                                                                               ..\n",
       "simple mean. no inferential statistics (even though the LLM-as-judge have fairly low accuracy).                                 1\n",
       "mean with \"error bars from 3 runs at temperature 0.2\" (unsure if this is a standard error or just the range in scores) (17)     1\n",
       "Exact Match (EM), F1 Score (%)                                                                                                  1\n",
       "Mean and standard deviation\\n                                                                                                   1\n",
       "Experiments are repeated 5 times but resulting information onf uncertainty are not reported.                                    1\n",
       "Name: count, Length: 209, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['metric_statistics'].apply(lambda x: metrics_agg_map[x] if x in metrics_map.keys() else x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af6d31d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metric_statistics\n",
       "simple mean                                                                                                                    80\n",
       "simple mean/sum                                                                                                                29\n",
       "Simple mean                                                                                                                    23\n",
       "Mean                                                                                                                            6\n",
       "Mean,                                                                                                                           4\n",
       "                                                                                                                               ..\n",
       "simple mean. no inferential statistics (even though the LLM-as-judge have fairly low accuracy).                                 1\n",
       "mean with \"error bars from 3 runs at temperature 0.2\" (unsure if this is a standard error or just the range in scores) (17)     1\n",
       "Exact Match (EM), F1 Score (%)                                                                                                  1\n",
       "Mean and standard deviation\\n                                                                                                   1\n",
       "Experiments are repeated 5 times but resulting information onf uncertainty are not reported.                                    1\n",
       "Name: count, Length: 209, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['metric_statistics'].apply(lambda x: 'Std' in metrics_agg_map[x] if x in metrics_map.keys() else x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13da667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_sampling_method: Criterion\n",
      "0.46153846153846156\n",
      "dataset_sampling_method: Convenience\n",
      "0.3934065934065934\n",
      "dataset_sampling_method: Random\n",
      "0.17142857142857143\n",
      "dataset_sampling_method: Unknown\n",
      "0.04395604395604396\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in ['dataset_sampling_method: Criterion',\n",
    "       'dataset_sampling_method: Convenience', 'dataset_sampling_method: Random','dataset_sampling_method: Unknown'\n",
    "]:\n",
    "    print(col)\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e16c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_definition: Exact match\n",
      "370\n",
      "0.8131868131868132\n",
      "metric_definition: Human ratings\n",
      "59\n",
      "0.12967032967032968\n",
      "metric_definition: LLM-as-a-Judge\n",
      "78\n",
      "0.17142857142857143\n",
      "metric_definition: LLM post-processing\n",
      "43\n",
      "0.0945054945054945\n",
      "metric_definition: Distribution\n",
      "55\n",
      "0.12087912087912088\n",
      "metric_definition: Correlation\n",
      "23\n",
      "0.05054945054945055\n",
      "metric_definition: Reward\n",
      "40\n",
      "0.08791208791208792\n",
      "metric_definition: Soft match\n",
      "95\n",
      "0.2087912087912088\n",
      "metric_definition: Unknown\n",
      "1\n",
      "0.002197802197802198\n"
     ]
    }
   ],
   "source": [
    "for col in ['metric_definition: Exact match',\n",
    "       'metric_definition: Human ratings', 'metric_definition: LLM-as-a-Judge',\n",
    "       'metric_definition: LLM post-processing',\n",
    "       'metric_definition: Distribution', 'metric_definition: Correlation',\n",
    "       'metric_definition: Reward', 'metric_definition: Soft match',\n",
    "       'metric_definition: Unknown'\n",
    "]:\n",
    "    print(col)\n",
    "    print(included_df[col].sum())\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "570a4523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_source: Author-crafted\n",
      "0.432967032967033\n",
      "task_source: Crowd-sourced\n",
      "0.1912087912087912\n",
      "task_source: Unknown\n",
      "0.008791208791208791\n",
      "task_source: Procedurally-generated\n",
      "0.2681318681318681\n",
      "task_source: Expert-crafted\n",
      "0.12967032967032968\n",
      "task_source: Another benchmark\n",
      "0.42637362637362636\n",
      "task_source: LLM-generated\n",
      "0.3120879120879121\n",
      "task_source: Human exams\n",
      "0.0945054945054945\n",
      "task_source: Real task\n",
      "0.2967032967032967\n"
     ]
    }
   ],
   "source": [
    "for col in ['task_source: Author-crafted','task_source: Crowd-sourced','task_source: Unknown','task_source: Procedurally-generated','task_source: Expert-crafted','task_source: Another benchmark','task_source: LLM-generated','task_source: Human exams','task_source: Real task']:\n",
    "    print(col)\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8471a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_format: Structured\n",
      "96\n",
      "0.210989010989011\n",
      "response_format: Interaction\n",
      "30\n",
      "0.06593406593406594\n",
      "response_format: Multiple choice\n",
      "182\n",
      "0.4\n",
      "response_format: Short free response\n",
      "175\n",
      "0.38461538461538464\n",
      "response_format: Free response\n",
      "211\n",
      "0.46373626373626375\n",
      "response_format: Logits\n",
      "6\n",
      "0.013186813186813187\n",
      "response_format: Unknown\n",
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for col in ['response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown',]:\n",
    "    print(col)\n",
    "    print(included_df[col].sum())\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fd084fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3       True\n",
       "4      False\n",
       "       ...  \n",
       "450     True\n",
       "451    False\n",
       "452     True\n",
       "453     True\n",
       "454    False\n",
       "Length: 455, dtype: bool"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(included_df['response_format: Free response'] | included_df['response_format: Short free response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37e8f7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5340659340659341"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['results_author_validity: Yes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4eb6d510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9318681318681319"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['task_face_validity: Yes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e769e956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9296703296703297"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['metric_face_validity: Yes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e34033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root\n",
       "NLP                      91\n",
       "Reasoning                84\n",
       "Agents                   40\n",
       "Alignment                37\n",
       "Language Modelling       36\n",
       "Code Generation          26\n",
       "VQA                      15\n",
       "Medicine                 15\n",
       "Knowledge                13\n",
       "Retrieval                13\n",
       "Grounding                11\n",
       "User Interaction         10\n",
       "Law                       9\n",
       "Multilinguality           8\n",
       "Instruction Following     8\n",
       "Biology                   7\n",
       "Theory of Mind            5\n",
       "General Science           5\n",
       "Psychology                3\n",
       "Finance                   3\n",
       "Factuality                3\n",
       "LLM as a Judge            3\n",
       "Data Analysis             2\n",
       "General Purpose           2\n",
       "Mental Health             1\n",
       "Business                  1\n",
       "History                   1\n",
       "Chemistry                 1\n",
       "Education                 1\n",
       "Sports                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['phenomenon_taxonomy_root'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9409d7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "main_coder",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "bibkey",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "inclusion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "exclusion_criteria",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "exclusion_criteria_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "short_summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "contribution",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_short",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target_phenomenon",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_defined",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "definition_scope",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "purpose_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_item_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_definition_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_dataset_size",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_dataset_metadata",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_metadata_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_sampling_method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "response_format",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_definition_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "authorship",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "benchmark_availability",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "procedural_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "notes_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_train_val",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_dataset_size_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "response_format_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_aggregation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_subscores",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_subscores_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_metascoring",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "benchmark_location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "benchmark",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_contested",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_face_validity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_face_validity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "result_interpretation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_comparison",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_comparison_explanation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_realism",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_human_baseline",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_author_validity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_author_validity_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_statistics",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_access",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_ecology",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_ecology_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "definition_integrity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "definition_integrity_detail",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_dataset_size_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_fewshot",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "new_bibkey",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_taxonomy_root",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_taxonomy_leaf",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_taxonomy_alternate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "validate_taxonomy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_source_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source: Author-crafted",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Crowd-sourced",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Unknown",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Procedurally-generated",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Expert-crafted",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Another benchmark",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: LLM-generated",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Human exams",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_source: Real task",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_sampling_method: Targeted",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method: Criterion",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method: Convenience",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method: Random",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dataset_sampling_method: Unknown",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "response_format: Structured",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Interaction",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Multiple choice",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Short free response",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Free response",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Logits",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "response_format: Unknown",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_definition: Exact match",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Human ratings",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: LLM-as-a-Judge",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: LLM post-processing",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Distribution",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Correlation",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Reward",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Soft match",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_definition: Unknown",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "phenomenon_contested_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_contested: Widely-agreed",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "phenomenon_contested: No definition",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "phenomenon_contested: Contested",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_face_validity_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_face_validity: Partially",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_face_validity: ",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_face_validity: Yes",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_face_validity: No",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_face_validity_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_face_validity: Partially",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_face_validity: ",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_face_validity: Yes",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_face_validity: No",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_realism: No comparison made",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism: Not possible",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism: Realistic",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism: Comparison made",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_realism: No",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_author_validity_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_author_validity: ",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_author_validity: Yes",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "results_author_validity: No",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_ecology: ",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology: Complete",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology: Partial",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology: Representative",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "task_ecology: Constructed",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "metric_statistics_clean",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d68894e9-1631-4582-8b93-58804872d816",
       "rows": [
        [
         "337",
         "4/20/2025 1:41:56",
         "Valentin Hoffman",
         "hallVisoGenderDatasetBenchmarking2023",
         "VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution",
         "Include",
         null,
         null,
         "This paper examines occupation-related gender bias in vision-language models. The authors introduce VisoGender, a new benchmark of images associated with captions containing pronoun relationships of the depicted subjects and objects. They use VisoGender to probe resolution bias (difference between pronoun resolution accuracies for image subjects with masculine versus feminine gender presentations) and retrieval bias (bias in ratios of professionals with masculine and feminine gender presentations retrieved for a gender-neutral search query). Experiments on VisoGender provide evidence for bias in state-of-the-art vision-language models.",
         null,
         "Specific form of bias",
         "The benchmark measures occupation-related gender bias in vision-language models.",
         "Yes",
         "\"stress-testing gender bias in visual-linguistic reasoning and coreference resolution capabilities\" (p. 2)",
         "Subset",
         null,
         "The authors define two tasks. In the resolution task, the model is provided with a single image (either of an occupation-object or occupation-participant scene) and must rank the likelihood of captions containing different gender pronouns. In the retrieval task, the model is provided with a single gender-neutral caption and must retrieve images from a set containing professionals with different perceived genders.",
         "Resolution task: image and candidate captions. Retrieval task: caption and candidate images.",
         null,
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         null,
         "Yes",
         "Difficulty (single subject, two subjects of the same perceived gender presentation, two subjects of different perceived gender presentations)",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Multiple choice, Ranking of images",
         "Resolution task: accuracy gap. Retrieval bias: Bias@K, Skew@K, NDKL.",
         null,
         "Authors specify templates and search for images matching those templates among existing image databases as well as search providers.",
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         "690",
         "Resolution task: multiple choice. Retrieval task: ranking of images.",
         "Simple Mean",
         "Yes",
         "Based on difficulty: single subject, two subjects of the same perceived gender presentation, two subjects of different perceived gender presentations",
         null,
         "https://github.com/oxai/visogender/tree/main",
         "VisoGender",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Representative task (e.g. answering medical licensing exam questions), Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         "Resolution bias: constructed. Retrieval bias: representative.",
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "hallVisoGenderDatasetBenchmarking2023",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Another benchmark', 'Procedurally-generated']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Targeted']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Multiple choice', 'Free response']",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "['Distribution']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Representative', 'Constructed']",
         "False",
         "False",
         "False",
         "True",
         "True",
         "['Mean']"
        ],
        [
         "338",
         "4/20/2025 2:45:15",
         "Valentin Hoffman",
         "hanInstinctiveBiasSpurious2024",
         "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs",
         "Include",
         null,
         null,
         "This paper examines the extent to which multimodal LMs are distracted by spurious information provided via images in the context of commonsense question answering, which the authors refer to as \"instinctive bias.\" The authors create CorrelationQA, a benchmark consisting of (i) commonsense questions, (ii) answer choices, and (iii) images in various forms that either support the correct or one of the incorrect answer choices. Applying CorrelationQA to a series of mulitmodal LMs, the authors find that all of them struggle if the information in the image does not support the correct answer choice.",
         null,
         "Specific form of bias",
         "They want to measure \"instinctive bias\" in multimodal LMs, i.e., the tendency to pick an incorrect answer choice in commonsense question answering if that choice is supported by an image provided in the prompt.",
         "Yes",
         "\"ignor[ing] the semantic information in reasoning quizzes and answer[ing] directly to the objects in the pictures instead of utilizing their reasoning ability\" (p. 16163)",
         "Subset",
         null,
         "The task is defined as answering commonsense questions (multiple choice) in the presence of an image that either depicts the correct or one of the incorrect answer choices.",
         "Each item consists of (i) a commonsense question, (ii) one correct and five incorrect answer choices, and (iii) an image depicting one of the six answer choices.",
         "Each question occurs six times in the benchmark (with an image for each of the six answer options).",
         "Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "7,308",
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Multiple choice",
         "Exact Match (accuracy, F1, precision, recall), They also report accuracy drop between cases where the image supports the correct answer choice and the cases where it supports one of the incorrect answer choices.",
         null,
         "LMs are used for all steps: creating the questions, answer choices, and images.",
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "13 subsets based on the class of the entity the question is about: animal, art, color, city, food, history, human, material, natural, objects, plant, sports, technology",
         null,
         "https://github.com/MasaiahHan/CorrelationQA",
         "CorrelationQA",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         "No",
         "hanInstinctiveBiasSpurious2024",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Procedurally-generated', 'LLM-generated']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Targeted']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Multiple choice']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Exact match', '']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "340",
         "4/20/2025 8:43:27",
         "Valentin Hoffman",
         "esiobuROBBIERobustBias2023",
         "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
         "Include",
         null,
         null,
         "This paper attempts to make bias evaluation more robust by examining several bias benchmarks at the same time, two of which are new contributions of the paper, an approach referred to as ROBBIE (Robust Bias Evaluation). The authors apply ROBBIE to several LMs and also examine the effects of different bias mitigation strategies.",
         null,
         "General form of bias",
         "They want to measure social bias against different demographic groups as manifested in LM generations.",
         "Yes",
         "\"we define ``bias'' in this work as the proportion of subgroups for which the frequency of toxicity and negative regard generations falls outside an acceptable threshold\"",
         "Comprehensive",
         null,
         "The task is to generate continuations of prompts that contain mentions of demographic groups. The LMs should avoid toxic generations in response to these prompts (but see the limitations of the specific metric used to operationalize this below).",
         "Each item in both AdvPromptSet and HolisticBiasR is a prompt that contains mentions of one or more demographic groups.",
         null,
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "AdvPromptSet: 199,403. HolisticBiasR: 214,460.",
         "Yes",
         "toxicity of prompt, identity labels for mentioned demographic group(s), number of toxicity and bias terms present in the prompt",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Free response (e.g. summary paragraph, executable code)",
         "BiasScore: percentage of demographic groups in a dataset for which the LM continuations are more negative (e.g., toxic) than the average percentage of negative generations across demographic groups",
         "BiasScore seems like an ill-defined metric. For example, if an LM outputs exclusively negative continuations, the BiasScore is zero.",
         null,
         "Industry",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         null,
         "No",
         null,
         null,
         "https://github.com/facebookresearch/ResponsibleNLP/tree/main/robbie",
         "AdvPromptSet, HolisticBiasR (ROBBIE, mentioned in the title, refers to a combination of several existing benchmarks with these two new benchmarks)",
         "Contested",
         "Yes",
         "It works since most LMs do not output toxic content all the time, but this does not make it a metric that is suitable for bias measurement in principle.",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "Yes",
         "The authors use converging evidence across several benchmarks (and comparisons across benchmarks) as a way to increase construct validity.",
         null,
         "Outputs alone",
         "Representative task (e.g. answering medical licensing exam questions)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "esiobuROBBIERobustBias2023",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Another benchmark', 'Procedurally-generated']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Free response']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Distribution']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Partially']",
         "True",
         "False",
         "False",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Representative']",
         "False",
         "False",
         "False",
         "True",
         "False",
         null
        ],
        [
         "359",
         "4/21/2025 0:11:29",
         "Valentin Hoffman",
         "felknerWinoQueerCommunityintheloopBenchmark2023",
         "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
         "Include",
         null,
         null,
         "This paper examines biases in LMs that harm the LGBTQ+ community. Specifically, the authors introduce WinoQueer, a community-sourced benchmark for measuring anti-queer and anti-trans bias in LMs. They evaluate several LMs on WinoQueer, finding substantial evidence of anti-LGBTQ+ bias.",
         null,
         "General form of bias",
         "They want to measure anti-LGBTQ+ bias in LMs.",
         "Yes",
         "\"the presence of stereotypes that have caused harm to specific subgroups of the LGBTQ+ community\" (p. 9126)",
         "Comprehensive",
         null,
         "The task is defined as processing sentence pairs, where one contains a harmful, anti-LGBTQ+ stereotype, and the other one is a counterfactual in which the identity descriptor (e.g., \"gay\") has been replaced with a corresponding non-LGBTQ+ identity (e.g., \"straight\"). WinoQueer measures the summed log probability of the tokens shared by both sentences; if the summed log probability is higher for the stereotypical sentence, this is counted as biased.",
         "A pair of sentences, where one contains an anti-LGBTQ+ stereotype, and the other one is a counterfactual in which the identity descriptor (e.g., \"gay\") has been replaced with a corresponding non-LGBTQ+ identity (e.g., \"straight\").",
         null,
         "Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "45,540",
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "This task is not based on model responses; it exclusively relies on the probability assigned to input tokens.",
         "Bias Score: percentage of examples for which the likelihood of the more stereotypical sentence is higher than the likelihood of the less stereotypical sentence.",
         null,
         "Templates, names, and identity descriptors are taken from existing resources. The predicates expressing stereotypes are based on an online survey conducted with individuals from the LGBTQ+ community.",
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Nine different LGBTQ+ identity groups: LGBTQ, Queer, Transgender, Nonbinary, Bisexual, Pansexual, Lesbian, Asexual, Gay",
         null,
         "https://github.com/katyfelkner/winoqueer/tree/main",
         "WinoQueer",
         "Contested",
         "Yes",
         "High validity for detecting the presence of stereotypes, but low validity for measuring the absence of stereotypes. The authors acknowledge this distinction.",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "Yes",
         "They acknowledge that a low score on their benchmark does not mean that the model is unbiased.",
         "simple mean",
         "Model access required (e.g. logits)",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "felknerWinoQueerCommunityintheloopBenchmark2023",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Crowd-sourced', 'Another benchmark', 'Procedurally-generated']",
         "False",
         "True",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Targeted']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Logits']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Distribution']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Partially']",
         "True",
         "False",
         "False",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "386",
         "4/21/2025 7:28:55",
         "Valentin Hoffman",
         "sahooIndiBiasBenchmarkDataset2024",
         "IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context",
         "Include",
         null,
         null,
         "This paper examines LMs' biases and stereotypes in the Indian context. Specifically, the authors introduce IndiBias, a benchmark consisting of an adapted, translated, and augmented version of CrowS-Pairs. The authors also conduct a variant of SEAT to measure intersectional biases in LMs. Using both their Indian variant of CrowS-Pairs and SEAT, the authors test a series of LMs, finding varying levels of social biases.",
         null,
         "General form of bias",
         "They want to measure \"LM's biases and stereotypes in the Indian context\" (p. 8786).",
         "No",
         "The authors do not provide an explicit definition but talk about \"preference for stereotypical associations in diverse social contexts,\" which can be seen as their definition of bias (p. 8786).",
         "Comprehensive",
         null,
         "IndiBias consists of two parts: an Indian variant of CrowS-Pairs, where sentence pairs are probability-scored by LMs to measure bias, and a variant of SEAT that aims to measure intersectional biases. Only the first part fulfils the inclusion criteria of a benchmark, so this description will focus on it.",
         "Each item is a sentence pair, where the first sentence contains an identity term and a stereotypical or anti-stereotypical association, and the second sentence contains a different identity term that makes the sentence more or less stereotypical compared to the first sentence.",
         null,
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "English portion: 800. Hindi portion (translation of English portion): 800.",
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "This task is not based on LM responses; it solely relies on measuring the probabilities assigned to tokens in the sentence pairs.",
         "Bias Percentage: percentage of sentence pairs for which the more stereotypical sentence has a higher probability than the less stereotypical sentence.",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "The link works, but the GitHub repo does only contain a sample of all 800 items.",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Seven social categories: gender, religion, caste, age, region, physical appearance, occupation/socioeconomic status",
         null,
         "https://github.com/sahoonihar/IndiBias",
         "IndiBias",
         "Contested",
         "Yes",
         "High values of the metric indicate presence of bias, but low values do not mean that a model is unbiased. The authors do not acknowledge that.",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "simple mean",
         "Model access required (e.g. logits)",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "sahooIndiBiasBenchmarkDataset2024",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Another benchmark', 'Procedurally-generated', 'LLM-generated']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "True",
         "True",
         "False",
         "False",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Logits']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Distribution']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Partially']",
         "True",
         "False",
         "False",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "387",
         "4/21/2025 9:12:20",
         "Valentin Hoffman",
         "marchiorimanerbaSocialBiasProbing2024",
         "Social Bias Probing: Fairness Benchmarking for Language Models",
         "Include",
         null,
         null,
         "This paper examines social biases in LMs, focusing on disparate treatment. The authors start by observing a limitation of prior benchmarks (e.g., CrowS-Pairs), specifically that they are limited to binary association tests. As a remedy, the authors collect SoFa (Social Fairness), a benchmark that makes it possible to compare stereotypical associations across several identities. They show in their experiments that the resulting rankings of LMs differ from prior benchmarks.",
         null,
         "General form of bias",
         "The authors want to measure social biases in LMs, which they view through the lens of disparate treatment (i.e., individuals are treated differently according to their affiliation with a sensitive demographic group).",
         "Yes",
         "\"Social bias can be defined as the manifestation through language of ``prejudices, stereotypes, and discriminatory attitudes against certain groups of people'' (Navigli et al., 2023). These biases are featured in training datasets and are carried over into downstream applications, resulting in, for instance, classification errors concerning specific minorities and the generation of harmful content when models are prompted with sensitive identities (Cui et al., 2024; Gallegos et al., 2023).\" (p. 14655)",
         "Subset",
         null,
         "SoFa measures the perplexity of an LM on several sentences expressing the same stereotype with different identities as subjects (all identities belong to the same category such as religion). Low variance in the normalized log perplexity across identities indicates low bias.",
         "Each item is a sentence that concatenates an identity descriptor with a stereotype.",
         "Within each category (e.g., gender), items are grouped by stereotype, which is necessary to compute the bias metric.",
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "1,490,120",
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "This task is not based on model responses; it relies solely on perplexity measurements.",
         "SoFa Score: variance in normalized log perplexity across grouped sentences (i.e., sentences with the same stereotype and different identities)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Four social categories: gender, religion, disability, nationality",
         null,
         "https://huggingface.co/datasets/copenlu/sofa",
         "SoFa (Social Fairness)",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "No",
         null,
         "simple mean",
         "Model access required (e.g. logits)",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "marchiorimanerbaSocialBiasProbing2024",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Another benchmark', 'Procedurally-generated']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Logits']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Distribution']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "402",
         "4/21/2025 19:14:13",
         "Valentin Hoffman",
         "nangiaCrowSpairsChallengeDataset2020",
         "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
         "Include",
         null,
         null,
         "This paper examines social biases in LMs against protected demographic groups in the United States. The authors introduce a benchmark, Crowdsourced Stereotype Pairs (CrowS-Pairs), that consists of sentences pairs, where one is more stereotyping, and the other one is less stereotyping. Measuring the probability that LMs assign to these sentence pairs, the authors find that all evaluated LMs manifest substantial social biases across all tested categories.",
         null,
         "General form of bias",
         "The authors want to measure social biases in LMs.",
         "Yes",
         "\"whether a model generally prefers more stereotypical sentences\" (p. 1953)",
         "Subset",
         null,
         "LMs are provided with two sentences, where one is more stereotypical than the other one. The two sentences only differ in the mentioned target group. The authors then measure the probability assigned to the two sentences, where they control for different prior probabilities of the two target groups. For their final bias score, the authors measure how often LMs assign a higher probability to the more stereotypical sentence.",
         "A pair of minimally distant sentences that only differ in the mentioned target group (e.g., \"female\" versus \"male\"). One of the two sentences is more stereotypical than the other one.",
         null,
         "Crowd-sourced task examples (e.g. Prolific-created tasks)",
         "1,508",
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "The task is not based on model responses; it solely relies on the probabilities assigned to the tokens in the two sentences.",
         "Percentage of items (i.e., sentence pairs) for which an LM assigns a higher (psuedo-)likelihood to the stereotyping sentence over the less stereotyping sentence",
         "The metric is defined for masked LMs exclusively; the authors leave extension to autoregressive LMs to future work.",
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Nine social categories: race/color, gender/gender identity or expression, socioeconomic status/occupation, nationality, religion, age, sexual orientation, physical appearance, disability",
         null,
         "https://github.com/nyu-mll/crows-pairs/tree/master",
         "CrowS-Pairs (Crowdsourced Stereotype Pairs)",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "Yes",
         "The authors conduct a crowdsourced annotation study comparing the validity of their benchmark with StereoSet, a similar benchmark for probing social biases in LMs. They find that examples from CrowS-Pairs are judged as substantially more valid by annotators.",
         "simple mean",
         "Model access required (e.g. logits)",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "nangiaCrowSpairsChallengeDataset2020",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Crowd-sourced']",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Logits']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Distribution']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "406",
         "4/21/2025 22:52:21",
         "Valentin Hoffman",
         "morabitoSTOPBenchmarkingLarge2024",
         "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions",
         "Include",
         null,
         null,
         "This paper examines social biases in LMs as they unfold in situations that exhibit gradually increasing levels of offensiveness. To this aim, the authors introduce STOP (Sensitivity Testing on Offensive Progressions), a benchmark containing sentences that describe situations escalating from less to more explicitly offensive. They find that all examined LMs are inconsistent at detecting explicitly offensive bias in STOP.",
         null,
         "General form of bias",
         "They want to measure social biases in LMs, with a focus on their situational evolution.",
         "Yes",
         "\"any ``skew that produces a type of harm'' and can exist both implicitly and explicitly (Crawford, 2017; Dong et al., 2023)\" (p. 4221)",
         "Subset",
         null,
         "The LM is provides with five consecutive sentences that progress the narrative of a scenario, with each sentence presenting an increase in problematic content compared to the previous sentence. After each sentence, the LM is asked whether the situation is appropriate. The LM is evaluated by measuring how often the model replies with \"no\" in problematic settings and \"yes\" in unproblematic settings. There is an additional evaluation in which the LM is provided with a counterfactual sentence that provides additional context, which is insufficient to justify problematic situations but is thought to distract the LM.",
         "Each item consists of (i) five consecutive sentences that progress the narrative of a scenario, with each sentence presenting an increase in problematic content compared to the previous sentence, (ii) a counterfactual sentence that provides additional context, which is insufficient to justify problematic situations but is thought to distract the LM, and (iii) additional information (e.g., severity level, target group).",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "450 (2,700 unique sentences: 2,250 for the progressions, 450 for the counterfactuals)",
         "Yes",
         "severity level (low, moderate, high), targeted demographic, targeted sub-demographic",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Multiple choice",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "In the main evaluation, subsets are based on severity level (low, moderate, high). In the appendix, the authors also report subsets based on social category.",
         null,
         "https://github.com/Robert-Morabito/STOP",
         "STOP (Sensitivity Testing on Offensive Progressions)",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "Yes",
         "Yes",
         "They show that by training on STOP, performance on other bias benchmarks goes up.",
         "simple mean, standard deviation",
         "Outputs alone",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "morabitoSTOPBenchmarkingLarge2024",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Author-crafted', 'Procedurally-generated', 'LLM-generated']",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Targeted']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Multiple choice']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Exact match']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean', 'Std']"
        ],
        [
         "413",
         "4/22/2025 0:16:12",
         "Valentin Hoffman",
         "halevyFlexTapeCan`t2024",
         "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models",
         "Include",
         null,
         null,
         "This paper examines the extent to which model edits amplify social biases in LMs. To this end, the authors introduce Seesaw-cf, a benchmark of edits with accompanying prompts that aim to detect any bias-related effects of the edits. Using Seesaw-cf with several LMs and editing methods , the authors find that edits can amplify social biases in LMs.",
         null,
         "Specific form of bias",
         "They want to measure how model edits can amplify social biases in LMs.",
         "Yes",
         "\"unintended impact of model editing on the representations of certain demographic groups in models\" (p. 8690-8691)",
         "Subset",
         null,
         "The LMs' parameters are altered using the knowledge edits from the benchmarks. Then, the LMs are prompted using both (i) cloze test prompts and (ii) open-ended prompts, and the generated completions are analyzed with respect to social biases.",
         "Each item consists of (i) a knowledge edit, (ii) accompanying cloze test prompts (cross-subject and/or cross-property), and (iii) open-ended prompts.",
         null,
         "Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "3,516",
         "Yes",
         "demographic information about edited subjects (race, geographic origin, gender)",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Output probability change of attribute",
         "Cross-subject cloze completions: output probability change of attribute; cross-property cloze-completions: accuracy change; open-ended generations: LLM-judged level of bias (e.g., racism) plus human annotation.",
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         "Cloze completions: probability of different short continuations corresponding to different attributes. Open-ended descriptions: free response.",
         "Simple Mean",
         "Yes",
         "Type of edited property: field of work, country of citizenship, gender, place of birth.",
         null,
         "https://github.com/ENSCMA2/flextape",
         "Seesaw-cf",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "simple mean",
         "Model access required (e.g. logits)",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "halevyFlexTapeCan`t2024",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Another benchmark', 'Procedurally-generated']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Short free response', 'Free response']",
         "False",
         "False",
         "False",
         "True",
         "True",
         "False",
         "False",
         "['Exact match', 'Human ratings', 'LLM-as-a-Judge', '']",
         "True",
         "True",
         "True",
         "False",
         "False",
         "False",
         "False",
         "False",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "422",
         "4/22/2025 6:37:31",
         "Valentin Hoffman",
         "chenCrosscareAssessingHealthcare2024",
         "Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias",
         "Include",
         null,
         null,
         "This paper examines how LMs associate disease prevalence with different demographic groups. The authors introduce Cross-Care, a benchmark probing this association across 89 diseases and nine demographic groups. Applying Cross-Care to a series of LMs, the authors find substantial misalignment between LM representation of disease prevalence and real disease prevalence rates across demographic groups.",
         null,
         "Specific form of bias",
         "They want to measure representational biases in LMs, focusing on medical information.",
         "Yes",
         "\"the representation of disease prevalence across diverse demographic groups\" (p. 1)",
         "Subset",
         null,
         "The task consists of measuring the probability assigned by LMs to sentences associating demographic groups with diseases (e.g., \"[DEMOGRAPHIC] patients usually have [DISEASE]\").",
         "Each item is a sentence associating a demographic group with a disease.",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "8,010 for each of four considered languages",
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "The task is not based on responses; it relies solely on the probability assigned to the tokens in the sentence.",
         "Mean of the output logits",
         null,
         "The basis for the benchmark are two dictionaries: a dictionary of demographic terms and a dictionary of diseases. Both are taken from prior resources and, in the latter case, expanded by the authors. The authors then use ten templates that are filled with a demographic term and a disease to yield one item of the benchmark.",
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "four languages, different demographic groups",
         null,
         "https://github.com/shan23chen/Cross-Care",
         "Cross-Care",
         "Widely-agreed",
         "Yes",
         "Computing the mean of the logits does not seem mathematically sound, but the general approach of examining the output probabilities is valid.",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "No",
         null,
         "simple mean",
         "Model access required (e.g. logits)",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         "The benchmark is not released as such; the authors solely release the templates and the dictionaries. The size of the benchmark is computed based on the size of these three components; it is not explicitly mentioned in the paper.",
         "No",
         "chenCrosscareAssessingHealthcare2024",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Author-crafted', 'Another benchmark', 'Procedurally-generated']",
         "True",
         "False",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "False",
         "['Targeted']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Logits']",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Distribution']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No']",
         "False",
         "False",
         "False",
         "True",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['No']",
         "False",
         "False",
         "True",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ],
        [
         "423",
         "4/22/2025 8:42:08",
         "Valentin Hoffman",
         "wanFactualityTaxDiversityintervened2024",
         "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention",
         "Include",
         null,
         null,
         "This paper examines the question of whether prompt-based diversity interventions for text-to-image models result in non-factual demographic distribution. The authors introduce DoFaiR, a benchmark to systematically analyze this question, finding that diversity-oriented instructions indeed lead to historically less accurate demographic distributions. They also propose a method to mitigate this factuality tax.",
         null,
         "Specific form of bias",
         "They want to measure whether prompt-based diversity interventions impair demographic factuality in text-to-image generations.",
         "Yes",
         "\"Would diversity interventions impair demographic factuality in text-to-image generations? Here, we define ``demographic factuality'' as the faithfulness to the real racial or gender distribution among individuals in historical events.\" (p. 9082-9083)",
         "Subset",
         null,
         "The task is to generate an image depicting the faces of participants in a historical event. The generated image is then evaluated with respect to its demographic factuality and diversity.",
         "Each item consists of a tuple of ground truths about a participant class in real historical events, and the demographic distribution among them (event name, role, dominant race/genders, involved race/genders).",
         null,
         "Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "756",
         "No",
         null,
         "Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)",
         "image",
         "Exact Match (accuracy, F1, precision, recall), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Factual Diversity Divergence (quantifies the divergence in the level of demographic diversity in model generations compared with the factual ground truth)",
         "Three exact match metrics:\n- Dominant Demographic Accuracy (accuracy of the dominant demographic  groups in generated images, compared with the ground truth)\n- Involved Demographic Accuracy (accuracy of the depicted demographic groups in generated images)\n- Involved Demographic F-1 (weighted F-1 score for involved and non-involved demographic groups)\n\nRace and gender of generated faces is determined using the pretrained FairFace classifier.",
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "two demographic categories (race, gender)",
         null,
         "https://github.com/elainew728/factuality-tax-t2i",
         "DoFaiR (DemOgraphic FActualIty Representation)",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "Yes",
         "They conduct a human verification of DoFaiR items.",
         "simple mean",
         "Outputs alone",
         "Representative task (e.g. answering medical licensing exam questions)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "wanFactualityTaxDiversityintervened2024",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Procedurally-generated', 'LLM-generated']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Random', 'Targeted']",
         "True",
         "False",
         "False",
         "True",
         "False",
         "['Free response']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Exact match', 'LLM-as-a-Judge', 'Distribution']",
         "True",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Widely-agreed']",
         "True",
         "False",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Representative']",
         "False",
         "False",
         "False",
         "True",
         "False",
         "['Mean']"
        ],
        [
         "426",
         "4/22/2025 23:06:39",
         "Valentin Hoffman",
         "jhaSeeGULLStereotypeBenchmark2023",
         "SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models",
         "Include",
         null,
         null,
         "This paper introduces SeeGULL, a broad-coverage dataset of stereotypes spanning 178 countries across six continents. SeeGULL is built using the generative capabilities of LMs, and it also includes offensiveness scores for the stereotypes as well as human annotations.",
         null,
         "General form of bias",
         "They want to measure social stereotypes in LMs.",
         "Yes",
         "\"Stereotypes are generalized beliefs about categories of people, and are often reflected in data as statistical associations, which the language models rely on to associate concepts.\" (p. 9851)",
         "Subset",
         null,
         "SeeGULL consists of (identity, attribute) tuples such as (Italian, gangsters) as well as metadata. The paper does not present a task per se; rather, SeeGULL forms a basis on which different tasks/evaluations can be performed.",
         "Each item consists of an (identity, attribute) tuple such as (Italian, gangsters), annotations from three raters indicating stereotypicality, and an offensiveness score.",
         null,
         "Crowd-sourced task examples (e.g. Prolific-created tasks), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "7,750",
         "Yes",
         "Each item is accompanied by annotations from three raters indicating stereotypicality and an offensiveness score.",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Multiple choice",
         "Mean entailment",
         "Mean entailment on the natural language inference task is meant to measure strereotype strength.",
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         "The authors only collect the dataset, without specifying a task. In their experiments, they apply the framework proposed by Dev at al. (2020), which measures bias using a natural language inference setup.",
         "Simple Mean",
         "Yes",
         "different regions",
         null,
         "https://github.com/google-research-datasets/seegull",
         "SeeGULL (Stereotypes Generated Using LLMs in the Loop)",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "Yes",
         "They conduct a human validation study.",
         "simple mean",
         "Outputs alone",
         "Constructed task (e.g. predicting medical diagnoses from clinicians' notes)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "jhaSeeGULLStereotypeBenchmark2023",
         "Alignment",
         "Bias",
         null,
         "Alignment - Bias",
         "['Crowd-sourced', 'Procedurally-generated', 'LLM-generated']",
         "False",
         "True",
         "False",
         "True",
         "False",
         "False",
         "True",
         "False",
         "False",
         "['Targeted', 'Criterion']",
         "True",
         "True",
         "False",
         "False",
         "False",
         "['Multiple choice']",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Distribution']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Contested']",
         "False",
         "False",
         "True",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['Yes']",
         "False",
         "False",
         "True",
         "False",
         "['No comparison made']",
         "True",
         "False",
         "False",
         "False",
         "False",
         "['Yes']",
         "False",
         "True",
         "False",
         "['Constructed']",
         "False",
         "False",
         "False",
         "False",
         "True",
         "['Mean']"
        ]
       ],
       "shape": {
        "columns": 128,
        "rows": 12
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>...</th>\n",
       "      <th>results_author_validity:</th>\n",
       "      <th>results_author_validity: Yes</th>\n",
       "      <th>results_author_validity: No</th>\n",
       "      <th>task_ecology_clean</th>\n",
       "      <th>task_ecology:</th>\n",
       "      <th>task_ecology: Complete</th>\n",
       "      <th>task_ecology: Partial</th>\n",
       "      <th>task_ecology: Representative</th>\n",
       "      <th>task_ecology: Constructed</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>4/20/2025 1:41:56</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>hallVisoGenderDatasetBenchmarking2023</td>\n",
       "      <td>VisoGender: A dataset for benchmarking gender ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines occupation-related gender ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>4/20/2025 2:45:15</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>hanInstinctiveBiasSpurious2024</td>\n",
       "      <td>The Instinctive Bias: Spurious Images lead to ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the extent to which multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>4/20/2025 8:43:27</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>esiobuROBBIERobustBias2023</td>\n",
       "      <td>ROBBIE: Robust Bias Evaluation of Large Genera...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper attempts to make bias evaluation mo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>4/21/2025 0:11:29</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>felknerWinoQueerCommunityintheloopBenchmark2023</td>\n",
       "      <td>WinoQueer: A Community-in-the-Loop Benchmark f...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines biases in LMs that harm th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>4/21/2025 7:28:55</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>sahooIndiBiasBenchmarkDataset2024</td>\n",
       "      <td>IndiBias: A Benchmark Dataset to Measure Socia...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines LMs' biases and stereotype...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>4/21/2025 9:12:20</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>marchiorimanerbaSocialBiasProbing2024</td>\n",
       "      <td>Social Bias Probing: Fairness Benchmarking for...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines social biases in LMs, focu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>4/21/2025 19:14:13</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>nangiaCrowSpairsChallengeDataset2020</td>\n",
       "      <td>CrowS-Pairs: A Challenge Dataset for Measuring...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines social biases in LMs again...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>4/21/2025 22:52:21</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>morabitoSTOPBenchmarkingLarge2024</td>\n",
       "      <td>STOP! Benchmarking Large Language Models with ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines social biases in LMs as th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>4/22/2025 0:16:12</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>halevyFlexTapeCan`t2024</td>\n",
       "      <td>\"Flex Tape Can't Fix That\": Bias and Misinform...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the extent to which model ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>4/22/2025 6:37:31</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>chenCrosscareAssessingHealthcare2024</td>\n",
       "      <td>Cross-Care: Assessing the Healthcare Implicati...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines how LMs associate disease ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>4/22/2025 8:42:08</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>wanFactualityTaxDiversityintervened2024</td>\n",
       "      <td>The Factuality Tax of Diversity-Intervened Tex...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the question of whether pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>4/22/2025 23:06:39</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>jhaSeeGULLStereotypeBenchmark2023</td>\n",
       "      <td>SeeGULL: A Stereotype Benchmark with Broad Geo...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces SeeGULL, a broad-coverag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Timestamp        main_coder  \\\n",
       "337   4/20/2025 1:41:56  Valentin Hoffman   \n",
       "338   4/20/2025 2:45:15  Valentin Hoffman   \n",
       "340   4/20/2025 8:43:27  Valentin Hoffman   \n",
       "359   4/21/2025 0:11:29  Valentin Hoffman   \n",
       "386   4/21/2025 7:28:55  Valentin Hoffman   \n",
       "387   4/21/2025 9:12:20  Valentin Hoffman   \n",
       "402  4/21/2025 19:14:13  Valentin Hoffman   \n",
       "406  4/21/2025 22:52:21  Valentin Hoffman   \n",
       "413   4/22/2025 0:16:12  Valentin Hoffman   \n",
       "422   4/22/2025 6:37:31  Valentin Hoffman   \n",
       "423   4/22/2025 8:42:08  Valentin Hoffman   \n",
       "426  4/22/2025 23:06:39  Valentin Hoffman   \n",
       "\n",
       "                                              bibkey  \\\n",
       "337            hallVisoGenderDatasetBenchmarking2023   \n",
       "338                   hanInstinctiveBiasSpurious2024   \n",
       "340                       esiobuROBBIERobustBias2023   \n",
       "359  felknerWinoQueerCommunityintheloopBenchmark2023   \n",
       "386                sahooIndiBiasBenchmarkDataset2024   \n",
       "387            marchiorimanerbaSocialBiasProbing2024   \n",
       "402             nangiaCrowSpairsChallengeDataset2020   \n",
       "406                morabitoSTOPBenchmarkingLarge2024   \n",
       "413                          halevyFlexTapeCan`t2024   \n",
       "422             chenCrosscareAssessingHealthcare2024   \n",
       "423          wanFactualityTaxDiversityintervened2024   \n",
       "426                jhaSeeGULLStereotypeBenchmark2023   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "337  VisoGender: A dataset for benchmarking gender ...   Include   \n",
       "338  The Instinctive Bias: Spurious Images lead to ...   Include   \n",
       "340  ROBBIE: Robust Bias Evaluation of Large Genera...   Include   \n",
       "359  WinoQueer: A Community-in-the-Loop Benchmark f...   Include   \n",
       "386  IndiBias: A Benchmark Dataset to Measure Socia...   Include   \n",
       "387  Social Bias Probing: Fairness Benchmarking for...   Include   \n",
       "402  CrowS-Pairs: A Challenge Dataset for Measuring...   Include   \n",
       "406  STOP! Benchmarking Large Language Models with ...   Include   \n",
       "413  \"Flex Tape Can't Fix That\": Bias and Misinform...   Include   \n",
       "422  Cross-Care: Assessing the Healthcare Implicati...   Include   \n",
       "423  The Factuality Tax of Diversity-Intervened Tex...   Include   \n",
       "426  SeeGULL: A Stereotype Benchmark with Broad Geo...   Include   \n",
       "\n",
       "    exclusion_criteria exclusion_criteria_detail  \\\n",
       "337                NaN                       NaN   \n",
       "338                NaN                       NaN   \n",
       "340                NaN                       NaN   \n",
       "359                NaN                       NaN   \n",
       "386                NaN                       NaN   \n",
       "387                NaN                       NaN   \n",
       "402                NaN                       NaN   \n",
       "406                NaN                       NaN   \n",
       "413                NaN                       NaN   \n",
       "422                NaN                       NaN   \n",
       "423                NaN                       NaN   \n",
       "426                NaN                       NaN   \n",
       "\n",
       "                                         short_summary contribution  \\\n",
       "337  This paper examines occupation-related gender ...          NaN   \n",
       "338  This paper examines the extent to which multim...          NaN   \n",
       "340  This paper attempts to make bias evaluation mo...          NaN   \n",
       "359  This paper examines biases in LMs that harm th...          NaN   \n",
       "386  This paper examines LMs' biases and stereotype...          NaN   \n",
       "387  This paper examines social biases in LMs, focu...          NaN   \n",
       "402  This paper examines social biases in LMs again...          NaN   \n",
       "406  This paper examines social biases in LMs as th...          NaN   \n",
       "413  This paper examines the extent to which model ...          NaN   \n",
       "422  This paper examines how LMs associate disease ...          NaN   \n",
       "423  This paper examines the question of whether pr...          NaN   \n",
       "426  This paper introduces SeeGULL, a broad-coverag...          NaN   \n",
       "\n",
       "          phenomenon_short  ... results_author_validity:   \\\n",
       "337  Specific form of bias  ...                     False   \n",
       "338  Specific form of bias  ...                     False   \n",
       "340   General form of bias  ...                     False   \n",
       "359   General form of bias  ...                     False   \n",
       "386   General form of bias  ...                     False   \n",
       "387   General form of bias  ...                     False   \n",
       "402   General form of bias  ...                     False   \n",
       "406   General form of bias  ...                     False   \n",
       "413  Specific form of bias  ...                     False   \n",
       "422  Specific form of bias  ...                     False   \n",
       "423  Specific form of bias  ...                     False   \n",
       "426   General form of bias  ...                     False   \n",
       "\n",
       "    results_author_validity: Yes results_author_validity: No  \\\n",
       "337                        False                        True   \n",
       "338                        False                        True   \n",
       "340                         True                       False   \n",
       "359                         True                       False   \n",
       "386                        False                        True   \n",
       "387                        False                        True   \n",
       "402                         True                       False   \n",
       "406                         True                       False   \n",
       "413                        False                        True   \n",
       "422                        False                        True   \n",
       "423                         True                       False   \n",
       "426                         True                       False   \n",
       "\n",
       "                task_ecology_clean task_ecology:  task_ecology: Complete  \\\n",
       "337  [Representative, Constructed]          False                  False   \n",
       "338                  [Constructed]          False                  False   \n",
       "340               [Representative]          False                  False   \n",
       "359                  [Constructed]          False                  False   \n",
       "386                  [Constructed]          False                  False   \n",
       "387                  [Constructed]          False                  False   \n",
       "402                  [Constructed]          False                  False   \n",
       "406                  [Constructed]          False                  False   \n",
       "413                  [Constructed]          False                  False   \n",
       "422                  [Constructed]          False                  False   \n",
       "423               [Representative]          False                  False   \n",
       "426                  [Constructed]          False                  False   \n",
       "\n",
       "    task_ecology: Partial task_ecology: Representative  \\\n",
       "337                 False                         True   \n",
       "338                 False                        False   \n",
       "340                 False                         True   \n",
       "359                 False                        False   \n",
       "386                 False                        False   \n",
       "387                 False                        False   \n",
       "402                 False                        False   \n",
       "406                 False                        False   \n",
       "413                 False                        False   \n",
       "422                 False                        False   \n",
       "423                 False                         True   \n",
       "426                 False                        False   \n",
       "\n",
       "    task_ecology: Constructed metric_statistics_clean  \n",
       "337                      True                  [Mean]  \n",
       "338                      True                  [Mean]  \n",
       "340                     False                     NaN  \n",
       "359                      True                  [Mean]  \n",
       "386                      True                  [Mean]  \n",
       "387                      True                  [Mean]  \n",
       "402                      True                  [Mean]  \n",
       "406                      True             [Mean, Std]  \n",
       "413                      True                  [Mean]  \n",
       "422                      True                  [Mean]  \n",
       "423                     False                  [Mean]  \n",
       "426                      True                  [Mean]  \n",
       "\n",
       "[12 rows x 128 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[included_df['phenomenon_taxonomy_leaf'] == 'Bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ba811",
   "metadata": {},
   "source": [
    "### Headline Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2dc40f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_map = {'Agents':'Agents',\n",
    "                'Alignment':'Alignment',\n",
    "                  'Reasoning':'Reasoning',\n",
    "                    'Retrieval':'Other',\n",
    "                      'NLP':'NLP',\n",
    "       'Factuality':'Other',\n",
    "         'Knowledge':'Other',\n",
    "           'Language Modelling':'Language Modelling',\n",
    "             'Law':'Domain Applications',\n",
    "       'Code Generation':'Code Generation',\n",
    "         'Multilinguality':'Other',\n",
    "           'Instruction Following':'Other',\n",
    "       'Finance':'Domain Applications',\n",
    "         'Biology':'Domain Applications',\n",
    "           'General Science':'Domain Applications',\n",
    "             'History':'Domain Applications',\n",
    "       'User Interaction':'Other',\n",
    "         'Mental Health':'Domain Applications',\n",
    "           'Domain Applications':'Domain Applications',\n",
    "       'Psychology':'Domain Applications',\n",
    "         'Business':'Domain Applications',\n",
    "           'Data Analysis':'Other',\n",
    "             'Medicine':'Domain Applications',\n",
    "               'Chemistry':'Domain Applications',\n",
    "       'Grounding':'Other',\n",
    "         'VQA':'Other',\n",
    "           'Education':'Domain Applications',\n",
    "             'Theory of Mind':'Other',\n",
    "       'LLM as a Judge':'Other',\n",
    "         'Sports':'Domain Applications'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c324efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root\n",
       "Other                  91\n",
       "NLP                    91\n",
       "Reasoning              84\n",
       "Domain Applications    48\n",
       "Agents                 40\n",
       "Alignment              37\n",
       "Language Modelling     36\n",
       "Code Generation        26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['phenomenon_taxonomy_root'].map(summary_map).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85b6d10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Choice\n",
      "182\n",
      "Short Free Response\n",
      "175\n",
      "Free Response\n",
      "211\n",
      "Structured\n",
      "96\n",
      "Interaction\n",
      "30\n",
      "Logits\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "['response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown',]\n",
    "\n",
    "print('Multiple Choice')\n",
    "print(included_df['response_format: Multiple choice'].sum())\n",
    "print('Short Free Response')\n",
    "print((included_df['response_format: Short free response']).sum())\n",
    "print('Free Response')\n",
    "print((included_df['response_format: Free response']).sum())\n",
    "print('Structured')\n",
    "print(included_df['response_format: Structured'].sum())\n",
    "print('Interaction')\n",
    "print(included_df['response_format: Interaction'].sum())\n",
    "print('Logits')\n",
    "print(included_df['response_format: Logits'].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc9458",
   "metadata": {},
   "source": [
    "## Complete Taxonomy Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24c69789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phenomenon_taxonomy(df, categories, label, caption):\n",
    "    header = \"\"\"\\\\begin{longtable}{p{.02\\\\textwidth}p{.2\\\\textwidth}p{.78\\\\textwidth}}\n",
    "        \\\\textbf{Category} && \\\\textbf{Included Papers} \\\\\\\\\n",
    "        \\\\toprule\n",
    "        \\\\endhead\n",
    "        \\\\bottomrule \\\\\\\\\n",
    "        \\caption{\\\\textbf{\"\"\"+caption+\"\"\"}}\n",
    "        \\label{\"\"\"+label+\"\"\"}\n",
    "        \\\\endlastfoot\n",
    "        \\\\bottomrule\n",
    "        \\\\endfoot\n",
    "    \"\"\"\n",
    "    \n",
    "    footer = \"\"\"\n",
    "        \\end{longtable}\n",
    "    \"\"\"\n",
    "\n",
    "    latex = header\n",
    "\n",
    "    for category in categories:\n",
    "        subcategories = df[df['phenomenon_taxonomy_root'] == category]['phenomenon_taxonomy_leaf'].value_counts(dropna=False).index.tolist()\n",
    "        subcategories = [x for x in subcategories if x != '']\n",
    "        if not all([isinstance(x,str) for x in subcategories]):\n",
    "            temp = [x for x in subcategories if isinstance(x,str)]\n",
    "            subcategories = temp + [x for x in subcategories if not isinstance(x,str)]\n",
    "        if len(subcategories) <= 1:\n",
    "            latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"}} & \"\"\"\n",
    "            papers = df[df['phenomenon_taxonomy_root'] == category]['new_bibkey'].unique()\n",
    "            papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "            latex += ', '.join(papers)+' \\\\\\\\ \\n'\n",
    "        else:\n",
    "            latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"}} & \\\\\\\\ \\n\"\"\"\n",
    "            for subcategory in subcategories:\n",
    "                if isinstance(subcategory,str):\n",
    "                    papers = df[(df['phenomenon_taxonomy_root'] == category) & (df['phenomenon_taxonomy_leaf'] == subcategory)]['new_bibkey'].unique()\n",
    "                    papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "                    latex += '& '+str(subcategory)+' & '+', '.join(papers)+' \\\\\\\\ \\n'\n",
    "                else:   \n",
    "                    other_categories = df['phenomenon_taxonomy_leaf'].apply(lambda x: isinstance(x,str)) & (df['phenomenon_taxonomy_root'] == category)\n",
    "                    papers = df[(df['phenomenon_taxonomy_root'] == category) & ~other_categories]['new_bibkey'].unique()\n",
    "                    papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "                    latex += '& Other & '+', '.join(papers)+' \\\\\\\\ \\n'\n",
    "        latex += \"\"\"\\\\midrule \\n\"\"\"\n",
    "\n",
    "    latex += footer\n",
    "\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac824672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{p{.02\\textwidth}p{.2\\textwidth}p{.78\\textwidth}}\n",
      "        \\textbf{Category} && \\textbf{Included Papers} \\\\\n",
      "        \\toprule\n",
      "        \\endhead\n",
      "        \\bottomrule \\\\\n",
      "        \\caption{\\textbf{Descriptive Taxonomy of LLM Benchmark Target General Phenomena.}}\n",
      "        \\label{tab:phenomena_general}\n",
      "        \\endlastfoot\n",
      "        \\bottomrule\n",
      "        \\endfoot\n",
      "    \\multicolumn{2}{l}{\\textbf{NLP}} & \\\\ \n",
      "& Understanding & \\textcite{senelCoDA21EvaluatingLanguage2022}, \\textcite{pengCOPENProbingConceptual2022a}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{shavrinaRussianSuperGLUERussianLanguage2020}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{zhangHumorAIMassive2024}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{herediaXNLIeuDatasetCrosslingual2024}, \\textcite{maruNibblingHardCore2022}, \\textcite{chenCurriculumBroadcoverageBenchmark2022}, \\textcite{bandarkarBelebeleBenchmarkParallel2024}, \\textcite{zhangMuCGECMultireferenceMultisource2022}, \\textcite{garcia-ferreroThisNotDataset2023a}, \\textcite{kumarVisionlanguageModelsUnderstand2024}, \\textcite{flachsGrammaticalErrorCorrection2020}, \\textcite{royBenchCLAMPBenchmarkEvaluating2023}, \\textcite{shivagundeLargerProbesTell2023}, \\textcite{devriesDUMBBenchmarkSmart2023}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{sheScoNeBenchmarkingNegation2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{aggarwalIndicXNLIEvaluatingMultilingual2022}, \\textcite{kotoLargeLanguageModels2023}, \\textcite{doddapaneniLeavingNoIndic2023}, \\textcite{zhaoCould`veAskedThat2024a}, \\textcite{chiyah-garciaRepairsBlockWorld2024} \\\\ \n",
      "& Long Context & \\textcite{maMMLONGBENCHDOCBenchmarkingLongcontext2024}, \\textcite{kuratovBABILongTestingLimits2024}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangMarathonRaceRealm2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{karpinskaOneThousandOne2024a}, \\textcite{castillo-boladoPromptsDynamicConversational2024}, \\textcite{liuExposingAttentionGlitches2023}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{maharanaEvaluatingVeryLongterm2024} \\\\ \n",
      "& Summarization & \\textcite{mahbubUnveilingEssencePoetry2023}, \\textcite{yeGlobeSummChallengingBenchmark2024}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{asthanaEvaluatingLLMsTargeted2024a}, \\textcite{huangEmbraceDivergenceRicher2024}, \\textcite{amarOpenAspBenchmarkMultidocument2023}, \\textcite{liuRevisitingGoldStandard2023}, \\textcite{cheangCanLMsGeneralize2023}, \\textcite{josephFactPICOFactualityEvaluation2024a}, \\textcite{ryanRevisitingNonEnglishText2023}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{ramprasadAnalyzingLLMBehavior2024a}, \\textcite{zhaoORCHIDChineseDebate2023} \\\\ \n",
      "& Extraction & \\textcite{wangIELMOpenInformation2022a}, \\textcite{qiPreservingKnowledgeInvariance2023}, \\textcite{wangMAVENARGCompletingPuzzle2024}, \\textcite{liTEGDBComprehensiveDataset2024}, \\textcite{duPAGEDBenchmarkProcedural2024}, \\textcite{wangCanLanguageModels2023}, \\textcite{yanComprehensiveStudyTextattributed2023}, \\textcite{liCanLargeLanguage2024}, \\textcite{zhangDTGBComprehensiveBenchmark2024}, \\textcite{merdjanovskaNoiseBenchBenchmarkingImpact2024}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{agrawalLargeLanguageModels2022} \\\\ \n",
      "& Detection & \\textcite{wuDetectRLBenchmarkingLLMgenerated2024}, \\textcite{chakrabortyCounterTuringTest2023}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{mackoMULTITuDELargescaleMultilingual2023} \\\\ \n",
      "& Other & \\textcite{naousReadMeBenchmarkingMultilingual2024}, \\textcite{taktashevaRuBLiMPRussianBenchmark2024}, \\textcite{casolaMultiPICoMultilingualPerspectivist2024a}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{edmanCUTEMeasuringLLMs2024}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{parkOpenKoLLMLeaderboard2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{labanSummEditsMeasuringLLM2023a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Reasoning}} & \\\\ \n",
      "& Logical & \\textcite{helweMAFALDABenchmarkComprehensive2024a}, \\textcite{sanyalRobustLRDiagnosticBenchmark2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{jacoviChainofthoughtStrongIts2024}, \\textcite{huangMetaLogicLogicalReasoning2022}, \\textcite{leeQASAAdvancedQuestion2023}, \\textcite{liWhenLlmsMeet2024}, \\textcite{tianDiagnosingFirstorderLogical2021}, \\textcite{zhouRICAEvaluatingRobust2021}, \\textcite{patelMultiLogiEvalEvaluatingMultistep2024}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{chenTheoremQATheoremdrivenQuestion2023a}, \\textcite{yeAnaloBenchBenchmarkingIdentification2024}, \\textcite{jinCanLargeLanguage2024}, \\textcite{hanFOLIONaturalLanguage2024}, \\textcite{chenPremiseOrderMatters2024} \\\\ \n",
      "& Mathematical & \\textcite{huSportsMetricsBlendingText2024}, \\textcite{liGSMplusComprehensiveBenchmark2024}, \\textcite{shiLargeLanguageModels2023}, \\textcite{kurticMathadorLMDynamicBenchmark2024}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{liFRoGEvaluatingFuzzy2024a}, \\textcite{heOlympiadBenchChallengingBenchmark2024a}, \\textcite{shiLanguageModelsAre2023}, \\textcite{xiongTRIGOBenchmarkingFormal2023a}, \\textcite{friederMathematicalCapabilitiesChatGPT2023a}, \\textcite{mishraNumGLUESuiteFundamental2022a}, \\textcite{paruchuriWhatAreOdds2024}, \\textcite{zhaoDocMathevalEvaluatingMath2024} \\\\ \n",
      "& Planning & \\textcite{nasirGameTraversalBenchmarkEvaluatingPlanning2024a}, \\textcite{valmeekamPlanBenchExtensibleBenchmark2023}, \\textcite{choiLoTabenchBenchmarkingLanguageoriented2024}, \\textcite{heExploringCapacityPretrained2023a}, \\textcite{maAgentBoardAnalyticalEvaluation2024}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{suActPlan1KBenchmarkingProcedural2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{xieTravelPlannerBenchmarkRealworld2024}, \\textcite{chenLLMArenaAssessingCapabilities2024} \\\\ \n",
      "& Compositional & \\textcite{yuksekgonulWhenWhyVisionlanguage2023}, \\textcite{rayColaBenchmarkCompositional2023}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{hsiehSugarCrepeFixingHackable2023}, \\textcite{huangConMeRethinkingEvaluation2024}, \\textcite{hareshClevrSkillsCompositionalLanguage2024} \\\\ \n",
      "& Commonsense & \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{bhargavaDiscoSenseCommonsenseReasoning2022}, \\textcite{spragueMuSRTestingLimits2024}, \\textcite{bittonWinoGAViLGamifiedAssociation2022a}, \\textcite{sunBenchmarkingChineseCommonsense2024} \\\\ \n",
      "& Temporal & \\textcite{suLivingMomentCan2024}, \\textcite{tanBenchmarkingImprovingTemporal2023}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{fierroMuLanStudyFact2024a} \\\\ \n",
      "& Spatial & \\textcite{mirzaeeSPARTQATextualQuestion2021}, \\textcite{wangPictureWorthThousand2024}, \\textcite{comsaBenchmarkReasoningSpatial2023a}, \\textcite{shiriEmpiricalAnalysisSpatial2024} \\\\ \n",
      "& Mathematics & \\textcite{zhaoFinanceMATHKnowledgeintensiveMath2024}, \\textcite{luMathVistaEvaluatingMathematical2024}, \\textcite{zhangCarefulExaminationLarge2024} \\\\ \n",
      "& Multimodal & \\textcite{yingMMTbenchComprehensiveMultimodal2024}, \\textcite{chenM3CoTNovelBenchmark2024} \\\\ \n",
      "& Other & \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{romanouCRABAssessingStrength2023}, \\textcite{ribeiroSTREETMULTITASKSTRUCTURED2023}, \\textcite{liMEQABenchmarkMultihop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{fanNPHardEvalDynamicBenchmark2024}, \\textcite{samdarshiConnectingDotsEvaluating2024}, \\textcite{jiangBRAINTEASERLateralThinking2023}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{huangOlympicArenaBenchmarkingMultidiscipline2024a}, \\textcite{kazemiBoardgameQADatasetNatural2023a}, \\textcite{bhuiyaSeeminglyPlausibleDistractors2024}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{wangSciBenchEvaluatingCollegelevel2024}, \\textcite{chenWeakevalstrongEvaluatingEliciting2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Agents}} & \\\\ \n",
      "& Web & \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{dengMobilebenchEvaluationBenchmark2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{drouinWorkArenaHowCapable2024}, \\textcite{boisvertWorkArenaCompositionalPlanning2024} \\\\ \n",
      "& Tool Use & \\textcite{yeRoTBenchMultilevelBenchmark2024}, \\textcite{huangMetaToolBenchmarkLarge2024}, \\textcite{zhuangToolQADatasetLLM2023}, \\textcite{zhangToolBeHonestMultilevelHallucination2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{basuAPIBLENDComprehensiveCorpora2024}, \\textcite{wangAppBenchPlanningMultiple2024} \\\\ \n",
      "& Coding & \\textcite{mundlerSWTbenchTestingValidating2024a}, \\textcite{wangMINTEvaluatingLLMs2024}, \\textcite{huangMLAgentBenchEvaluatingLanguage2024}, \\textcite{yangInterCodeStandardizingBenchmarking2023}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{mathaiKGymPlatformDataset2024}, \\textcite{jainR2ETurningAny2024} \\\\ \n",
      "& Other & \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{pressCiteMECanLanguage2024a}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{wuSmartPlayBenchmarkLLMs2024}, \\textcite{fanR2HBuildingMultimodal2023}, \\textcite{zhouHAZARDChallengeEmbodied2024}, \\textcite{mialonGAIABenchmarkGeneral2024}, \\textcite{boginSUPEREvaluatingAgents2024}, \\textcite{abdelnabiCooperationCompetitionMaliciousness2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Alignment}} & \\\\ \n",
      "& Alignment & \\textcite{davidsonEvaluatingLanguageModel2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{chaoJailbreakBenchOpenRobustness2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{renValueBenchComprehensivelyEvaluating2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{marraffiniGreatestGoodBenchmark2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{huangFlamesBenchmarkingValue2024}, \\textcite{liuAlignBenchBenchmarkingChinese2024a}, \\textcite{hendrycksAligningAIShared2020a}, \\textcite{panRewardsJustifyMeans2023}, \\textcite{ramamurthyReinforcementLearningNot2023} \\\\ \n",
      "& Bias & \\textcite{hallVisoGenderDatasetBenchmarking2023}, \\textcite{hanInstinctiveBiasSpurious2024}, \\textcite{esiobuROBBIERobustBias2023}, \\textcite{felknerWinoQueerCommunityintheloopBenchmark2023}, \\textcite{sahooIndiBiasBenchmarkDataset2024}, \\textcite{marchiorimanerbaSocialBiasProbing2024}, \\textcite{nangiaCrowSpairsChallengeDataset2020}, \\textcite{morabitoSTOPBenchmarkingLarge2024}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{chenCrosscareAssessingHealthcare2024}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{jhaSeeGULLStereotypeBenchmark2023} \\\\ \n",
      "& Safety & \\textcite{yinSafeWorldGeodiverseSafety2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{alamCTIBenchBenchmarkEvaluating2024}, \\textcite{jinJailbreakingLargeLanguage2024}, \\textcite{dengCOLDBenchmarkChinese2022}, \\textcite{liWMDPBenchmarkMeasuring2024}, \\textcite{toyerTensorTrustInterpretable2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{zhangSafetyBenchEvaluatingSafety2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Language Modelling}} & \\\\ \n",
      "& Robustness & \\textcite{dumpalaSUGARCREPEDatasetVisionlanguage2024}, \\textcite{caoWorstPromptPerformance2024}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{liNaturalBenchEvaluatingVisionlanguage2024}, \\textcite{zhangUnveilingTapestryConsistency2024}, \\textcite{siREADINChineseMultitask2023}, \\textcite{yangCanLargeLanguage2024}, \\textcite{tamkinTaskAmbiguityHumans2023} \\\\ \n",
      "& Updating & \\textcite{dengNewTermBenchmarkingRealtime2024a}, \\textcite{jangTemporalWikiLifelongBenchmark2022a}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{kasaiRealTimeQAWhats2023}, \\textcite{srinivasanCLiMBContinualLearning2022}, \\textcite{yinALCUNALargeLanguage2023a} \\\\ \n",
      "& Hallucination & \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{sunHeadtotailHowKnowledgeable2024a}, \\textcite{pratoLargeLanguageModels2024}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{chenFELMBenchmarkingFactuality2023a} \\\\ \n",
      "& In-context Learning & \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{tanzerBenchmarkLearningTranslate2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{asaiBUFFETBenchmarkingLarge2024} \\\\ \n",
      "& Adaptability & \\textcite{albalakFETABenchmarkFewsample2022a}, \\textcite{itoGeneralizationCapacityNeural2024}, \\textcite{zhouVLUEMultitaskMultidimension2022} \\\\ \n",
      "& Unlearning & \\textcite{jinRWKUBenchmarkingRealworld2024a} \\\\ \n",
      "& Copyright & \\textcite{chenCopyBenchMeasuringLiteral2024} \\\\ \n",
      "& Calibration & \\textcite{yeBenchmarkingLlmsUncertainty2024} \\\\ \n",
      "& Other & \\textcite{tanDevBenchMultimodalDevelopmental2024}, \\textcite{magnussonPalomaBenchmarkEvaluating2024a}, \\textcite{zhangM3ExamMultilingualMultimodal2023a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Code Generation}} & \\\\ \n",
      "& Natural Language & \\textcite{saparinaAMBROSIABenchmarkParsing2024a}, \\textcite{bhaskarBenchmarkingImprovingTexttoSQL2023a}, \\textcite{changDrspiderDiagnosticEvaluation2023}, \\textcite{shahStackEvalBenchmarkingLlms2024a} \\\\ \n",
      "& Other & \\textcite{tangStrucbenchAreLarge2024}, \\textcite{athiwaratkunMultilingualEvaluationCode2023}, \\textcite{liInfiBenchEvaluatingQuestionanswering2024}, \\textcite{duMercuryCodeEfficiency2024}, \\textcite{zhaoQTSummQueryfocusedSummarization2023}, \\textcite{zhangSelenePioneeringAutomated2024}, \\textcite{maSpreadsheetBenchChallengingReal2024}, \\textcite{konIaCevalCodeGeneration2024}, \\textcite{waghjaleECCOCanWe2024}, \\textcite{huangEffiBenchBenchmarkingEfficiency2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{huangDAcodeAgentData2024}, \\textcite{liEvoCodeBenchEvolvingCode2024}, \\textcite{gongEvaluationLLMsSyntaxaware2024}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{liuRepoBenchBenchmarkingRepositorylevel2024}, \\textcite{zhangBenchmarkingDataScience2024}, \\textcite{huInfiAgentDABenchEvaluatingAgents2024}, \\textcite{yinNaturalLanguageCode2023}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{liCanLLMAlready2023a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{VQA}} & \\\\ \n",
      "& Understanding & \\textcite{liMultimodalArXivDataset2024}, \\textcite{zhangMIntRec20LargescaleBenchmark2024} \\\\ \n",
      "& Long Context & \\textcite{wangNeedleMultimodalHaystack2024} \\\\ \n",
      "& Other & \\textcite{chenAreWeRight2024}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{yuMMvetEvaluatingLarge2024}, \\textcite{parcalabescuVALSETaskindependentBenchmark2022}, \\textcite{wuEvaluatingAnalyzingRelationship2024a}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{dasEXAMSVMultidisciplineMultilingual2024}, \\textcite{gingOpenendedVQABenchmarking2024}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{luoCODISBenchmarkingContextdependent2024}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{duEmbSpatialbenchBenchmarkingSpatial2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Knowledge}} & \\\\ \n",
      "& Cultural & \\textcite{huangCevalMultilevelMultidiscipline2023a}, \\textcite{myungBLEnDBenchmarkLlms2024a}, \\textcite{romeroCVQACulturallydiverseMultilingual2024}, \\textcite{bhatiaLocalConceptsUniversals2024}, \\textcite{kannenAestheticsCulturalCompetence2024}, \\textcite{yinGeoMLAMAGeodiverseCommonsense2022a}, \\textcite{caoWenMindComprehensiveBenchmark2024a} \\\\ \n",
      "& General & \\textcite{jiangXFACTRMultilingualFactual2020a}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{wangMMLUproMoreRobust2024} \\\\ \n",
      "& Conflicts & \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{wuClashEvalQuantifyingTugofwar2024a}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Retrieval}} & \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{kalyanWikiDONewBenchmark2024a}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{krojerImageRetrievalContextual2022}, \\textcite{wuSTaRKBenchmarkingLLM2024a}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{monteiroRepLiQAQuestionansweringDataset2024a}, \\textcite{gaoEnablingLargeLanguage2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{pratoEpiKevalEvaluationLanguage2023a}, \\textcite{zhuFanOutQAMultihopMultidocument2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Grounding}} & \\textcite{piUOUOUncontextualizedUncommon2024a}, \\textcite{lyuMMScanMultimodal3D2024}, \\textcite{wangCanLanguageModels2024}, \\textcite{krojerAreDiffusionModels2023a}, \\textcite{kesenViLMAZeroshotBenchmark2024}, \\textcite{chungCanVisualLanguage2024}, \\textcite{luoMMMRSMultimodalMultiGSD2024}, \\textcite{liCanLanguageModels2023}, \\textcite{moneaGlitchMatrixLocating2024a}, \\textcite{guLanguageModelsHave2023}, \\textcite{hanReadingBooksGreat2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{User Interaction}} & \\textcite{kwanMTevalMultiturnCapabilities2024}, \\textcite{panchalWhatSayWhen2024}, \\textcite{liuConvBenchMultiturnConversation2024}, \\textcite{liuMMDUMultiturnMultiimage2024}, \\textcite{chevalierLanguageModelsScience2024}, \\textcite{baiMTbench101FinegrainedBenchmark2024}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ouDialogBenchEvaluatingLLMs2024}, \\textcite{liDiplomatDialogueDataset2023}, \\textcite{tuCharacterEvalChineseBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Multilinguality}} & \\textcite{augustyniakThisWayDesigning2022}, \\textcite{zhangMELAMultilingualEvaluation2024}, \\textcite{etxanizLatxaOpenLanguage2024}, \\textcite{riemenschneiderExploringLargeLanguage2023}, \\textcite{marchisioUnderstandingMitigatingLanguage2024}, \\textcite{zhangXSemPLRCrosslingualSemantic2023}, \\textcite{sunInformalLanguageProcessing2024}, \\textcite{songSLINGSinoLinguistic2022a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Instruction Following}} & \\textcite{xiaFOFOBenchmarkEvaluate2024}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{jiangFollowBenchMultilevelFinegrained2024}, \\textcite{zengEvaluatingLargeLanguage2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{liEvaluatingInstructionfollowingRobustness2024}, \\textcite{wenBenchmarkingComplexInstructionfollowing2024}, \\textcite{abdinKITABEvaluatingLLMs2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Theory of Mind}} & \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{jinMMToMQAMultimodalTheory2024}, \\textcite{gandhiUnderstandingSocialReasoning2023}, \\textcite{chenToMBenchBenchmarkingTheory2024}, \\textcite{xuOpenToMComprehensiveBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Factuality}} & \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{suTextttConflictBankBenchmarkEvaluating2024}, \\textcite{ohERBenchEntityrelationshipBased2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{LLM as a Judge}} & \\textcite{chenMLLMasajudgeAssessingMultimodal2024}, \\textcite{wattsPARIKSHALargescaleInvestigation2024}, \\textcite{lanCriticEvalEvaluatingLargescale2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Data Analysis}} & \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{zhuAreLargeLanguage2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{General Purpose}} & \\textcite{zhengLMSYSchat1MLargescaleRealworld2024}, \\textcite{wangUsercentricMultiintentBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\n",
      "        \\end{longtable}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "categories = included_df['phenomenon_taxonomy_root'].value_counts(dropna=True).index.tolist()\n",
    "specific_categories = [\"Sports\",\"Medicine\",\"Law\",\"Finance\",\"Biology\",\"Chemistry\",\"Education\",\"Mental Health\",\"Psychology\",\"General Science\",\"History\",\"Business\"]\n",
    "categories = [x for x in categories if x not in specific_categories]\n",
    "\n",
    "\n",
    "print(create_phenomenon_taxonomy(included_df, categories,\"tab:phenomena_general\",\"Descriptive Taxonomy of LLM Benchmark Target General Phenomena.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c090e9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{p{.02\\textwidth}p{.2\\textwidth}p{.78\\textwidth}}\n",
      "        \\textbf{Category} && \\textbf{Included Papers} \\\\\n",
      "        \\toprule\n",
      "        \\endhead\n",
      "        \\bottomrule \\\\\n",
      "        \\caption{\\textbf{Descriptive Taxonomy of LLM Benchmark Target Specific Application Phenomena.}}\n",
      "        \\label{tab:phenomena_specific}\n",
      "        \\endlastfoot\n",
      "        \\bottomrule\n",
      "        \\endfoot\n",
      "    \\multicolumn{2}{l}{\\textbf{Sports}} & \\textcite{xiaSportQABenchmarkSports2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Medicine}} & \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{liuRevisitingDeidentificationElectronic2023a}, \\textcite{hanMedSafetyBenchEvaluatingImproving2024}, \\textcite{khandekarMedCalcbenchEvaluatingLarge2024a}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{sivasubramaniamSM3texttoquerySyntheticMultimodel2024a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{xiangCAREMIChineseBenchmark2023a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Law}} & \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{zuoPatentEvalUnderstandingErrors2024}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{chiPLUELanguageUnderstanding2023}, \\textcite{sanchetiAgentspecificDeonticModality2022}, \\textcite{braunAGBDECorpusAutomated2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Finance}} & \\textcite{shahWhenFLUEMeets2022}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{zhaoFinDVerExplainableClaim2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Biology}} & \\textcite{wangPretrainingLanguageModel2023}, \\textcite{xuPEERComprehensiveMultitask2022}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{tsurutaSARSCoV2InteractionDataset2024}, \\textcite{guoCanLlmsSolve2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Chemistry}} & \\textcite{guoWhatCanLarge2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Education}} & \\textcite{chenDrAcademyBenchmarkEvaluating2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Mental Health}} & \\textcite{hengleStillNotQuite2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Psychology}} & \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{sabourEmoBenchEvaluatingEmotional2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{General Science}} & \\textcite{bajpaiCanLLMsReplace2024}, \\textcite{sadatMSciNLIDiverseBenchmark2024a}, \\textcite{liangSceMQAScientificCollege2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{dinhSciExBenchmarkingLarge2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{History}} & \\textcite{hauserLargeLanguageModelsExpertlevel2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Business}} & \\textcite{mitaStrikingGoldAdvertising2024} \\\\ \n",
      "\\midrule \n",
      "\n",
      "        \\end{longtable}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_phenomenon_taxonomy(included_df, specific_categories,\"tab:phenomena_specific\",\"Descriptive Taxonomy of LLM Benchmark Target Specific Application Phenomena.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09984588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metric_taxonomy(df, categories, label, caption):\n",
    "    header = \"\"\"\\\\begin{longtable}{p{.02\\\\textwidth}p{.2\\\\textwidth}p{.78\\\\textwidth}}\n",
    "        \\\\textbf{Category} && \\\\textbf{Included Papers} \\\\\\\\\n",
    "        \\\\toprule\n",
    "        \\\\endhead\n",
    "        \\\\bottomrule \\\\\\\\\n",
    "        \\caption{\\\\textbf{\"\"\"+caption+\"\"\"}}\n",
    "        \\label{\"\"\"+label+\"\"\"}\n",
    "        \\\\endlastfoot\n",
    "        \\\\bottomrule\n",
    "        \\\\endfoot\n",
    "    \"\"\"\n",
    "    \n",
    "    footer = \"\"\"\n",
    "        \\end{longtable}\n",
    "    \"\"\"\n",
    "\n",
    "    latex = header\n",
    "\n",
    "    for category in categories:\n",
    "        latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"}} & \"\"\"\n",
    "        papers = df[df['metric_definition_clean'].apply(lambda x: category in x)]['new_bibkey'].unique()\n",
    "        papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "        if len(papers) > 200:\n",
    "            papers1 = papers[:200]\n",
    "            papers2 = papers[200:]\n",
    "            latex += ', '.join(papers1)+' \\\\\\\\ \\n'\n",
    "            latex += \"\"\"\\\\midrule \\n\"\"\"\n",
    "            latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"} (Cont.)} & \"\"\"\n",
    "            latex += ', '.join(papers2)+' \\\\\\\\ \\n'\n",
    "        else:\n",
    "            latex += ', '.join(papers)+' \\\\\\\\ \\n'\n",
    "        latex += \"\"\"\\\\midrule \\n\"\"\"\n",
    "    latex += footer\n",
    "\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6de6ee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{p{.02\\textwidth}p{.2\\textwidth}p{.78\\textwidth}}\n",
      "        \\textbf{Category} && \\textbf{Included Papers} \\\\\n",
      "        \\toprule\n",
      "        \\endhead\n",
      "        \\bottomrule \\\\\n",
      "        \\caption{\\textbf{Descriptive Taxonomy of LLM Benchmark Metric Definitions.}}\n",
      "        \\label{tab:metric_definitions}\n",
      "        \\endlastfoot\n",
      "        \\bottomrule\n",
      "        \\endfoot\n",
      "    \\multicolumn{2}{l}{\\textbf{Exact match}} & \\textcite{davidsonEvaluatingLanguageModel2024}, \\textcite{helweMAFALDABenchmarkComprehensive2024a}, \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{wangIELMOpenInformation2022a}, \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{huangCevalMultilevelMultidiscipline2023a}, \\textcite{myungBLEnDBenchmarkLlms2024a}, \\textcite{sanyalRobustLRDiagnosticBenchmark2022a}, \\textcite{albalakFETABenchmarkFewsample2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{nasirGameTraversalBenchmarkEvaluatingPlanning2024a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{yuksekgonulWhenWhyVisionlanguage2023}, \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{saparinaAMBROSIABenchmarkParsing2024a}, \\textcite{augustyniakThisWayDesigning2022}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{wangMINTEvaluatingLLMs2024}, \\textcite{valmeekamPlanBenchExtensibleBenchmark2023}, \\textcite{zhangMELAMultilingualEvaluation2024}, \\textcite{etxanizLatxaOpenLanguage2024}, \\textcite{riemenschneiderExploringLargeLanguage2023}, \\textcite{qiPreservingKnowledgeInvariance2023}, \\textcite{shahWhenFLUEMeets2022}, \\textcite{kalyanWikiDONewBenchmark2024a}, \\textcite{marchisioUnderstandingMitigatingLanguage2024}, \\textcite{itoGeneralizationCapacityNeural2024}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{zhangXSemPLRCrosslingualSemantic2023}, \\textcite{sunInformalLanguageProcessing2024}, \\textcite{wangPretrainingLanguageModel2023}, \\textcite{bajpaiCanLLMsReplace2024}, \\textcite{hauserLargeLanguageModelsExpertlevel2024}, \\textcite{sadatMSciNLIDiverseBenchmark2024a}, \\textcite{dengNewTermBenchmarkingRealtime2024a}, \\textcite{yeRoTBenchMultilevelBenchmark2024}, \\textcite{maMMLONGBENCHDOCBenchmarkingLongcontext2024}, \\textcite{kuratovBABILongTestingLimits2024}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{senelCoDA21EvaluatingLanguage2022}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangMarathonRaceRealm2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{mahbubUnveilingEssencePoetry2023}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{suLivingMomentCan2024}, \\textcite{krojerImageRetrievalContextual2022}, \\textcite{rayColaBenchmarkCompositional2023}, \\textcite{bhaskarBenchmarkingImprovingTexttoSQL2023a}, \\textcite{xuPEERComprehensiveMultitask2022}, \\textcite{jangTemporalWikiLifelongBenchmark2022a}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{huangMetaToolBenchmarkLarge2024}, \\textcite{huSportsMetricsBlendingText2024}, \\textcite{choiLoTabenchBenchmarkingLanguageoriented2024}, \\textcite{songSLINGSinoLinguistic2022a}, \\textcite{pengCOPENProbingConceptual2022a}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{kwanMTevalMultiturnCapabilities2024}, \\textcite{hengleStillNotQuite2024}, \\textcite{shavrinaRussianSuperGLUERussianLanguage2020}, \\textcite{taktashevaRuBLiMPRussianBenchmark2024}, \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{wuSTaRKBenchmarkingLLM2024a}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{wangMAVENARGCompletingPuzzle2024}, \\textcite{jiangFollowBenchMultilevelFinegrained2024}, \\textcite{romanouCRABAssessingStrength2023}, \\textcite{zhaoFinanceMATHKnowledgeintensiveMath2024}, \\textcite{zhaoFinDVerExplainableClaim2024a}, \\textcite{casolaMultiPICoMultilingualPerspectivist2024a}, \\textcite{jinRWKUBenchmarkingRealworld2024a}, \\textcite{jiangXFACTRMultilingualFactual2020a}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{karpinskaOneThousandOne2024a}, \\textcite{suTextttConflictBankBenchmarkEvaluating2024}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{liTEGDBComprehensiveDataset2024}, \\textcite{jacoviChainofthoughtStrongIts2024}, \\textcite{ribeiroSTREETMULTITASKSTRUCTURED2023}, \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{liangSceMQAScientificCollege2024}, \\textcite{zhangHumorAIMassive2024}, \\textcite{liMEQABenchmarkMultihop2024}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{huangMetaLogicLogicalReasoning2022}, \\textcite{mirzaeeSPARTQATextualQuestion2021}, \\textcite{bhargavaDiscoSenseCommonsenseReasoning2022}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{maSpreadsheetBenchChallengingReal2024}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{zhuAreLargeLanguage2024}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{wuClashEvalQuantifyingTugofwar2024a}, \\textcite{pressCiteMECanLanguage2024a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{zhuangToolQADatasetLLM2023}, \\textcite{chenCopyBenchMeasuringLiteral2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{duPAGEDBenchmarkProcedural2024}, \\textcite{zhangToolBeHonestMultilevelHallucination2024}, \\textcite{yeBenchmarkingLlmsUncertainty2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{wangCanLanguageModels2023}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{piUOUOUncontextualizedUncommon2024a}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{changDrspiderDiagnosticEvaluation2023}, \\textcite{zengEvaluatingLargeLanguage2024}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{yanComprehensiveStudyTextattributed2023}, \\textcite{liCanLargeLanguage2024}, \\textcite{zhangDTGBComprehensiveBenchmark2024}, \\textcite{luMathVistaEvaluatingMathematical2024}, \\textcite{liuRevisitingGoldStandard2023}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{dengMobilebenchEvaluationBenchmark2024}, \\textcite{romeroCVQACulturallydiverseMultilingual2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{chenAreWeRight2024}, \\textcite{dumpalaSUGARCREPEDatasetVisionlanguage2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{wangPictureWorthThousand2024}, \\textcite{kasaiRealTimeQAWhats2023}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{tanBenchmarkingImprovingTemporal2023}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{lyuMMScanMultimodal3D2024}, \\textcite{heExploringCapacityPretrained2023a}, \\textcite{edmanCUTEMeasuringLLMs2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{liWhenLlmsMeet2024}, \\textcite{wangNeedleMultimodalHaystack2024}, \\textcite{zhangMIntRec20LargescaleBenchmark2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{huangDAcodeAgentData2024}, \\textcite{parcalabescuVALSETaskindependentBenchmark2022}, \\textcite{huangConMeRethinkingEvaluation2024}, \\textcite{bhatiaLocalConceptsUniversals2024}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{zhangCarefulExaminationLarge2024}, \\textcite{wuEvaluatingAnalyzingRelationship2024a}, \\textcite{liNaturalBenchEvaluatingVisionlanguage2024}, \\textcite{liuRepoBenchBenchmarkingRepositorylevel2024}, \\textcite{zhangBenchmarkingDataScience2024}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{huInfiAgentDABenchEvaluatingAgents2024}, \\textcite{fanNPHardEvalDynamicBenchmark2024}, \\textcite{wangCanLanguageModels2024}, \\textcite{liGSMplusComprehensiveBenchmark2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{castillo-boladoPromptsDynamicConversational2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{srinivasanCLiMBContinualLearning2022}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{samdarshiConnectingDotsEvaluating2024}, \\textcite{herediaXNLIeuDatasetCrosslingual2024}, \\textcite{alamCTIBenchBenchmarkEvaluating2024}, \\textcite{jinJailbreakingLargeLanguage2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{liuExposingAttentionGlitches2023}, \\textcite{merdjanovskaNoiseBenchBenchmarkingImpact2024}, \\textcite{jinMMToMQAMultimodalTheory2024}, \\textcite{dengCOLDBenchmarkChinese2022}, \\textcite{tsurutaSARSCoV2InteractionDataset2024}, \\textcite{maAgentBoardAnalyticalEvaluation2024}, \\textcite{maruNibblingHardCore2022}, \\textcite{chenCurriculumBroadcoverageBenchmark2022}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{dasEXAMSVMultidisciplineMultilingual2024}, \\textcite{jiangBRAINTEASERLateralThinking2023}, \\textcite{gingOpenendedVQABenchmarking2024}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{shiLargeLanguageModels2023}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{comsaBenchmarkReasoningSpatial2023a}, \\textcite{luoCODISBenchmarkingContextdependent2024}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{yinGeoMLAMAGeodiverseCommonsense2022a}, \\textcite{kesenViLMAZeroshotBenchmark2024}, \\textcite{kurticMathadorLMDynamicBenchmark2024}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{gandhiUnderstandingSocialReasoning2023}, \\textcite{bandarkarBelebeleBenchmarkParallel2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Exact match} (Cont.)} & \\textcite{tianDiagnosingFirstorderLogical2021}, \\textcite{zhouRICAEvaluatingRobust2021}, \\textcite{agrawalLargeLanguageModels2022}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{chenToMBenchBenchmarkingTheory2024}, \\textcite{wuDetectRLBenchmarkingLLMgenerated2024}, \\textcite{patelMultiLogiEvalEvaluatingMultistep2024}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{zhangMuCGECMultireferenceMultisource2022}, \\textcite{spragueMuSRTestingLimits2024}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{sabourEmoBenchEvaluatingEmotional2024a}, \\textcite{garcia-ferreroThisNotDataset2023a}, \\textcite{liuRevisitingDeidentificationElectronic2023a}, \\textcite{zhangUnveilingTapestryConsistency2024}, \\textcite{fierroMuLanStudyFact2024a}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{duEmbSpatialbenchBenchmarkingSpatial2024}, \\textcite{chungCanVisualLanguage2024}, \\textcite{kumarVisionlanguageModelsUnderstand2024}, \\textcite{flachsGrammaticalErrorCorrection2020}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{royBenchCLAMPBenchmarkEvaluating2023}, \\textcite{siREADINChineseMultitask2023}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{yinNaturalLanguageCode2023}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{hareshClevrSkillsCompositionalLanguage2024}, \\textcite{shivagundeLargerProbesTell2023}, \\textcite{wuSmartPlayBenchmarkLLMs2024}, \\textcite{liWMDPBenchmarkMeasuring2024}, \\textcite{renValueBenchComprehensivelyEvaluating2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{chakrabortyCounterTuringTest2023}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{devriesDUMBBenchmarkSmart2023}, \\textcite{hsiehSugarCrepeFixingHackable2023}, \\textcite{toyerTensorTrustInterpretable2024}, \\textcite{mackoMULTITuDELargescaleMultilingual2023}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{shiriEmpiricalAnalysisSpatial2024}, \\textcite{yingMMTbenchComprehensiveMultimodal2024}, \\textcite{chenM3CoTNovelBenchmark2024}, \\textcite{ohERBenchEntityrelationshipBased2024a}, \\textcite{zhangM3ExamMultilingualMultimodal2023a}, \\textcite{khandekarMedCalcbenchEvaluatingLarge2024a}, \\textcite{liCanLanguageModels2023}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{sivasubramaniamSM3texttoquerySyntheticMultimodel2024a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{liEvaluatingInstructionfollowingRobustness2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{sheScoNeBenchmarkingNegation2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{yinALCUNALargeLanguage2023a}, \\textcite{drouinWorkArenaHowCapable2024}, \\textcite{hanInstinctiveBiasSpurious2024}, \\textcite{boisvertWorkArenaCompositionalPlanning2024}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{parkOpenKoLLMLeaderboard2024}, \\textcite{fanR2HBuildingMultimodal2023}, \\textcite{chiPLUELanguageUnderstanding2023}, \\textcite{aggarwalIndicXNLIEvaluatingMultilingual2022}, \\textcite{sanchetiAgentspecificDeonticModality2022}, \\textcite{kotoLargeLanguageModels2023}, \\textcite{braunAGBDECorpusAutomated2024}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{doddapaneniLeavingNoIndic2023}, \\textcite{zhouHAZARDChallengeEmbodied2024}, \\textcite{yangCanLargeLanguage2024}, \\textcite{guoCanLlmsSolve2024}, \\textcite{mathaiKGymPlatformDataset2024}, \\textcite{liFRoGEvaluatingFuzzy2024a}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{wenBenchmarkingComplexInstructionfollowing2024}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{tamkinTaskAmbiguityHumans2023}, \\textcite{shiLanguageModelsAre2023}, \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{mialonGAIABenchmarkGeneral2024}, \\textcite{sunHeadtotailHowKnowledgeable2024a}, \\textcite{monteiroRepLiQAQuestionansweringDataset2024a}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{abdinKITABEvaluatingLLMs2024}, \\textcite{xiaSportQABenchmarkSports2024a}, \\textcite{xiongTRIGOBenchmarkingFormal2023a}, \\textcite{chenTheoremQATheoremdrivenQuestion2023a}, \\textcite{huangOlympicArenaBenchmarkingMultidiscipline2024a}, \\textcite{mishraNumGLUESuiteFundamental2022a}, \\textcite{kazemiBoardgameQADatasetNatural2023a}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{xieTravelPlannerBenchmarkRealworld2024}, \\textcite{zhangSafetyBenchEvaluatingSafety2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{basuAPIBLENDComprehensiveCorpora2024}, \\textcite{bhuiyaSeeminglyPlausibleDistractors2024}, \\textcite{pratoLargeLanguageModels2024}, \\textcite{boginSUPEREvaluatingAgents2024}, \\textcite{wangAppBenchPlanningMultiple2024}, \\textcite{gaoEnablingLargeLanguage2023a}, \\textcite{tanDevBenchMultimodalDevelopmental2024}, \\textcite{morabitoSTOPBenchmarkingLarge2024}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{zhaoCould`veAskedThat2024a}, \\textcite{moneaGlitchMatrixLocating2024a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{labanSummEditsMeasuringLLM2023a}, \\textcite{xiangCAREMIChineseBenchmark2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{pratoEpiKevalEvaluationLanguage2023a}, \\textcite{ramprasadAnalyzingLLMBehavior2024a}, \\textcite{chenFELMBenchmarkingFactuality2023a}, \\textcite{lanCriticEvalEvaluatingLargescale2024a}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{hendrycksAligningAIShared2020a}, \\textcite{panRewardsJustifyMeans2023}, \\textcite{wangSciBenchEvaluatingCollegelevel2024}, \\textcite{chenWeakevalstrongEvaluatingEliciting2024}, \\textcite{yeAnaloBenchBenchmarkingIdentification2024}, \\textcite{zhaoORCHIDChineseDebate2023}, \\textcite{paruchuriWhatAreOdds2024}, \\textcite{zhuFanOutQAMultihopMultidocument2024}, \\textcite{zhaoDocMathevalEvaluatingMath2024}, \\textcite{jinCanLargeLanguage2024}, \\textcite{hanFOLIONaturalLanguage2024}, \\textcite{sunBenchmarkingChineseCommonsense2024}, \\textcite{guLanguageModelsHave2023}, \\textcite{jainR2ETurningAny2024}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{ouDialogBenchEvaluatingLLMs2024}, \\textcite{liDiplomatDialogueDataset2023}, \\textcite{xuOpenToMComprehensiveBenchmark2024}, \\textcite{chenPremiseOrderMatters2024}, \\textcite{hanReadingBooksGreat2023}, \\textcite{wangMMLUproMoreRobust2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Human ratings}} & \\textcite{saparinaAMBROSIABenchmarkParsing2024a}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{songSLINGSinoLinguistic2022a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{asthanaEvaluatingLLMsTargeted2024a}, \\textcite{zhaoQTSummQueryfocusedSummarization2023}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{mitaStrikingGoldAdvertising2024}, \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{zhangHumorAIMassive2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{leeQASAAdvancedQuestion2023}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{chenCopyBenchMeasuringLiteral2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{huangEmbraceDivergenceRicher2024}, \\textcite{cheangCanLMsGeneralize2023}, \\textcite{kannenAestheticsCulturalCompetence2024}, \\textcite{zhangCarefulExaminationLarge2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{suActPlan1KBenchmarkingProcedural2024}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{josephFactPICOFactualityEvaluation2024a}, \\textcite{dinhSciExBenchmarkingLarge2024a}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{ryanRevisitingNonEnglishText2023}, \\textcite{zuoPatentEvalUnderstandingErrors2024}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{sivasubramaniamSM3texttoquerySyntheticMultimodel2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{wattsPARIKSHALargescaleInvestigation2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{friederMathematicalCapabilitiesChatGPT2023a}, \\textcite{marraffiniGreatestGoodBenchmark2024}, \\textcite{chevalierLanguageModelsScience2024}, \\textcite{bittonWinoGAViLGamifiedAssociation2022a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{tuCharacterEvalChineseBenchmark2024}, \\textcite{wangUsercentricMultiintentBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{LLM-as-a-Judge}} & \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{xiaFOFOBenchmarkEvaluate2024}, \\textcite{tangStrucbenchAreLarge2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{kwanMTevalMultiturnCapabilities2024}, \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{jiangFollowBenchMultilevelFinegrained2024}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{asthanaEvaluatingLLMsTargeted2024a}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{zhangHumorAIMassive2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{hsiehSugarCrepeFixingHackable2023}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{chenCopyBenchMeasuringLiteral2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{liCanLargeLanguage2024}, \\textcite{cheangCanLMsGeneralize2023}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{caoWorstPromptPerformance2024}, \\textcite{chaoJailbreakBenchOpenRobustness2024}, \\textcite{chenDrAcademyBenchmarkEvaluating2024}, \\textcite{liWhenLlmsMeet2024}, \\textcite{yuMMvetEvaluatingLarge2024}, \\textcite{kannenAestheticsCulturalCompetence2024}, \\textcite{yinSafeWorldGeodiverseSafety2024}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{panchalWhatSayWhen2024}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{liuConvBenchMultiturnConversation2024}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{hanMedSafetyBenchEvaluatingImproving2024}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{liuMMDUMultiturnMultiimage2024}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{wattsPARIKSHALargescaleInvestigation2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{sunHeadtotailHowKnowledgeable2024a}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{chevalierLanguageModelsScience2024}, \\textcite{baiMTbench101FinegrainedBenchmark2024}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{liuAlignBenchBenchmarkingChinese2024a}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{zhengLMSYSchat1MLargescaleRealworld2024}, \\textcite{zhuFanOutQAMultihopMultidocument2024}, \\textcite{shahStackEvalBenchmarkingLlms2024a}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{wangUsercentricMultiintentBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Distribution}} & \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{senelCoDA21EvaluatingLanguage2022}, \\textcite{jangTemporalWikiLifelongBenchmark2022a}, \\textcite{naousReadMeBenchmarkingMultilingual2024}, \\textcite{tanDevBenchMultimodalDevelopmental2024}, \\textcite{taktashevaRuBLiMPRussianBenchmark2024}, \\textcite{wuSTaRKBenchmarkingLLM2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{magnussonPalomaBenchmarkEvaluating2024a}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{mitaStrikingGoldAdvertising2024}, \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{yeBenchmarkingLlmsUncertainty2024}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{bhatiaLocalConceptsUniversals2024}, \\textcite{liGSMplusComprehensiveBenchmark2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{samdarshiConnectingDotsEvaluating2024}, \\textcite{krojerAreDiffusionModels2023a}, \\textcite{huangConMeRethinkingEvaluation2024}, \\textcite{yinGeoMLAMAGeodiverseCommonsense2022a}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{wuDetectRLBenchmarkingLLMgenerated2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{flachsGrammaticalErrorCorrection2020}, \\textcite{shivagundeLargerProbesTell2023}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{chakrabortyCounterTuringTest2023}, \\textcite{luoMMMRSMultimodalMultiGSD2024}, \\textcite{mackoMULTITuDELargescaleMultilingual2023}, \\textcite{liCanLanguageModels2023}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{hallVisoGenderDatasetBenchmarking2023}, \\textcite{esiobuROBBIERobustBias2023}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{kotoLargeLanguageModels2023}, \\textcite{yangCanLargeLanguage2024}, \\textcite{felknerWinoQueerCommunityintheloopBenchmark2023}, \\textcite{sahooIndiBiasBenchmarkDataset2024}, \\textcite{marchiorimanerbaSocialBiasProbing2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{nangiaCrowSpairsChallengeDataset2020}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{lanCriticEvalEvaluatingLargescale2024a}, \\textcite{chenCrosscareAssessingHealthcare2024}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{jhaSeeGULLStereotypeBenchmark2023}, \\textcite{ramamurthyReinforcementLearningNot2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Correlation}} & \\textcite{zhangMELAMultilingualEvaluation2024}, \\textcite{sunInformalLanguageProcessing2024}, \\textcite{wangPretrainingLanguageModel2023}, \\textcite{xuPEERComprehensiveMultitask2022}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{chenCurriculumBroadcoverageBenchmark2022}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{chenMLLMasajudgeAssessingMultimodal2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{mackoMULTITuDELargescaleMultilingual2023}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{ramamurthyReinforcementLearningNot2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Reward}} & \\textcite{mundlerSWTbenchTestingValidating2024a}, \\textcite{davidsonEvaluatingLanguageModel2024}, \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{nasirGameTraversalBenchmarkEvaluatingPlanning2024a}, \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{huangMLAgentBenchEvaluatingLanguage2024}, \\textcite{athiwaratkunMultilingualEvaluationCode2023}, \\textcite{liInfiBenchEvaluatingQuestionanswering2024}, \\textcite{duMercuryCodeEfficiency2024}, \\textcite{zhangSelenePioneeringAutomated2024}, \\textcite{konIaCevalCodeGeneration2024}, \\textcite{waghjaleECCOCanWe2024}, \\textcite{changDrspiderDiagnosticEvaluation2023}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{huangEffiBenchBenchmarkingEfficiency2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{huangDAcodeAgentData2024}, \\textcite{liEvoCodeBenchEvolvingCode2024}, \\textcite{gongEvaluationLLMsSyntaxaware2024}, \\textcite{yangInterCodeStandardizingBenchmarking2023}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{jinJailbreakingLargeLanguage2024}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{yinNaturalLanguageCode2023}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{fanR2HBuildingMultimodal2023}, \\textcite{zhouHAZARDChallengeEmbodied2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{chenLLMArenaAssessingCapabilities2024}, \\textcite{liCanLLMAlready2023a}, \\textcite{panRewardsJustifyMeans2023}, \\textcite{chenWeakevalstrongEvaluatingEliciting2024}, \\textcite{abdelnabiCooperationCompetitionMaliciousness2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Soft match}} & \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{liMultimodalArXivDataset2024}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{zhangXSemPLRCrosslingualSemantic2023}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{mahbubUnveilingEssencePoetry2023}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{yeGlobeSummChallengingBenchmark2024}, \\textcite{liInfiBenchEvaluatingQuestionanswering2024}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{zhaoQTSummQueryfocusedSummarization2023}, \\textcite{mitaStrikingGoldAdvertising2024}, \\textcite{tanzerBenchmarkLearningTranslate2024}, \\textcite{ribeiroSTREETMULTITASKSTRUCTURED2023}, \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{zhangHumorAIMassive2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{leeQASAAdvancedQuestion2023}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{duPAGEDBenchmarkProcedural2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{wangCanLanguageModels2023}, \\textcite{piUOUOUncontextualizedUncommon2024a}, \\textcite{amarOpenAspBenchmarkMultidocument2023}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{lyuMMScanMultimodal3D2024}, \\textcite{wangNeedleMultimodalHaystack2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{liuRepoBenchBenchmarkingRepositorylevel2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{castillo-boladoPromptsDynamicConversational2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{gingOpenendedVQABenchmarking2024}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{suActPlan1KBenchmarkingProcedural2024}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{panchalWhatSayWhen2024}, \\textcite{zhangMuCGECMultireferenceMultisource2022}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{ryanRevisitingNonEnglishText2023}, \\textcite{siREADINChineseMultitask2023}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{guoCanLlmsSolve2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{heOlympiadBenchChallengingBenchmark2024a}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{basuAPIBLENDComprehensiveCorpora2024}, \\textcite{gaoEnablingLargeLanguage2023a}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{chiyah-garciaRepairsBlockWorld2024}, \\textcite{zhaoORCHIDChineseDebate2023}, \\textcite{zhuFanOutQAMultihopMultidocument2024}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{hanReadingBooksGreat2023} \\\\ \n",
      "\\midrule \n",
      "\n",
      "        \\end{longtable}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_metric_taxonomy(included_df, \n",
    "['Exact match','Human ratings','LLM-as-a-Judge','Distribution','Correlation','Reward','Soft match'],'tab:metric_definitions','Descriptive Taxonomy of LLM Benchmark Metric Definitions.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44b14375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task_taxonomy(df, categories, label, caption):\n",
    "    header = \"\"\"\\\\begin{longtable}{p{.02\\\\textwidth}p{.2\\\\textwidth}p{.78\\\\textwidth}}\n",
    "        \\\\textbf{Category} && \\\\textbf{Included Papers} \\\\\\\\\n",
    "        \\\\toprule\n",
    "        \\\\endhead\n",
    "        \\\\bottomrule \\\\\\\\\n",
    "        \\caption{\\\\textbf{\"\"\"+caption+\"\"\"}}\n",
    "        \\label{\"\"\"+label+\"\"\"}\n",
    "        \\\\endlastfoot\n",
    "        \\\\bottomrule\n",
    "        \\\\endfoot\n",
    "    \"\"\"\n",
    "    \n",
    "    footer = \"\"\"\n",
    "        \\end{longtable}\n",
    "    \"\"\"\n",
    "\n",
    "    latex = header\n",
    "\n",
    "    for category in categories:\n",
    "        latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"}} & \"\"\"\n",
    "        papers = df[df['response_format_clean'].apply(lambda x: category in x)]['new_bibkey'].unique()\n",
    "        papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "        latex += ', '.join(papers)+' \\\\\\\\ \\n'\n",
    "        latex += \"\"\"\\\\midrule \\n\"\"\"\n",
    "    latex += footer\n",
    "\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c504547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{p{.02\\textwidth}p{.2\\textwidth}p{.78\\textwidth}}\n",
      "        \\textbf{Category} && \\textbf{Included Papers} \\\\\n",
      "        \\toprule\n",
      "        \\endhead\n",
      "        \\bottomrule \\\\\n",
      "        \\caption{\\textbf{Descriptive Taxonomy of LLM Benchmark Task Definitions.}}\n",
      "        \\label{tab:task_definitions}\n",
      "        \\endlastfoot\n",
      "        \\bottomrule\n",
      "        \\endfoot\n",
      "    \\multicolumn{2}{l}{\\textbf{Structured}} & \\textcite{mundlerSWTbenchTestingValidating2024a}, \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{wangIELMOpenInformation2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{nasirGameTraversalBenchmarkEvaluatingPlanning2024a}, \\textcite{saparinaAMBROSIABenchmarkParsing2024a}, \\textcite{augustyniakThisWayDesigning2022}, \\textcite{xiaFOFOBenchmarkEvaluate2024}, \\textcite{valmeekamPlanBenchExtensibleBenchmark2023}, \\textcite{tangStrucbenchAreLarge2024}, \\textcite{qiPreservingKnowledgeInvariance2023}, \\textcite{shahWhenFLUEMeets2022}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{wangPretrainingLanguageModel2023}, \\textcite{yeRoTBenchMultilevelBenchmark2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{senelCoDA21EvaluatingLanguage2022}, \\textcite{zhangMarathonRaceRealm2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{xuPEERComprehensiveMultitask2022}, \\textcite{huSportsMetricsBlendingText2024}, \\textcite{choiLoTabenchBenchmarkingLanguageoriented2024}, \\textcite{athiwaratkunMultilingualEvaluationCode2023}, \\textcite{duMercuryCodeEfficiency2024}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{wangMAVENARGCompletingPuzzle2024}, \\textcite{zhaoFinanceMATHKnowledgeintensiveMath2024}, \\textcite{zhangSelenePioneeringAutomated2024}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{huangMetaLogicLogicalReasoning2022}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{maSpreadsheetBenchChallengingReal2024}, \\textcite{zhuAreLargeLanguage2024}, \\textcite{konIaCevalCodeGeneration2024}, \\textcite{waghjaleECCOCanWe2024}, \\textcite{zhuangToolQADatasetLLM2023}, \\textcite{zhangToolBeHonestMultilevelHallucination2024}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{dengMobilebenchEvaluationBenchmark2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{huangEffiBenchBenchmarkingEfficiency2024}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{huangDAcodeAgentData2024}, \\textcite{liEvoCodeBenchEvolvingCode2024}, \\textcite{gongEvaluationLLMsSyntaxaware2024}, \\textcite{yangInterCodeStandardizingBenchmarking2023}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{liuRepoBenchBenchmarkingRepositorylevel2024}, \\textcite{zhangBenchmarkingDataScience2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{samdarshiConnectingDotsEvaluating2024}, \\textcite{liuExposingAttentionGlitches2023}, \\textcite{merdjanovskaNoiseBenchBenchmarkingImpact2024}, \\textcite{maAgentBoardAnalyticalEvaluation2024}, \\textcite{suActPlan1KBenchmarkingProcedural2024}, \\textcite{agrawalLargeLanguageModels2022}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{liuRevisitingDeidentificationElectronic2023a}, \\textcite{dinhSciExBenchmarkingLarge2024a}, \\textcite{royBenchCLAMPBenchmarkEvaluating2023}, \\textcite{siREADINChineseMultitask2023}, \\textcite{yinNaturalLanguageCode2023}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{sivasubramaniamSM3texttoquerySyntheticMultimodel2024a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{guoCanLlmsSolve2024}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{xiongTRIGOBenchmarkingFormal2023a}, \\textcite{xieTravelPlannerBenchmarkRealworld2024}, \\textcite{basuAPIBLENDComprehensiveCorpora2024}, \\textcite{wangAppBenchPlanningMultiple2024}, \\textcite{liCanLLMAlready2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{ramprasadAnalyzingLLMBehavior2024a}, \\textcite{chenFELMBenchmarkingFactuality2023a}, \\textcite{lanCriticEvalEvaluatingLargescale2024a}, \\textcite{kotturSIMMC20Taskoriented2021} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Interaction}} & \\textcite{davidsonEvaluatingLanguageModel2024}, \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{wangMINTEvaluatingLLMs2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{dengMobilebenchEvaluationBenchmark2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{yangInterCodeStandardizingBenchmarking2023}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{panchalWhatSayWhen2024}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{hareshClevrSkillsCompositionalLanguage2024}, \\textcite{wuSmartPlayBenchmarkLLMs2024}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{drouinWorkArenaHowCapable2024}, \\textcite{boisvertWorkArenaCompositionalPlanning2024}, \\textcite{chenLLMArenaAssessingCapabilities2024}, \\textcite{chenWeakevalstrongEvaluatingEliciting2024}, \\textcite{jainR2ETurningAny2024}, \\textcite{tuCharacterEvalChineseBenchmark2024}, \\textcite{abdelnabiCooperationCompetitionMaliciousness2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Multiple choice}} & \\textcite{helweMAFALDABenchmarkComprehensive2024a}, \\textcite{huangCevalMultilevelMultidiscipline2023a}, \\textcite{myungBLEnDBenchmarkLlms2024a}, \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{sanyalRobustLRDiagnosticBenchmark2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{yuksekgonulWhenWhyVisionlanguage2023}, \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{zhangMELAMultilingualEvaluation2024}, \\textcite{etxanizLatxaOpenLanguage2024}, \\textcite{riemenschneiderExploringLargeLanguage2023}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{sunInformalLanguageProcessing2024}, \\textcite{bajpaiCanLLMsReplace2024}, \\textcite{hauserLargeLanguageModelsExpertlevel2024}, \\textcite{sadatMSciNLIDiverseBenchmark2024a}, \\textcite{dengNewTermBenchmarkingRealtime2024a}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{rayColaBenchmarkCompositional2023}, \\textcite{songSLINGSinoLinguistic2022a}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{naousReadMeBenchmarkingMultilingual2024}, \\textcite{hengleStillNotQuite2024}, \\textcite{shavrinaRussianSuperGLUERussianLanguage2020}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{romanouCRABAssessingStrength2023}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{karpinskaOneThousandOne2024a}, \\textcite{suTextttConflictBankBenchmarkEvaluating2024}, \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{jacoviChainofthoughtStrongIts2024}, \\textcite{liangSceMQAScientificCollege2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{mirzaeeSPARTQATextualQuestion2021}, \\textcite{bhargavaDiscoSenseCommonsenseReasoning2022}, \\textcite{hsiehSugarCrepeFixingHackable2023}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{yeBenchmarkingLlmsUncertainty2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{zengEvaluatingLargeLanguage2024}, \\textcite{yanComprehensiveStudyTextattributed2023}, \\textcite{zhangDTGBComprehensiveBenchmark2024}, \\textcite{luMathVistaEvaluatingMathematical2024}, \\textcite{liuRevisitingGoldStandard2023}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{romeroCVQACulturallydiverseMultilingual2024}, \\textcite{chenAreWeRight2024}, \\textcite{dumpalaSUGARCREPEDatasetVisionlanguage2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{wangPictureWorthThousand2024}, \\textcite{tanBenchmarkingImprovingTemporal2023}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{liWhenLlmsMeet2024}, \\textcite{wangNeedleMultimodalHaystack2024}, \\textcite{zhangMIntRec20LargescaleBenchmark2024}, \\textcite{parcalabescuVALSETaskindependentBenchmark2022}, \\textcite{huangConMeRethinkingEvaluation2024}, \\textcite{bhatiaLocalConceptsUniversals2024}, \\textcite{liNaturalBenchEvaluatingVisionlanguage2024}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{alamCTIBenchBenchmarkEvaluating2024}, \\textcite{jinJailbreakingLargeLanguage2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{jinMMToMQAMultimodalTheory2024}, \\textcite{dengCOLDBenchmarkChinese2022}, \\textcite{tsurutaSARSCoV2InteractionDataset2024}, \\textcite{maruNibblingHardCore2022}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{dasEXAMSVMultidisciplineMultilingual2024}, \\textcite{jiangBRAINTEASERLateralThinking2023}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{kesenViLMAZeroshotBenchmark2024}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{gandhiUnderstandingSocialReasoning2023}, \\textcite{bandarkarBelebeleBenchmarkParallel2024}, \\textcite{tianDiagnosingFirstorderLogical2021}, \\textcite{zhouRICAEvaluatingRobust2021}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{chenToMBenchBenchmarkingTheory2024}, \\textcite{wuDetectRLBenchmarkingLLMgenerated2024}, \\textcite{patelMultiLogiEvalEvaluatingMultistep2024}, \\textcite{spragueMuSRTestingLimits2024}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{sabourEmoBenchEvaluatingEmotional2024a}, \\textcite{zhangUnveilingTapestryConsistency2024}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{duEmbSpatialbenchBenchmarkingSpatial2024}, \\textcite{kumarVisionlanguageModelsUnderstand2024}, \\textcite{siREADINChineseMultitask2023}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{liWMDPBenchmarkMeasuring2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{chakrabortyCounterTuringTest2023}, \\textcite{mackoMULTITuDELargescaleMultilingual2023}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{shiriEmpiricalAnalysisSpatial2024}, \\textcite{yingMMTbenchComprehensiveMultimodal2024}, \\textcite{chenM3CoTNovelBenchmark2024}, \\textcite{zhangM3ExamMultilingualMultimodal2023a}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{sheScoNeBenchmarkingNegation2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{yinALCUNALargeLanguage2023a}, \\textcite{hallVisoGenderDatasetBenchmarking2023}, \\textcite{hanInstinctiveBiasSpurious2024}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{parkOpenKoLLMLeaderboard2024}, \\textcite{chiPLUELanguageUnderstanding2023}, \\textcite{aggarwalIndicXNLIEvaluatingMultilingual2022}, \\textcite{sanchetiAgentspecificDeonticModality2022}, \\textcite{kotoLargeLanguageModels2023}, \\textcite{braunAGBDECorpusAutomated2024}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{doddapaneniLeavingNoIndic2023}, \\textcite{zhouHAZARDChallengeEmbodied2024}, \\textcite{liFRoGEvaluatingFuzzy2024a}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{mialonGAIABenchmarkGeneral2024}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{xiaSportQABenchmarkSports2024a}, \\textcite{huangOlympicArenaBenchmarkingMultidiscipline2024a}, \\textcite{marraffiniGreatestGoodBenchmark2024}, \\textcite{zhangSafetyBenchEvaluatingSafety2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{morabitoSTOPBenchmarkingLarge2024}, \\textcite{moneaGlitchMatrixLocating2024a}, \\textcite{labanSummEditsMeasuringLLM2023a}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{jhaSeeGULLStereotypeBenchmark2023}, \\textcite{hendrycksAligningAIShared2020a}, \\textcite{panRewardsJustifyMeans2023}, \\textcite{yeAnaloBenchBenchmarkingIdentification2024}, \\textcite{zhaoORCHIDChineseDebate2023}, \\textcite{jinCanLargeLanguage2024}, \\textcite{hanFOLIONaturalLanguage2024}, \\textcite{sunBenchmarkingChineseCommonsense2024}, \\textcite{guLanguageModelsHave2023}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ouDialogBenchEvaluatingLLMs2024}, \\textcite{liDiplomatDialogueDataset2023}, \\textcite{xuOpenToMComprehensiveBenchmark2024}, \\textcite{hanReadingBooksGreat2023}, \\textcite{wangMMLUproMoreRobust2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Short free response}} & \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{myungBLEnDBenchmarkLlms2024a}, \\textcite{albalakFETABenchmarkFewsample2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{yuksekgonulWhenWhyVisionlanguage2023}, \\textcite{augustyniakThisWayDesigning2022}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{wangMINTEvaluatingLLMs2024}, \\textcite{shahWhenFLUEMeets2022}, \\textcite{kalyanWikiDONewBenchmark2024a}, \\textcite{itoGeneralizationCapacityNeural2024}, \\textcite{liMultimodalArXivDataset2024}, \\textcite{maMMLONGBENCHDOCBenchmarkingLongcontext2024}, \\textcite{kuratovBABILongTestingLimits2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{suLivingMomentCan2024}, \\textcite{krojerImageRetrievalContextual2022}, \\textcite{rayColaBenchmarkCompositional2023}, \\textcite{jangTemporalWikiLifelongBenchmark2022a}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{pengCOPENProbingConceptual2022a}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{tanDevBenchMultimodalDevelopmental2024}, \\textcite{shavrinaRussianSuperGLUERussianLanguage2020}, \\textcite{taktashevaRuBLiMPRussianBenchmark2024}, \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{wuSTaRKBenchmarkingLLM2024a}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{casolaMultiPICoMultilingualPerspectivist2024a}, \\textcite{jinRWKUBenchmarkingRealworld2024a}, \\textcite{jiangXFACTRMultilingualFactual2020a}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{karpinskaOneThousandOne2024a}, \\textcite{suTextttConflictBankBenchmarkEvaluating2024}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{liangSceMQAScientificCollege2024}, \\textcite{liMEQABenchmarkMultihop2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{zhuAreLargeLanguage2024}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{wuClashEvalQuantifyingTugofwar2024a}, \\textcite{pressCiteMECanLanguage2024a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{zhangToolBeHonestMultilevelHallucination2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{wangCanLanguageModels2023}, \\textcite{piUOUOUncontextualizedUncommon2024a}, \\textcite{luMathVistaEvaluatingMathematical2024}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{kasaiRealTimeQAWhats2023}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{heExploringCapacityPretrained2023a}, \\textcite{edmanCUTEMeasuringLLMs2024}, \\textcite{wangNeedleMultimodalHaystack2024}, \\textcite{yuMMvetEvaluatingLarge2024}, \\textcite{kannenAestheticsCulturalCompetence2024}, \\textcite{zhangCarefulExaminationLarge2024}, \\textcite{wuEvaluatingAnalyzingRelationship2024a}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{huInfiAgentDABenchEvaluatingAgents2024}, \\textcite{fanNPHardEvalDynamicBenchmark2024}, \\textcite{liGSMplusComprehensiveBenchmark2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{castillo-boladoPromptsDynamicConversational2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{srinivasanCLiMBContinualLearning2022}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{herediaXNLIeuDatasetCrosslingual2024}, \\textcite{alamCTIBenchBenchmarkEvaluating2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{liuExposingAttentionGlitches2023}, \\textcite{maAgentBoardAnalyticalEvaluation2024}, \\textcite{chenCurriculumBroadcoverageBenchmark2022}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{gingOpenendedVQABenchmarking2024}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{shiLargeLanguageModels2023}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{comsaBenchmarkReasoningSpatial2023a}, \\textcite{luoCODISBenchmarkingContextdependent2024}, \\textcite{yinGeoMLAMAGeodiverseCommonsense2022a}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{chenMLLMasajudgeAssessingMultimodal2024}, \\textcite{zhouRICAEvaluatingRobust2021}, \\textcite{agrawalLargeLanguageModels2022}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{garcia-ferreroThisNotDataset2023a}, \\textcite{zhangUnveilingTapestryConsistency2024}, \\textcite{fierroMuLanStudyFact2024a}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{siREADINChineseMultitask2023}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{shivagundeLargerProbesTell2023}, \\textcite{renValueBenchComprehensivelyEvaluating2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{devriesDUMBBenchmarkSmart2023}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{khandekarMedCalcbenchEvaluatingLarge2024a}, \\textcite{liCanLanguageModels2023}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{yinALCUNALargeLanguage2023a}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{parkOpenKoLLMLeaderboard2024}, \\textcite{chiPLUELanguageUnderstanding2023}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{yangCanLargeLanguage2024}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{tamkinTaskAmbiguityHumans2023}, \\textcite{shiLanguageModelsAre2023}, \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{sunHeadtotailHowKnowledgeable2024a}, \\textcite{monteiroRepLiQAQuestionansweringDataset2024a}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenTheoremQATheoremdrivenQuestion2023a}, \\textcite{huangOlympicArenaBenchmarkingMultidiscipline2024a}, \\textcite{mishraNumGLUESuiteFundamental2022a}, \\textcite{kazemiBoardgameQADatasetNatural2023a}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{bhuiyaSeeminglyPlausibleDistractors2024}, \\textcite{dumpalaSUGARCREPEDatasetVisionlanguage2024}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{moneaGlitchMatrixLocating2024a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{hendrycksAligningAIShared2020a}, \\textcite{wangSciBenchEvaluatingCollegelevel2024}, \\textcite{chiyah-garciaRepairsBlockWorld2024}, \\textcite{paruchuriWhatAreOdds2024}, \\textcite{zhuFanOutQAMultihopMultidocument2024}, \\textcite{zhaoDocMathevalEvaluatingMath2024}, \\textcite{liDiplomatDialogueDataset2023}, \\textcite{chenPremiseOrderMatters2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Free response}} & \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{valmeekamPlanBenchExtensibleBenchmark2023}, \\textcite{marchisioUnderstandingMitigatingLanguage2024}, \\textcite{liMultimodalArXivDataset2024}, \\textcite{zhangXSemPLRCrosslingualSemantic2023}, \\textcite{yeRoTBenchMultilevelBenchmark2024}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{mahbubUnveilingEssencePoetry2023}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{bhaskarBenchmarkingImprovingTexttoSQL2023a}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{huangMetaToolBenchmarkLarge2024}, \\textcite{huangMLAgentBenchEvaluatingLanguage2024}, \\textcite{yeGlobeSummChallengingBenchmark2024}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{kwanMTevalMultiturnCapabilities2024}, \\textcite{liInfiBenchEvaluatingQuestionanswering2024}, \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{jiangFollowBenchMultilevelFinegrained2024}, \\textcite{zhaoFinanceMATHKnowledgeintensiveMath2024}, \\textcite{zhaoFinDVerExplainableClaim2024a}, \\textcite{magnussonPalomaBenchmarkEvaluating2024a}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{asthanaEvaluatingLLMsTargeted2024a}, \\textcite{zhaoQTSummQueryfocusedSummarization2023}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{liTEGDBComprehensiveDataset2024}, \\textcite{mitaStrikingGoldAdvertising2024}, \\textcite{tanzerBenchmarkLearningTranslate2024}, \\textcite{ribeiroSTREETMULTITASKSTRUCTURED2023}, \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{zhangHumorAIMassive2024}, \\textcite{liMEQABenchmarkMultihop2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{huangMetaLogicLogicalReasoning2022}, \\textcite{leeQASAAdvancedQuestion2023}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{pressCiteMECanLanguage2024a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{zhuangToolQADatasetLLM2023}, \\textcite{chenCopyBenchMeasuringLiteral2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{duPAGEDBenchmarkProcedural2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{wangCanLanguageModels2023}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{changDrspiderDiagnosticEvaluation2023}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{liCanLargeLanguage2024}, \\textcite{zhangDTGBComprehensiveBenchmark2024}, \\textcite{huangEmbraceDivergenceRicher2024}, \\textcite{amarOpenAspBenchmarkMultidocument2023}, \\textcite{cheangCanLMsGeneralize2023}, \\textcite{romeroCVQACulturallydiverseMultilingual2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{caoWorstPromptPerformance2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{huangEffiBenchBenchmarkingEfficiency2024}, \\textcite{chaoJailbreakBenchOpenRobustness2024}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{chenDrAcademyBenchmarkEvaluating2024}, \\textcite{lyuMMScanMultimodal3D2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{liWhenLlmsMeet2024}, \\textcite{yuMMvetEvaluatingLarge2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{yinSafeWorldGeodiverseSafety2024}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{wangCanLanguageModels2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{krojerAreDiffusionModels2023a}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{kurticMathadorLMDynamicBenchmark2024}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{panchalWhatSayWhen2024}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{zhangMuCGECMultireferenceMultisource2022}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{josephFactPICOFactualityEvaluation2024a}, \\textcite{liuConvBenchMultiturnConversation2024}, \\textcite{dinhSciExBenchmarkingLarge2024a}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{chungCanVisualLanguage2024}, \\textcite{flachsGrammaticalErrorCorrection2020}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{royBenchCLAMPBenchmarkEvaluating2023}, \\textcite{ryanRevisitingNonEnglishText2023}, \\textcite{siREADINChineseMultitask2023}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{zuoPatentEvalUnderstandingErrors2024}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{hanMedSafetyBenchEvaluatingImproving2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{luoMMMRSMultimodalMultiGSD2024}, \\textcite{devriesDUMBBenchmarkSmart2023}, \\textcite{toyerTensorTrustInterpretable2024}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{hareshClevrSkillsCompositionalLanguage2024}, \\textcite{ohERBenchEntityrelationshipBased2024a}, \\textcite{liuMMDUMultiturnMultiimage2024}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{liEvaluatingInstructionfollowingRobustness2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{sheScoNeBenchmarkingNegation2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{drouinWorkArenaHowCapable2024}, \\textcite{hallVisoGenderDatasetBenchmarking2023}, \\textcite{wattsPARIKSHALargescaleInvestigation2024}, \\textcite{esiobuROBBIERobustBias2023}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{fanR2HBuildingMultimodal2023}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{yangCanLargeLanguage2024}, \\textcite{mathaiKGymPlatformDataset2024}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{wenBenchmarkingComplexInstructionfollowing2024}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{heOlympiadBenchChallengingBenchmark2024a}, \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{abdinKITABEvaluatingLLMs2024}, \\textcite{friederMathematicalCapabilitiesChatGPT2023a}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{xieTravelPlannerBenchmarkRealworld2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{chevalierLanguageModelsScience2024}, \\textcite{pratoLargeLanguageModels2024}, \\textcite{huangFlamesBenchmarkingValue2024}, \\textcite{baiMTbench101FinegrainedBenchmark2024}, \\textcite{boginSUPEREvaluatingAgents2024}, \\textcite{gaoEnablingLargeLanguage2023a}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{bittonWinoGAViLGamifiedAssociation2022a}, \\textcite{zhaoCould`veAskedThat2024a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{xiangCAREMIChineseBenchmark2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{pratoEpiKevalEvaluationLanguage2023a}, \\textcite{liuAlignBenchBenchmarkingChinese2024a}, \\textcite{ramprasadAnalyzingLLMBehavior2024a}, \\textcite{lanCriticEvalEvaluatingLargescale2024a}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{zhengLMSYSchat1MLargescaleRealworld2024}, \\textcite{zhaoORCHIDChineseDebate2023}, \\textcite{shahStackEvalBenchmarkingLlms2024a}, \\textcite{jainR2ETurningAny2024}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{wangUsercentricMultiintentBenchmark2024}, \\textcite{hanReadingBooksGreat2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Logits}} & \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{felknerWinoQueerCommunityintheloopBenchmark2023}, \\textcite{sahooIndiBiasBenchmarkDataset2024}, \\textcite{marchiorimanerbaSocialBiasProbing2024}, \\textcite{nangiaCrowSpairsChallengeDataset2020}, \\textcite{chenCrosscareAssessingHealthcare2024} \\\\ \n",
      "\\midrule \n",
      "\n",
      "        \\end{longtable}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_task_taxonomy(included_df,\n",
    "['Structured','Interaction','Multiple choice','Short free response','Free response','Logits'],'tab:task_definitions','Descriptive Taxonomy of LLM Benchmark Task Definitions.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd11b18",
   "metadata": {},
   "source": [
    "## Codebook Detailed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df0f2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = ['main_coder','Timestamp',\n",
    "               'task_source: Author-crafted','task_source: Crowd-sourced','task_source: Unknown','task_source: Procedurally-generated','task_source: Expert-crafted','task_source: Another benchmark','task_source: LLM-generated','task_source: Human exams','task_source: Real task','dataset_sampling_method: Targeted', 'dataset_sampling_method: Criterion',\n",
    "       'dataset_sampling_method: Convenience', 'dataset_sampling_method: Random','dataset_sampling_method: Unknown','response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown',\n",
    "       'metric_definition: Exact match',\n",
    "'metric_definition: Human ratings',\n",
    "'metric_definition: LLM-as-a-Judge',\n",
    "'metric_definition: LLM post-processing',\n",
    "'metric_definition: Distribution',\n",
    "'metric_definition: Correlation',\n",
    "'metric_definition: Reward',\n",
    "'metric_definition: Soft match',\n",
    "'metric_definition: Unknown',\n",
    "'validate_taxonomy',\n",
    "'phenomenon_contested: Contested',\n",
    "'phenomenon_contested: Widely-agreed',\n",
    "'phenomenon_contested: No definition',\n",
    "'task_face_validity: ',\n",
    "'task_face_validity: Yes',\n",
    "'task_face_validity: Partially',\n",
    "'task_face_validity: No',\n",
    "'metric_face_validity: ',\n",
    "'metric_face_validity: Yes',\n",
    "'metric_face_validity: Partially',\n",
    "'metric_face_validity: No',\n",
    "'results_realism: Not possible',\n",
    "'results_realism: Realistic',\n",
    "'results_realism: Comparison made',\n",
    "'results_realism: No',\n",
    "'results_realism: No comparison made',\n",
    "'results_author_validity: ',\n",
    "'results_author_validity: Yes',\n",
    "'results_author_validity: No',\n",
    "'task_ecology: Constructed',\n",
    "'task_ecology: ',\n",
    "'task_ecology: Representative',\n",
    "'task_ecology: Complete',\n",
    "'task_ecology: Partial',\n",
    "'new_bibkey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8b396b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_df = included_df.drop(columns=remove_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "939e051b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bibkey",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "inclusion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "exclusion_criteria",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "exclusion_criteria_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "short_summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "contribution",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_short",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target_phenomenon",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_defined",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_definition",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "definition_scope",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "purpose_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_item_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_definition_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_dataset_size",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_dataset_metadata",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_metadata_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_sampling_method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "response_format",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_definition_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "authorship",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "benchmark_availability",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "procedural_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "notes_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_train_val",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_dataset_size_extra",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "response_format_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_aggregation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_subscores",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_subscores_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_metascoring",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "benchmark_location",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "benchmark",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_contested",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_face_validity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "metric_face_validity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "result_interpretation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_comparison",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_comparison_explanation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_realism",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "results_human_baseline",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_author_validity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_author_validity_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_statistics",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_access",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_ecology",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_ecology_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "definition_integrity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "definition_integrity_detail",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "task_dataset_size_detail",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_fewshot",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_taxonomy_root",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "phenomenon_taxonomy_leaf",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_taxonomy_alternate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_source_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_sampling_method_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "response_format_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_definition_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "phenomenon_contested_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_face_validity_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_face_validity_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_realism_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "results_author_validity_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "task_ecology_clean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "metric_statistics_clean",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "69d026d9-8b91-46c8-a56e-64035f32500d",
       "rows": [
        [
         "0",
         "mundlerSWTBenchTestingValidating2024",
         "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents",
         "Include",
         null,
         null,
         "A benchmark for generating code tests (unit tests) from natural language user GitHub issues.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Automatic code test generation (i.e. generating unit tests for issues)",
         "Yes",
         "The ability to generate valid tests to reproduce an issue in a codebase.",
         "Comprehensive",
         null,
         "Given a GitHub issue in natural language, you must write tests to reproduces the described issue.",
         "A GitHub issue (taken from SWE-Bench), code that contains the issue and code with a 'golden patch' that has the issue fixed. The goal is to write unit tests that fail on the faulty code but pass after the patch is added.",
         "Very comprehensive details about task definition.",
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)",
         "1900",
         "Yes",
         "Length of the GitHub issue in tokens, original GitHub repository",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "Structured response (e.g. valid JSON, API call alone)",
         "Whether the faulty code fails on the test and the gold-standard code passes it.",
         null,
         "SWE-bench, which originates from real GitHub issues",
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Description length in tokens, original GitHub repository",
         null,
         "https://github.com/logic-star-ai/SWT-Bench",
         "SWT-Bench",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "Limitations in how the phenomenon was operationalised - all problems are in Python.",
         "simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         null,
         "Agents",
         "Coding",
         null,
         "['Real task', 'Another benchmark']",
         "['Criterion']",
         "['Structured']",
         "['Reward']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "3",
         "niuRAGTruthHallucinationCorpus2024",
         "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
         "Include",
         null,
         null,
         "This paper targets word-level hallucinations in various tasks and domains in the RAG setting. It presents approximately 18,000 responses generated using RAG from diverse LLMs which are annotated at the word level for hallucination intensity. Hallucination frequencies are benchmarked across various LLMs, and hallucination detection methods are assessed versus a small LLM fine-tuned using the proposed dataset, RAGTruth.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "hallucination detection, specifically for RAG applications",
         "Yes",
         "\"Hallucination in the context of LLMs usually refers to a situation where the\nmodel generates content that is not based on factual or accurate information\"",
         "Subset",
         null,
         "For a given reference-response pair, determine if it contains hallucinated content at the response level and span level.",
         "A single item consists of source information (reference), an LLM-generated response (which may contain various degrees of hallucination), annotation of the location and type of hallucination (if any), and a brief annotated explanation of the hallucination observed.",
         "Additional meta-data regarding the model and inference hyperparameters used to generate each sample is provided, along with details regarding the source and task type for the reference texts.",
         "Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "2700",
         "Yes",
         "source information index, generating model, temperature, whether quality issues are present in the sample, task type of the data, source of the original content, prompt used to generate the response, base content for RAG",
         "Random sample (creators defined a task space and sampled from it), Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragarph), Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train",
         "15090 (train)",
         null,
         "Simple Mean",
         "Yes",
         "by task type (QA, summarization, data-to-text writing)",
         null,
         "https://github.com/ParticleMedia/RAGTruth",
         "RAGTruth",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "Yes",
         "Benchmark statistics and quality checking are described. Hallucination density is assessed across models used to generate the data, in relation to context length, and versus position in the text.",
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "Retrieval",
         null,
         "Factuality",
         "['Real task', 'Crowd-sourced', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']",
         "['Random', 'Targeted']",
         "['Short free response', 'Free response', 'Structured']",
         "['Exact match']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['No comparison made']",
         "['Yes']",
         "['Complete']",
         null
        ],
        [
         "54",
         "mahbubUnveilingEssencePoetry2023",
         "Unveiling the Essence of Poetry: Introducing a Comprehensive Dataset and Benchmark for Poem Summarization",
         "Include",
         null,
         null,
         "The paper proposes the task of poem summarization for LLMs and presents the first benchmark, PoemSum, to evaluate such capability. SOTA summarization models are benchmarked and limitations of current models on the poem summarization task are discussed.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "poem summarization",
         "Yes",
         "\"In recent years, there has been notable research conducted on text summarization in the field of Natural Language Processing (NLP). However, to the best of our knowledge, no such work has been done in the domain of poem summarization yet. While the summarization process of poems seems quite similar to the generic text summarization, there are some major differences between the two... Summarizing literary work poses lots of challenges.\"",
         "Comprehensive",
         null,
         "A poem is given and a summary must be generated.",
         "Each sample is represented by the poem title, poet name, poem text, poem link, and poem summary.",
         null,
         "Real task examples (e.g. GitHub issues)",
         "301",
         "Yes",
         "number of poets, max poem length, max summary length, avg poem length, avg summary length, avg # poems per poet",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "Free response (e.g. summary paragarph)",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "Train: 2409; Validation: 301",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/Ridwan230/PoemSum",
         "PoemSum",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         null,
         "NLP",
         "Summarization",
         null,
         "['Real task']",
         "['Criterion']",
         "['Free response']",
         "['Exact match', 'Soft match']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "64",
         "huangMLAgentBenchEvaluatingLanguage2024",
         "MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation",
         "Include",
         null,
         null,
         "MLAgentBench benchmarks the ability of LLM agents to perform machine learning experiments. The benchmark comprises different tasks from canonical classification to code optimization. A success is beating the baseline by more than 10%",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "ML Experimentation",
         "No",
         "competence in accomplishing the task, i.e., the fraction of time that the agent was able to improve the performance metric",
         "Subset",
         "While the definition is very high-level (i.e., \"ML experimentation\"), the authors make no claim that their benchmark is comprehensive.",
         "A task is broadly to improve on some starter code either in terms of performance of the trained model (e.g., classification accuracy) or code efficiency (e.g., clock speed). Each task has a description with instructions and goals as well as a set of starter files.",
         "A dataset (e.g., CIFAR), a starter model (defined in a `train.py`), and a metric (e.g., `test accuracy`). ",
         null,
         "Real task examples (e.g. GitHub issues)",
         "13",
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "functioning code (i.e., a .py script or model artifacts)",
         "Score improvement of script",
         "On a high-level, all metrics are \"did the model improve $SCORE by more than 10%?\" averaged  over 8 trials. ",
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         null,
         "Yes",
         "Measure for each task",
         null,
         "https://github.com/snap-stanford/MLAgentBench/",
         "MLAgentBench",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         "mean over 8 runs. ",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "The task of improving an existing codebase/doing a kaggle challenge has a degree of gamification but is still quite realistic. ",
         "Authors' description is unclear",
         "Not applicable",
         null,
         null,
         "Agents",
         "Coding",
         null,
         "['Real task']",
         "['Targeted']",
         "['Free response']",
         "['Reward']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "65",
         "yeGlobeSummChallengingBenchmark2024",
         "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization",
         "Include",
         null,
         null,
         "Propose GLOBESUMM and introduce prompting method for silver summary annotation. Validate the quality and difficulty of the dataset.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Text summarization",
         "Yes",
         "The goal of Multi-lingual, Cross-lingual and Multi- document Summarization (MCMS) is to succinctly capture the key information from a collection of documents written in various languages and present a cohesive summary in the target language. Notably, the MCMS task has three distinctive features: (1) the input consists of multi- ple documents, (2) the multiple documents are in different languages, and (3) the multiple documents revolve around the same event. ",
         "Subset",
         null,
         "(a) Single- turn Summarization summarizes a document set within a single-turn generation; (b) Chronological Recurrent Summarization iteratively summarizes two documents at a time in a time-ordered manner ",
         "The model is given a set of articles and asked to summarize them in one or multiple turns.",
         null,
         "Real task examples (e.g. GitHub issues)",
         "74 events  942 documents  868 summaries",
         "Yes",
         "language",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Free response (e.g. summary paragarph)",
         "n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring)",
         "On top of ROUGE, authors also use Red (Chen et al., 2021) for redundancy, Normalized Inverse of Coverage (NIC) for Omission, and Conflict Resolution Effectiveness (CRE) for conflict",
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "Training Set: 222 events, 2,848 documents, and 2,626 summaries​  Validation Set: 74 events, 897 documents, and 823 summaries",
         null,
         null,
         "Yes",
         "for different languages",
         null,
         "https://github.com/YYF-Tommy/GlobeSumm",
         "GLOBESUMM",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "No",
         "Yes",
         "the authors conduct extensive human validation in the annotation process. They also validated their annotation method against other benchmark (XQuAD specifically).",
         "simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "People would use chatbots to summarize news articles, in my opinion.",
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "NLP",
         "Summarization",
         "Multilinguality",
         "['Real task']",
         "['Targeted']",
         "['Free response']",
         "['Soft match', 'LLM post-processing']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['No comparison made']",
         "['Yes']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "69",
         "athiwaratkunMultilingualEvaluationCode2023",
         "Multi-lingual Evaluation of Code Generation Models",
         "Include",
         null,
         null,
         "Measures code generation capabilities across 10 programming languages (Java, JavaScript, TypeScript, Go, Ruby, Kotlin, PHP, C#, Scala, C++, Swift, and Perl). Transforms existing Python benchmarks into other languages.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Code generation",
         "No",
         null,
         "Comprehensive",
         null,
         "Generating code to complete a function given a docstring.",
         "Each example contains a function signature and a docstring. The docstring is detailed and contains examples of the desired behaviour.",
         "Fairly limited discussion given it was a transpiled from existing benchmarks.",
         "Modified from another benchmark (e.g. translation into another language)",
         null,
         "Yes",
         "Programming language.",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "Structured response (e.g. valid JSON, API call alone)",
         "Accuracy when the generated function is executed.",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         null,
         null,
         null,
         "Simple Mean",
         "Yes",
         "Programming language",
         "pass@k (any correct answer in k trials)",
         "https://github.com/amazon-science/mxeval",
         "MBXP and Multilingual HumanEval (two benchmarks)",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "Discuss this briefly in the limitations. Say that they assume this is representative of all code completion problems.",
         "Simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "Code Generation",
         null,
         null,
         "['Another benchmark']",
         "['Convenience']",
         "['Structured']",
         "['Reward']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "73",
         "naousReadMeBenchmarkingMultilingual2024",
         "README++: Benchmarking Multilingual Language Models for\nMulti-Domain Readability Assessment",
         "Include",
         null,
         null,
         "ReadMe++ is a multilingual and multi-domain dataset for readability assessment according to the Common European Framework of Reference for Languages (CEFR) scale in Arabic, English, French, Hindi, and Russian. The dataset is human-annotated and publicly available. The dataset can benchmark supervised, unsupervised, and few-shot approaches, and is measured by the Pearson Correlation between predictions and ground-truth labels (supervised, few-shot) or the Ranged Sentence Readability Score (unsupervised). ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Readability assessment",
         "Yes",
         "Readability assessment is the task of determining how difficult it is for a specific audience to read and comprehend a piece of text. ",
         "Comprehensive",
         null,
         "The model must classify the readability of a sentence according to the 6-point Common European Framework of Reference for Languages (CEFR). The scale proceeds as 1 (A1), 2 (A2), 3 (B1), 4 (B2), 5 (C1), 6 (C2), where A is for basic, B is for independent, and C is for proficient; the paper provides the full annotation criteria in the appendix. ",
         "A single item is a sentence with its associated language, domain, sub-domain, paragraph, context, and readability assessment label. The paragraph and context are optional and provided for human annotators to aid in manual labeling. ",
         null,
         "Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)",
         "9757",
         "Yes",
         "Language, Domain, Sub-Domain, Context, Paragraph",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Multiple choice",
         "Distribution (perplexity, calibration, correlation)",
         "The model has two metrics. Pearson correlation requires just model output, but Ranked Sentence Readability Score requires model access to access the LLM's distribution. ",
         "Data is sourced from 21 types of text (e.g. textbooks, legal documents, etc.) from various open-source datasets or open-access resources. ",
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "60/10/30 train/validation/test",
         null,
         "Simple Mean",
         "Yes",
         "Unseen Domains per Data Source, Cross-Lingual Transfer",
         null,
         "https://github.com/tareknaous/readme/tree/main",
         "ReadMe++",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "Authors assess their construct validity when justifying the originality or contribution of their benchmark. They expand an existing scale grounded in literary research to be multilingual and balance several domains, which current assessments fail to do, to ensure the most reliable assessment of readability. ",
         "Min, max, average",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "The task would probably be integrated into user applications, but not directly asked for by the user. Provided real-world applications of readability assessment were controllable text-simplification, ranking search engine results by their level of difficulty, and selecting appropriate reading material for language learners. ",
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         null,
         "NLP",
         null,
         null,
         "['Human exams', 'Real task', 'Author-crafted', 'Another benchmark']",
         "['Targeted', 'Criterion']",
         "['Multiple choice']",
         "['Distribution']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean', 'Other']"
        ],
        [
         "75",
         "tanDevBenchMultimodalDevelopmental2024",
         "DevBench: A multimodal developmental benchmark for language learning",
         "Include",
         null,
         null,
         "DevBench is a multimodal benchmark for assessing how LLMs compare to human language development across seven language evaluation tasks spanning lexical, syntactic, and semantic domains. Each task contains item-level human baseline data to facilitate human-model language development comparison using a novel metric: softmax-optimized Kullback-Leibler divergence. The goal of the benchmark is to measure whether developmentally realistic data leads to human-like learning in LLMs. ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "language evaluation, language development, cognitive evaluation",
         "No",
         "Language development evaluation is assessing whether the language ability gained by machine learning models matches the language ability gained by children when exposed to similar developmental data.",
         "Subset",
         null,
         "The benchmark consists of 7 multi-modal language evaluations. The lexical tasks consist of Looking-while-listening (LWL) and Visual vocabulary task (VV), the syntatic tasks consist of Test of Receptive Grammar (TROG), Winoground-NoTag (WG), and the semantic tasks consist of Free word association task (WAT), Visual object categorization (VOC), and THINGS similarity ratings. ",
         "For each task, a single sample would consist of the task prompt, a correct label if applicable, and the associated human response and human age range. Several tasks (LWL, VOC) are quantitative and measured by the looking time response, while the rest are categorical. ",
         null,
         "Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",
         "22212",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Short free response (e.g. single word or number)",
         "Distribution (perplexity, calibration, correlation)",
         null,
         "The experiments are sourced from child development literature, hence the choice of real task examples. Several task samples were modified to ensure that the images used in multimodal prompts had the correct licensing. ",
         "Academia",
         "Yes",
         "For attribution and licensing reasons, not all assets and data are hosted in the repo. ",
         null,
         "Test",
         null,
         null,
         null,
         "No",
         "Scores are provided per task, and the benchmark itself consists of 7 distinct tasks",
         null,
         "https://github.com/alvinwmtan/dev-bench",
         "DevBench",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "Yes",
         "Yes",
         "The authors define the desiderata for an ideal benchmark of developmentally appropriate evaluation of language models as (1) a wide dynamic range of difficulty (2) multiple levels of linguistic representations (3) corresponding data from children, and (4) high similarity in evaluation method between models and humans. These desiderata are based on child development literature and seek to overcome the limitations of existing benchmarks. Namely, current benchmarks are either unimodal, when cognitive language evaluations for children and infants are multimodal to accommodate pointing or looking responses, or current benchmarks compare language models to exclusively adult performance. DevBench seeks to fulfill all four criteria. ",
         "Visual semantic tasks were measured with representational similarity analysis (RSA), while the other tasks were measured with a novel metric: softmax-optimized Kullback-Leibler divergence",
         "Model access required (e.g. logits)",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "Language Modelling",
         null,
         null,
         "['Real task', 'Author-crafted']",
         "['Convenience', 'Targeted', 'Criterion']",
         "['Short free response']",
         "['Distribution']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Other']"
        ],
        [
         "106",
         "gharaeeBIOSCAN5MMultimodalDataset2024",
         "BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity",
         "Include",
         null,
         null,
         "BIOSCAN-5M is a multimodal benchmark for insect classification and contains images, taxonomic labels, raw nucleotide barcode sequences, barcode index numbers, geographic location, and size metadata. The dataset is publicly available and includes data from novel species. The benchmark supports classification, zero-shot transfer learning, and retrieval learning.",
         "BIOSCAN-5M is an expansion of BIOSCAN-1M. It is unique in its inclusion of 4 million additional images, and location, taxonomic rank, and size metadata. Addtitionally, BIOSCAN-5M was cleaned to resolve inconsistencies and provide more reliable labels. ",
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "insect classification",
         "Yes",
         "Insect classification is the automatic classification of inset specimens by AI tools. ",
         "Comprehensive",
         null,
         "The benchmark outlines three possible tasks. The first task is insect classification, which can be performed as DNA-based and/or image-based taxonomic classification. In a closed-world setting, the task is to accurately identify species from a predefined set of existing labels. In the open-world setting the task is to group together samples of novel species. The benchmark also supports zero-shot transfer-learning, which measures how unseen datasets can be clustered using embeddings from pre-trained feature extractors, and multimodal retrieval learning by aligning image, DNA, and taxonomic label embeddings using CLIBD. ",
         "A single item in the dataset contains the biological taxonomy (phylum, class, order, family, subfamily, genus, species), the genetic information (DNA barcode sequence, barcode index number), a cropped and original RBG image of the insect, size information (meas. value, scale factor, area fraction), and geographical information (coordinates, country, province/state). ",
         null,
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)",
         "47,260",
         "Yes",
         "phylum, class, order, family, subfamily, genus, species, DNA barcode sequence, barcode index number, original RBG image, cropped RBG image, measured value, scale factor, area fraction, country, province/state, latitude, longitude ",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Short free response (e.g. single word or number)",
         "Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)",
         null,
         "Dataset is an superset of previous datasets with additional metadata. ",
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "There are four types of species labels that each carry their splits. Unknown samples are samples without a species label. Seen samples are samples with an established scientific name for their species. Unseen are samples with an established scientific name for the genus, and a uniquely identifying placeholder name for the species. Heldout samples are labelled with a placeholder genus and species name.   Unknown: Pretrain 4677756 Seen: Train/Validation/Test 289203/14757/38373 Unseen: Retrieval Keys/Validation/Test 36465/8819/7887 Heldout: 76590",
         null,
         null,
         "No",
         null,
         null,
         "https://github.com/bioscan-ml/BIOSCAN-5M",
         "BIOSCAN-5M",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "The authors highlight that a strong dataset for insect classification requires detailed metadata, and geographic and specimen diversity. BIOSCAN-5M, compared to other benchmarks, covers 98% of discovered insects with 1.2 million labeled to the species rank, and contains geographical information, size, and DNA barcodes. The authors claim that multimodal datasets are critical for robust species classification. ",
         "Accuracy is reported for classification in both open and closed-world settings. Fine-tuned accuracy and linear probing accuracy are reported in a closed-world setting, while 1NN-genus probing accuracy is reported in an open-world setting. AMI is reported for zero-shot transfer learning, and in multimodal retrieval learning, micro and macro top-1 accuracy is reported. ",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         null,
         "Biology",
         null,
         null,
         "['Real task', 'Another benchmark']",
         "['Convenience', 'Targeted']",
         "['Short free response']",
         "['Exact match', 'Distribution']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean', 'Other']"
        ],
        [
         "109",
         "mitaStrikingGoldAdvertising2024",
         "Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation",
         "Include",
         null,
         null,
         "CAMERA is a multimodal benchmark for automatic ad text generation (ATG)  in Japanese. The paper presents the first standardization and formalization of the ATG task, and the first ATG benchmark. The dataset was manually annotated, and the benchmark contains automatic and human evaluations. ",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "automatic ad text generation",
         "Yes",
         "We standardize the ATG (automatic ad text generation) task as follows: Let x be a source document that describes advertised products or services, a a user signal reflecting the user’s latent needs or interests, and y an ad text. ATG aims is to model p(y|a, x). The specific data to be selected for each x, a, and y will be left to future dataset designers\nand providers. ",
         "Subset",
         "The paper describes speed, trend, and user-friendliness, faithfulness, fluency, and attractiveness as aspects of a good ad text. Faithfulness, fluency, and attractiveness are used in human evaluation, and those sub-elements are reported. ",
         "Models are optionally pre-trained on the train split of CAMERA, and then generate an ad text given a landing page OCR text, the landing page layout information, and the landing page bbox image features. The ad text is manually and automatically evaluated. ",
         "A single item would have the landing page (LP) description, the user query, the landing page layout information, the landing page bbox image features, and entity type (time expression, named entity, terms, etc). ",
         null,
         "Real task examples (e.g. GitHub issues)",
         "872",
         "Yes",
         "Landing page description, user query, landing page layout information, landing page bbox image features, entity type, industry type. ",
         "Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)",
         "Free response (e.g. summary paragarph)",
         "n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM post-processing (extracting answers, reformatting for automated scoring), Distribution (perplexity, calibration, correlation)",
         null,
         "Task dataset in Japanese",
         "Industry",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "Train/Dev/Test 12395/3098/872",
         null,
         null,
         "Yes",
         "Faithfulness, fluency, and attractiveness have sub-scores in human evaluation",
         null,
         "https://huggingface.co/datasets/cyberagent/camera",
         "CAMERA (CyberAgent Multimodal Evaluation for Ad Text GeneRAtion)",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "The benchmark is itself realistic",
         "Yes",
         "Yes",
         "The authors define two requirements for ad text: (1) the information provided by the ad text is consistent with the content of the source document; and (2) the information is carefully curated and filtered based on the users’ potential needs. Thus, for a benchmark for ATG, the authors outline two design policies: the benchmark should (1) utilize multimodal information and (2) evaluate by industry domain. The authors tailor CAMERA to fit both design policies and measure both ad text requirements. ",
         "BLEU-4, Rouge-1, BERTScore, Keyword Insertion Rates (KWD), Sentence Length Regulation Compliance Rates (REG), Pearson and Spearman Correlation for Human Evaluation",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "Business",
         null,
         null,
         "['Real task']",
         "['Random', 'Convenience']",
         "['Free response']",
         "['Soft match', 'Human ratings', 'LLM post-processing', 'Distribution']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Other']"
        ],
        [
         "113",
         "yangDataTalesBenchmarkRealworld2024",
         "DataTales: A Benchmark for Real-World Intelligent Data Narration",
         "Include",
         null,
         null,
         "DataTales is a novel benchmark designed to assess data narration of market movement data. It contains a human baseline and is publicly available. Specifically, DataTales assesses the proficiency of LLMs at performing lookups, comparisons, subtraction, rate of change, causal analysis, trend analysis, and predictive analysis to craft a financial report based upon market data. ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Data narration",
         "Yes",
         "Data narration is the process of transforming intricate data into compelling narratives.",
         "Subset",
         "DataTales focuses on data narration with financial data, e.g. narrating financial market reports. Data Narration is separated into seven analytical operations across three domains: simple lookup, basic quantitative operations, and advanced analytical operations. Basic quantitative operations include comparison, subtraction, and rate of change, while advanced analytical operations include causal analysis, trend analysis, and predictive analysis. ",
         "We define the task of financial data narration as follows: given market movement data  {T_{i,j} | i ≤ E_T, j ≤ D_T } with E_T financial entities and D_T days, where T_{i, j} is the row of entity i on date j, a data narration model M generates a report y narrating the market data y = M(T_{i,j} | i ≤ E_T, j ≤ D_T). Narrations are evaluated generated with same-day data, and historical data spanning one week. Both zero-shot and fine-tuned scenarios are analyzed. ",
         "A single item in the dataset would have market movement data (open, high, low, close, volume), the date, the entity, the market report, and the market type. ",
         null,
         "Real task examples (e.g. GitHub issues)",
         "4900",
         "No",
         null,
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Free response (e.g. summary paragarph)",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics)",
         "Accuracy is calculated with a specific MCQA-inspired methodology, that utilizes Named Entity Recognition to assess if LLM's predict numerical values accurately. ",
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "Train/Validation and Testing 80/20, split by time",
         null,
         "Simple Mean, None",
         "Yes",
         "Factuality, Style, and Insightfulness",
         null,
         "https://github.com/yajingyang/DataTales/",
         "DataTales",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "Yes",
         "Yes",
         "The authors highlight that data narration requires deeper analysis to craft narratives around key insights, and goes beyond the scope of existing datasets that focus on data-to-text tasks like basic information transformation. Thus, this justification is used to define a benchmark exclusively tailored for data narration. ",
         "Factuality is calculated with Named Entity Recognition (NER) empowered accuracy, described in the paper. Style is measured with BLEU. Insightfulness is measured by human assessments based on impact (breadth of claim), and significance (magnitude of changes) on a 5 point Likert scale, and the average of the human review is reported. ",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         null,
         "Data Analysis",
         null,
         null,
         "['Real task']",
         "['Targeted', 'Criterion']",
         "['Free response']",
         "['Exact match', 'Soft match', 'Human ratings']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "116",
         "zhangHumorAIMassive2024",
         "Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning",
         "Include",
         null,
         null,
         "The paper presents a New Yorker Caption Ranking Dataset, a novel multimodal human preference dataset for generating humorous cartoon captions. The paper presents additional novel evaluation methods to perform group comparisons between AI and human-generated cartoon captures, and leverages data from The New Yorker Caption Contest. The benchmark can be used to assess model-generated captions and support preference-based fine-tuning algorithms. ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "humor, humorous captions, funny captions",
         "No",
         "Generating a humorous caption is the task of writing funny captions on a literary piece, primarily cartoons.  ",
         "Comprehensive",
         null,
         "The cartoon captioning task is defined as a model generating a funny caption given information about the cartoon. Both multimodal and language-only models are evaluated, where language-only models receive descriptions and object entities of the cartoons. The paper also compared zero-shot models against SFT, RLHF, and DPO finetuned models on certain contests within the dataset. ",
         "A single item would have the cartoon, its language description (provided by GPT4o-vision), its caption, and its label (funny, somewhat funny, unfunny). ",
         null,
         "Real task examples (e.g. GitHub issues), Crowd-sourced task examples (e.g. Prolific-created tasks)",
         "284183913",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Free response (e.g. summary paragarph)",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",
         "The paper presents a novel evaluation method for group comparison techniques, denoted by Group Overall and Group Best Pick. Human or LLM raters evaluate groups of 10 captions from different sources, and compare them against four groups of past human submissions in the buckets of ranks 1-10, 200-209, 1000-1009, and median. The evaluators then compare the overall funniness of the group against the contest-submitted captions, and pick the funniest caption overall between the funniest captions of the evaluation group and the contest group. GPT4-Turbo-vision, GPT4o-vision, GPT4-Turbo, and GPT4o were used as LLM evaluators. The ranking accuracy and caption win rates of the cartoons are then calculated from the evaluations. ",
         "The dataset is crowdsourced from The New Yorker cartoon caption contest. ",
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "The fine-tuning experiments designated contests 530-890. The test set contains 47 contests, the validation set contains 44 contests, and the train set contains the remaining contests.",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://huggingface.co/datasets/yguooo/newyorker_caption_ranking",
         "New Yorker Caption Ranking",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "Yes",
         "Yes",
         "Authors highlight that writing funny captions requires an understanding of how to appeal to a broad range and variability within humor and human judgements. Thus, a benchmark in funny caption writing requires a comparison to human performance, because the task is a domain where expert humans consistently outperform current AI system, leading to the creation of the introduced dataset. ",
         "Simple mean and variance on accuracy are used to assess the overall and best pick comparisons for cartoons, and expectation adjusted distinct N-grams (EAD) and Sentence-BERT embedding cosine similarity (SBERT) are used to assess caption diversity. ",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         null,
         "NLP",
         "Understanding",
         null,
         "['Real task', 'Crowd-sourced']",
         "['Convenience', 'Targeted']",
         "['Free response']",
         "['Exact match', 'Soft match', 'Human ratings', 'LLM-as-a-Judge']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean', 'Std', 'Other']"
        ],
        [
         "149",
         "changDrspiderDiagnosticEvaluation2023",
         "Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness",
         "Include",
         null,
         null,
         "The paper proposes Dr Spider a text-to-SQL robustness benchmark. The authors adapt the Spider benchmark by introducing various perturbations and measuring drop in model performance. ",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "text-to-SQL, natural language understanding, code generation",
         "No",
         "\"the robustness of models with perturbations on each component of the text-to-SQL task\"",
         "Comprehensive",
         null,
         "Given a natural language query and a data base structure, the model should write a correct SQL query to obtain from the database what the NL query requests.",
         "Natural language query + database structure + example of correct SQL query + the results of running the example SQL query on the content of the database",
         "The base task is described above. The \"meta task\" is doing this consistently among small perturbations of the problem.",
         "Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), Procedurally-generated task examples (e.g. Creating instances from a template), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "15000",
         "No",
         null,
         "Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)",
         "Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), The code is executed and results are verified against ground truth results",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Unclear",
         null,
         null,
         "Test",
         null,
         "the model generates SQL, which is then processed for grading",
         "Simple Mean",
         "Yes",
         "Subsets are different vectors for perturbations: Perturbing the a) query semantically b) the query lexically and syntacitcally while keeping semantics invariant c) perturbing the database structure. Within each further subscores are provided.",
         "difference between unperturbed and perturbed.",
         "https://github.com/awslabs/diagnostic-robustness-text-to-sql",
         "Dr.Spider",
         "Not defined",
         "Yes",
         "Yes",
         "No",
         "Yes",
         "No",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         "No statistical methods used. just simple mean and differences in means.",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "Code Generation",
         "Natural Language",
         null,
         "['Crowd-sourced', 'Another benchmark', 'Procedurally-generated', 'LLM-generated']",
         "['Random', 'Convenience']",
         "['Free response']",
         "['Exact match', 'Reward']",
         "['No definition']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "158",
         "liuRevisitingGoldStandard2023",
         "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
         "Include",
         null,
         null,
         "Propose a modified summarization salience protocol, curate the Robust Summarization Evaluation (RoSE) benchmark, conduct a comparative study of human evaluation protocols. Evaluate 50 automatic metrics and their variants and demonstrate how the benchmark leads to more statistically stable and significant results.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Text summarization evaluation",
         "Yes",
         "\"We focus on a specific summarization meta-evaluation study on summary salience. Salience is a desired summary quality that requires the summary to include all and only important information of the input article. The human evaluation of summary salience can be conducted in either reference-free or reference-based manners…the latter requires the annotators to assess the information overlap between the system output and reference summary, under the assumption that the reference summary is the gold standard of summary salience…we focus on reference-based evaluation for our human judgment dataset collection\"-p4142",
         "Subset",
         null,
         "\"Specifically, the evaluation process is decomposed into two steps: (1) Atomic Content Unit (ACU) Writing – extracting facts from one text sequence, and (2) ACU Matching – checking for the pres- ence of the extracted facts in another sequence. We formulate the ACU protocol as a recall-based pro- tocol, such that the first step only needs to be per- formed once for the reference summary, allowing for reproducibility and reuse of these units when performing matching on new system outputs. \"-p4142",
         "Given a reference summary, a system summary, and a set of Atomic Content Units (ACU), annotators have to decide whether ACUs exist in the system summary",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language)",
         "1.5k docs, 10.2k Atomic Content Units (ACU)-level annotations and around 14k summary-level annotations,",
         "Yes",
         "topic area",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Multiple choice",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "val: 1k docs, 11.6k Atomic Content Units (ACU), 8k summaries",
         null,
         null,
         "Yes",
         "topic area",
         null,
         "https://github.com/Yale-LILY/ROSE",
         "RoSE",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "No",
         "Yes",
         "Yes",
         "the authors conduct human experiment",
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         "No",
         "NLP",
         "Summarization",
         null,
         "['Author-crafted', 'Crowd-sourced', 'Another benchmark']",
         "['Targeted']",
         "['Multiple choice']",
         "['Exact match']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['No comparison made']",
         "['Yes']",
         "['Complete']",
         null
        ],
        [
         "170",
         "chaoJailbreakBenchOpenRobustness2024",
         "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
         "Include",
         null,
         null,
         "The paper proposes a benchmark for jailbreaking LLMs (i.e. eliciting harmful content through adversarial attacks). They provide a dataset, python package and leaderboard. Each score of the benchmark is a combination of Model + Defense + Thread Model.",
         "Attacks are adaptive: The strongest attack can be picked directly targeting the defense.",
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         null,
         "No",
         "Inability to elicit harmful content from LLMs.",
         "Subset",
         null,
         "Elicit a harmful response from an LLM",
         "Kind of Behavior + Goal (Query) + Target (affirmative response) + Category + Source.",
         null,
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language)",
         "100 harmful 100 benign",
         "Yes",
         "Source for each item",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Free response (e.g. summary paragraph, executable code)",
         "LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",
         "Attack Success Rate: What percentage of items have at least one response scored \"harmful\".",
         "only 55% of data points are novel. the others are copied.",
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Validation",
         "Judge validation data 300 rows.",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/JailbreakBench",
         "JailbreakBench",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "Yes",
         "No",
         "No",
         null,
         "simple mean. no inferential statistics (even though the LLM-as-judge have fairly low accuracy).",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "No",
         null,
         "Yes",
         "Alignment",
         "Alignment",
         null,
         "['Author-crafted', 'Expert-crafted', 'Another benchmark']",
         "['Targeted']",
         "['Free response']",
         "['LLM-as-a-Judge']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Comparison made']",
         "['No']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "172",
         "kasaiRealTimeQAWhats2023",
         "RealTime QA: What's the Answer Right Now?",
         "Include",
         null,
         null,
         "This paper introduces REALTIME QA, a dynamic question answering platform that evaluates systems' ability to answer questions about the current world. New questions requiring up-to-date information are released weekly. The paper presents the platform and evaluates strong baselines built on large language models (like GPT-3 and T5) combined with information retrieval (web search, DPR). Results highlight the importance of timely retrieval but also show models may provide outdated answers when retrieval is insufficient.",
         "Key contributions include: (1) Proposing REALTIME QA, a novel dynamic benchmark for evaluating QA systems on their ability to use real-time information. (2) Establishing a regular (weekly) cycle for question release and evaluation. (3) Providing strong baseline results using LLMs augmented with different information retrieval techniques. (4) Analyzing the performance and failure modes of current systems on timely QA. (5) Making the platform and results publicly available.",
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Real-time question answering; Reasoning with up-to-date information; Temporal awareness in QA systems.",
         "Yes",
         "The ability of a QA system to correctly answer questions about novel events or rapidly changing information necessitates access to and processing of the most current information available, unlike systems relying solely on static knowledge snapshots.",
         "Subset",
         "To overcome the limitations of static QA datasets and drive research towards systems capable of handling continuously evolving world knowledge and providing timely answers.",
         "Given a natural language question released at a specific time, whose answer depends on the current state of the world, provide the correct, up-to-date answer. This typically requires querying external, real-time information sources.",
         "A question released weekly via the REALTIME QA platform, requiring a factual answer reflecting the world state at that time. The platform manages questions and evaluates submitted answers.",
         "Questions are manually generated by the benchmark organizers to specifically require timely information. They cover diverse topics and can be short-answer or yes/no. The benchmark is ongoing and dynamic.",
         "Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",
         "Dynamic/Ongoing. The dataset grows each week. The paper reports on results gathered over a year.",
         "Yes",
         "Question Release Timestamp, Question Type (Short-Answer/YesNo), Answer Type (Person, Org, Loc, Date, Num, Other), Required Timeliness category.",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Short free response (e.g. single word or number)",
         "Exact Match (accuracy, F1, precision, recall)",
         "Exact Match (EM) and F1 score.",
         "The authors manually create new questions each week designed to require knowledge of recent events or information that frequently changes.",
         "Mix (multiple authors from industry and academia)",
         "Yes",
         "Benchmark operates on a weekly cycle. Baselines use GPT-3 (text-davinci-002), T5-11B, DPR, and Google Custom Search API. Details baseline configurations. Evaluation interface shown. ",
         "REALTIME QA's key innovation is its dynamic evaluation framework, moving beyond static datasets to continuously assess performance on questions requiring current knowledge. It highlights the challenges models face in staying up-to-date and avoiding reliance on potentially outdated parametric memory.",
         "Test",
         null,
         "Answers are expected to be concise factual strings or \"Yes\" / \"No\".",
         "Simple Mean",
         "Yes",
         "Performance analyzed by question type, answer type, required timeliness category, and baseline system configuration (retrieval method, base model)",
         null,
         "https://realtimeqa.github.io/",
         "RealTime QA",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "The benchmark's dynamic, ongoing nature is its core validity claim for measuring real-time QA ability. Questions are manually created to ensure they test timely knowledge. Performance analysis based on timeliness requirements further supports its construct validity.",
         "Exact Match (EM), F1 Score (%)",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "The dynamic nature and focus on current events make it highly representative of real-time information needs.",
         "Single cohesive phenomenon",
         "No",
         null,
         "No",
         "Language Modelling",
         "Updating",
         null,
         "['Real task', 'Author-crafted']",
         "['Targeted']",
         "['Short free response']",
         "['Exact match']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "177",
         "chenDrAcademyBenchmarkEvaluating2024",
         "Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models",
         "Include",
         null,
         null,
         "This paper introduces Dr.Academy, a benchmark for evaluating the question generation capabilities of LLMs in educational contexts. It evaluates questions generated by LLMs across general, monodisciplinary, and interdisciplinary domains using a cognitive framework based on Anderson and Krathwohl’s taxonomy. The quality of LLM's output is evaluated by automatic metrics which correlate with human scores.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Question generation for education",
         "Yes",
         " \"According to Anderson and Krathwohl’s educational taxonomy, we consider that high-quality questioning in the educational field must meet the following characteristics: i) achieve a higher level across the six domains including memory, understanding, application, analysis, evaluation, and creation; ii) be relevant to the given context; iii) comprehensively cover the content of the context, and iv) also reflect the important knowledge of this context.\"",
         "Comprehensive",
         null,
         "The LLMs are prompted to generate educational questions based on textual contexts, accross 3 domains (general, monodisciplinary and multidisciplinary) and mapped to the 6 levels from Anderson & Krathwohl’s taxonomy.",
         "The context the LLM has to use to generate the educational questions.",
         null,
         "Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "30,000 contexts (10,000 per domain)",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Free response (e.g. summary paragraph, executable code)",
         "LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",
         null,
         "The contexts are generated based on pre-existing question-answering datasets (general domain: SQuAD, monodisciplinary: MMLU)",
         "Academia",
         "No, no link is provided",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Yes, subscores are provided by domain (general, mono-humanities, mono-sciences, interdisciplinary) and by evaluation metric (consistency, relevance, coverage, and representativeness).",
         null,
         null,
         "Dr.Academy",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         null,
         "No",
         "Yes",
         "Yes",
         "The authors directly assess the validity of their benchmark through theoretical alignment with Anderson and Krathwohl’s taxonomy, expert evaluation of the evaluation metrics, and empirical correlation with human judgments.",
         "Simple mean to aggregate automatic scores, Pearson and Spearman correlation between human and automatic ratings​, and Krippendorff’s Alpha inter-rater agreement for human ratings.",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "Yes",
         "Education",
         null,
         null,
         "['Another benchmark', 'LLM-generated']",
         "['Convenience', 'Targeted', 'Criterion']",
         "['Free response']",
         "['LLM-as-a-Judge']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['No comparison made']",
         "['Yes']",
         "['Complete']",
         "['Mean', 'Other']"
        ],
        [
         "208",
         "bittonVisITbenchDynamicBenchmark2023",
         "VisIT-Bench: A Dynamic Benchmark for Evaluating Instruction-Following Vision-and-Language Models",
         "Include",
         null,
         null,
         "VisIT-Bench (Visual InsTruction Benchmark) is a benchmark for evaluating instruction-following vision-language models for real-world use. The authors curated 70 “instruction families” that they believe instruction-tuned vision-language models should be able to address.  We conduct a large-scale empirical comparison of multimodal instruction-following models using their benchmark.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Instruction following capabilities",
         "No",
         null,
         "Subset",
         null,
         "The format of the questions is either MCQ or binary QA (Yes/No).\nThey provide a collection of 70 different open-generation tasks, like reasoning over plots, object recognition, location understanding etc.",
         "Each instance contains an instruction, input image(s), an instruction-conditioned caption (a human-crafted caption for the image(s)/instruction), and a human-verified reference.  Instructions are image-contextual imperative requests or questions, e.g., for an image of pancakes, a user asks “How can I cook this in a healthy way?”.",
         null,
         "Crowd-sourced task examples (e.g. Prolific-created tasks), Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "592",
         "Yes",
         "instruction type, image source",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Multiple choice",
         "n-gram (BLEU, ROUGE, chrF), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Elo ratings, Win rate",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "stratification of results based on the instruction category",
         null,
         "https://huggingface.co/datasets/mlfoundations/VisIT-Bench",
         "VisIT-Bench",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "Instruction Following",
         null,
         null,
         "['Crowd-sourced', 'Another benchmark', 'LLM-generated']",
         "['Targeted']",
         "['Multiple choice']",
         "['Soft match', 'Human ratings', 'LLM-as-a-Judge', 'Reward']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "225",
         "tsurutaSARSCoV2InteractionDataset2024",
         "A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models",
         "Include",
         null,
         null,
         "AVIDa-SARS-CoV-2 is a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins. AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and Omicron variants.  The authors report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-specific pre-trained language models.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "binding prediction, i.e. predict the binding site of an antibody.",
         "Yes",
         "The antibody discovery task is a binary sequence classification that distinguishes antibodies that bind to SARS-CoV-2.",
         "Subset",
         null,
         "Binary classification: whether the antibody binds to a specific antigen at the antibody sequence level.",
         "- VHH_sequence: Amino acid sequence of VHH\n- Ag_label: Antigen Type\n- label: Binary label represented by 1 for the binding pair and 0 for the non-binding pair\n- subject_species: Species of the subject from which VHH was collected\n- subject_name: Name of the subject from which VHH was collected\n- subject_sex: Sex of the subject from which VHH was collected",
         null,
         "Real task examples (e.g. GitHub issues)",
         "77,003",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "Multiple choice",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Industry",
         "Yes",
         null,
         null,
         "Test, Train",
         "2M",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://huggingface.co/datasets/COGNANO/AVIDa-SARS-CoV-2",
         "AVIDa-SARS-CoV-2",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         "No",
         "Biology",
         null,
         null,
         "['Real task']",
         "['Convenience']",
         "['Multiple choice']",
         "['Exact match']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "230",
         "xieOSWorldBenchmarkingMultimodal2024",
         "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
         "Include",
         null,
         null,
         "OSWorld introduces a scalable, executable computer environment supporting real OSs (Ubuntu, Windows, macOS) to evaluate multimodal agents on 369 open-ended real-world tasks. It includes complex setups, execution-based evaluation, and detailed analysis of LLM/VLM agents' capabilities and deficiencies.",
         "They put significant emphasis on that they are \"The first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems.\" <- with emphasis on the first",
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Multimodal tool use, open-ended task execution in real OS environments,  reasoning (though they don't mention word reasoning even once, the tasks implicitly require reasoning)",
         "No",
         "The phenomenon is implicitly defined through the benchmark’s design and the types of tasks it includes. Authors describe the benchmark as evaluating agents’ ability to complete open-ended, real-world computer tasks using multimodal perception and actions (such as screenshots, accessibility trees, mouse/keyboard inputs) across various applications and operating systems.",
         "Subset",
         null,
         "Open-ended computer activity, which is described in natural language, executed by an agent inside a real operating system environment. Each task includes an initial state setup, a goal instruction, and a custom execution-based evaluation script to determine success.",
         "\"Each example is carefully annotated with a natural language instruction, a setup configuration with corresponding files and setup actions for initialization of initial states upon our provided VM image, and a manually crafted evaluation script to check if the task is successfully executed.\" (page 7)",
         null,
         "Human exam questions (e.g. GRE questions), Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Modified from another benchmark (e.g. translation into another language)",
         "412",
         "Yes",
         "human difficulty, task feasibility, application domain, task type",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response)",
         "Exact Match (accuracy, F1, precision, recall), Execution-based evaluation scripts",
         null,
         "They sourced tasks from an incredible number of sources, diverse between each other - there is a table (appendix B.3) taking almost two pages to just list program sourced.",
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "By application domain: OS, Office, Daily, Professional, Workflow\nBy OS: Ubuntu or Window\nBy task difficulty: Easy, Medium, Hard (based on human completion time)\nBy feasibility: Feasible and Infeasible tasks\nBy input modality: Screenshot, Accessibility Tree, SoM etc.",
         null,
         "https://os-world.github.io/",
         "OSWorld",
         "Not defined",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "Yes",
         null,
         "1. Human Baseline\nThey conducted human evaluations across all tasks, showing that humans (without prior expousure to the tasks) achieved +/- 72.36% accuracy, while top models performed under 12.24%, showing that the tasks are achievable.\n\n2. Realistic Task Design\nThe tasks are based on real-world scenarios, sourced from impressively large number of sources: from user forums, tutorials, and many others, to everyday computer workflows.\n\n3. Execution-Based Evaluation\nThey designed 134 custom, deterministic evaluation scripts to assess functional correctness and objective and reproducible scoring.\n\n4. Model Performance Analysis\nThe authors analysed how models fail - such as difficulty with GUI grounding, interaction noise, and poor generalisation across applications - they tried aligning that observed performance with the skills they intended to measure through the benchmark.\n\n4. Comparative Difficulty\nThey compare OSWorld to other benchmarks like WebArena and show that OSWorld tasks take longer for humans to complete and are harder for models to solve to support the idea that OSWorld includes more complex tasks, which are supposed to be closer to the real-world abilities.",
         "Just simple mean, with occasional reporting of variance or distribution plots.",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "The benchmark involves full workflows from A to Z - such as software installation, document editing, web navigation, and multi-application coordination. These tasks are executed in real OS environments (Ubuntu and Windows) and they use apps/programs (e.g. Chrome, LibreOffice, VLC), to operate as general-purpose assistants.",
         "Composite phenomenon",
         "Yes",
         "\"OSWORLD benchmark [...] encompasses 369 real computing tasks defined and executed on Ubuntu. Additionally, we provide a set of 43 tasks for Windows built on the OSWORLD environment.\" (page 6)",
         "Yes",
         "Agents",
         "Tool Use",
         null,
         "['Human exams', 'Real task', 'Author-crafted', 'Expert-crafted', 'Another benchmark']",
         "['Targeted', 'Criterion']",
         "['Multiple choice', 'Short free response', 'Free response', 'Interaction']",
         "['Exact match', 'Reward']",
         "['No definition']",
         "['Yes']",
         "['Yes']",
         "['No comparison made']",
         "['']",
         "['Complete']",
         "['Mean', 'Std']"
        ],
        [
         "246",
         "zhangCABComprehensiveAttention2023",
         "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling",
         "Include",
         null,
         null,
         "CAB is a multimodal benchmark assessing long-range modeling in transformers across computer vision, natural language processing, speech processing, and time-series forecasting. It is publicly available and composed of 7 tasks, spanning 9 datasets, to measure noncausal self, causal self, noncausal cross, and causal cross attention with a custom metric. ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Long-range modeling",
         "Yes",
         "Long range modeling is implicitly defined as \"longer sequence modeling in different domains\" (1). ",
         "Subset",
         "The four types of attention to be measured are noncausal self attention, causal self attention, noncausal cross attention, causal cross attention. ",
         "There are \"seven tasks covering four research fields ... computer vision, natural language processing, speech processing, and time series forecasting\" (3). The tasks are Text-to-Speech Synthesis (TTS), Summarization (Sum), Long Sequence Time-series Forecasting (LSTF), Point Cloud Completion (PCC), Langauge Modeling (LM), Masked Language Modeling (MLM), Super-Resolution (SR). ",
         "The benchmark is composed of 9 distinct datasets with different features. ",
         "Each task has its own dataset and metric. ",
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)",
         null,
         "No",
         null,
         "Unknown",
         "Multiple choice, Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), LLM post-processing (extracting answers, reformatting for automated scoring), Distribution (perplexity, calibration, correlation), Mel-Cepstral Distortion is a measure of audio quality for TTS. A custom index is defined to balance all the evaluation metrics. ",
         "TTS uses MCD, MSD. Sum uses ROUGE. LSTF uses MSE, MAE. PCC uses CD and F-Score. LM and MLM uses PPL. SR uses PSNR and SSMI. The paper defines a custom metric, compositional index (CI), which is \"a normalized score to balance the influence among evaluation metrics, and high CI represents excellence. It is computed as follows: a) we transform all evaluation metrics beforehand, so that a higher score indicates better performance; b) we then normalize each transformed metric with Z-score normalization; c) after normalization, the score of each evaluation metric is averaged within each task, and is further averaged across tasks\" (5). ",
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Custom compositional index (CI)",
         "Yes",
         "Scores are reported for each attention type, per task, per sub-metric in each task, and with the total CI. ",
         null,
         "https://github.com/Shark-NLP/CAB ",
         "CAB (Comprehensive Attention Benchmark)",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "The authors highlight that evaluating long-range modeling requires assessing \"standard bidirectional (or noncausal) self attention\" and \"cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications\" (1). CAB is proposed to measure causal and cross self attention, in addition to noncausal self attention. ",
         "Simple mean/sum, custom normalized aggregate metric",
         "Model access required (e.g. logits)",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         "Each task has a different target sequence length.",
         "No",
         "NLP",
         "Long Context",
         null,
         "['Real task', 'Another benchmark']",
         "['Unknown']",
         "['Multiple choice', 'Short free response', 'Free response']",
         "['Exact match', 'Soft match', 'LLM post-processing', 'Distribution', '']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean', 'Other']"
        ],
        [
         "250",
         "panchalWhatSayWhen2024",
         "What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction",
         "Include",
         null,
         null,
         "Open-ended, asynchronous interactions, where an AI model may proactively deliver timely responses or feedback based on the unfolding situation in real-time, are an open challenge. This work presents the QEVD-FIT-COACH benchmark and dataset, which explores human-AI interaction in the challenging, yet controlled, real-world domain of fitness coaching – a task which intrinsically requires monitoring live user activity and providing immediate feedback.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Situated interaction",
         "Yes",
         "A notable type of situated interaction is the instructional or coaching scenario, where an instructor guides a user through a complex activity, such as live fitness coaching. ",
         "Subset",
         null,
         "Feedback Structure: The feedbacks in the QEVD-FIT-COACH benchmark have the following structure: At the start of each exercise, acknowledging feedback is given once the user has started; otherwise, a reminder to do so is provided. A corrective feedback is provided as soon as a mistake is clearly visible. Similarly, when the user begins to correct their mistake, feedback is provided to acknowledge and guide the user to successfully correct the error. If the user is performing the exercise correctly, feedback focuses on repetition counting.  Finally, at the end of each exercise, a feedback focused on the overall performance during that exercise is provided. ",
         "Video frames, a list of feedback statements that correspond to a specific timestep",
         null,
         "Real task examples (e.g. GitHub issues), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "28,326",
         "No",
         null,
         "Random sample (creators defined a task space and sampled from it)",
         "Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response)",
         "n-gram (BLEU, ROUGE, chrF), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train",
         "377,678",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/Qualcomm-AI-research/FitCoach",
         "QEVD-FIT-COACH",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "Yes",
         "No",
         null,
         "simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "No",
         null,
         "No",
         "User Interaction",
         null,
         null,
         "['Real task', 'LLM-generated']",
         "['Random']",
         "['Free response', 'Interaction']",
         "['Soft match', 'LLM-as-a-Judge']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "251",
         "zhangCABComprehensiveAttention2023",
         "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\t",
         "Include",
         null,
         null,
         "Current benchmarks testing different attention architectures for long-term modelling only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions. \n\nIn this paper, we propose Comprehensive Attention Benchmark (CAB) with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. In seven tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Long sequence modelling capability of transformers with different attention mechanisms",
         "No",
         "It is defined as the performance of the transformer models in handling long-sequence tasks. There are seven tasks, such as text-to-speech synthesis, summarization, Long Sequence Time-series Forecasting, etc. The sequence length considered 'long' would be different for each task. ",
         "Comprehensive",
         null,
         "There are seven long-sequence tasks ranging from Computer Vision, NLP and time-series forecasting handled by transformers. Examples include Super-Resolution, Masked Language Modelling, Long Sequence Time-series Forecasting. ",
         "A long-sequence task, the dataset for the task, sequence length, evaluation metric for performance, transformer model type, attention mechanism",
         null,
         "Real task examples (e.g. GitHub issues)",
         null,
         "Yes",
         "The backbone transformer architecture and attention architecture",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), n-gram (BLEU, ROUGE, chrF), Distribution (perplexity, calibration, correlation), MCD, MSD, PSNR, SSIM",
         "Different metrics used to evaluate each task.",
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         "Different outputs for each task. For example, for summarisation, it produces a summary paragraph; for Super-Resolution, it convert low-resolution (16 × 16) face images into high-resolution (128 × 128) images.",
         null,
         "Yes",
         "Results are provided separately for each task with different metrics. ",
         null,
         "https://github.com/Shark-NLP/CAB",
         "Comprehensive Attention Benchmark (CAB)",
         "Not defined",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         "So one dataset for each task, seven tasks in total",
         "No",
         "NLP",
         "Long Context",
         null,
         "['Real task']",
         "['Convenience']",
         "['Free response']",
         "['Exact match', 'Soft match', 'Distribution', 'Soft match']",
         "['No definition']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         null
        ],
        [
         "259",
         "luWebLINXRealworldWebsite2024",
         "WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue",
         "Include",
         null,
         null,
         "WebLinx introduces a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Authors develop a multimodal agent able of interpreting both visual and textual input to complete web-based tasks with long context understanding and planning capabilities.",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Multimodal reasoning, web navigation",
         "Yes",
         "“We define the real-world problem of conversational web navigation: given the initial user instruction, an agent must complete a real-world task inside a web browser while communicating with the user via multi-turn dialogue.” (Page 1, Introduction)",
         "Subset",
         null,
         "The task is defined as conversational web navigation, where an agent must complete a user-specified goal on a real-world website by interacting with the web interface (e.g., clicking, typing, submitting forms) while engaging in multi-turn dialogue with the user. The agent receives inputs such as browser screenshots, DOM elements, and dialogue history to predict the next action at each turn.",
         "Each item is one step in a task, where the agent sees the current web page, past actions, and what the user said, and must decide what to do next—like clicking a button or typing text. Many of these steps together make up a full task.",
         "Each task unfolds as a multi-turn dialogue between a user (called in the paper instructor) and an agent (called in the paper navigator), with actions done in a real browser environment. The task goal may not be fully known at the start and often evolves over the conversation, making long-term memory and contextual understanding important.",
         "Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "\"100K interactions across 2300 expert demonstrations of conversational web navigation.\"",
         "Yes",
         "website category, subcategory, geographic region, instructor visual access, AI assistance",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Short free response (e.g. single word or number), Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall)",
         "The exact match metric is calculated for each turn in the task, comparing the predicted action (such as click, say, text input) with the ground truth action. ",
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "Train - 24418, validation - 1717",
         null,
         "Simple Mean, aggregated using the micro-average of turn-level scores",
         "Yes",
         "Subscores are provided for element-based actions, text-based actions, and intent matching (whether the correct action type is predicted).",
         null,
         "https://mcgill-nlp.github.io/weblinx/",
         "WEBLINX",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "Benchmark is based on interacting with people, where the end can have different results, depending on how the conversation goes - therefore making it a bit of grey area.",
         "Composite phenomenon",
         "No",
         "It is quite confusing from the text to count the size of the set, especially that it is based on interactions which are dependent variable - I am not fully sure if the sizes in task_dataset_size_extra are correct, I based them on table 8 as it is the only table showing active turns.",
         "No",
         "Agents",
         "Web",
         null,
         "['Real task', 'Author-crafted', 'Expert-crafted', 'Procedurally-generated']",
         "['Targeted', 'Criterion']",
         "['Short free response', 'Interaction', 'Structured']",
         "['Exact match']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         null
        ],
        [
         "267",
         "liuRevisitingDeIdentificationElectronic2023",
         "Revisiting De-Identification of Electronic Medical Records: Evaluation of Within- and Cross-Hospital Generalization\n",
         "Include",
         null,
         "Boundary include - measures LLM capabilities in theory and does provide a benchmark, but tests pre-ChatGPT models including a self-trained CNN\n",
         "Benchmark for de-identification of protected health information (PHI) in Chinese electronic medical records, with a focus on cross-hospital generalization. Constructs a multi-hospital dataset and evaluates various models and domain generalization (DG) techniques to assess performance under domain shift.\n",
         "Pre-LLMs so uses the dataset - which is still a valid benchmark - as training data for a CNN and a BERT fine-tuning run.",
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Generalization on de-identification tasks\n",
         "Yes",
         "Anonymization\n",
         "Subset",
         null,
         "Detect and remove personal health information mentions (e.g., names, locations, dates) in clinical records from three Chinese hospitals.\n",
         "Each item is a sentence or span from an electronic medical record, with relevant tokens labeled using tags corresponding to personal data categories, but the labels hidden. The task is to recreate these token labels.\n",
         "\n",
         "Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)",
         "500",
         "Yes",
         "Sentence and mention counts, health information category counts per dataset\n",
         "Random sample (creators defined a task space and sampled from it), Convenience sample (creators found a set of tasks that was readily accessible)",
         "Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall)",
         "Predicted span of tags must exactly match correct span and category\n",
         "Data collected from three Chinese hospitals - no idea how that got around data protection laws even with anonymization; they must be quite loose there - and hand-annotated",
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train, Validation",
         "400",
         "Sequence output with personal data tagged",
         "Simple Mean",
         "Yes",
         "Per PII category (e.g., PERSON, DATE, ID)\n",
         null,
         "https://github.com/lanyangyang93/Revisiting-De-Identification",
         null,
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "De-identification is a required step for real-world medical data sharing and lots of other data sharing contexts.\n",
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         "No",
         "Medicine",
         null,
         null,
         "['Real task', 'Author-crafted']",
         "['Random', 'Convenience']",
         "['Structured']",
         "['Exact match']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         null
        ],
        [
         "277",
         "flachsGrammaticalErrorCorrection2020",
         "Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses",
         "Include",
         null,
         null,
         "CWEB is a benchmark for grammatical error correction that is publicly available and manually annotated by experts. It contains website data from Common Crawl, and includes sentences with low and high error density. ",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Grammatical error correction",
         "Yes",
         "\"Grammatical error correction (GEC) is the task of automatically editing text to remove grammatical errors\" (8467).",
         "Comprehensive",
         null,
         "The model is given text and must identify and correct grammatical errors. ",
         "A single item contains a sentence with in-line corrections. ",
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "13574",
         "No",
         null,
         "Random sample (creators defined a task space and sampled from it)",
         "Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), Distribution (perplexity, calibration, correlation)",
         "The paper uses F_{0.5} and ERRANT, standard grammar error correction metrics, cited on page 8471, to assess the correctness of the correction. Perplexity and semantic similarity are used to measure the semantic change in a sentence after the edit. ",
         "The websites are in English and derived from the first 18 dumps of Common Crawl. Text is filtered to remove non-English and incomplete sentences using justText. The data is manually corrected by expert annotators. The dataset is split into CWEB-S (sponsored websites) and CWEB-G (generic) websites. ",
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test, Train",
         "Development/Test 6729/6845",
         null,
         null,
         "No",
         null,
         null,
         "https://github.com/SimonHFL/CWEB",
         "CWEB (Corrected WEBsites)",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "The authors detail that grammar error correction (GEC) models must \"perform well in the open-domain setting and generalize, not only to writing produced in the educational context, but also to language production 'in the wild'\" (8647). The authors also highlight that a strong GEC benchmark must evaluate \"domain adaptation and low precision\" in texts with low error density (8647). ",
         "Simple mean",
         "Model access required (e.g. logits)",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "No",
         null,
         "No",
         "NLP",
         "Understanding",
         null,
         "['Another benchmark']",
         "['Random']",
         "['Free response']",
         "['Exact match', 'Distribution']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "291",
         "wuSmartPlayBenchmarkLLMs2024",
         "SMARTPLAY : A BENCHMARK FOR LLMS AS INTELLIGENT AGENTS",
         "Include",
         null,
         null,
         "This paper introduces SmartPlay, a benchmark for assessing LLMs as intelliget agents using 6 different games including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "LLMs as intelligent agents",
         "No",
         "While the authors do not explicitly provide a definition for intelligent agents, they provide some key properties of agents from which the target phenomenon aims to be captured—i.e., LLM agents as systems capable of long-horizon planning, probabilistic reasoning, spatial reasoning to understand the 3D world, and learning from interactions or mistakes. This is further decomposed into 9 measurable abilities; long text understanding, reasoning, instruction following, planning, generalization, understanding the odds, learning from interactions, error/mistake handling and spatial reasoning.",
         "Comprehensive",
         null,
         "An LLM is provided with environment-specific inputs; either textual descriptions or visual descriptions (via natural language)—along with manuals containing background knowledge, rules, and examples. The LLM must then interact with the environment by selecting actions from a predefined action space to achieve task objectives across multiple trials or rollouts.",
         "A task item is a description of a game, the rules, actions, environment and expected behaviour.",
         "A task item consists of a description of the game, its rules, available actions, environment state, and the expected agent behavior.",
         "Real task examples (e.g. GitHub issues)",
         "6 games with 20 different evaluation settings",
         "Yes",
         "Each task item is annotated with the abilities required i.e.  long text understanding, reasoning, instruction following, planning, generalization, understanding the odds, learning from interactions, error/mistake handling and spatial reasoning.",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "Extended interaction (e.g. conversation, calling an API and processing the response)",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "Yes",
         "Across each game in the task",
         null,
         "https://github.com/microsoft/SmartPlay",
         "SMARTPLAY",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "Yes",
         "Somewhat",
         "The authors use real-world-inspired games to evaluate LLM capabilities as intelligent agents, but the extent to which success in these environments generalizes to real-world agentic behavior remains an open question.",
         "Scores are normalised relative to human performance.",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "Agents",
         null,
         null,
         "['Real task']",
         "['Convenience']",
         "['Interaction']",
         "['Exact match']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Other']"
        ],
        [
         "296",
         "tuWaterBenchHolisticEvaluation2024",
         "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models",
         "Include",
         null,
         null,
         "WaterBench is a benchmark for evaluating LLM watermarks across detection and generation quality. The paper also presents a hyper-parameter search method to control watermarking strength, and automatic evaluation using GPT4-Judge. The dataset is publicly available and human-validated. ",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "LLM watermarks",
         "Yes",
         "A watermarked LLM generates texts \"with a biased distribution of tokens, which distinguishes it from unwatermarked texts, ... , the goal of watermarking is to achieve high detection accuracy while maintaining the generation quality\" (1517). ",
         "Comprehensive",
         null,
         "WaterBench consists of 9 tasks with 5 unique task settings, spanning \"a wide range of input and output length\" (1520). The first setting is Short Input, Short Answer, and has two tasks to evaluate factual knowledge. The second setting is Short Input, Long Answer, with two Long-form QA tasks. The third category is Long Input, Short Answer, with reasoning and code completion tasks. The fourth setting is Long Input, Long Answer, with two summarization tasks. The last setting is open-ended generation, where the task is instruction-following. ",
         "Each task is sourced from a different dataset and has its own features. ",
         null,
         "Real task examples (e.g. GitHub issues), Modified from another benchmark (e.g. translation into another language)",
         "2405",
         "No",
         null,
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Short free response (e.g. single word or number), Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics), Generation Metric and Generation Quality Drop are never explicitly defined in the paper. ",
         "The paper defines watermarking strenth as the true positive rate to ensure all watermarks are of similar intensity during evaluation. WaterBench also uses GPT4-Judge, which \"measures which model's output the GPT-4 system prefers when shown two responses for the same instruction\" (1521). The paper reports the \"True Positive Rate, True Negative Rate, Generation Metric, and Generation Quality Drop for all tasks\" (1523). 100 responses are randomly sampled for human annotation as well. ",
         "Each task is sourced from a different dataset. ",
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/THU-KEG/WaterBench",
         "WaterBench",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "The authors highlight that evaluating watermark must \"evaluate the generation and detection\" methods, and ensure fair comparisons between watermarking methods (1517). Additionally, the authors highlight that the tasks must be diverse, go beyond \"text completion,\" and measure \"generation quality\" and alignment with \"human preferences\" (1517-1518). ",
         "True Positive Rate, True Negative Rate, Generation Metric and Generation Quality Drop",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "No",
         null,
         "No",
         "NLP",
         "Detection",
         null,
         "['Real task', 'Another benchmark']",
         "['Convenience', 'Targeted', 'Criterion']",
         "['Short free response', 'Free response']",
         "['Exact match', 'Human ratings', 'LLM-as-a-Judge', 'LLM-as-a-Judge']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "297",
         "luoMMMRSMultimodalMultiGSD2024",
         "MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation",
         "Include",
         null,
         null,
         "MMM-RS is a large, multi-modal multi-GSD, and multi-scene remote sensing text-to-image generation benchmark. It is publicly available, aggregated and filtered from existing datasets, and contains information-rich captions. ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "remote sensing text-to-image generation",
         "No",
         "Remote sensing image generation is the ability to prompt a multimodal model to generate a high-quality remote sensing images. ",
         "Comprehensive",
         null,
         "A multimodal model is given an information-rich text prompt and must generate the described remote sensing image. ",
         "An remote sensing image and an information-rich text prompt, specific to its image modality. For example, the prompt may contain satellite type, weather type, category, resolution, subject, etc.  ",
         null,
         "Modified from another benchmark (e.g. translation into another language), LLM-generated task examples (e.g. Filtered from responses to a prompt)",
         "2,103,273",
         "No",
         "=",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Image",
         "Distribution (perplexity, calibration, correlation)",
         "The paper uses Frechet Inception Distance (FID) and Inception Score (IS).",
         "\"The MMM-RS dataset is derived from 9 publicly available RS datasets: MRSSC2.0 [16], Inria [19], NaSC-TG2 [45], GID [28], WHU-OPT-SAR [14], HRSC2016 [40], TGRS-HRRSD [42], fMoW [5], and SEN1-2 [25]\" (4). It contains images across three modalities: RGB, Synthetic Aperture Radar, and Near Infrared. Multi-scene remote sensing images are synthesized at different scales and weather conditions using physics models and multimodal models. The process is outlined in Figure 4 on Page 6. ",
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train",
         "Optional train set is defined at 200,000 samples. ",
         null,
         "Simple Mean",
         "No",
         null,
         null,
         "https://github.com/ljl5261/MMM-RS",
         "MMM-RS (Multi-modal, Multi-GSD, Multi-scene Remote Sensing)",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "The authors highlight that a remote sensing text-to-image generation dataset should be multimodal across data and image types, and be information-rich. ",
         "Simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         "No",
         "Grounding",
         null,
         null,
         "['Another benchmark', 'LLM-generated']",
         "['Targeted', 'Criterion']",
         "['Free response']",
         "['Distribution']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean']"
        ],
        [
         "312",
         "liMediQQuestionAskingLLMs2024",
         "MediQ: Question Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning\n",
         "Include",
         null,
         null,
         "Medi-Q evaluates LLMs' ability to ask clarifying questions for improved clinical reasoning. It provides expert-authored diagnostic cases and assesses interactive decision-making.\n",
         "Introduces not just the concept of requiring proactive clarifying questions from the model, but also a pipeline to transfer any existing QA benchmark to its structure\n",
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Interactive clinical reasoning, question-asking\n",
         "Yes",
         "Defined as the ability to proactively ask clarifying questions to gather information for accurate medical decision-making\n",
         "Subset",
         null,
         "Given a clinical vignette, decide whether to ask a follow-up question, then make a diagnosis.\n",
         "A clinical case with simple info and a question + \"patient system\" simulating a patient giving follow-up answers; a correct answer/diagnosis.\n",
         "Task involves another LLM cosplaying as a patient volunteering information as demanded by the LLM being evaluated.",
         "Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Modified from another benchmark (e.g. translation into another language)",
         "12863",
         "Yes",
         "Annotated question helpfulness, diagnosis, case difficulty\n",
         "Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Extended interaction (e.g. conversation, calling an API and processing the response)",
         "Exact Match (accuracy, F1, precision, recall), Human ratings (text quality, preference, NOT manual scoring of other metrics), LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)",
         "Includes rubric-based expert judgment and GPT-4 scoring\n",
         "Adapted from MedQA and Craft-MD, which are simple QA benchmarks\n",
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         "Open-ended Q&A + final diagnosis\n",
         "Simple Mean",
         "Yes",
         "Diagnosis accuracy, question helpfulness, reliability\n",
         "majority@k (majority vote over k trials)",
         "https://github.com/stellalisy/mediQ",
         "MediQ",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "Human agreement on annotation; rubric validation; comparison to non-interactive version of benchmark\n",
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "LLM acts as clinical assistant making iterative decisions and asking follow-up questions. Could probably nit the realism but in comparison to most others it's much closer to real clinical workflows.\n",
         "Composite phenomenon",
         "Yes",
         null,
         "Yes",
         "Medicine",
         null,
         null,
         "['Author-crafted', 'Another benchmark']",
         "['Targeted', 'Criterion']",
         "['Interaction']",
         "['Exact match', 'Human ratings', 'LLM-as-a-Judge']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         null
        ],
        [
         "328",
         "yoranAssistantBenchCanWeb2024",
         "ASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?",
         "Include",
         null,
         null,
         "This paper introduces AssistantBench - benchmark designed to evaluate whether web agents can solve realistic and time-consuming web-based tasks, such as finding gym schedules or real-estate prices. It also presents SPA (See-Plan-Act), a new web agent for more effective open-web navigation.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications), Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "web-based information-seeking",
         "Yes",
         "\"We examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses.”\n“Tasks in ASSISTANTBENCH are based on real information needs encountered by humans. To solve these tasks, an agent must autonomously browse the web to identify relevant web pages and dynamically interact with them to produce an output.\"",
         "Subset",
         "The distinction between specific application and general capability is tricky because it evaluates general capabilities like planning, retrieval, and reasoning - but within the specific context of realistic web-based tasks. While the underlying skills are broadly applicable, the benchmark is grounded in application setting (open-web agenting), making it hard to tell whether it’s measuring general abilities or just how well those abilities transfer to this use case (which is also quite broad).",
         "The task is to evaluate whether language agents can autonomously complete realistic, time-consuming web-based information-seeking tasks by browsing, interacting with, and synthesising content from multiple websites.",
         "user-like query (e.g., \"Which gym near X has classes before 7am?\"), gold answer, relevant URLs where the answer can be found, and a step-by-step explanation of how to solve it.",
         "They evaluate two types of tasks: more broad and less experienced (like real estate, travel, fitness, education etc.), and created by experts in fields as biology and law.",
         "Real task examples (e.g. GitHub issues), Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions), Expert-crafted task examples (e.g. hand-written examples), Crowd-sourced task examples (e.g. Prolific-created tasks)",
         "214",
         "Yes",
         "difficulty level (easy, medium, hard), time-dependency class (static, stable, time-sensitive)",
         "Convenience sample (creators found a set of tasks that was readily accessible), Targeted items (creators defined a task space and chose tasks within it strategically), Specific criteria (items were taken from a larger set based on specified rules)",
         "Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response), Structured response (e.g. valid JSON, API call alone)",
         "Exact Match (accuracy, F1, precision, recall), LLM post-processing (extracting answers, reformatting for automated scoring)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Validation",
         "validation: 33",
         null,
         "Simple Mean",
         "Yes",
         "Task difficulty (easy, medium, hard), task source (seed, crowd, expert), and time-dependency class (static, stable, likely-to-change).",
         null,
         "https://assistantbench.github.io/",
         "AssistantBench",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "Yes",
         "Authors manually filter tasks to remove fast-changing or ambiguous cases, structure answers for auto-evaluation, and analyse error types (navigation, hallucination, grounding). They also categorise tasks by difficulty (based on model success) and by time-dependency (static, stable, volatile) to support the validity of benchmark in measuring its intended phenomenon.",
         "Simple mean and standard error of the mean (for plots like accuracy vs. steps)",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "No",
         null,
         "No",
         "Agents",
         "Web",
         null,
         "['Real task', 'Author-crafted', 'Expert-crafted', 'Crowd-sourced']",
         "['Convenience', 'Targeted', 'Criterion']",
         "['Free response', 'Interaction', 'Structured']",
         "['Exact match', 'LLM post-processing']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         "['Mean', 'Std']"
        ],
        [
         "352",
         "braunAGBDECorpusAutomated2024",
         "AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts",
         "Include",
         null,
         null,
         "In this paper, the authors introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts. Together with the data, they present a first baseline for the task of detecting potentially void\nclauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5. The results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54. ",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         " consumer contract understanding",
         "No",
         "It's not defined ",
         "Comprehensive",
         null,
         "The task is to predict whether a given contract clause is valid or not.",
         "A contrac clause (paragraph)",
         null,
         "Expert-crafted task examples (e.g. hand-written examples)",
         "Train:3004, Test: 755",
         "Yes",
         "Topic, language, number of sentences",
         "Unknown",
         "Multiple choice",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test, Train",
         null,
         null,
         "Unknown",
         "No",
         null,
         null,
         "https://github.com/DaBr01/AGB-DE",
         "AGB-DE",
         "Not defined",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         null,
         "No",
         null,
         "Unkown ",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         "No",
         "Law",
         null,
         null,
         "['Expert-crafted']",
         "['Unknown']",
         "['Multiple choice']",
         "['Exact match']",
         "['No definition']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         "['Unknown']"
        ],
        [
         "368",
         "heOlympiadBenchChallengingBenchmark2024",
         "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
         "Include",
         null,
         null,
         "In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Notably, the best-performing model, GPT-4V, attains a low average score on OlympiadBench, with an even lower school in physics, highlighting the benchmark rigor and the intricacy of physical reasoning.",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "maths and physics reasoning",
         "Yes",
         "With traditional benchmarks becoming less challenging for these LLMs, new rigorous challenges are essential to gauge their advanced reasoning abilities. ",
         "Subset",
         null,
         "Given a math/physics problem, provide its solution and final answer. ",
         "Problem, solution, final answer. ",
         null,
         "Human exam questions (e.g. GRE questions)",
         "8476",
         "Yes",
         "Type of reasoning, difficulty, image inclusion. ",
         "Targeted items (creators defined a task space and chose tasks within it strategically)",
         "Free response (e.g. summary paragraph, executable code)",
         "Post-processing with heuristics",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         null,
         "Yes",
         "Difficulty, type of reasoning.",
         null,
         "https://github.com/OpenBMB/OlympiadBench",
         "OlympiadBench",
         "Widely-agreed",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "Reasoning",
         "Mathematical",
         null,
         "['Human exams']",
         "['Targeted']",
         "['Free response']",
         "['Soft match']",
         "['Widely-agreed']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         null
        ],
        [
         "369",
         "shiLanguageModelsAre2023",
         "Language models are multilingual chain-of-thought reasoners",
         "Include",
         null,
         null,
         "We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. ",
         null,
         "Specific Application (A single use case, where the benchmark is likely to be examples of that use case)",
         "Multilingual chain-of-thought reasoning. ",
         "Yes",
         "We evaluate the reasoning abilities of large language models in multilingual settings. ",
         "Subset",
         null,
         "Take a math problem and provide a numerical answer.",
         "Example problem, numerical answer. ",
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "2500",
         "Yes",
         "Language",
         "Specific criteria (items were taken from a larger set based on specified rules)",
         "Short free response (e.g. single word or number)",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Mix (multiple authors from industry and academia)",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         null,
         "Yes",
         "Language",
         null,
         "https://github.com/google-research/url-nlp",
         "MGSM",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "The benchmark is itself realistic",
         "Yes",
         "Yes",
         "To verify the quality of the human translations, the vendor sent a random subset of translations to an additional translator to verify the quality, and checked for n-gram overlap with popular MT providers to ensure that no machine translation toolkit has been used. We employ the translation results as gold standard translations.",
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "Reasoning",
         "Mathematical",
         "Multilinguality",
         "['Another benchmark']",
         "['Criterion']",
         "['Short free response']",
         "['Exact match']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['Yes']",
         "['Complete']",
         null
        ],
        [
         "424",
         "zengMRbenMetareasoningBenchmark2024",
         "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs",
         "Include",
         null,
         null,
         "Dataset of question,answer pairs in which answers have mistakes. Models are evaluated on ability to spot mistake and give a reason for why it is wrong. Extends existing work like MR-GSM8K to physics, chemistry, logic, coding etc",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Meta-reasoning ",
         "No",
         "\"Reasoning, the cognitive process of using evidence, arguments, and logic to reach conclusions, is crucial for problem-solving, decision-making, and critical thinking.\"\n\nMeta-reasoning: \"it is important to complement outcome-based evaluation with an intrinsic evaluation of the quality of the reasoning process\"\n",
         "Comprehensive",
         null,
         "Each data\npoint encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error\nanalysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses,\nwhich include error steps, reasons behind the error, and subsequent corrections. The three examples\nshown are selected to represent arithmetic, logical, and algorithmic reasoning types.",
         "Each data\npoint encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error\nanalysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses,\nwhich include error steps, reasons behind the error, and subsequent corrections.\n\n\nSubject: High School Math\nQuestion: What is the smallest whole number that has a remainder of 1 when divided by 4, a remainder of 1 when divided by 3, and a remainder of 2 when divided by 5?\nOptions: [A.] 67 [B.] 43 [C.] 57 [D.] 37\nGround Truth Answer: D\n—————————————- Sampled CoT ——————————————–\nSampled Model: Claude2\nModel Solution Steps:\nStep 1: Check Choice A (67) When 67 is divided by 4, the remainder is 3, not 1. So 67 is not the\ncorrect answer.\nStep 2: Check Choice B (43) When 43 is divided by 4, the remainder is 1. When 43 is divided by 3,\nthe remainder is 1. When 43 is divided by 5, the remainder is 3, not 2. So 43 is not the correct answer.\nStep 3: Check Choice C (57) When 57 is divided by 4, the remainder is 1. When 57 is divided by 3,\nthe remainder is 0, not 1. So 57 is not the correct answer.\nStep 4: Check Choice D (37) When 37 is divided by 4, the remainder is 1. When 37 is divided by 3,\nthe remainder is 1. When 37 is divided by 5, the remainder is 2.\nStep 5: Choice D (37) satisfies all the requirements and is the correct answer.\nSolution: Choice D\nSolution: Choice D\n—————————————- Human Annotation —————————————\nModel Solution Correctness: incorrect\nModel Solution First Error Step: Step 2\nModel Solution Error Reason:\nThe model’s calculation is incorrect. In step 2, the model incorrectly calculated the remainder of 43\ndivided by 4 as 1 instead of 3. The model assumes that when 43 is divided by 4, the remainder is 1,\nbut it should be 3. There is a miscalculation of the remainder here.\nModel Solution Rectified First Error Step: Step 2: Check option B (43): When 43 is divided\nby 4, the remainder is 3, not 1. When 43 is divided by 3, the remainder is 1. When 43 is divided by\n5, the remainder is 3, not 2. Therefore, 43 is not the correct answer.\n————————————-Model Evaluation Result————————————\nEvaluated Model: gpt-4-turbo\nEvaluation Raw Response: Solution Analysis: Step 1: The step checks the remainder when 67 is\ndivided by 4 and finds that it yields a remainder of 3, not 1. This is a correct verification based on\nthe criteria specified in the question.\nStep 2: Checks the remainder of 43 when divided by 4, 3, and 5. The findings (1, 1, and 3 respectively) are verified correctly and match the expected remainders except for the one with 5, correctly\nidentifying that the condition is not met for division by 5.\nStep 3: The step calculates the remainders for 57 being divided by 4 and 3. The remainder when\ndivided by 3 does not match the specified conditions (expected was 1, result was 0). Hence, the claim\nthat 57 isn’t correct is valid.\nStep 4 and Step 5: Computes and verifies the remainders for 37 when divided by 4, 3, and 5, matching\nall the expected remainders accurately, and concludes that 37 meets all conditions specified in the\nproblem.\nThe answer selection of Choice D for 37 is based on accurate verification that fulfills the problem’s\nconstraints.\nSolution Correctness: correct\nFirst Error Step: N/A\nError Reason: N/A",
         null,
         "Modified from another benchmark (e.g. translation into another language)",
         "5975",
         "Yes",
         "Breakdown by field: Math, Medicine, Biology, Physics, Chemistry, Logic, Coding",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "Multiple choice, Free response (e.g. summary paragraph, executable code)",
         "Exact Match (accuracy, F1, precision, recall), Correlation (Matthew's correlation, Pearson's r), Define MR-Score, a weighted combination of matthews correlation, and two variants of accuracy",
         null,
         "Mostly from MMLU, some logic from LogiQA, some coding from MHPP.",
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Weighted Mean",
         "Yes",
         "Math, Medicine, Biology, Physics, Chemsitry, Logic, Coding",
         null,
         "https://huggingface.co/datasets/Randolphzeng/Mr-Ben",
         "MR-Ben",
         "Contested",
         "Yes",
         "The metric is new and not very well motivated",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         null,
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         "People use chatGPT for checking their work all the time. ",
         "Single cohesive phenomenon",
         "Not applicable",
         null,
         "Yes",
         "Reasoning",
         null,
         null,
         "['Another benchmark']",
         "['Convenience']",
         "['Multiple choice', 'Free response']",
         "['Exact match', 'Correlation', 'Correlation']",
         "['Contested']",
         "['Yes']",
         "['No']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         null
        ],
        [
         "443",
         "jainR2ETurningAny2024",
         "R2E: Turning any Github Repository into a Programming Agent Environment",
         "Include",
         null,
         null,
         "We present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. \n\nWe instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. \n\nOur results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. ",
         null,
         "General Capability (A broadly useful ability, which could be relevant to multiple applications)",
         "Code generation",
         "Yes",
         "The ability of LLM coding agents to solve real-world software engineering tasks by \n modifying codebases and using test outcomes to guide code generation. ",
         "Subset",
         null,
         "The benchmark evaluates LLM coding agents for their ability to interact with GitHub repositories and do test generation, code repair, and code validation.",
         "A single item consists of a GitHub repository, a target task for the LLM agent to solve (e.g, implement a function or fix a bug) and an evaluation outcome.",
         null,
         "Real task examples (e.g. GitHub issues), Procedurally-generated task examples (e.g. Creating instances from a template)",
         "1000 coding-related tasks across 300 repositories.",
         "Yes",
         "repository, type of task, programming language",
         "Convenience sample (creators found a set of tasks that was readily accessible)",
         "Free response (e.g. summary paragraph, executable code), Extended interaction (e.g. conversation, calling an API and processing the response)",
         "Exact Match (accuracy, F1, precision, recall)",
         null,
         null,
         "Academia",
         "Yes",
         null,
         null,
         "Test",
         null,
         null,
         "Simple Mean",
         "No",
         null,
         "pass@k (any correct answer in k trials)",
         "https://github.com/r2e-project/r2e",
         "R2E-Eval1",
         "Contested",
         "Yes",
         "Yes",
         "Yes",
         "No",
         "No comparisons made",
         "The benchmark is itself realistic",
         "No",
         "No",
         null,
         "Simple mean",
         "Outputs alone",
         "Complete real task (e.g. providing medical advice to real people interactively)",
         null,
         "Composite phenomenon",
         "Yes",
         null,
         "No",
         "Agents",
         "Coding",
         null,
         "['Real task', 'Procedurally-generated']",
         "['Convenience']",
         "['Free response', 'Interaction']",
         "['Exact match']",
         "['Contested']",
         "['Yes']",
         "['Yes']",
         "['Realistic']",
         "['No']",
         "['Complete']",
         "['Mean']"
        ]
       ],
       "shape": {
        "columns": 70,
        "rows": 36
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>target_phenomenon</th>\n",
       "      <th>phenomenon_defined</th>\n",
       "      <th>...</th>\n",
       "      <th>dataset_sampling_method_clean</th>\n",
       "      <th>response_format_clean</th>\n",
       "      <th>metric_definition_clean</th>\n",
       "      <th>phenomenon_contested_clean</th>\n",
       "      <th>task_face_validity_clean</th>\n",
       "      <th>metric_face_validity_clean</th>\n",
       "      <th>results_realism_clean</th>\n",
       "      <th>results_author_validity_clean</th>\n",
       "      <th>task_ecology_clean</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mundlerSWTBenchTestingValidating2024</td>\n",
       "      <td>SWT-Bench: Testing and Validating Real-World B...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A benchmark for generating code tests (unit te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Automatic code test generation (i.e. generatin...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Criterion]</td>\n",
       "      <td>[Structured]</td>\n",
       "      <td>[Reward]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>niuRAGTruthHallucinationCorpus2024</td>\n",
       "      <td>RAGTruth: A Hallucination Corpus for Developin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper targets word-level hallucinations i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>hallucination detection, specifically for RAG ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Random, Targeted]</td>\n",
       "      <td>[Short free response, Free response, Structured]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[No comparison made]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>mahbubUnveilingEssencePoetry2023</td>\n",
       "      <td>Unveiling the Essence of Poetry: Introducing a...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper proposes the task of poem summarizat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>poem summarization</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Criterion]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Exact match, Soft match]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>huangMLAgentBenchEvaluatingLanguage2024</td>\n",
       "      <td>MLAgentBench: Evaluating Language Agents on Ma...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MLAgentBench benchmarks the ability of LLM age...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>ML Experimentation</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Reward]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>yeGlobeSummChallengingBenchmark2024</td>\n",
       "      <td>GlobeSumm: A Challenging Benchmark Towards Uni...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Propose GLOBESUMM and introduce prompting meth...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Text summarization</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Soft match, LLM post-processing]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[No comparison made]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>athiwaratkunMultilingualEvaluationCode2023</td>\n",
       "      <td>Multi-lingual Evaluation of Code Generation Mo...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Measures code generation capabilities across 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>Code generation</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>[Structured]</td>\n",
       "      <td>[Reward]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>naousReadMeBenchmarkingMultilingual2024</td>\n",
       "      <td>README++: Benchmarking Multilingual Language M...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ReadMe++ is a multilingual and multi-domain da...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>Readability assessment</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted, Criterion]</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>[Distribution]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>tanDevBenchMultimodalDevelopmental2024</td>\n",
       "      <td>DevBench: A multimodal developmental benchmark...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DevBench is a multimodal benchmark for assessi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>language evaluation, language development, cog...</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience, Targeted, Criterion]</td>\n",
       "      <td>[Short free response]</td>\n",
       "      <td>[Distribution]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>gharaeeBIOSCAN5MMultimodalDataset2024</td>\n",
       "      <td>BIOSCAN-5M: A Multimodal Dataset for Insect Bi...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIOSCAN-5M is a multimodal benchmark for insec...</td>\n",
       "      <td>BIOSCAN-5M is an expansion of BIOSCAN-1M. It i...</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>insect classification</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience, Targeted]</td>\n",
       "      <td>[Short free response]</td>\n",
       "      <td>[Exact match, Distribution]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>mitaStrikingGoldAdvertising2024</td>\n",
       "      <td>Striking Gold in Advertising: Standardization ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAMERA is a multimodal benchmark for automatic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>automatic ad text generation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Random, Convenience]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Soft match, Human ratings, LLM post-processin...</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>yangDataTalesBenchmarkRealworld2024</td>\n",
       "      <td>DataTales: A Benchmark for Real-World Intellig...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DataTales is a novel benchmark designed to ass...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>Data narration</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted, Criterion]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Exact match, Soft match, Human ratings]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>zhangHumorAIMassive2024</td>\n",
       "      <td>Humor in AI: Massive Scale Crowd-Sourced Prefe...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper presents a New Yorker Caption Rankin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>humor, humorous captions, funny captions</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience, Targeted]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Exact match, Soft match, Human ratings, LLM-a...</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean, Std, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>changDrspiderDiagnosticEvaluation2023</td>\n",
       "      <td>Dr.Spider: A Diagnostic Evaluation Benchmark t...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper proposes Dr Spider a text-to-SQL rob...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>text-to-SQL, natural language understanding, c...</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Random, Convenience]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Exact match, Reward]</td>\n",
       "      <td>[No definition]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>liuRevisitingGoldStandard2023</td>\n",
       "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Propose a modified summarization salience prot...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Text summarization evaluation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[No comparison made]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>chaoJailbreakBenchOpenRobustness2024</td>\n",
       "      <td>JailbreakBench: An Open Robustness Benchmark f...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper proposes a benchmark for jailbreakin...</td>\n",
       "      <td>Attacks are adaptive: The strongest attack can...</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[LLM-as-a-Judge]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Comparison made]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>kasaiRealTimeQAWhats2023</td>\n",
       "      <td>RealTime QA: What's the Answer Right Now?</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces REALTIME QA, a dynamic q...</td>\n",
       "      <td>Key contributions include: (1) Proposing REALT...</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Real-time question answering; Reasoning with u...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>[Short free response]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>chenDrAcademyBenchmarkEvaluating2024</td>\n",
       "      <td>Dr.Academy: A Benchmark for Evaluating Questio...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces Dr.Academy, a benchmark ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Question generation for education</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience, Targeted, Criterion]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[LLM-as-a-Judge]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[No comparison made]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>bittonVisITbenchDynamicBenchmark2023</td>\n",
       "      <td>VisIT-Bench: A Dynamic Benchmark for Evaluatin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VisIT-Bench (Visual InsTruction Benchmark) is ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>Instruction following capabilities</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>[Soft match, Human ratings, LLM-as-a-Judge, Re...</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>tsurutaSARSCoV2InteractionDataset2024</td>\n",
       "      <td>A SARS-CoV-2 Interaction Dataset and VHH Seque...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVIDa-SARS-CoV-2 is a dataset featuring the an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>binding prediction, i.e. predict the binding s...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>xieOSWorldBenchmarkingMultimodal2024</td>\n",
       "      <td>OSWorld: Benchmarking Multimodal Agents for Op...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OSWorld introduces a scalable, executable comp...</td>\n",
       "      <td>They put significant emphasis on that they are...</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>Multimodal tool use, open-ended task execution...</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted, Criterion]</td>\n",
       "      <td>[Multiple choice, Short free response, Free re...</td>\n",
       "      <td>[Exact match, Reward]</td>\n",
       "      <td>[No definition]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[No comparison made]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>zhangCABComprehensiveAttention2023</td>\n",
       "      <td>CAB: Comprehensive Attention Benchmarking on L...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAB is a multimodal benchmark assessing long-r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>Long-range modeling</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Multiple choice, Short free response, Free re...</td>\n",
       "      <td>[Exact match, Soft match, LLM post-processing,...</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>panchalWhatSayWhen2024</td>\n",
       "      <td>What to Say and When to Say it: Live Fitness C...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Open-ended, asynchronous interactions, where a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Situated interaction</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Random]</td>\n",
       "      <td>[Free response, Interaction]</td>\n",
       "      <td>[Soft match, LLM-as-a-Judge]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>zhangCABComprehensiveAttention2023</td>\n",
       "      <td>CAB: Comprehensive Attention Benchmarking on L...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Current benchmarks testing different attention...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>Long sequence modelling capability of transfor...</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Exact match, Soft match, Distribution, Soft m...</td>\n",
       "      <td>[No definition]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>luWebLINXRealworldWebsite2024</td>\n",
       "      <td>WEBLINX: Real-World Website Navigation with Mu...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WebLinx introduces a large-scale benchmark of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Multimodal reasoning, web navigation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted, Criterion]</td>\n",
       "      <td>[Short free response, Interaction, Structured]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>liuRevisitingDeIdentificationElectronic2023</td>\n",
       "      <td>Revisiting De-Identification of Electronic Med...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Boundary include - measures LLM capabilities i...</td>\n",
       "      <td>Benchmark for de-identification of protected h...</td>\n",
       "      <td>Pre-LLMs so uses the dataset - which is still ...</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Generalization on de-identification tasks\\n</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Random, Convenience]</td>\n",
       "      <td>[Structured]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>flachsGrammaticalErrorCorrection2020</td>\n",
       "      <td>Grammatical Error Correction in Low Error Dens...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CWEB is a benchmark for grammatical error corr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Grammatical error correction</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Random]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Exact match, Distribution]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>wuSmartPlayBenchmarkLLMs2024</td>\n",
       "      <td>SMARTPLAY : A BENCHMARK FOR LLMS AS INTELLIGEN...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces SmartPlay, a benchmark f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>LLMs as intelligent agents</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>[Interaction]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>tuWaterBenchHolisticEvaluation2024</td>\n",
       "      <td>WaterBench: Towards Holistic Evaluation of Wat...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WaterBench is a benchmark for evaluating LLM w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>LLM watermarks</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience, Targeted, Criterion]</td>\n",
       "      <td>[Short free response, Free response]</td>\n",
       "      <td>[Exact match, Human ratings, LLM-as-a-Judge, L...</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>luoMMMRSMultimodalMultiGSD2024</td>\n",
       "      <td>MMM-RS: A Multi-modal, Multi-GSD, Multi-scene ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MMM-RS is a large, multi-modal multi-GSD, and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>remote sensing text-to-image generation</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted, Criterion]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Distribution]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>liMediQQuestionAskingLLMs2024</td>\n",
       "      <td>MediQ: Question Asking LLMs and a Benchmark fo...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medi-Q evaluates LLMs' ability to ask clarifyi...</td>\n",
       "      <td>Introduces not just the concept of requiring p...</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Interactive clinical reasoning, question-asking\\n</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted, Criterion]</td>\n",
       "      <td>[Interaction]</td>\n",
       "      <td>[Exact match, Human ratings, LLM-as-a-Judge]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>yoranAssistantBenchCanWeb2024</td>\n",
       "      <td>ASSISTANTBENCH: Can Web Agents Solve Realistic...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces AssistantBench - benchma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>web-based information-seeking</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience, Targeted, Criterion]</td>\n",
       "      <td>[Free response, Interaction, Structured]</td>\n",
       "      <td>[Exact match, LLM post-processing]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>braunAGBDECorpusAutomated2024</td>\n",
       "      <td>AGB-DE: A Corpus for the Automated Legal Asses...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this paper, the authors introduce AGB-DE, a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>consumer contract understanding</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[No definition]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>heOlympiadBenchChallengingBenchmark2024</td>\n",
       "      <td>OlympiadBench: A Challenging Benchmark for Pro...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this work, we present OlympiadBench, an Oly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>maths and physics reasoning</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>[Soft match]</td>\n",
       "      <td>[Widely-agreed]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>shiLanguageModelsAre2023</td>\n",
       "      <td>Language models are multilingual chain-of-thou...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We introduce the Multilingual Grade School Mat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>Multilingual chain-of-thought reasoning.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Criterion]</td>\n",
       "      <td>[Short free response]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>zengMRbenMetareasoningBenchmark2024</td>\n",
       "      <td>MR-Ben: A Meta-Reasoning Benchmark for Evaluat...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset of question,answer pairs in which answ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>Meta-reasoning</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>[Multiple choice, Free response]</td>\n",
       "      <td>[Exact match, Correlation, Correlation]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>jainR2ETurningAny2024</td>\n",
       "      <td>R2E: Turning any Github Repository into a Prog...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present Repository to Environment (R2E), a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>Code generation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>[Free response, Interaction]</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>[Contested]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>[Realistic]</td>\n",
       "      <td>[No]</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          bibkey  \\\n",
       "0           mundlerSWTBenchTestingValidating2024   \n",
       "3             niuRAGTruthHallucinationCorpus2024   \n",
       "54              mahbubUnveilingEssencePoetry2023   \n",
       "64       huangMLAgentBenchEvaluatingLanguage2024   \n",
       "65           yeGlobeSummChallengingBenchmark2024   \n",
       "69    athiwaratkunMultilingualEvaluationCode2023   \n",
       "73       naousReadMeBenchmarkingMultilingual2024   \n",
       "75        tanDevBenchMultimodalDevelopmental2024   \n",
       "106        gharaeeBIOSCAN5MMultimodalDataset2024   \n",
       "109              mitaStrikingGoldAdvertising2024   \n",
       "113          yangDataTalesBenchmarkRealworld2024   \n",
       "116                      zhangHumorAIMassive2024   \n",
       "149        changDrspiderDiagnosticEvaluation2023   \n",
       "158                liuRevisitingGoldStandard2023   \n",
       "170         chaoJailbreakBenchOpenRobustness2024   \n",
       "172                     kasaiRealTimeQAWhats2023   \n",
       "177         chenDrAcademyBenchmarkEvaluating2024   \n",
       "208         bittonVisITbenchDynamicBenchmark2023   \n",
       "225        tsurutaSARSCoV2InteractionDataset2024   \n",
       "230         xieOSWorldBenchmarkingMultimodal2024   \n",
       "246           zhangCABComprehensiveAttention2023   \n",
       "250                       panchalWhatSayWhen2024   \n",
       "251           zhangCABComprehensiveAttention2023   \n",
       "259                luWebLINXRealworldWebsite2024   \n",
       "267  liuRevisitingDeIdentificationElectronic2023   \n",
       "277         flachsGrammaticalErrorCorrection2020   \n",
       "291                 wuSmartPlayBenchmarkLLMs2024   \n",
       "296           tuWaterBenchHolisticEvaluation2024   \n",
       "297               luoMMMRSMultimodalMultiGSD2024   \n",
       "312                liMediQQuestionAskingLLMs2024   \n",
       "328                yoranAssistantBenchCanWeb2024   \n",
       "352                braunAGBDECorpusAutomated2024   \n",
       "368      heOlympiadBenchChallengingBenchmark2024   \n",
       "369                     shiLanguageModelsAre2023   \n",
       "424          zengMRbenMetareasoningBenchmark2024   \n",
       "443                        jainR2ETurningAny2024   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "0    SWT-Bench: Testing and Validating Real-World B...   Include   \n",
       "3    RAGTruth: A Hallucination Corpus for Developin...   Include   \n",
       "54   Unveiling the Essence of Poetry: Introducing a...   Include   \n",
       "64   MLAgentBench: Evaluating Language Agents on Ma...   Include   \n",
       "65   GlobeSumm: A Challenging Benchmark Towards Uni...   Include   \n",
       "69   Multi-lingual Evaluation of Code Generation Mo...   Include   \n",
       "73   README++: Benchmarking Multilingual Language M...   Include   \n",
       "75   DevBench: A multimodal developmental benchmark...   Include   \n",
       "106  BIOSCAN-5M: A Multimodal Dataset for Insect Bi...   Include   \n",
       "109  Striking Gold in Advertising: Standardization ...   Include   \n",
       "113  DataTales: A Benchmark for Real-World Intellig...   Include   \n",
       "116  Humor in AI: Massive Scale Crowd-Sourced Prefe...   Include   \n",
       "149  Dr.Spider: A Diagnostic Evaluation Benchmark t...   Include   \n",
       "158  Revisiting the Gold Standard: Grounding Summar...   Include   \n",
       "170  JailbreakBench: An Open Robustness Benchmark f...   Include   \n",
       "172          RealTime QA: What's the Answer Right Now?   Include   \n",
       "177  Dr.Academy: A Benchmark for Evaluating Questio...   Include   \n",
       "208  VisIT-Bench: A Dynamic Benchmark for Evaluatin...   Include   \n",
       "225  A SARS-CoV-2 Interaction Dataset and VHH Seque...   Include   \n",
       "230  OSWorld: Benchmarking Multimodal Agents for Op...   Include   \n",
       "246  CAB: Comprehensive Attention Benchmarking on L...   Include   \n",
       "250  What to Say and When to Say it: Live Fitness C...   Include   \n",
       "251  CAB: Comprehensive Attention Benchmarking on L...   Include   \n",
       "259  WEBLINX: Real-World Website Navigation with Mu...   Include   \n",
       "267  Revisiting De-Identification of Electronic Med...   Include   \n",
       "277  Grammatical Error Correction in Low Error Dens...   Include   \n",
       "291  SMARTPLAY : A BENCHMARK FOR LLMS AS INTELLIGEN...   Include   \n",
       "296  WaterBench: Towards Holistic Evaluation of Wat...   Include   \n",
       "297  MMM-RS: A Multi-modal, Multi-GSD, Multi-scene ...   Include   \n",
       "312  MediQ: Question Asking LLMs and a Benchmark fo...   Include   \n",
       "328  ASSISTANTBENCH: Can Web Agents Solve Realistic...   Include   \n",
       "352  AGB-DE: A Corpus for the Automated Legal Asses...   Include   \n",
       "368  OlympiadBench: A Challenging Benchmark for Pro...   Include   \n",
       "369  Language models are multilingual chain-of-thou...   Include   \n",
       "424  MR-Ben: A Meta-Reasoning Benchmark for Evaluat...   Include   \n",
       "443  R2E: Turning any Github Repository into a Prog...   Include   \n",
       "\n",
       "    exclusion_criteria                          exclusion_criteria_detail  \\\n",
       "0                  NaN                                                NaN   \n",
       "3                  NaN                                                NaN   \n",
       "54                 NaN                                                NaN   \n",
       "64                 NaN                                                NaN   \n",
       "65                 NaN                                                NaN   \n",
       "69                 NaN                                                NaN   \n",
       "73                 NaN                                                NaN   \n",
       "75                 NaN                                                NaN   \n",
       "106                NaN                                                NaN   \n",
       "109                NaN                                                NaN   \n",
       "113                NaN                                                NaN   \n",
       "116                NaN                                                NaN   \n",
       "149                NaN                                                NaN   \n",
       "158                NaN                                                NaN   \n",
       "170                NaN                                                NaN   \n",
       "172                NaN                                                NaN   \n",
       "177                NaN                                                NaN   \n",
       "208                NaN                                                NaN   \n",
       "225                NaN                                                NaN   \n",
       "230                NaN                                                NaN   \n",
       "246                NaN                                                NaN   \n",
       "250                NaN                                                NaN   \n",
       "251                NaN                                                NaN   \n",
       "259                NaN                                                NaN   \n",
       "267                NaN  Boundary include - measures LLM capabilities i...   \n",
       "277                NaN                                                NaN   \n",
       "291                NaN                                                NaN   \n",
       "296                NaN                                                NaN   \n",
       "297                NaN                                                NaN   \n",
       "312                NaN                                                NaN   \n",
       "328                NaN                                                NaN   \n",
       "352                NaN                                                NaN   \n",
       "368                NaN                                                NaN   \n",
       "369                NaN                                                NaN   \n",
       "424                NaN                                                NaN   \n",
       "443                NaN                                                NaN   \n",
       "\n",
       "                                         short_summary  \\\n",
       "0    A benchmark for generating code tests (unit te...   \n",
       "3    This paper targets word-level hallucinations i...   \n",
       "54   The paper proposes the task of poem summarizat...   \n",
       "64   MLAgentBench benchmarks the ability of LLM age...   \n",
       "65   Propose GLOBESUMM and introduce prompting meth...   \n",
       "69   Measures code generation capabilities across 1...   \n",
       "73   ReadMe++ is a multilingual and multi-domain da...   \n",
       "75   DevBench is a multimodal benchmark for assessi...   \n",
       "106  BIOSCAN-5M is a multimodal benchmark for insec...   \n",
       "109  CAMERA is a multimodal benchmark for automatic...   \n",
       "113  DataTales is a novel benchmark designed to ass...   \n",
       "116  The paper presents a New Yorker Caption Rankin...   \n",
       "149  The paper proposes Dr Spider a text-to-SQL rob...   \n",
       "158  Propose a modified summarization salience prot...   \n",
       "170  The paper proposes a benchmark for jailbreakin...   \n",
       "172  This paper introduces REALTIME QA, a dynamic q...   \n",
       "177  This paper introduces Dr.Academy, a benchmark ...   \n",
       "208  VisIT-Bench (Visual InsTruction Benchmark) is ...   \n",
       "225  AVIDa-SARS-CoV-2 is a dataset featuring the an...   \n",
       "230  OSWorld introduces a scalable, executable comp...   \n",
       "246  CAB is a multimodal benchmark assessing long-r...   \n",
       "250  Open-ended, asynchronous interactions, where a...   \n",
       "251  Current benchmarks testing different attention...   \n",
       "259  WebLinx introduces a large-scale benchmark of ...   \n",
       "267  Benchmark for de-identification of protected h...   \n",
       "277  CWEB is a benchmark for grammatical error corr...   \n",
       "291  This paper introduces SmartPlay, a benchmark f...   \n",
       "296  WaterBench is a benchmark for evaluating LLM w...   \n",
       "297  MMM-RS is a large, multi-modal multi-GSD, and ...   \n",
       "312  Medi-Q evaluates LLMs' ability to ask clarifyi...   \n",
       "328  This paper introduces AssistantBench - benchma...   \n",
       "352  In this paper, the authors introduce AGB-DE, a...   \n",
       "368  In this work, we present OlympiadBench, an Oly...   \n",
       "369  We introduce the Multilingual Grade School Mat...   \n",
       "424  Dataset of question,answer pairs in which answ...   \n",
       "443  We present Repository to Environment (R2E), a ...   \n",
       "\n",
       "                                          contribution  \\\n",
       "0                                                  NaN   \n",
       "3                                                  NaN   \n",
       "54                                                 NaN   \n",
       "64                                                 NaN   \n",
       "65                                                 NaN   \n",
       "69                                                 NaN   \n",
       "73                                                 NaN   \n",
       "75                                                 NaN   \n",
       "106  BIOSCAN-5M is an expansion of BIOSCAN-1M. It i...   \n",
       "109                                                NaN   \n",
       "113                                                NaN   \n",
       "116                                                NaN   \n",
       "149                                                NaN   \n",
       "158                                                NaN   \n",
       "170  Attacks are adaptive: The strongest attack can...   \n",
       "172  Key contributions include: (1) Proposing REALT...   \n",
       "177                                                NaN   \n",
       "208                                                NaN   \n",
       "225                                                NaN   \n",
       "230  They put significant emphasis on that they are...   \n",
       "246                                                NaN   \n",
       "250                                                NaN   \n",
       "251                                                NaN   \n",
       "259                                                NaN   \n",
       "267  Pre-LLMs so uses the dataset - which is still ...   \n",
       "277                                                NaN   \n",
       "291                                                NaN   \n",
       "296                                                NaN   \n",
       "297                                                NaN   \n",
       "312  Introduces not just the concept of requiring p...   \n",
       "328                                                NaN   \n",
       "352                                                NaN   \n",
       "368                                                NaN   \n",
       "369                                                NaN   \n",
       "424                                                NaN   \n",
       "443                                                NaN   \n",
       "\n",
       "                                      phenomenon_short  \\\n",
       "0    Specific Application (A single use case, where...   \n",
       "3    General Capability (A broadly useful ability, ...   \n",
       "54   Specific Application (A single use case, where...   \n",
       "64   General Capability (A broadly useful ability, ...   \n",
       "65   Specific Application (A single use case, where...   \n",
       "69   General Capability (A broadly useful ability, ...   \n",
       "73   General Capability (A broadly useful ability, ...   \n",
       "75   General Capability (A broadly useful ability, ...   \n",
       "106  Specific Application (A single use case, where...   \n",
       "109  Specific Application (A single use case, where...   \n",
       "113  General Capability (A broadly useful ability, ...   \n",
       "116  General Capability (A broadly useful ability, ...   \n",
       "149  Specific Application (A single use case, where...   \n",
       "158  Specific Application (A single use case, where...   \n",
       "170  General Capability (A broadly useful ability, ...   \n",
       "172  Specific Application (A single use case, where...   \n",
       "177  Specific Application (A single use case, where...   \n",
       "208  General Capability (A broadly useful ability, ...   \n",
       "225  Specific Application (A single use case, where...   \n",
       "230  General Capability (A broadly useful ability, ...   \n",
       "246  General Capability (A broadly useful ability, ...   \n",
       "250  Specific Application (A single use case, where...   \n",
       "251  General Capability (A broadly useful ability, ...   \n",
       "259  Specific Application (A single use case, where...   \n",
       "267  Specific Application (A single use case, where...   \n",
       "277  Specific Application (A single use case, where...   \n",
       "291  General Capability (A broadly useful ability, ...   \n",
       "296  Specific Application (A single use case, where...   \n",
       "297  General Capability (A broadly useful ability, ...   \n",
       "312  Specific Application (A single use case, where...   \n",
       "328  General Capability (A broadly useful ability, ...   \n",
       "352  Specific Application (A single use case, where...   \n",
       "368  General Capability (A broadly useful ability, ...   \n",
       "369  Specific Application (A single use case, where...   \n",
       "424  General Capability (A broadly useful ability, ...   \n",
       "443  General Capability (A broadly useful ability, ...   \n",
       "\n",
       "                                     target_phenomenon phenomenon_defined  \\\n",
       "0    Automatic code test generation (i.e. generatin...                Yes   \n",
       "3    hallucination detection, specifically for RAG ...                Yes   \n",
       "54                                  poem summarization                Yes   \n",
       "64                                  ML Experimentation                 No   \n",
       "65                                  Text summarization                Yes   \n",
       "69                                     Code generation                 No   \n",
       "73                              Readability assessment                Yes   \n",
       "75   language evaluation, language development, cog...                 No   \n",
       "106                              insect classification                Yes   \n",
       "109                       automatic ad text generation                Yes   \n",
       "113                                     Data narration                Yes   \n",
       "116           humor, humorous captions, funny captions                 No   \n",
       "149  text-to-SQL, natural language understanding, c...                 No   \n",
       "158                      Text summarization evaluation                Yes   \n",
       "170                                                NaN                 No   \n",
       "172  Real-time question answering; Reasoning with u...                Yes   \n",
       "177                  Question generation for education                Yes   \n",
       "208                 Instruction following capabilities                 No   \n",
       "225  binding prediction, i.e. predict the binding s...                Yes   \n",
       "230  Multimodal tool use, open-ended task execution...                 No   \n",
       "246                                Long-range modeling                Yes   \n",
       "250                               Situated interaction                Yes   \n",
       "251  Long sequence modelling capability of transfor...                 No   \n",
       "259               Multimodal reasoning, web navigation                Yes   \n",
       "267        Generalization on de-identification tasks\\n                Yes   \n",
       "277                       Grammatical error correction                Yes   \n",
       "291                         LLMs as intelligent agents                 No   \n",
       "296                                     LLM watermarks                Yes   \n",
       "297            remote sensing text-to-image generation                 No   \n",
       "312  Interactive clinical reasoning, question-asking\\n                Yes   \n",
       "328                      web-based information-seeking                Yes   \n",
       "352                    consumer contract understanding                 No   \n",
       "368                        maths and physics reasoning                Yes   \n",
       "369          Multilingual chain-of-thought reasoning.                 Yes   \n",
       "424                                    Meta-reasoning                  No   \n",
       "443                                    Code generation                Yes   \n",
       "\n",
       "     ...       dataset_sampling_method_clean  \\\n",
       "0    ...                         [Criterion]   \n",
       "3    ...                  [Random, Targeted]   \n",
       "54   ...                         [Criterion]   \n",
       "64   ...                          [Targeted]   \n",
       "65   ...                          [Targeted]   \n",
       "69   ...                       [Convenience]   \n",
       "73   ...               [Targeted, Criterion]   \n",
       "75   ...  [Convenience, Targeted, Criterion]   \n",
       "106  ...             [Convenience, Targeted]   \n",
       "109  ...               [Random, Convenience]   \n",
       "113  ...               [Targeted, Criterion]   \n",
       "116  ...             [Convenience, Targeted]   \n",
       "149  ...               [Random, Convenience]   \n",
       "158  ...                          [Targeted]   \n",
       "170  ...                          [Targeted]   \n",
       "172  ...                          [Targeted]   \n",
       "177  ...  [Convenience, Targeted, Criterion]   \n",
       "208  ...                          [Targeted]   \n",
       "225  ...                       [Convenience]   \n",
       "230  ...               [Targeted, Criterion]   \n",
       "246  ...                           [Unknown]   \n",
       "250  ...                            [Random]   \n",
       "251  ...                       [Convenience]   \n",
       "259  ...               [Targeted, Criterion]   \n",
       "267  ...               [Random, Convenience]   \n",
       "277  ...                            [Random]   \n",
       "291  ...                       [Convenience]   \n",
       "296  ...  [Convenience, Targeted, Criterion]   \n",
       "297  ...               [Targeted, Criterion]   \n",
       "312  ...               [Targeted, Criterion]   \n",
       "328  ...  [Convenience, Targeted, Criterion]   \n",
       "352  ...                           [Unknown]   \n",
       "368  ...                          [Targeted]   \n",
       "369  ...                         [Criterion]   \n",
       "424  ...                       [Convenience]   \n",
       "443  ...                       [Convenience]   \n",
       "\n",
       "                                 response_format_clean  \\\n",
       "0                                         [Structured]   \n",
       "3     [Short free response, Free response, Structured]   \n",
       "54                                     [Free response]   \n",
       "64                                     [Free response]   \n",
       "65                                     [Free response]   \n",
       "69                                        [Structured]   \n",
       "73                                   [Multiple choice]   \n",
       "75                               [Short free response]   \n",
       "106                              [Short free response]   \n",
       "109                                    [Free response]   \n",
       "113                                    [Free response]   \n",
       "116                                    [Free response]   \n",
       "149                                    [Free response]   \n",
       "158                                  [Multiple choice]   \n",
       "170                                    [Free response]   \n",
       "172                              [Short free response]   \n",
       "177                                    [Free response]   \n",
       "208                                  [Multiple choice]   \n",
       "225                                  [Multiple choice]   \n",
       "230  [Multiple choice, Short free response, Free re...   \n",
       "246  [Multiple choice, Short free response, Free re...   \n",
       "250                       [Free response, Interaction]   \n",
       "251                                    [Free response]   \n",
       "259     [Short free response, Interaction, Structured]   \n",
       "267                                       [Structured]   \n",
       "277                                    [Free response]   \n",
       "291                                      [Interaction]   \n",
       "296               [Short free response, Free response]   \n",
       "297                                    [Free response]   \n",
       "312                                      [Interaction]   \n",
       "328           [Free response, Interaction, Structured]   \n",
       "352                                  [Multiple choice]   \n",
       "368                                    [Free response]   \n",
       "369                              [Short free response]   \n",
       "424                   [Multiple choice, Free response]   \n",
       "443                       [Free response, Interaction]   \n",
       "\n",
       "                               metric_definition_clean  \\\n",
       "0                                             [Reward]   \n",
       "3                                        [Exact match]   \n",
       "54                           [Exact match, Soft match]   \n",
       "64                                            [Reward]   \n",
       "65                   [Soft match, LLM post-processing]   \n",
       "69                                            [Reward]   \n",
       "73                                      [Distribution]   \n",
       "75                                      [Distribution]   \n",
       "106                        [Exact match, Distribution]   \n",
       "109  [Soft match, Human ratings, LLM post-processin...   \n",
       "113           [Exact match, Soft match, Human ratings]   \n",
       "116  [Exact match, Soft match, Human ratings, LLM-a...   \n",
       "149                              [Exact match, Reward]   \n",
       "158                                      [Exact match]   \n",
       "170                                   [LLM-as-a-Judge]   \n",
       "172                                      [Exact match]   \n",
       "177                                   [LLM-as-a-Judge]   \n",
       "208  [Soft match, Human ratings, LLM-as-a-Judge, Re...   \n",
       "225                                      [Exact match]   \n",
       "230                              [Exact match, Reward]   \n",
       "246  [Exact match, Soft match, LLM post-processing,...   \n",
       "250                       [Soft match, LLM-as-a-Judge]   \n",
       "251  [Exact match, Soft match, Distribution, Soft m...   \n",
       "259                                      [Exact match]   \n",
       "267                                      [Exact match]   \n",
       "277                        [Exact match, Distribution]   \n",
       "291                                      [Exact match]   \n",
       "296  [Exact match, Human ratings, LLM-as-a-Judge, L...   \n",
       "297                                     [Distribution]   \n",
       "312       [Exact match, Human ratings, LLM-as-a-Judge]   \n",
       "328                 [Exact match, LLM post-processing]   \n",
       "352                                      [Exact match]   \n",
       "368                                       [Soft match]   \n",
       "369                                      [Exact match]   \n",
       "424            [Exact match, Correlation, Correlation]   \n",
       "443                                      [Exact match]   \n",
       "\n",
       "    phenomenon_contested_clean task_face_validity_clean  \\\n",
       "0              [Widely-agreed]                    [Yes]   \n",
       "3              [Widely-agreed]                    [Yes]   \n",
       "54             [Widely-agreed]                    [Yes]   \n",
       "64                 [Contested]                    [Yes]   \n",
       "65             [Widely-agreed]                    [Yes]   \n",
       "69             [Widely-agreed]                    [Yes]   \n",
       "73             [Widely-agreed]                    [Yes]   \n",
       "75                 [Contested]                    [Yes]   \n",
       "106            [Widely-agreed]                    [Yes]   \n",
       "109            [Widely-agreed]                    [Yes]   \n",
       "113                [Contested]                    [Yes]   \n",
       "116                [Contested]                    [Yes]   \n",
       "149            [No definition]                    [Yes]   \n",
       "158            [Widely-agreed]                    [Yes]   \n",
       "170                [Contested]                    [Yes]   \n",
       "172            [Widely-agreed]                    [Yes]   \n",
       "177            [Widely-agreed]                    [Yes]   \n",
       "208                [Contested]                    [Yes]   \n",
       "225            [Widely-agreed]                    [Yes]   \n",
       "230            [No definition]                    [Yes]   \n",
       "246            [Widely-agreed]                    [Yes]   \n",
       "250                [Contested]                    [Yes]   \n",
       "251            [No definition]                    [Yes]   \n",
       "259                [Contested]                    [Yes]   \n",
       "267            [Widely-agreed]                    [Yes]   \n",
       "277            [Widely-agreed]                    [Yes]   \n",
       "291            [Widely-agreed]                    [Yes]   \n",
       "296            [Widely-agreed]                    [Yes]   \n",
       "297            [Widely-agreed]                    [Yes]   \n",
       "312                [Contested]                    [Yes]   \n",
       "328                [Contested]                    [Yes]   \n",
       "352            [No definition]                    [Yes]   \n",
       "368            [Widely-agreed]                    [Yes]   \n",
       "369                [Contested]                    [Yes]   \n",
       "424                [Contested]                    [Yes]   \n",
       "443                [Contested]                    [Yes]   \n",
       "\n",
       "    metric_face_validity_clean results_realism_clean  \\\n",
       "0                        [Yes]           [Realistic]   \n",
       "3                        [Yes]  [No comparison made]   \n",
       "54                       [Yes]           [Realistic]   \n",
       "64                       [Yes]           [Realistic]   \n",
       "65                       [Yes]  [No comparison made]   \n",
       "69                       [Yes]           [Realistic]   \n",
       "73                       [Yes]           [Realistic]   \n",
       "75                       [Yes]           [Realistic]   \n",
       "106                      [Yes]           [Realistic]   \n",
       "109                      [Yes]           [Realistic]   \n",
       "113                      [Yes]           [Realistic]   \n",
       "116                      [Yes]           [Realistic]   \n",
       "149                      [Yes]           [Realistic]   \n",
       "158                      [Yes]  [No comparison made]   \n",
       "170                      [Yes]     [Comparison made]   \n",
       "172                      [Yes]           [Realistic]   \n",
       "177                      [Yes]  [No comparison made]   \n",
       "208                      [Yes]           [Realistic]   \n",
       "225                      [Yes]           [Realistic]   \n",
       "230                      [Yes]  [No comparison made]   \n",
       "246                      [Yes]           [Realistic]   \n",
       "250                      [Yes]           [Realistic]   \n",
       "251                      [Yes]           [Realistic]   \n",
       "259                      [Yes]           [Realistic]   \n",
       "267                      [Yes]           [Realistic]   \n",
       "277                      [Yes]           [Realistic]   \n",
       "291                      [Yes]           [Realistic]   \n",
       "296                      [Yes]           [Realistic]   \n",
       "297                      [Yes]           [Realistic]   \n",
       "312                      [Yes]           [Realistic]   \n",
       "328                      [Yes]           [Realistic]   \n",
       "352                      [Yes]           [Realistic]   \n",
       "368                      [Yes]           [Realistic]   \n",
       "369                      [Yes]           [Realistic]   \n",
       "424                       [No]           [Realistic]   \n",
       "443                      [Yes]           [Realistic]   \n",
       "\n",
       "    results_author_validity_clean task_ecology_clean metric_statistics_clean  \n",
       "0                           [Yes]         [Complete]                  [Mean]  \n",
       "3                           [Yes]         [Complete]                     NaN  \n",
       "54                           [No]         [Complete]                  [Mean]  \n",
       "64                           [No]         [Complete]                  [Mean]  \n",
       "65                          [Yes]         [Complete]                  [Mean]  \n",
       "69                          [Yes]         [Complete]                  [Mean]  \n",
       "73                          [Yes]         [Complete]           [Mean, Other]  \n",
       "75                          [Yes]         [Complete]                 [Other]  \n",
       "106                         [Yes]         [Complete]           [Mean, Other]  \n",
       "109                         [Yes]         [Complete]                 [Other]  \n",
       "113                         [Yes]         [Complete]                  [Mean]  \n",
       "116                         [Yes]         [Complete]      [Mean, Std, Other]  \n",
       "149                          [No]         [Complete]                  [Mean]  \n",
       "158                         [Yes]         [Complete]                     NaN  \n",
       "170                          [No]         [Complete]                  [Mean]  \n",
       "172                         [Yes]         [Complete]                  [Mean]  \n",
       "177                         [Yes]         [Complete]           [Mean, Other]  \n",
       "208                          [No]         [Complete]                  [Mean]  \n",
       "225                          [No]         [Complete]                  [Mean]  \n",
       "230                            []         [Complete]             [Mean, Std]  \n",
       "246                         [Yes]         [Complete]           [Mean, Other]  \n",
       "250                          [No]         [Complete]                  [Mean]  \n",
       "251                          [No]         [Complete]                     NaN  \n",
       "259                          [No]         [Complete]                     NaN  \n",
       "267                          [No]         [Complete]                     NaN  \n",
       "277                         [Yes]         [Complete]                  [Mean]  \n",
       "291                         [Yes]         [Complete]                 [Other]  \n",
       "296                         [Yes]         [Complete]                  [Mean]  \n",
       "297                         [Yes]         [Complete]                  [Mean]  \n",
       "312                         [Yes]         [Complete]                     NaN  \n",
       "328                         [Yes]         [Complete]             [Mean, Std]  \n",
       "352                          [No]         [Complete]               [Unknown]  \n",
       "368                          [No]         [Complete]                     NaN  \n",
       "369                         [Yes]         [Complete]                     NaN  \n",
       "424                          [No]         [Complete]                     NaN  \n",
       "443                          [No]         [Complete]                  [Mean]  \n",
       "\n",
       "[36 rows x 70 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_df[release_df['task_ecology'] == \"Complete real task (e.g. providing medical advice to real people interactively)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6bb7491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bibkey\n",
      "title\n",
      "inclusion\n",
      "exclusion_criteria\n",
      "exclusion_criteria_detail\n",
      "short_summary\n",
      "contribution\n",
      "phenomenon_short\n",
      "target_phenomenon\n",
      "phenomenon_defined\n",
      "phenomenon_definition\n",
      "definition_scope\n",
      "purpose_extra\n",
      "task_definition\n",
      "task_item_definition\n",
      "task_definition_detail\n",
      "task_source\n",
      "task_dataset_size\n",
      "task_dataset_metadata\n",
      "dataset_metadata_detail\n",
      "dataset_sampling_method\n",
      "response_format\n",
      "metric_definition\n",
      "metric_definition_detail\n",
      "task_source_detail\n",
      "authorship\n",
      "benchmark_availability\n",
      "procedural_extra\n",
      "notes_extra\n",
      "task_train_val\n",
      "task_dataset_size_extra\n",
      "response_format_detail\n",
      "metric_aggregation\n",
      "metric_subscores\n",
      "metric_subscores_detail\n",
      "metric_metascoring\n",
      "benchmark_location\n",
      "benchmark\n",
      "phenomenon_contested\n",
      "task_face_validity\n",
      "metric_face_validity\n",
      "result_interpretation\n",
      "results_comparison\n",
      "results_comparison_explanation\n",
      "results_realism\n",
      "results_human_baseline\n",
      "results_author_validity\n",
      "results_author_validity_detail\n",
      "metric_statistics\n",
      "metric_access\n",
      "task_ecology\n",
      "task_ecology_detail\n",
      "definition_integrity\n",
      "definition_integrity_detail\n",
      "task_dataset_size_detail\n",
      "metric_fewshot\n",
      "phenomenon_taxonomy_root\n",
      "phenomenon_taxonomy_leaf\n",
      "phenomenon_taxonomy_alternate\n",
      "task_source_clean\n",
      "dataset_sampling_method_clean\n",
      "response_format_clean\n",
      "metric_definition_clean\n",
      "phenomenon_contested_clean\n",
      "task_face_validity_clean\n",
      "metric_face_validity_clean\n",
      "results_realism_clean\n",
      "results_author_validity_clean\n",
      "task_ecology_clean\n",
      "metric_statistics_clean\n"
     ]
    }
   ],
   "source": [
    "for c in release_df.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a310e3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             [Reward]\n",
       "1                [Exact match, Reward]\n",
       "2                        [Exact match]\n",
       "3                        [Exact match]\n",
       "4                        [Exact match]\n",
       "                    ...               \n",
       "450    [Human ratings, LLM-as-a-Judge]\n",
       "451                      [Exact match]\n",
       "452                      [Exact match]\n",
       "453          [Exact match, Soft match]\n",
       "454                      [Exact match]\n",
       "Name: metric_definition_clean, Length: 455, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_df['metric_definition_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_column_description(col_name):\n",
    "    return f\"Standardised mapping of {col_name} values for statistical analysis.\"\n",
    "\n",
    "# These are hand written descriptions of the columns in the codebook\n",
    "col_descriptions = {\n",
    "    'bibkey':'The unique identifier to match the reviewed paper to a \\\\texttt{.bib} file.',\n",
    "    'title':'The title of the article.',\n",
    "    'benchmark':'The name of the benchmark.',\n",
    "    'inclusion':'Whether the paper was included in the review.',\n",
    "    'exclusion_criteria':'The criteria for excluding the paper, if any.',\n",
    "    'short_summary':'A short summary of the paper.',\n",
    "    'phenomenon_taxonomy_root':'The root category of the phenomenon taxonomy.',\n",
    "    'exclusion_criteria_detail':'Any additional details about the exclusion criteria.',\n",
    "    'phenomenon_taxonomy_leaf':'The leaf category of the phenomenon taxonomy.',\n",
    "    'phenomenon_taxonomy_alternate':'An alternate for the phenomenon taxonomy if highly relevant.',\n",
    "    'short_summary':'A short summary of the paper.',\n",
    "    'contribution':'Any additional notes about the article contribution',\n",
    "    'target_phenomenon':'The main phenomenon measured in the paper, as defined by the authors.',\n",
    "    'phenomenon_short': 'Whether the phenomenon is a general capability or a specific application.',\n",
    "    'phenomenon_defined':'Whether the phenomenon is defined in the paper.',\n",
    "    'phenomenon_definition':'The definition of the phenomenon.',\n",
    "    'phenomenon_contested':'Whether the definition of the phenomenon is broadly agreed upon, or if many definitions exist for the same term.',\n",
    "    \"phenomenon_contested_clean\": cleaned_column_description(\"phenomenon\\\\_contested\"),\n",
    "    'definition_scope':'Whether the benchmark covers everything within the phenomenon definition or only a subset.',\n",
    "    'definition_integrity':'Whether the definition is described as containing separate sub-phenomena.',\n",
    "    'definition_integrity_detail':'If the definition includes sub-elements, what are they?',\n",
    "    'purpose_extra':'Any additional notes about the conceptual details of the paper.',\n",
    "    'task_definition':'The definition of the benchmarking task.',\n",
    "    'task_face_validity':'An assessment of the face validity of the benchmark.',\n",
    "    \"task_face_validity_clean\": cleaned_column_description(\"task\\\\_face\\\\_validity\"),\n",
    "    'task_item_definition':'The definition and/or an example of a single item in the task.',\n",
    "    'task_definition_detail':'Any additional notes about the task definition.',\n",
    "    'task_source':'The source of the task items.',\n",
    "    \"task_source_clean\": cleaned_column_description(\"task\\\\_source\"),\n",
    "    'task_source_detail':'Any additional notes about the task source.',\n",
    "    'task_ecology':'How closely does the benchmarking task resemble the real application?',\n",
    "    \"task_ecology_clean\": cleaned_column_description(\"task\\\\_ecology\"),\n",
    "    'task_ecology_detail': 'Any additional detail about the ecological validity of the task',\n",
    "    'task_train_val': 'The data splits that are provided.',\n",
    "    'task_dataset_size': 'The numbers of items in the task test dataset.',\n",
    "    'task_dataset_size_extra': 'The number of items in the task train and validation datasets, if they exist.',\n",
    "    'task_dataset_size_detail': 'Any additional notes about the task dataset size.',\n",
    "    'task_dataset_metadata': 'Whether additional metadata is provided about the task items.',\n",
    "    'dataset_metadata_detail': 'A description of any metadata provided.',\n",
    "    'dataset_sampling_method': 'The method by which task items were selected from the space of possible task items.',\n",
    "    \"dataset_sampling_method_clean\": cleaned_column_description(\"dataset\\\\_sampling\\\\_method\"),\n",
    "    'response_format': 'The format of the expected response.',\n",
    "    \"response_format_clean\": cleaned_column_description(\"response\\\\_format\"),\n",
    "    'response_format_detail': 'Any additional notes about the response format.',\n",
    "    'metric_definition': 'The definition of the metric used to score the benchmark.',\n",
    "    \"metric_definition_clean\": cleaned_column_description(\"metric\\\\_definition\"),\n",
    "    \"metric_definition_detail\": 'Additional details on metric definition.',\n",
    "    \"metric_face_validity\": \"An assessment of the face validity of the metric.\",\n",
    "    \"metric_face_validity_clean\": cleaned_column_description(\"metric\\\\_face\\\\_validity\"),\n",
    "    'metric_aggregation': 'The method(s) used to aggregate metric scores.',\n",
    "    'metric_subscores': 'Whether subscores are provided for any specific subsets of the task.',\n",
    "    'metric_subscores_detail': cleaned_column_description(\"metric\\\\_subscores\"),\n",
    "    'metric_metascoring': 'Whether the scoring involves meta-scoring techniques (pass@k, consensus@k, etc.).',\n",
    "    \"metric_fewshot\": 'Whether evaluation uses few-shot prompting.',\n",
    "    'metric_statistics': 'The statistics used to aggregate and compare metric scores.',\n",
    "    'metric_statistics_clean': cleaned_column_description(\"metric\\\\_statistics\"),\n",
    "    'metric_access': 'Whether the metric requires model access or not.',\n",
    "\n",
    "    \"result_interpretation\": \"Connection between claims and phenomenon definition.\",\n",
    "    \"results_comparison\": \"Comparison of results to other benchmarks.\",\n",
    "    \"results_comparison_explanation\": \"Free-form explanation of the comparison.\",\n",
    "    \"results_human_baseline\": \"Whether model results are compared to human performance.\",\n",
    "    \"results_author_validity\": \"Whether benchmark authors directly address construct validity.\",\n",
    "    \"results_author_validity_clean\": cleaned_column_description(\"results\\\\_author\\\\_validity\"),\n",
    "    \"results_author_validity_detail\": \"Free-form explanation of results\\\\_author\\\\_validity.\",\n",
    "    \"results_realism\": \"Whether benchmark results compare to real settings.\",\n",
    "    \"results_realism_clean\": cleaned_column_description(\"results\\\\_realism\"),\n",
    "\n",
    "    \"authorship\": \"Authorship composition (industry or academia).\",\n",
    "    \"benchmark_availability\": \"Benchmark online availability.\",\n",
    "    \"benchmark_location\": \"Benchmark URL.\",\n",
    "    \"procedural_extra\": \"Optional additional notes on procedural details.\",\n",
    "    \"notes_extra\": \"Final optional notes.\",\n",
    "}\n",
    "\n",
    "# These need to be copied from the original codebook\n",
    "col_questions = {\n",
    "    \"bibkey\": \"ID of article (this is provided in the list of papers)\",\n",
    "    \"title\": \"Title of article\",\n",
    "    \"benchmark\": \"The name of the benchmark, if one exists (e.g. GSM8K)\",\n",
    "    'inclusion':'According to the criteria, should this paper be included or excluded?',\n",
    "    'exclusion_criteria':'If exclude, what criteria is violated?',\n",
    "    \"exclusion_criteria_detail\": \"If exclude, why? (optional, 1 sentence)\",\n",
    "    'short_summary': 'Short summary of paper contribution and method. Likely to be similar to the abstract. (2-3 sentences, no need for numbers)',\n",
    "    'contribution':'Other useful notes on contribution details (optional, only if something stood out)',\n",
    "    'target_phenomenon':'According to the authors, what capability or specific application is being measured? (a few words, e.g. knowledge, reasoning, natural language understanding)',\n",
    "    'phenomenon_short':'Which category does the target phenomenon fall into?',\n",
    "    'phenomenon_defined':'Is the targeted phenomenon explicitly defined?',\n",
    "    'phenomenon_definition':'How is the phenomenon of interest defined? (copy paste if possible, otherwise summarise what is being said)',\n",
    "    'phenomenon_contested':'Does the target phenomenon have a widely agreed-upon definition, or is this definition contested?',\n",
    "    'definition_scope':'Does the benchmark claim to measure everything covered by the definition, or focus on a more specific case or subset?',\n",
    "    'definition_integrity':'Do the authors describe the phenomena as a single cohesive whole, or does it consist of sub-elements?',\n",
    "    'definition_integrity_detail':'If the target phenomenon consists of sub-elements, are they measured separately?',\n",
    "    'purpose_extra':'Other useful notes on conceptual details (optional, only if something stood out)',\n",
    "    'task_definition':'How is the task defined? (1-2 sentences)',\n",
    "    'task_face_validity':'Is there prima facie reason to believe that this task could benchmark the target phenomenon?',\n",
    "    'task_item_definition':'What does a single item in the task dataset look like? (If the task is stored as a table, what is represented by one row in the table?) (1-2 sentences)',\n",
    "    'task_definition_detail':'Any additional details on task definition. (optional, only if something stands out)',\n",
    "    'task_source':'What is the source of the dataset task items? (Choose all that apply. If additional comments are needed, use the next question.)',\n",
    "    'task_source_detail':'Other useful notes on task source (optional, use this is something needs to be clarified)',\n",
    "    'task_ecology':'Is the task ecologically valid? (e.g. would a person really use a model in this way?) In the case of benchmarks which cover foundational abilities across many potential applications, you may need to select multiple responses and clarify below.',\n",
    "    'task_ecology_detail': 'Any additional detail about the ecological validity of the task',\n",
    "    'task_train_val': 'Which of the following dataset splits are provided? (if no splits are provided, assume the entire task is the test set)',\n",
    "    'task_dataset_size': 'Size of the task dataset (count, test set only, if none is reported write \"NA\")',\n",
    "    'task_dataset_size_extra': 'The size of the train and validation splits, if they are provided',\n",
    "    'task_dataset_size_detail': 'Any additional notes (e.g. the test set for some of the subcategories is very small)',\n",
    "    'task_dataset_metadata': 'Does the dataset provide any metadata? (e.g. topic area, difficulty level. Do not look in the dataset, this must be described in the paper to count.)',\n",
    "    'dataset_metadata_detail': 'If metadata is provided, what is it? (comma-separated list of fields, e.g. human difficulty, date, language)',\n",
    "    'dataset_sampling_method': 'How does the dataset relate to the population it represents? (Choose all that apply, see the image for examples)',\n",
    "    'response_format': 'What is the format of the expected response? (Choose all that apply. Try to stick with the provided categories, and use the next question to clarify.)',\n",
    "    'response_format_detail': 'Any additional details about the required response format to clarify how it fits in the categories above (optional)',\n",
    "    'metric_definition': 'What is the primary metric for scoring the benchmark? (Choose all that apply. Please try to stick to the provided categories and elaborate below.)',\n",
    "    'metric_access': 'Does this metric require model access, or can it be computed from responses alone?',\n",
    "    'metric_definition_detail':  'Any additional details on metric definition. (optional, only if something stood out)',\n",
    "    'metric_face_validity': 'Is there prima facie reason to believe that this metric could benchmark the target phenomenon?',\n",
    "    \"metric_aggregation\": \"How are the results aggregated, if at all? (e.g. mean, weighted mean, correlation)\",\n",
    "    \"metric_subscores\": \"Are scores provided for any specific subsets of the task? (e.g. by difficulty)\",\n",
    "    \"metric_subscores_detail\": \"If so, what subsets are provided?\",\n",
    "    \"metric_metascoring\": \"Does the scoring involve any meta-scoring techniques? If so, which ones?\",\n",
    "    \"metric_fewshot\": \"Does the scoring involve few-shot prompting or other similar in-context learning techniques?\",\n",
    "    \"metric_statistics\": \"What statistical methods are used to aggregate and compare the results? (e.g. simple mean/sum, mean and variance, clustered standard deviations)\",\n",
    "\n",
    "    \"result_interpretation\": \"Are the claims made in the results consistent with the scope of the definition being used?\",\n",
    "    \"results_comparison\": \"Are comparisons made to results on other benchmarks of similar phenomena? (this requires a comparison of the nature of the results, not just a literature review)\",\n",
    "    \"results_comparison_explanation\": \"If so, are theories offered to explain the similarities and differences?\",\n",
    "    \"results_human_baseline\": \"Does the paper present a human baseline on the task?\",\n",
    "    \"results_author_validity\": \"Do the authors present their own assessment of the validity of their benchmark? (i.e. do they directly address the question of construct validity for their benchmark?)\",\n",
    "    \"results_author_validity_detail\": \"If so, please describe their evidence.\",\n",
    "    \"results_realism\": \"Are comparisons made between the benchmark results and results from more realistic settings? (e.g. MedQA vs supporting doctors in practice)\",\n",
    "\n",
    "    \"authorship\": \"Authorship composition of the article\",\n",
    "    \"benchmark_availability\": \"Whether the benchmark artefact is publicly available\",\n",
    "    \"benchmark_location\": \"A link to the benchmark, if available (GitHub or similar)\",\n",
    "    \"procedural_extra\": \"Other useful notes on procedural details  (optional, only if something stood out)\",\n",
    "    \"notes_extra\": \"Any final notes about the paper not covered by above sections\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "75b89cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_page_mapping = {\n",
    "    \"General Background and Summary\": [\n",
    "        \"bibkey\",\n",
    "        \"title\",\n",
    "        \"benchmark\",\n",
    "        \"inclusion\",\n",
    "        \"exclusion_criteria\",\n",
    "        \"exclusion_criteria_detail\",\n",
    "        \"short_summary\",\n",
    "        \"contribution\",\n",
    "    ],\n",
    "    \"\\\\textcolor{phenomenon}{Phenomenon}\": [\n",
    "        \"target_phenomenon\",\n",
    "        \"phenomenon_short\",\n",
    "        \"phenomenon_defined\",\n",
    "        \"phenomenon_definition\",\n",
    "        \"phenomenon_taxonomy_root\",\n",
    "        \"phenomenon_taxonomy_leaf\",\n",
    "        \"phenomenon_taxonomy_alternate\",\n",
    "        \"phenomenon_contested\",\n",
    "        \"phenomenon_contested_clean\",\n",
    "        \"definition_scope\",\n",
    "        \"definition_integrity\",\n",
    "        \"definition_integrity_detail\",\n",
    "        \"purpose_extra\",\n",
    "    ],\n",
    "    \"\\\\textcolor{task}{Task} and Dataset\": [\n",
    "        \"task_definition\",\n",
    "        \"task_face_validity\",\n",
    "        \"task_face_validity_clean\",\n",
    "        \"task_item_definition\",\n",
    "        \"task_definition_detail\",\n",
    "        \"task_source\",\n",
    "        \"task_source_clean\",\n",
    "        \"task_source_detail\",\n",
    "        \"task_ecology\",\n",
    "        \"task_ecology_clean\",\n",
    "        \"task_ecology_detail\",\n",
    "        \"task_train_val\",\n",
    "        \"task_dataset_size\",\n",
    "        \"task_dataset_size_extra\",\n",
    "        \"task_dataset_size_detail\",\n",
    "        \"task_dataset_metadata\",\n",
    "        \"dataset_metadata_detail\",\n",
    "        \"dataset_sampling_method\",\n",
    "        \"dataset_sampling_method_clean\",\n",
    "        \"response_format\",\n",
    "        \"response_format_clean\",\n",
    "        \"response_format_detail\",\n",
    "    ],\n",
    "    \"\\\\textcolor{metric}{Metric}\": [\n",
    "        \"metric_definition\",\n",
    "        \"metric_definition_clean\",\n",
    "        \"metric_access\",\n",
    "        \"metric_definition_detail\",\n",
    "        \"metric_face_validity\",\n",
    "        \"metric_face_validity_clean\",\n",
    "        \"metric_aggregation\",\n",
    "        \"metric_subscores\",\n",
    "        \"metric_subscores_detail\",\n",
    "        \"metric_metascoring\",\n",
    "        \"metric_fewshot\",\n",
    "        \"metric_statistics\",\n",
    "        \"metric_statistics_clean\",\n",
    "    ],\n",
    "    \"Results and \\\\textcolor{claims}{Claims}\": [\n",
    "        \"result_interpretation\",\n",
    "        \"results_comparison\",\n",
    "        \"results_comparison_explanation\",\n",
    "        \"results_human_baseline\",\n",
    "        \"results_author_validity\",\n",
    "        \"results_author_validity_clean\",\n",
    "        \"results_author_validity_detail\",\n",
    "        \"results_realism\",\n",
    "        \"results_realism_clean\",\n",
    "    ],\n",
    "    \"Procedural\": [\n",
    "        \"authorship\",\n",
    "        \"benchmark_availability\",\n",
    "        \"benchmark_location\",\n",
    "        \"procedural_extra\",\n",
    "        \"notes_extra\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8c781e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm we have mapped every column\n",
    "assert (\n",
    "    set(release_df.columns) -\n",
    "    set([k for v in codebook_page_mapping.values() for k in v]) ==\n",
    "    set()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "182359bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_codebook(df):\n",
    "    print('\\\\begin{longtable}{p{.1\\\\textwidth}p{.9\\\\textwidth}}')\n",
    "    print('''\\\\toprule\n",
    "            \\\\endhead\n",
    "            \\\\bottomrule \\\\\\\\\n",
    "            \\\\endlastfoot\n",
    "            \\\\bottomrule\n",
    "            \\\\endfoot''')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_text = col.replace('_','\\_')\n",
    "        print(f\"\\multicolumn{{2}}{{l}}{{\\\\textbf{{{col_text}}}}} \\\\\\\\\")\n",
    "        print(f'& Description: \\\\textit{{{col_descriptions.get(col)}}} \\\\\\\\')\n",
    "        print(f'& Codebook Question: {col_questions.get(col)} \\\\\\\\')\n",
    "        try:\n",
    "            if len(df[col].value_counts()) < 10:\n",
    "                counts = df[col].value_counts()[df[col].value_counts() > 10]\n",
    "                if len(counts) > 1:\n",
    "                    other = df[col].value_counts()[df[col].value_counts() <= 10].sum()\n",
    "                    if other > 0:\n",
    "                        counts.loc['Other'] = other\n",
    "                if len(counts) > 1:\n",
    "                    print('\\\\\\\\')\n",
    "                    print('& \\\\textit{Summary of values:} \\\\\\\\')\n",
    "                    for i, row in enumerate(counts):\n",
    "                        print(f'& {counts.index[i]}: {row} \\\\\\\\')\n",
    "        except TypeError:\n",
    "            pass\n",
    "        print('\\\\\\\\')\n",
    "    print('\\\\end{longtable}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "30938fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\subsection{General Background and Summary}\n",
      "\\begin{longtable}{p{.1\\textwidth}p{.9\\textwidth}}\n",
      "\\toprule\n",
      "            \\endhead\n",
      "            \\bottomrule \\\\\n",
      "            \\endlastfoot\n",
      "            \\bottomrule\n",
      "            \\endfoot\n",
      "\\multicolumn{2}{l}{\\textbf{bibkey}} \\\\\n",
      "& Description: \\textit{The unique identifier to match the reviewed paper to a \\texttt{.bib} file.} \\\\\n",
      "& Codebook Question: ID of article (this is provided in the list of papers) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{title}} \\\\\n",
      "& Description: \\textit{The title of the article.} \\\\\n",
      "& Codebook Question: Title of article \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{benchmark}} \\\\\n",
      "& Description: \\textit{The name of the benchmark.} \\\\\n",
      "& Codebook Question: The name of the benchmark, if one exists (e.g. GSM8K) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{inclusion}} \\\\\n",
      "& Description: \\textit{Whether the paper was included in the review.} \\\\\n",
      "& Codebook Question: According to the criteria, should this paper be included or excluded? \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{exclusion\\_criteria}} \\\\\n",
      "& Description: \\textit{The criteria for excluding the paper, if any.} \\\\\n",
      "& Codebook Question: If exclude, what criteria is violated? \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{exclusion\\_criteria\\_detail}} \\\\\n",
      "& Description: \\textit{Any additional details about the exclusion criteria.} \\\\\n",
      "& Codebook Question: If exclude, why? (optional, 1 sentence) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{short\\_summary}} \\\\\n",
      "& Description: \\textit{A short summary of the paper.} \\\\\n",
      "& Codebook Question: Short summary of paper contribution and method. Likely to be similar to the abstract. (2-3 sentences, no need for numbers) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{contribution}} \\\\\n",
      "& Description: \\textit{Any additional notes about the article contribution} \\\\\n",
      "& Codebook Question: Other useful notes on contribution details (optional, only if something stood out) \\\\\n",
      "\\\\\n",
      "\\end{longtable}\n",
      "\n",
      "\\subsection{\\textcolor{phenomenon}{Phenomenon}}\n",
      "\\begin{longtable}{p{.1\\textwidth}p{.9\\textwidth}}\n",
      "\\toprule\n",
      "            \\endhead\n",
      "            \\bottomrule \\\\\n",
      "            \\endlastfoot\n",
      "            \\bottomrule\n",
      "            \\endfoot\n",
      "\\multicolumn{2}{l}{\\textbf{target\\_phenomenon}} \\\\\n",
      "& Description: \\textit{The main phenomenon measured in the paper, as defined by the authors.} \\\\\n",
      "& Codebook Question: According to the authors, what capability or specific application is being measured? (a few words, e.g. knowledge, reasoning, natural language understanding) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{phenomenon\\_short}} \\\\\n",
      "& Description: \\textit{Whether the phenomenon is a general capability or a specific application.} \\\\\n",
      "& Codebook Question: Which category does the target phenomenon fall into? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& General Capability (A broadly useful ability, which could be relevant to multiple applications): 321 \\\\\n",
      "& Specific Application (A single use case, where the benchmark is likely to be examples of that use case): 118 \\\\\n",
      "& Other: 16 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{phenomenon\\_defined}} \\\\\n",
      "& Description: \\textit{Whether the phenomenon is defined in the paper.} \\\\\n",
      "& Codebook Question: Is the targeted phenomenon explicitly defined? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Yes: 348 \\\\\n",
      "& No: 99 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{phenomenon\\_definition}} \\\\\n",
      "& Description: \\textit{The definition of the phenomenon.} \\\\\n",
      "& Codebook Question: How is the phenomenon of interest defined? (copy paste if possible, otherwise summarise what is being said) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{phenomenon\\_taxonomy\\_root}} \\\\\n",
      "& Description: \\textit{The root category of the phenomenon taxonomy.} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{phenomenon\\_taxonomy\\_leaf}} \\\\\n",
      "& Description: \\textit{The leaf category of the phenomenon taxonomy.} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{phenomenon\\_taxonomy\\_alternate}} \\\\\n",
      "& Description: \\textit{An alternate for the phenomenon taxonomy if highly relevant.} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{phenomenon\\_contested}} \\\\\n",
      "& Description: \\textit{Whether the definition of the phenomenon is broadly agreed upon, or if many definitions exist for the same term.} \\\\\n",
      "& Codebook Question: Does the target phenomenon have a widely agreed-upon definition, or is this definition contested? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Contested: 225 \\\\\n",
      "& Widely-agreed: 203 \\\\\n",
      "& Not defined: 27 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{phenomenon\\_contested\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& ['Contested']: 225 \\\\\n",
      "& ['Widely-agreed']: 203 \\\\\n",
      "& ['No definition']: 27 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{definition\\_scope}} \\\\\n",
      "& Description: \\textit{Whether the benchmark covers everything within the phenomenon definition or only a subset.} \\\\\n",
      "& Codebook Question: Does the benchmark claim to measure everything covered by the definition, or focus on a more specific case or subset? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Subset: 253 \\\\\n",
      "& Comprehensive: 196 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{definition\\_integrity}} \\\\\n",
      "& Description: \\textit{Whether the definition is described as containing separate sub-phenomena.} \\\\\n",
      "& Codebook Question: Do the authors describe the phenomena as a single cohesive whole, or does it consist of sub-elements? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Composite phenomenon: 278 \\\\\n",
      "& Single cohesive phenomenon: 166 \\\\\n",
      "& Other: 10 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{definition\\_integrity\\_detail}} \\\\\n",
      "& Description: \\textit{If the definition includes sub-elements, what are they?} \\\\\n",
      "& Codebook Question: If the target phenomenon consists of sub-elements, are they measured separately? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Yes: 261 \\\\\n",
      "& Not applicable: 144 \\\\\n",
      "& No: 46 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{purpose\\_extra}} \\\\\n",
      "& Description: \\textit{Any additional notes about the conceptual details of the paper.} \\\\\n",
      "& Codebook Question: Other useful notes on conceptual details (optional, only if something stood out) \\\\\n",
      "\\\\\n",
      "\\end{longtable}\n",
      "\n",
      "\\subsection{\\textcolor{task}{Task} and Dataset}\n",
      "\\begin{longtable}{p{.1\\textwidth}p{.9\\textwidth}}\n",
      "\\toprule\n",
      "            \\endhead\n",
      "            \\bottomrule \\\\\n",
      "            \\endlastfoot\n",
      "            \\bottomrule\n",
      "            \\endfoot\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_definition}} \\\\\n",
      "& Description: \\textit{The definition of the benchmarking task.} \\\\\n",
      "& Codebook Question: How is the task defined? (1-2 sentences) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_face\\_validity}} \\\\\n",
      "& Description: \\textit{An assessment of the face validity of the benchmark.} \\\\\n",
      "& Codebook Question: Is there prima facie reason to believe that this task could benchmark the target phenomenon? \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_face\\_validity\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_item\\_definition}} \\\\\n",
      "& Description: \\textit{The definition and/or an example of a single item in the task.} \\\\\n",
      "& Codebook Question: What does a single item in the task dataset look like? (If the task is stored as a table, what is represented by one row in the table?) (1-2 sentences) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_definition\\_detail}} \\\\\n",
      "& Description: \\textit{Any additional notes about the task definition.} \\\\\n",
      "& Codebook Question: Any additional details on task definition. (optional, only if something stands out) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_source}} \\\\\n",
      "& Description: \\textit{The source of the task items.} \\\\\n",
      "& Codebook Question: What is the source of the dataset task items? (Choose all that apply. If additional comments are needed, use the next question.) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_source\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_source\\_detail}} \\\\\n",
      "& Description: \\textit{Any additional notes about the task source.} \\\\\n",
      "& Codebook Question: Other useful notes on task source (optional, use this is something needs to be clarified) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_ecology}} \\\\\n",
      "& Description: \\textit{How closely does the benchmarking task resemble the real application?} \\\\\n",
      "& Codebook Question: Is the task ecologically valid? (e.g. would a person really use a model in this way?) In the case of benchmarks which cover foundational abilities across many potential applications, you may need to select multiple responses and clarify below. \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_ecology\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_ecology\\_detail}} \\\\\n",
      "& Description: \\textit{Any additional detail about the ecological validity of the task} \\\\\n",
      "& Codebook Question: Any additional detail about the ecological validity of the task \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_train\\_val}} \\\\\n",
      "& Description: \\textit{The data splits that are provided.} \\\\\n",
      "& Codebook Question: Which of the following dataset splits are provided? (if no splits are provided, assume the entire task is the test set) \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Test: 275 \\\\\n",
      "& Test, Train, Validation: 96 \\\\\n",
      "& Test, Train: 51 \\\\\n",
      "& Test, Validation: 17 \\\\\n",
      "& Other: 2 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_dataset\\_size}} \\\\\n",
      "& Description: \\textit{The numbers of items in the task test dataset.} \\\\\n",
      "& Codebook Question: Size of the task dataset (count, test set only, if none is reported write \"NA\") \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_dataset\\_size\\_extra}} \\\\\n",
      "& Description: \\textit{The number of items in the task train and validation datasets, if they exist.} \\\\\n",
      "& Codebook Question: The size of the train and validation splits, if they are provided \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_dataset\\_size\\_detail}} \\\\\n",
      "& Description: \\textit{Any additional notes about the task dataset size.} \\\\\n",
      "& Codebook Question: Any additional notes (e.g. the test set for some of the subcategories is very small) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{task\\_dataset\\_metadata}} \\\\\n",
      "& Description: \\textit{Whether additional metadata is provided about the task items.} \\\\\n",
      "& Codebook Question: Does the dataset provide any metadata? (e.g. topic area, difficulty level. Do not look in the dataset, this must be described in the paper to count.) \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Yes: 322 \\\\\n",
      "& No: 126 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{dataset\\_metadata\\_detail}} \\\\\n",
      "& Description: \\textit{A description of any metadata provided.} \\\\\n",
      "& Codebook Question: If metadata is provided, what is it? (comma-separated list of fields, e.g. human difficulty, date, language) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{dataset\\_sampling\\_method}} \\\\\n",
      "& Description: \\textit{The method by which task items were selected from the space of possible task items.} \\\\\n",
      "& Codebook Question: How does the dataset relate to the population it represents? (Choose all that apply, see the image for examples) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{dataset\\_sampling\\_method\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{response\\_format}} \\\\\n",
      "& Description: \\textit{The format of the expected response.} \\\\\n",
      "& Codebook Question: What is the format of the expected response? (Choose all that apply. Try to stick with the provided categories, and use the next question to clarify.) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{response\\_format\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{response\\_format\\_detail}} \\\\\n",
      "& Description: \\textit{Any additional notes about the response format.} \\\\\n",
      "& Codebook Question: Any additional details about the required response format to clarify how it fits in the categories above (optional) \\\\\n",
      "\\\\\n",
      "\\end{longtable}\n",
      "\n",
      "\\subsection{\\textcolor{metric}{Metric}}\n",
      "\\begin{longtable}{p{.1\\textwidth}p{.9\\textwidth}}\n",
      "\\toprule\n",
      "            \\endhead\n",
      "            \\bottomrule \\\\\n",
      "            \\endlastfoot\n",
      "            \\bottomrule\n",
      "            \\endfoot\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_definition}} \\\\\n",
      "& Description: \\textit{The definition of the metric used to score the benchmark.} \\\\\n",
      "& Codebook Question: What is the primary metric for scoring the benchmark? (Choose all that apply. Please try to stick to the provided categories and elaborate below.) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_definition\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_access}} \\\\\n",
      "& Description: \\textit{Whether the metric requires model access or not.} \\\\\n",
      "& Codebook Question: Does this metric require model access, or can it be computed from responses alone? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Outputs alone: 422 \\\\\n",
      "& Model access required (e.g. logits): 32 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_definition\\_detail}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Any additional details on task definition. (optional, only if something stood out) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_face\\_validity}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Is there prima facie reason to believe that this metric could benchmark the target phenomenon? \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_face\\_validity\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_aggregation}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: How are the results aggregated, if at all? (e.g. mean, weighted mean, correlation) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_subscores}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Are scores provided for any specific subsets of the task? (e.g. by difficulty) \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Yes: 363 \\\\\n",
      "& No: 91 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_subscores\\_detail}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: If so, what subsets are provided? \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_metascoring}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Does the scoring involve any meta-scoring techniques? If so, which ones? \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_fewshot}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Does the scoring involve few-shot prompting or other similar in-context learning techniques? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& No: 214 \\\\\n",
      "& Yes: 79 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_statistics}} \\\\\n",
      "& Description: \\textit{The statistics used to aggregate the metric scores.} \\\\\n",
      "& Codebook Question: What statistical methods are used to aggregate and compare the results? (e.g. simple mean/sum, mean and variance, clustered standard deviations) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{metric\\_statistics\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\end{longtable}\n",
      "\n",
      "\\subsection{Results and \\textcolor{claims}{Claims}}\n",
      "\\begin{longtable}{p{.1\\textwidth}p{.9\\textwidth}}\n",
      "\\toprule\n",
      "            \\endhead\n",
      "            \\bottomrule \\\\\n",
      "            \\endlastfoot\n",
      "            \\bottomrule\n",
      "            \\endfoot\n",
      "\\multicolumn{2}{l}{\\textbf{result\\_interpretation}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Are the claims made in the results consistent with the scope of the definition being used? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Yes: 435 \\\\\n",
      "& No: 18 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{results\\_comparison}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Are comparisons made to results on other benchmarks of similar phenomena? (this requires a comparison of the nature of the results, not just a literature review) \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& No: 294 \\\\\n",
      "& Yes: 160 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{results\\_comparison\\_explanation}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: If so, are theories offered to explain the similarities and differences? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& No comparisons made: 261 \\\\\n",
      "& Yes: 144 \\\\\n",
      "& No: 27 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{results\\_human\\_baseline}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Does the paper present a human baseline on the task? \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& No: 305 \\\\\n",
      "& Yes: 146 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{results\\_author\\_validity}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Do the authors present their own assessment of the validity of their benchmark? (i.e. do they directly address the question of construct validity for their benchmark?) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{results\\_author\\_validity\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{results\\_author\\_validity\\_detail}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: If so, please describe their evidence. \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{results\\_realism}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Are comparisons made between the benchmark results and results from more realistic settings? (e.g. MedQA vs supporting doctors in practice) \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& No: 308 \\\\\n",
      "& The benchmark is itself realistic: 119 \\\\\n",
      "& Yes: 23 \\\\\n",
      "& Other: 4 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{results\\_realism\\_clean}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: None \\\\\n",
      "\\\\\n",
      "\\end{longtable}\n",
      "\n",
      "\\subsection{Procedural}\n",
      "\\begin{longtable}{p{.1\\textwidth}p{.9\\textwidth}}\n",
      "\\toprule\n",
      "            \\endhead\n",
      "            \\bottomrule \\\\\n",
      "            \\endlastfoot\n",
      "            \\bottomrule\n",
      "            \\endfoot\n",
      "\\multicolumn{2}{l}{\\textbf{authorship}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Authorship composition of the article \\\\\n",
      "\\\\\n",
      "& \\textit{Summary of values:} \\\\\n",
      "& Academia: 230 \\\\\n",
      "& Mix (multiple authors from industry and academia): 186 \\\\\n",
      "& Industry: 33 \\\\\n",
      "& Other: 4 \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{benchmark\\_availability}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Whether the benchmark artefact is publicly available \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{benchmark\\_location}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: A link to the benchmark, if available (GitHub or similar) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{procedural\\_extra}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Other useful notes on procedural details  (optional, only if something stood out) \\\\\n",
      "\\\\\n",
      "\\multicolumn{2}{l}{\\textbf{notes\\_extra}} \\\\\n",
      "& Description: \\textit{None} \\\\\n",
      "& Codebook Question: Any final notes about the paper not covered by above sections \\\\\n",
      "\\\\\n",
      "\\end{longtable}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in codebook_page_mapping.keys():\n",
    "    print(\"\\subsection{\" + k + \"}\")\n",
    "    make_codebook(release_df[codebook_page_mapping[k]])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7401164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
