{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ed0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bddd6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df = pd.read_csv('../data/coding_responses.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fd325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df = codebook_df[codebook_df['Timestamp'].apply(lambda x: datetime.strptime(x,\"%m/%d/%Y %H:%M:%S\") > datetime.strptime('2025-04-05', \"%Y-%m-%d\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91900f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5df24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df.columns = pd.Series(codebook_df.columns).apply(lambda x: x.split(\":\")[0])\n",
    "codebook_df.drop(['Column 1'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed8991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>...</th>\n",
       "      <th>results_author_validity</th>\n",
       "      <th>results_author_validity_detail</th>\n",
       "      <th>metric_statistics</th>\n",
       "      <th>metric_access</th>\n",
       "      <th>task_ecology</th>\n",
       "      <th>task_ecology_detail</th>\n",
       "      <th>definition_integrity</th>\n",
       "      <th>definition_integrity_detail</th>\n",
       "      <th>task_dataset_size_detail</th>\n",
       "      <th>metric_fewshot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4/8/2025 15:27:11</td>\n",
       "      <td>Harry Mayne</td>\n",
       "      <td>mundlerSWTBenchTestingValidating2024</td>\n",
       "      <td>SWT-Bench: Testing and Validating Real-World B...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A benchmark for generating code tests (unit te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Limitations in how the phenomenon was operatio...</td>\n",
       "      <td>simple mean</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Complete real task (e.g. providing medical adv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single cohesive phenomenon</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4/8/2025 15:57:43</td>\n",
       "      <td>Jonathan Rystrøm</td>\n",
       "      <td>yangLLMCBenchBenchmarkingLarge2024</td>\n",
       "      <td>LLMCBench: Benchmarking Large Language Model C...</td>\n",
       "      <td>Exclude</td>\n",
       "      <td>Topic Exclusion (Is the paper about measuring ...</td>\n",
       "      <td>It's about compression algorithms rather than ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4/8/2025 16:50:41</td>\n",
       "      <td>Jonathan Rystrøm</td>\n",
       "      <td>davidsonEvaluatingLanguageModel2024</td>\n",
       "      <td>EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces a dynamic framework for e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mean with variance</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Constructed task (e.g. predicting medical diag...</td>\n",
       "      <td>The tasks simulates agent negotiations (so no ...</td>\n",
       "      <td>Composite phenomenon</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4/8/2025 17:08:34</td>\n",
       "      <td>Lennart Luettgau</td>\n",
       "      <td>helweMAFALDABenchmarkComprehensive2024</td>\n",
       "      <td>MAFALDA: A Benchmark and Comprehensive Study o...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces MAFALD, a benchmark that ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>simple mean/sum</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Representative task (e.g. answering medical li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Composite phenomenon</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4/8/2025 18:20:47</td>\n",
       "      <td>Kaili Liu</td>\n",
       "      <td>niuRAGTruthHallucinationCorpus2024</td>\n",
       "      <td>RAGTruth: A Hallucination Corpus for Developin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper targets word-level hallucinations i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Benchmark statistics and quality checking are ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Complete real task (e.g. providing medical adv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Composite phenomenon</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp        main_coder  \\\n",
       "13  4/8/2025 15:27:11       Harry Mayne   \n",
       "14  4/8/2025 15:57:43  Jonathan Rystrøm   \n",
       "15  4/8/2025 16:50:41  Jonathan Rystrøm   \n",
       "16  4/8/2025 17:08:34  Lennart Luettgau   \n",
       "17  4/8/2025 18:20:47         Kaili Liu   \n",
       "\n",
       "                                    bibkey  \\\n",
       "13    mundlerSWTBenchTestingValidating2024   \n",
       "14      yangLLMCBenchBenchmarkingLarge2024   \n",
       "15     davidsonEvaluatingLanguageModel2024   \n",
       "16  helweMAFALDABenchmarkComprehensive2024   \n",
       "17      niuRAGTruthHallucinationCorpus2024   \n",
       "\n",
       "                                                title inclusion  \\\n",
       "13  SWT-Bench: Testing and Validating Real-World B...   Include   \n",
       "14  LLMCBench: Benchmarking Large Language Model C...   Exclude   \n",
       "15  EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...   Include   \n",
       "16  MAFALDA: A Benchmark and Comprehensive Study o...   Include   \n",
       "17  RAGTruth: A Hallucination Corpus for Developin...   Include   \n",
       "\n",
       "                                   exclusion_criteria  \\\n",
       "13                                                NaN   \n",
       "14  Topic Exclusion (Is the paper about measuring ...   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "                            exclusion_criteria_detail  \\\n",
       "13                                                NaN   \n",
       "14  It's about compression algorithms rather than ...   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "                                        short_summary contribution  \\\n",
       "13  A benchmark for generating code tests (unit te...          NaN   \n",
       "14                                                NaN          NaN   \n",
       "15  The paper introduces a dynamic framework for e...          NaN   \n",
       "16  The paper introduces MAFALD, a benchmark that ...          NaN   \n",
       "17  This paper targets word-level hallucinations i...          NaN   \n",
       "\n",
       "                                     phenomenon_short  ...  \\\n",
       "13  Specific Application (A single use case, where...  ...   \n",
       "14                                                NaN  ...   \n",
       "15  General Capability (A broadly useful ability, ...  ...   \n",
       "16  General Capability (A broadly useful ability, ...  ...   \n",
       "17  General Capability (A broadly useful ability, ...  ...   \n",
       "\n",
       "   results_author_validity                     results_author_validity_detail  \\\n",
       "13                     Yes  Limitations in how the phenomenon was operatio...   \n",
       "14                     NaN                                                NaN   \n",
       "15                      No                                                NaN   \n",
       "16                      No                                                NaN   \n",
       "17                     Yes  Benchmark statistics and quality checking are ...   \n",
       "\n",
       "     metric_statistics  metric_access  \\\n",
       "13         simple mean  Outputs alone   \n",
       "14                 NaN            NaN   \n",
       "15  mean with variance  Outputs alone   \n",
       "16     simple mean/sum  Outputs alone   \n",
       "17                 NaN  Outputs alone   \n",
       "\n",
       "                                         task_ecology  \\\n",
       "13  Complete real task (e.g. providing medical adv...   \n",
       "14                                                NaN   \n",
       "15  Constructed task (e.g. predicting medical diag...   \n",
       "16  Representative task (e.g. answering medical li...   \n",
       "17  Complete real task (e.g. providing medical adv...   \n",
       "\n",
       "                                  task_ecology_detail  \\\n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15  The tasks simulates agent negotiations (so no ...   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "          definition_integrity definition_integrity_detail  \\\n",
       "13  Single cohesive phenomenon              Not applicable   \n",
       "14                         NaN                         NaN   \n",
       "15        Composite phenomenon                         Yes   \n",
       "16        Composite phenomenon                         Yes   \n",
       "17        Composite phenomenon                         Yes   \n",
       "\n",
       "   task_dataset_size_detail metric_fewshot  \n",
       "13                      NaN            NaN  \n",
       "14                      NaN            NaN  \n",
       "15                      NaN            NaN  \n",
       "16                      NaN            NaN  \n",
       "17                      NaN            NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebook_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b68b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bibkeys_df = pd.read_csv('../data/final_list_bibtex.csv')[['bibkey','title']]\n",
    "\n",
    "codebook_df['new_bibkey'] = ''\n",
    "for key in bibkeys_df['bibkey']:\n",
    "    if key.lower() in codebook_df['bibkey'].apply(lambda x: x.lower()).values:\n",
    "        codebook_df.loc[codebook_df['bibkey'].apply(lambda x: x.lower()) == key.lower(), 'new_bibkey'] = key\n",
    "    elif (key +\"a\").lower() in codebook_df['bibkey'].apply(lambda x: x.lower()).values:\n",
    "        codebook_df.loc[codebook_df['bibkey'].apply(lambda x: x.lower()) == (key+\"a\").lower(), 'new_bibkey'] = key\n",
    "    elif (key[:-1]).lower() in codebook_df['bibkey'].apply(lambda x: x.lower()).values:\n",
    "        codebook_df.loc[codebook_df['bibkey'].apply(lambda x: x.lower()) == (key[:-1]).lower(), 'new_bibkey'] = key\n",
    "    else:\n",
    "        print(key)\n",
    "        print(bibkeys_df[bibkeys_df['bibkey'] == key]['title'].values[0])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1e56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df = codebook_df[codebook_df['inclusion'] == 'Include']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3daa08",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25eb8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_embedding(text):\n",
    "\n",
    "    client = openai.OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c592297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_helper(series,labels):\n",
    "    series.index = labels\n",
    "    plt.pie(series, labels=series.index, startangle=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293aba08",
   "metadata": {},
   "source": [
    "## Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f167225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = codebook_df['exclusion_criteria'].value_counts()\n",
    "temp['Include'] = codebook_df['inclusion'].value_counts()['Include']\n",
    "pie_helper(temp, ['Novelty','Topic','Empirical','Modality','Include'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f49760",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenomena_df = pd.read_csv('../data/phenomena_taxonomy.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6b7f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df = included_df.merge(phenomena_df,left_on='bibkey',right_on='bibkey',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "284e3f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = codebook_df['phenomenon_defined'].value_counts()\n",
    "pie_helper(temp, temp.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c208bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenomenon_map = {'General Capability (A broadly useful ability, which could be relevant to multiple applications)': 'General Capability',\n",
    "                     'Specific Application (A single use case, where the benchmark is likely to be examples of that use case)': 'Specific Application',\n",
    "                     'General form of bias':'General Capability',\n",
    "                     'Specific form of bias':'General Capability',\n",
    "                     'General Capability (A broadly useful ability, which could be relevant to multiple applications), MERA as a whole tries to measure general \"capabilities\", but the individual tasks evaluate more specific applications (e.g., question answering).': 'General Capability',\n",
    "                     'General Capability (A broadly useful ability, which could be relevant to multiple applications), Specific Application (A single use case, where the benchmark is likely to be examples of that use case)': 'Both'}\n",
    "\n",
    "\n",
    "contested_map = {'Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)':'Contested',\n",
    "                 'Contested':'Contested',\n",
    "                 'Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)':'Widely-agreed',\n",
    "                 'Widely-agreed': 'Widely-agreed',\n",
    "                 'No definition provided': 'Not defined',\n",
    "                 'Very specialized phenomenon without any standard definition':'Widely-agreed',\n",
    "                 'A new problem is presented so agreement is unclear; but the definition is generally interpretable in a widely-agreeable way.':'Widely-agreed',\n",
    "                 'Very special phenomenon that is only introduced by the authors. It is neither widely-agreed upon nor contested.':'Widely-agreed',\n",
    "                 'Very specific phenomenon discovered in this paper (i.e., no prior definition exists)':'Widely-agreed',\n",
    "                 'Very specialized phenomenon with no standard definition ': 'Widely-agreed',}\n",
    "\n",
    "scope_map = {'The benchmark measures the effects of multi-turn and code interpretor on the target phenomena.':'Subset',\n",
    "             'The title claims the benchmark is \"comprehensive\", but in the Limitations section they say that the \"evaluation might not comprehensively assess LLM’s abilities.\"':'Comprehensive'}\n",
    "\n",
    "col_maps = {'phenomenon_contested': contested_map,\n",
    "            'phenomenon_short':phenomenon_map,\n",
    "            'definition_scope':scope_map,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd77e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_map = {'Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)': 'Author-crafted',\n",
    "              'Modified from another benchmark (e.g. translation into another language)':'Another benchmark',\n",
    "              'LLM-generated task examples (e.g. Filtered from responses to a prompt)':'LLM-generated',\n",
    "              'Real task examples (e.g. GitHub issues)':'Real task',\n",
    "              'Procedurally-generated task examples (e.g. Creating instances from a template)':'Procedurally-generated',\n",
    "              'Crowd-sourced task examples (e.g. Prolific-created tasks)':'Crowd-sourced',\n",
    "              'Expert-crafted task examples (e.g. hand-written examples)':'Expert-crafted',\n",
    "              'Human exam questions (e.g. GRE questions)':'Human exams',\n",
    "              'LLM- and VLM- generated task examples':'LLM-generated',\n",
    "              'Text snippets from books':'Author-crafted',\n",
    "              'Human-crafted task examples from an existing human game (Choose-Your-Own-Adventure)':'Procedurally-generated',\n",
    "              'Scraped from social media (Reddit)':'Procedurally-generated',\n",
    "              'The dataset is derived from the open BYTESIZED32 corpus.':'Another benchmark',\n",
    "              '':'Unknown',\n",
    "              'Produced media (TV sitcom scenes)':'Author-crafted',\n",
    "              'Unclear':'Unknown',\n",
    "              'Expert-annotated task examples (PhD students)':'Expert-crafted',\n",
    "              'The examples are created by a linguist ':'Expert-crafted',\n",
    "              'Domain expert annotators':'Expert-crafted',\n",
    "              'Wikidata':'Crowd-sourced',\n",
    "              'Human expert created the examples':'Expert-crafted',\n",
    "              'Not explained ':'Unknown',\n",
    "              'For some part of the data they include human generated prompts ':'Author-crafted',\n",
    "              'hand-written by linguists in respective languages, taken from textbooks, handbooks and journal articles in theoretical syntax + some examples taken from previous benchmarks ':'Expert-crafted',\n",
    "              'Human TV show; Human chitchat dialogues':'Author-crafted',\n",
    "              'Based on knowledge graphs (KG) e.g. Wikidata':'Procedurally-generated',\n",
    "              'Original benchmark modified through an agent automatically and through crowdsourcing it was filtered for quality.':'Crowd-sourced',\n",
    "              'Human-sourced task examples (not crowdworkers per say as these are non-paid real-users)':'Real task', \n",
    "              }\n",
    "sources_raw_list = ['Human exam questions (e.g. GRE questions)','Real task examples (e.g. GitHub issues)','Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)','Expert-crafted task examples (e.g. hand-written examples)','Crowd-sourced task examples (e.g. Prolific-created tasks)','Modified from another benchmark (e.g. translation into another language)','Procedurally-generated task examples (e.g. Creating instances from a template)','LLM-generated task examples (e.g. Filtered from responses to a prompt)']\n",
    "sources_list = ['Author-crafted','Crowd-sourced','Unknown','Procedurally-generated','Expert-crafted','Another benchmark','LLM-generated','Human exams','Real task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "932fdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_map = {'Targeted items (creators defined a task space and chose tasks within it strategically)':'Targeted',\n",
    "                'Specific criteria (items were taken from a larger set based on specified rules)':'Criterion',\n",
    "                'Convenience sample (creators found a set of tasks that was readily accessible)':'Convenience',\n",
    "                'Random sample (creators defined a task space and sampled from it)':'Random',\n",
    "                'Unknown':'Unknown',\n",
    "                '':'Unknown'\n",
    "                }\n",
    "sampling_raw_list = ['Specific criteria (items were taken from a larger set based on specified rules)','Targeted items (creators defined a task space and chose tasks within it strategically)','Convenience sample (creators found a set of tasks that was readily accessible)','Random sample (creators defined a task space and sampled from it)']\n",
    "sampling_list = ['Targeted','Criterion','Convenience','Random','Unknown']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f83a7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_map = {'Structured response (e.g. valid JSON, API call alone)': 'Structured',\n",
    "       'Extended interaction (e.g. conversation, calling an API and processing the response)':'Interaction',\n",
    "       'Multiple choice':'Multiple choice',\n",
    "       'Short free response (e.g. single word or number)': 'Short free response',\n",
    "       'Free response (e.g. summary paragarph)' : 'Free response',\n",
    "       'Depends on the subtask category (Utterance Classification, Dialogue Classification, Multiple Choice, Span Extraction)':'Short free response',\n",
    "       'Retrieval ': 'Short free response',\n",
    "       'functioning code (i.e., a .py script or model artifacts)':'Free response',\n",
    "       'Choice of one input sentence':'Multiple choice',\n",
    "       'predicted label':'Multiple choice',\n",
    "       'Log-likelihood of a given free response':'Logits',\n",
    "       'Free response (e.g. summary paragraph, executable code)':'Free response',\n",
    "       '':'',\n",
    "       'Generated image':'Free response',\n",
    "       'Sequencing':'Free response', \n",
    "       'image':'Free response', \n",
    "       'Image':'Free response',\n",
    "       'Movement trajectory to complete task':'Free response', \n",
    "       'Ranking of images':'Free response',\n",
    "       'This task is not based on model responses; it exclusively relies on the probability assigned to input tokens.':'Logits',\n",
    "       'This task is not based on LM responses; it solely relies on measuring the probabilities assigned to tokens in the sentence pairs.':'Logits',\n",
    "       'This task is not based on model responses; it relies solely on perplexity measurements.':'Logits',\n",
    "       'The task is not based on model responses; it solely relies on the probabilities assigned to the tokens in the two sentences.':'Logits',\n",
    "       'The task is not based on responses; it relies solely on the probability assigned to the tokens in the sentence.':'Logits',\n",
    "       'Numeric response (for utilitarian task)':'Short free response'}\n",
    "\n",
    "response_raw_list = ['Multiple choice','Short free response (e.g. single word or number)','Free response (e.g. summary paragraph, executable code)','Free response (e.g. summary paragarph)','Extended interaction (e.g. conversation, calling an API and processing the response)','Structured response (e.g. valid JSON, API call alone)']\n",
    "response_list = ['Structured', 'Interaction', 'Multiple choice', 'Short free response', 'Free response', 'Logits', 'Unknown']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1592de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map = {'Whether the faulty code fails on the test and the gold-standard code passes it.':'Reward',\n",
    "       'Exact Match (accuracy, F1, precision, recall)':\"Exact match\",\n",
    "       'Number of rounds completted': 'Reward',\n",
    "       'n-gram (BLEU, ROUGE, chrF)': 'Soft match',\n",
    "       'Distribution (perplexity, calibration, correlation)':'Distribution',\n",
    "       'LLM post-processing (extracting answers, reformatting for automated scoring)': 'LLM post-processing',\n",
    "       'reward is computed based on the final product chosen by the agent, compared against known attributes, options, and price of the target product.':'Reward',\n",
    "       'The paper defines a reward score':'Reward',\n",
    "       'LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)':'LLM-as-a-Judge',\n",
    "       'Win rate':'Reward',\n",
    "       'Human ratings (text quality, preference, NOT manual scoring of other metrics)':'Human ratings',\n",
    "       'Matthews Correlation Coefficient (MCC, Matthews), which is a measure of similarity between binary distributions taking values from -1 to 1 and always yielding 0 for any two uncorrelated distributions, regardless of class imbalance.':'Correlation',\n",
    "       'P-Score (Prompting Score) and H-Score (Heuristical Score)':'LLM-as-a-Judge',\n",
    "       'The paper introduces 2 new metrics for language confusion. Line-level pass rate (LPR) and Word-level pass rate (WPR).':'Exact match',\n",
    "       'They also report two metrics to compare an LLM’s predictive confidence in slang usages relative to their literal counterparts.':'Correlation',\n",
    "       'Matthews Correlation Coefficient (MCC), and AUC (Area Under the ROC Curve)':'Correlation',\n",
    "       'instruction following rate':'Exact match',\n",
    "       'Human accuracy evaluation':'Human ratings',\n",
    "       'cosine similarity, log generation probability':'Distribution',\n",
    "       'pass rate':'Exact match',\n",
    "       'Normalized score relative to GPT-3.5-Turbo-16K performance':'',\n",
    "       'Spearman’s ρ, L/5 precision, RMSE':'Correlation',\n",
    "       'Score improvement of script':'Reward',\n",
    "       'Accuracy when the generated function is executed.':'Reward',\n",
    "       'Pear./Spear. Corr , Avg. Precision':'Correlation',\n",
    "       'exact match, MCC (Matthews Correlation Coefficient)':'Exact match',\n",
    "       'Also consider unit tests for some questions.':'Reward',\n",
    "       'runtime percentile of the LLM-generated code on the runtime distribution supported by corresponding solutions (the Leetcode solutions)':'Reward',\n",
    "       'Krippendorff’s α':'Correlation',\n",
    "       'Memorization score':'Exact match',\n",
    "       'Generated proof verified by an independent prover system.':'Reward',\n",
    "       'Reasoning Graph Accuracy and Reasoning Graph Similarity based on graph edit distance and textual similarity e.g. BLEURT.':'Soft match',\n",
    "       'Meteor':'Soft match',\n",
    "       'Explanation Completeness P/R/F1, Explanation Logical Consistency %':'Exact match',\n",
    "       'Unordered/Ordered BERT-F1 using DeBERTa-based BERTScore':'LLM-as-a-Judge',\n",
    "       'Binary F1 for Evidence IDs':'Exact match',\n",
    "       'Landmark Coverage Rate (LCR(%)) for route-planning':'Exact match',\n",
    "       'recall@1':'Exact match',\n",
    "       'Macro-F1 for the multi-class certainty prediction':'Exact match',\n",
    "       'Top L Precision, Top-k ACC, R^2, AUC, MCRMSE, Spearmann core':'Correlation',\n",
    "       'Execute the code and evaluate exact match of table vs ground truth table.':'Exact match',\n",
    "       'Functional correctness checks. Evaluated by (1) producing a dependency graph from the code (2) using an IaC policy engine to check whether the instruction specification are in the program.':'Reward',\n",
    "       'Code \"Speedup\" and \"Memory Reduction\" versus reference solutions.':'Reward',\n",
    "       'They use an ambiguity classifier as well from previous work':'LLM-as-a-Judge',\n",
    "       'partial credit':'Soft match',\n",
    "       'Mean IoU (Intersection over Union)':'Soft match',\n",
    "       'The code is executed and results are verified against ground truth results':'Reward',\n",
    "       'checkpoint coverage':'Exact match',\n",
    "       'Custom metrics: multi-modal gain, multi-modal leakage':'Exact match',\n",
    "       'For dialogue assessment, they introduce four metrics: average turn (AT), average dialogue refinement (ADR), average turn refinement (ATR), and refinement ratio (RR).':'Reward',\n",
    "       'Execution time and memory usage efficiency; unit test correctness':'Reward',\n",
    "       '3D IoU-based Average Precision':'Soft match', \n",
    "       'Unit test cases':'Reward',\n",
    "       'Soft Accuracy for counting task':'Soft match',\n",
    "       \"Correlation (Matthew's correlation, Pearson's r)\":'Correlation',\n",
    "       \"Execution-based evaluation. e.g. run the agent's code and see if it matches the ground-truth results. Plus different rubrics for each task.\":'Reward',\n",
    "       'Execution-based / functional correctness. Pass unit tests.':'Reward',\n",
    "       'Execution-Based Evaluation (unit tests)':'Reward',\n",
    "       'Diversity@k: that measures the cultural diversity among the retrieved images, helping to identify models’ bias towards specific countries or regions.':'Distribution',\n",
    "       'Execution-based evaluation (unit tests)':'Reward',\n",
    "       'Execution-based metrics.':'Reward', \n",
    "       'Elo ratings, Win rate':'Reward', \n",
    "       'VQAScore':'Exact match',\n",
    "       'Intersection over Union (IoU)':'Soft match',\n",
    "       'A non-defined \"jailbreak success rate\". likely LLM-as-a-Judge but unclear.':'Reward',\n",
    "       'Execution-based evaluation scripts':'Reward',\n",
    "       '- Contains: A less restrictive option is to consider a response correct if the prediction contains the true class name after preprocessing - ClipMatch: matching the prediction and label using cosine similarity in a vector embedding space':'Soft match',\n",
    "       'Mean Absolute Error':'Soft match',\n",
    "       'Mel-Cepstral Distortion is a measure of audio quality for TTS. A custom index is defined to balance all the evaluation metrics. ':'',\n",
    "       'MCD, MSD, PSNR, SSIM':'Soft match', \n",
    "       'BERTScore, GLEU':'Soft match',\n",
    "       'Execution-based (unit tests)':'Reward',\n",
    "       'Generation Metric and Generation Quality Drop are never explicitly defined in the paper. ':'LLM-as-a-Judge',\n",
    "       '(school) grade':'Soft match',\n",
    "       'Custom reward functions (e.g. must_include, eval_vqa, eval_fuzzy_image_match)':'Exact match',\n",
    "       'Execution-based scoring.':'Reward',\n",
    "       'Execution-based evaluation. Fairly comprehensive.':'Reward',\n",
    "       'Execution-based evaluation':'Reward',\n",
    "       'execution-based verification, file-based comparison, information-based validation':'Reward',\n",
    "       'edit-f1':'Soft match',\n",
    "       \"must_include', 'fuzzy_match', and programmatic checks which don't fit standard categories\":'Soft match',\n",
    "       'Resolution task: accuracy gap. Retrieval bias: Bias@K, Skew@K, NDKL.':'Distribution',\n",
    "       'They also report accuracy drop between cases where the image supports the correct answer choice and the cases where it supports one of the incorrect answer choices.':'',\n",
    "       'BiasScore: percentage of demographic groups in a dataset for which the LM continuations are more negative (e.g., toxic) than the average percentage of negative generations across demographic groups':'Distribution',\n",
    "       'Toxicity Score, Entailment Score':'LLM-as-a-Judge', \n",
    "       'Ko-H5 score':'Unknown',\n",
    "       \"The primary metrics (GP, SR, SPL, PWSR) measure the task performer's navigation success when guided by the helper. Helper effectiveness is inferred from these outcomes, with SPL and PWSR combining success and path efficiency.\":'Reward',\n",
    "       'For multiple correct multiple choice questions, if there were 4 correct answers and the taker selects 3, they score 0.75. If they selected 4 correct and an additional 5th incorrect, they score 0. This is to mimic actual IEE exam scoring. ':'Soft match',\n",
    "       'rescued value rate, averaged rescue step, averaged damaged rate':'Reward',\n",
    "       'Consistency Score, Relative Consistency Score':'Distribution',\n",
    "       'Many chemistry specific metrics, such as molecule validity, Fingerprint Tanimoto Similarity etc.':'Soft match',\n",
    "       'Bias Score: percentage of examples for which the likelihood of the more stereotypical sentence is higher than the likelihood of the less stereotypical sentence.':'Distribution',\n",
    "       'Win Rate, Population Block Ratio (PBR), Resource Utilisation Ratio (RUR), Average Population Utilization (APU), Technology Rate (TR)':'Reward',\n",
    "       'Post-processing with heuristics':'Soft match',\n",
    "       'Bias Percentage: percentage of sentence pairs for which the more stereotypical sentence has a higher probability than the less stereotypical sentence.':'Distribution',\n",
    "       'SoFa Score: variance in normalized log perplexity across grouped sentences (i.e., sentences with the same stereotype and different identities)':'Distribution',\n",
    "       'TrueScore, win rates, reward (game specific)':'Reward',\n",
    "       'Percentage of items (i.e., sentence pairs) for which an LM assigns a higher (psuedo-)likelihood to the stereotyping sentence over the less stereotyping sentence':'Distribution',\n",
    "       'BertScore, kwPrec':'Soft match',\n",
    "       'Semantic Similarity, BARTScore, Char-level edit distance':'Soft match',\n",
    "       'Execution Accuracy (EX) and Valid Efficiency Score (VES)':'Reward',\n",
    "       'Jaccard Index between model predictions and human-labeled associations':'Human ratings',\n",
    "       'Output probability change of attribute':'',\n",
    "       'Mean of the output logits':'Distribution',\n",
    "       'Factual Diversity Divergence (quantifies the divergence in the level of demographic diversity in model generations compared with the factual ground truth)':'Distribution',\n",
    "       'Define MR-Score, a weighted combination of matthews correlation, and two variants of accuracy':'Correlation',\n",
    "       'FactScore (Min et al., 2023), a method that evaluates the factuality of generated text by decomposing both the reference and hypothesis into atomic facts; MMRelevance':'Exact match',\n",
    "       'Mean entailment':'Distribution',\n",
    "       'A key metric is: Score = # harms committed by agent / # harms committed by random baseline (aka a normalised ratio relative to random baseline of 1000 random trajectories)':'Reward',\n",
    "       'Define 2 new metrics, RND and OCC which handle intricacies of the mutli-turn evaluation':'Reward',\n",
    "       'IOU':'Soft match',\n",
    "       'Reward in the environment':'Reward',}\n",
    "\n",
    "metric_raw_list = ['Exact Match (accuracy, F1, precision, recall)','n-gram (BLEU, ROUGE, chrF)','Human ratings (text quality, preference, NOT manual scoring of other metrics)','LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)','LLM post-processing (extracting answers, reformatting for automated scoring)','Distribution (perplexity, calibration, correlation)',\"Correlation (Matthew's correlation, Pearson's r)\"]\n",
    "metric_list = ['Exact match', 'Human ratings', 'LLM-as-a-Judge', 'LLM post-processing', 'Distribution', 'Correlation', 'Reward', 'Soft match', 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddf6b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "contested_map = {'Widely-agreed':'Widely-agreed',\n",
    "                 'Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)':'Widely-agreed',\n",
    "                 'Contested':'Contested',\n",
    "                 'Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)':'Contested',\n",
    "                 'No definition provided':'No definition',\n",
    "                 'A new problem is presented so agreement is unclear; but the definition is generally interpretable in a widely-agreeable way.':'Widely-agreed',\n",
    "                 'Very special phenomenon that is only introduced by the authors. It is neither widely-agreed upon nor contested.':'Widely-agreed',\n",
    "                 'Very specific phenomenon discovered in this paper (i.e., no prior definition exists)':'Widely-agreed',\n",
    "                 'Very specialized phenomenon with no standard definition ':'Widely-agreed',\n",
    "                 'Very specialized phenomenon without any standard definition':'Widely-agreed'\n",
    "                 }\n",
    "contested_raw_list = ['Widely-agreed','Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)','Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)','No definition provided'] \n",
    "contested_list = list(set(contested_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81b8f3",
   "metadata": {},
   "source": [
    "#### Face Validity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77434782",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_validity_map = {'Yes':'Yes',\n",
    "                     'Partially':'Partially',\n",
    "                     \"They follow the lead of popular knowledge and reasoning benchmarks, so it's hard to say here. \":'Partially',\n",
    "                     '':'',\n",
    "                    'It is evaluating temporal misaglignment through the specific lens of factual information on Wikipedia.':'Partially',\n",
    "                    'Too vaguely defined phenomenon':'No',\n",
    "                    'No':'No',\n",
    "                    'but fairly poor task definition.':'Partially',\n",
    "                    'Somewhat. Certain tasks in the benchmark align well with how real-world analysts evaluate cyber threat intelligence, suggesting some face validity. However, other tasks focus more on knowledge retrieval, which may not reflect the full nature of cyber threat intelligence, where knowledge retrieval, understanding, reasoning, and application are all important. These aspects are tested separately, so the benchmark doesn’t provide a full picture of end-to-end evaluation.':'Partially',\n",
    "                    \"There is no specified phenomenon besides the models' ability to answer open-ended questions.\":'No',\n",
    "                    'Highly simplified version of the phenomena':'Partially',\n",
    "                    'Tricky to say since the paper does not provide a principled definition of the target phenomenon. It just talks of general \"capabilities,\" as well as the ten skills mentioned above. As for the ten skills, face validity varies -- for some (e.g., mathematics) it seems higher than for others (e.g., ethics).':'Partially',\n",
    "                    'yes, but only a small subset':'Partially',\n",
    "                    'Too broad to tell.':'No',\n",
    "                    'It really depends on the phenomena and task':'Partially',\n",
    "                    'Only partly':'Partially',\n",
    "                    'Not sure about this. Compared to other similar benchmarks, yes. In general, probably not. ':'Partially',\n",
    "                    \"rima facie reason to believe that perplexity on factual completions is a valid metric for benchmarking a language model's ability to adapt to changing knowledge over time (the target phenomenon of temporal misalignment). But the task format is very synthetic.\":'Yes',\n",
    "                    'Mixed. Keywords/n-grams are a limited way of assessing performance.':'Partially',\n",
    "                    'The Vera and Grammar models may be well-established and commonly used in compositionality or linguistic tasks, but it is not apparent in the paper. No justification is provided for the use of Vera and Grammar.':'Partially',\n",
    "                    'Mixed. For the execution-based tasks, yes, but for the code summarisation tasks they use BLEU/CodeBLEU':'Partially',\n",
    "                    'It works since most LMs do not output toxic content all the time, but this does not make it a metric that is suitable for bias measurement in principle.':'Partially',\n",
    "                    'High validity for detecting the presence of stereotypes, but low validity for measuring the absence of stereotypes. The authors acknowledge this distinction.':'Partially',\n",
    "                    'High values of the metric indicate presence of bias, but low values do not mean that a model is unbiased. The authors do not acknowledge that.':'Partially',\n",
    "                    \"Maybe - it relies on the GPT-4 evaluator being able to assess the correctness of the answer relative to human key points. If the GPT-4-as-a-judge lacked nuanced scientific understanding, it may fail to evaluate another LLM's response against the key points (e.g., requires capabilities for classic entailment,contradiction task). Howver, they do show high correlation empirically between GPT-4-as-a-judge and human evaluators. \":'Partially',\n",
    "                    'Maybe - they only keep very non-ambiguous examples but this only covers a subset of human values which can have disagreements and be ambigous in some settings':'Partially',\n",
    "                    'Depends on the game':'Partially',\n",
    "                    'Computing the mean of the logits does not seem mathematically sound, but the general approach of examining the output probabilities is valid.':'No',\n",
    "                    'The metric is new and not very well motivated':'No',\n",
    "                    'Maybe - good on ecological validity but a very small and specific set of 200 prompts':'Partially',\n",
    "                    \"Probablistic reasoning is a wide ranging and difficult to estimate phenomenon, and whilst these tasks do measure a subset of this phenomenon they don't come close to measuring everything.\":'Partially',\n",
    "                    \"Maybe: You could imagine that GPT-4 is of lower capability than the model being evaluated which would mean it couldn't necessarily judge what a good or correct answer is.\":'Partially',\n",
    "                    'The task is too unclear to know':'No',\n",
    "                    'It seems unlikely that so broad a concept could be measured well, but this is a good effort to cast a wide net.':'Partially',\n",
    "                    'Very limited scope':'Partially',\n",
    "                    'It measures the ability to solve STEM multiple choice questions, but not as the authors claim \"expert level intelligence across a diverse range of tasks\". ':'No',\n",
    "                    'Whilst relevant for this task,  it is debatable whether Theory of Mind can be boiled down to yes/no classifcation tasks. Ie therapists getting an idea for how their patient feels.':'Partially',   }\n",
    "face_validity_raw_list = ['Yes','No']\n",
    "face_validity_list = list(set(face_validity_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81558203",
   "metadata": {},
   "source": [
    "#### Realism Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69a7d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "realism_map = {'The benchmark is itself realistic':'Realistic',\n",
    " 'It is an entirely constructed scenario (no available realistic setting)':'Not possible',\n",
    " 'No':'No comparison made', 'Yes':'Comparison made',\n",
    " 'They do a partial study with actual human feedback on the benchmark tasks.':'Comparison made',\n",
    " 'Given that the benchmark is trying to measure general capabilities, it is unclear how a more realistic setting would look like.':'Not possible',\n",
    " '':'No',\n",
    " 'No - but you could say the commonsense morality task is scraped from social media so has some realism':'Realistic'}\n",
    "realism_raw_list = ['Yes','No','The benchmark is itself realistic']\n",
    "realism_list = list(set(realism_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaff4f4",
   "metadata": {},
   "source": [
    "#### Author Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9d04b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_map = {'Yes':'Yes', 'No':'No',\n",
    " 'for short-answer questions, there is a human evaluation, which to some extent can represent the validity of the questions':'Yes',\n",
    " '':'', 'Somewhat':'Yes', 'Somehwat':'Yes',\n",
    " 'The papers justify the improvement of the task design displayed in their benchmark, but not the choice of the task itself.':'Yes',\n",
    " 'Partially; addressed their own limitations.':'Yes', 'indirectly address it':'Yes',\n",
    " 'They indirectly address it':'Yes', 'Indirectly address it':'Yes', 'implicitly':'Yes',\n",
    " 'They compare scores on benchmark to human judgment':'Yes',\n",
    " 'Partial pre-analysis.':'Yes', 'A bit (but not a strong Yes)':'Yes',\n",
    " 'They acknowledge the lateral thinking is hard to measure: \"In this paper, we seek to explore and elicit the lateral thinking ability of LLMs. However, accurately evaluating this capability poses significant challenges due to the complexity of measuring creative thinking [29 , 19 ] and the difficulty of obtaining relevant data. The generation of novel ideas is inherently non-trivial, even for humans [13 , 14 ]. Considering these challenges, we propose the exploration of lateral thinking in LLMs by situation puzzles as a primary research tool\"':'Yes'}\n",
    "author_raw_list = ['Yes','No']\n",
    "author_list = list(set(author_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd1324",
   "metadata": {},
   "source": [
    "#### Ecology Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01c5092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecology_map = {'Complete real task (e.g. providing medical advice to real people interactively)':'Complete',\n",
    " \"Constructed task (e.g. predicting medical diagnoses from clinicians' notes)\":'Constructed',\n",
    " 'Representative task (e.g. answering medical licensing exam questions)':'Representative',\n",
    " 'Partial real task (e.g. answering medical questions collected from real people)':'Partial',\n",
    " '':'', 'Low ecology':'Constructed',\n",
    " 'Low ecology, humans wouldn’t usually ask LLMs to do these tasks.':'Constructed',\n",
    " 'Artificial task':'Constructed',\n",
    " 'Proxy task - tries to get at real-world scenarios of agents via fictional adventures':'Representative'}\n",
    "\n",
    "ecology_raw_list = ['Complete real task (e.g. providing medical advice to real people interactively)','Partial real task (e.g. answering medical questions collected from real people)','Representative task (e.g. answering medical licensing exam questions)',\"Constructed task (e.g. predicting medical diagnoses from clinicians' notes)\"]\n",
    "ecology_list = list(set(ecology_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b872f9",
   "metadata": {},
   "source": [
    "#### Metrics Agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f964e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_agg_map = {\n",
    " 'simple mean': ['Mean'],\n",
    " 'simple mean/sum': ['Mean'],\n",
    " 'Simple mean': ['Mean'],\n",
    " 'Mean': ['Mean'],\n",
    " 'Mean, ': ['Mean'],\n",
    " 'Simple mean and standard deviation': ['Mean','Std'],\n",
    " 'mean and variance': ['Mean','Std'],\n",
    " 'simple mean and std': ['Mean','Std'],\n",
    " 'simple mean, std': ['Mean','Std'],\n",
    " 'mean and variance, t-tests': ['Mean','Std','Tests'],\n",
    " 'mean': ['Mean'],\n",
    " 'Simple Mean': ['Mean'],\n",
    " 'Averages and win/tie percentages in human eval; no advanced statistics reported\\n': ['Mean'],\n",
    " 'Means, standard deviations, and Spearman/Pearson correlations with expert rankings.\\n': ['Mean','Std','Other'],\n",
    " 'simple mean, statistical tests': ['Mean','Tests'],\n",
    " 'Flesch Kincaid, Rouge-L, Kendalls, Spearmans\\n' :['Other'],\n",
    " 'simple mean, standard error of the mean': ['Mean'],\n",
    " 'Pearson correlation, RMSE, differences to student baselines\\n': ['Other'],\n",
    " 'Simple mean, average':['Mean'],\n",
    " 'Average':['Mean'],\n",
    " 'mean with variance':['Mean','Std'],\n",
    "'The authors carry out some error analysis: \"We argue that we are measuring a lower bound for what LMs know. To further understand the shortcomings of the current method, we conduct an error analysis of the errors in precision on all datasets. We choose BERTLARGE for the study. We sample 100 documents from the Wikidata-OIE dataset, and manually check the reasons for the errors\"':['Other'],\n",
    "\"Simple means for performance metrics; agreement percentages and Cohen's Kappa for annotation reliability.\":['Mean','Other'],\n",
    "'simple mean, Anova for p-values, Tukey-HSD':['Mean','Tests'],\n",
    "'The authors report average task score and success rate across trials. They also include standard deviation/error bars in some result plots (e.g. Figure 4), mainly to show the variation across multiple runs.':['Mean','Std'],\n",
    "'mean of weighted-F1 scores':['Mean'],\n",
    "\"Mean, and they they show a delta (for change in aggregate sources across all tasks). It is unclear if this is a range or a standard deviation. I think it's a range.\":['Mean'],\n",
    "'The authors use a weighted mean in calculating an approximate human performance threshold but not for model performance. They take a weighted average of the annual medal thresholds for ‘Advanced’ problems. ':['Mean'],\n",
    "'simple mean and STD':['Mean','Std'],\n",
    "'Simple means and macro-averaging (mean across tasks, which is identical here because each task has same # of instances)':['Mean'],\n",
    "'macro-accuracy':['Mean'], \n",
    "'Simple mean (no variance or standard reported)':['Mean'],\n",
    "'mean and standard deviation':['Mean','Std'],\n",
    "'Simple mean/sum; % improvement between contexts':['Mean','Other'],\n",
    "'simple mean and standard deviation ':['Mean','Std'],\n",
    "'accuracy, F1, standard deviation':['Mean','Std','Other'],\n",
    "'retrieval rate R@K metric':['Other'],\n",
    "'For each tuple, the F1 is computed, then across a clique the minimum is computed and aggregated across the dataset as mean.':['Mean'],\n",
    "'Simple mean: F1 scores and accuracy. MSE. nDCG and MRR. Perplexity':['Mean','Other'],\n",
    "'Mean and standard deviation':['Mean','Std'], \n",
    "'simple mean (as percentage)':['Mean'],\n",
    "'Simple average of perplexity for different snapshots of the wikipedia data.':['Mean'],\n",
    "'Aggregated scores (no additional stats)':['Mean'], \n",
    "'mean over 8 runs. ':['Mean'],\n",
    "'Simple summary stats. ':['Mean'], \n",
    "'Success rate':['Mean'],\n",
    "'simple mean, for tasks with more than one metric (like Pearson and Spearman correlation for sentiment regression), scores are averaged to get a single task score':['Mean'],\n",
    "'Min, max, average':['Mean','Other'],\n",
    "'Weighted Precision, Recall, F1 scores, and macro-F1 scores for binary and multi-class classification. Hamming loss is also reported for multi-class classification. ':['Mean','Other'],\n",
    "'Visual semantic tasks were measured with representational similarity analysis (RSA), while the other tasks were measured with a novel metric: softmax-optimized Kullback-Leibler divergence':['Other'],\n",
    "'simple mean,  inter-annotator agreement with WAWA and the Dawid-Skene method for vote aggregation.  delta-scores to measure performance differences between models under different dataset filtering conditions':['Mean','Other'],\n",
    "'Mean, standard deviation.':['Mean','Std'],\n",
    "'Simple mean/average scores (MSQA Correctness Score C, MSNN Accuracy) are used to aggregate results. Different models or settings are compared directly based on these mean scores presented in tables.':['Mean'],\n",
    "'Simple mean/average of Hit@k, Recall@k, and MRR over the test sets.':['Mean'],\n",
    "'Accuracy, MRR, Precision, Recall, F1, BLEU, ROUGE-L, Keyword Recall, Mean Likert scores.':['Mean','Other'],\n",
    "'BLEU-4, ROUGE-L, MRR, Precision@3. Mean scores are reported, sometimes with standard deviation (e.g., for text lengths in Table 2 ).':['Mean','Std'],\n",
    "'Precision, Recall, F1 score, Exact Match (EM)':['Mean','Other'],\n",
    "'Macro F1 score, Exact Match (EM)':['Mean'], \n",
    "' simple mean/sum':['Mean'],\n",
    "'simple mean, mean and std, averaging across multiple metrics':['Mean','Std'],\n",
    "'simple mean/sum, t-tests':['Mean','Tests'], \n",
    "'simple mean/sum, GLMs':['Mean','Tests'],\n",
    "'Accuracy is reported for classification in both open and closed-world settings. Fine-tuned accuracy and linear probing accuracy are reported in a closed-world setting, while 1NN-genus probing accuracy is reported in an open-world setting. AMI is reported for zero-shot transfer learning, and in multimodal retrieval learning, micro and macro top-1 accuracy is reported. ':['Mean','Other'],\n",
    "'The metrics are averaged and normalized against human performance':['Mean'],\n",
    "'BLEU-4, Rouge-1, BERTScore, Keyword Insertion Rates (KWD), Sentence Length Regulation Compliance Rates (REG), Pearson and Spearman Correlation for Human Evaluation':['Other'],\n",
    "' Macro F1 score, per-class F1 score':['Mean'], \n",
    "'Unknown':['Unknown'],\n",
    "'Answer Accuracy (Exact Match %), Reasoning Graph Accuracy (%), Reasoning Graph Similarity (%).':['Mean'],\n",
    "'Factuality is calculated with Named Entity Recognition (NER) empowered accuracy, described in the paper. Style is measured with BLEU. Insightfulness is measured by human assessments based on impact (breadth of claim), and significance (magnitude of changes) on a 5 point Likert scale, and the average of the human review is reported. ':['Mean'],\n",
    "'F1, EM, R1, R2, MET':['Mean'],\n",
    "'Accuracy (%). Kappa score used for error analysis inter-rater reliability.':['Mean','Other'],\n",
    "'Simple mean and variance on accuracy are used to assess the overall and best pick comparisons for cartoons, and expectation adjusted distinct N-grams (EAD) and Sentence-BERT embedding cosine similarity (SBERT) are used to assess caption diversity. ':['Mean','Std','Other'],\n",
    "'Precision, Recall, F1 score, Completeness (P/R/F1), Logical Consistency (%).':['Mean'],\n",
    "'BERT-F1, ROUGE-L F1, Human Judgement Proportions (%), Pearson Correlation (r) for metric validation.':['Mean'],\n",
    "'Accuracy, F1 score, BERTScore F1, Average score (1-5 scale).':['Mean'],\n",
    "'simple mean/sum, percentage point improvements':['Mean'],\n",
    "'F1 score, AllCorrect (Exact Match), Accuracy, Macro F1':['Mean'],\n",
    "'Precision, Recall, F1, ROUGE-1, ROUGE-2, ROUGE-L, Human evaluation win/tie/lose rates (%).':['Mean'],\n",
    "'Accuracy (%), F1 Score (%)':['Mean'],\n",
    "'Accuracy (%), Standard Deviation, Error Rate (%)':['Mean','Std'],\n",
    "'Reports average scores for commonsense Vera score gap and Grammar score gap. The paper also reports the pairwise better ratio between SugarCrepe and ARO+CREPE. ':['Mean'],\n",
    "'Mean and std':['Mean','Std'], 'Mean, variance':['Mean','Std'], 'simple average ':['Mean'],\n",
    "'mean and standard dev':['Mean','Std'],\n",
    "'No statistical methods used. just simple mean and differences in means.':['Mean'],\n",
    "'simple mean and for rating-based evaluations they measure \"hedging rate\"':['Mean'],\n",
    "'simple mean to aggregate performance over scenarios and roles':['Mean'],\n",
    "'simple mean/sum, plus comparisons to scores from the base LLMs comprising the multi-modal models (called \"multi-modal gain\" and \"multi-modal leakage\" statistics)':['Mean'],\n",
    "'Mean, worst and best out of 11':['Mean'],\n",
    "'simple mean. no inferential statistics (even though the LLM-as-judge have fairly low accuracy).':['Mean'],\n",
    "'mean with \"error bars from 3 runs at temperature 0.2\" (unsure if this is a standard error or just the range in scores) (17)':['Mean','Std'],\n",
    "'Exact Match (EM), F1 Score (%)':['Mean'], 'Mean and standard deviation\\n':['Mean'],\n",
    "'Accuracy (%)':['Mean'], 'Accuracy (%), BLEU-4, ROUGE-L':['Mean'],\n",
    "'Simple mean to aggregate automatic scores, Pearson and Spearman correlation between human and automatic ratings\\u200b, and Krippendorff’s Alpha inter-rater agreement for human ratings.':['Mean','Other'],\n",
    "'mean/sum, where problem correct means all subproblems must be correct':['Mean'],\n",
    "'simple means to report F1 scores and ROUGE metrics':['Mean'], 'Mean,':['Mean'],\n",
    "'simple mean + std':['Mean','Std'], 'Mean, standard errors':['Mean','Std'],\n",
    "'Mean, standard deviation':['Mean','Std'],\n",
    "'Mean, Spearman/Pearson Correlations\\n(For completeness, they also report the standard Pearson but also mention that Pearson is not the ideal metric due to the curve-of-best-fit not appearing linear.)':['Mean','Other'],\n",
    "'mean,':['Mean'], 'Means, comparisons with percentage point gaps.':['Mean'],\n",
    "'Wilcoxon tests, variance analysis':['Other'],\n",
    "'Means and percentage differences':['Mean'],\n",
    "'simple mean/sum, correlation between overall rankings and general capabilities based on MMBench':['Mean','Other'],\n",
    "'Mean, percentage':['Mean'],\n",
    "'simple mean, std, relative performance changes':['Mean','Std'],\n",
    "'Simple Means, the percentage of questions answered correctly.':['Mean'],\n",
    "'simple mean, weighted and unweighted clustering scores, frequency counts, Fleiss Kappa':['Mean','Other'],\n",
    "'Experiments are repeated 5 times but resulting information onf uncertainty are not reported.':['Unknown'],\n",
    "'Mean error, scatter plots, attention heatmaps':['Mean'],\n",
    "'Micro-averaged entity-level F1 score reported as means across 3 runs with standard deviations. Simple means used for comparing approaches across different noise types.':['Mean','Std'],\n",
    "'Reporting accuracy, precision, recall, and F1 scores, both as macro averages across all categories and separately for offensive and non-offensive classes.':['Mean'],\n",
    "'Image retrieval error, effect score bias':['Mean'],\n",
    "'F1 scores (micro-averaged and macro-averaged) as the primary statistical method.':['Mean'],\n",
    "'Simple mean/accuracy':['Mean'],\n",
    "'Just simple mean, with occasional reporting of variance or distribution plots.':['Mean','Std'],\n",
    "'Simple proportion, human/model comparisons':['Mean'],\n",
    "\"Simple mean scores for each metric. For correlation analysis between automatic metrics and human judgments: Spearman's rank correlation coefficient.\":['Mean','Other'],\n",
    "'simple mean and variance':['Mean','Std'], \n",
    "'Percentage, comparison':['Mean'],\n",
    "'Simple means for the main metrics':['Mean'],\n",
    "'Means, standard deviations, weighted Fleiss‑k':['Mean','Std'],\n",
    "'simple mean, 95% confidence interval, percentage point of performance gains over baselines':['Mean','Tests'],\n",
    "'Simple mean/sum, custom normalized aggregate metric':['Mean','Other'],\n",
    "'Mean with 95% Confidence Interval ':['Mean','Tests'], 'Simple mean accuracy':['Mean'],\n",
    "'Simple mean + standard deviations':['Mean','Std'],\n",
    "'Primary metrics used are F1 scores, accuracy, recall, and precision. Partially, micro and macro averages are reported.':['Mean'],\n",
    "'Mean and Standard deviation':['Mean','Std'], 'simple mean with percentage point':['Mean'],\n",
    "'Simple mean to aggregate across different settings. For each detector, they report AUROC and F1 Score values for each specific condition and the average across those conditions. ':['Mean'],\n",
    "'simple mean ':['Mean'],\n",
    "'Simple mean, char-based F0.5 scores for overall performance, along with precision and recall.':['Mean'],\n",
    "'mean + standard deviatin, significance test. proportion':['Mean','Std'],\n",
    "'Stratified human agreement evaluation on LLM-graded items; comparisons to BLEU/F1 for scoring validity.\\n':['Mean'],\n",
    "'Accuracy, Fleiss’ k for human agreement\\n':['Mean','Other'],\n",
    "'Mean scores across different data splits and standard deviation for low-resource settings':['Mean','Std'],\n",
    "\"Automatic Evaluation Metrics (SARI; BLEU); Measure inter-annotator agreement using Krippendorff's alpha\":['Mean'],\n",
    "'Micro-Average, Worst-Average':['Mean'],\n",
    "\"Win and draw rates from pairwise comparisons. For automated metrics and human judgment evaluation: Kendall's Tau \":['Mean','Other'],\n",
    "'Mean, error bars on figures in appendix.':['Mean','Std'],\n",
    "'Dataset score is calculated as a macro-average of the per-language score.':['Mean'],\n",
    "'Simple means, McNemar test, Minimum detectable effect (MDE)':['Mean','Other'],\n",
    "'Scores are normalised relative to human performance.':['Other'],\n",
    "'simple mean, weighted mean':['Mean'],\n",
    "\"mean, standard deviation, entropy calculations, z-scores, p-values, bootstrapping, Le Cam's lemma, multiplicative damping factors\":['Mean','Std','Tests','Other'],\n",
    "'True Positive Rate, True Negative Rate, Generation Metric and Generation Quality Drop':['Mean'],\n",
    "'Binomial mixed effects regression models':['Tests'],\n",
    "'Mean, ANOVA, post-hoc pairwise tests':['Mean','Tests'],\n",
    "'Means, standard deviations, comparisons':['Mean','Std'],\n",
    "'Descriptive accuracy only\\n':['Mean'],\n",
    "'Binomial standard error reported\\n':['Tests'],\n",
    "'Inter-rater agreement (Krippendorff’s alpha), statistical comparisons\\n':['Other'],\n",
    "'Basic comparisons across models; no significance tests\\n':['Mean'],\n",
    "'Simple mean success rates across tasks and subsets. There is no formal hypothesis testing or statistical significance tests.':['Mean'],\n",
    "'Mean, error bars on some plots':['Mean','Std'],\n",
    "'Simple mean and standard error of the mean (for plots like accuracy vs. steps)':['Mean','Std'],\n",
    "'simple mean across samples, then the relative decline in accuracy due to injection is computed.':['Mean'],\n",
    "'Simple mean/sum (Success Rate %)':['Mean'],\n",
    "'Mean and standard error, with bootstrap confidence intervals':['Mean','Std'],\n",
    "'Simple mean (for direct assessment scores), Elo rating via Maximum Likelihood Estimation (MLE) (for pairwise comparisons), Fleiss’ Kappa and Percentage Agreement (for inter-annotator agreement and human-LLM agreement), and Kendall’s Tau (for human-LLM leaderboard rank correlation).':['Mean','Other'],\n",
    "'simple mean\\xa0and\\xa0standard error':['Mean','Std'],\n",
    "\"Simple mean/sum, correlation (Pearson's r)\":['Mean','Other'],\n",
    "'simple mean, correlation':['Mean','Other'], 'Simple means with error bars':['Mean','Std'],\n",
    "'simple mean, correlation, calibration':['Mean','Other'], 'Unkown ':['Unknown'],\n",
    "'simple mean, optionally the negative scoring as above':['Mean'],\n",
    "'Simple mean, standard deviation':['Mean','Std'],\n",
    "'Simple sum/mean - raw percentages':['Mean'],\n",
    "'Mean, standard deviation, Spearman correlation for difficulty alignment (IRT vs human/GPT4)':['Mean','Std'],\n",
    "'Mean and pairwise agreement figures;':['Mean','Other'],\n",
    "'Simple mean scores, simple sum scores, inter‑annotator agreement percentages.':['Mean'],\n",
    "'Mean accuracy is used to aggregate model performance across tasks and conditions':['Mean'],\n",
    "'They report simple percentage means, and compute Spearman’s\\xa0ρ and Pearson’s\\xa0r between rule‑ and LLM‑based scores to show consistency; and give a 98\\xa0% human‑vs‑ChatGPT agreement figure (percent agreement, not kappa).':['Mean','Other'],\n",
    "'Mean, recall, F1 with standard error comparisons':['Mean','Std'],\n",
    "'Mean and standard error; calibration (ECE) and AUROC for confidence analyses.':['Mean','Std'],\n",
    "'Mean; McNemar x^2 test for mining':['Mean','Tests'],\n",
    "'simple mean, significance clusters':['Mean'],\n",
    "'Mean and breakdown by category':['Mean'],\n",
    "'simple mean + standard errors, standard deviations':['Mean','Std'],\n",
    "'The models are primarily ordered by rank.':['Mean'],\n",
    "'In the main table, they only reported the weighted mean. In the appendix, they say \"Where plotted, the error bars show what the standard error would be if estimated using bootstrapping. To estimate the standard error of our reported model scores, we analytically estimate the standard error.\"':['Mean','Std'],\n",
    "'Weighted mean based on test samples.':['Mean'],\n",
    "'Simple mean across questions for each category. They also compute correlation metrics between human and GPT-4 evaluations: Pearson, Spearman, and Kendall-τ coefficients to validate GPT-4 as a reliable evaluator.':['Mean','Other'],\n",
    "'Simple mean over examples (accuracy)':['Mean'],\n",
    "'The paper reports simple means for calculating the overall accuracy as a macro average across all five dimensions':['Mean'],\n",
    "'mostly mean - variance only reported for best model. ':['Mean','Std'],\n",
    "'simple mean. ':['Mean'],\n",
    "'Mean, and human–automatic correlation (Cohen’s Kappa coefficient) for validation.':['Mean','Other'],\n",
    "'simple mean, standard deviation':['Mean','Std'],\n",
    "'Simple mean, and for annotation agreement Cohen’s Kappa coefficient was used.':['Mean','Other'],\n",
    "'Simple mean is used for aggregation.':['Mean'],\n",
    "'Mean; authors additionally report t‑tests for MGCT effect differences and classification accuracy for the detector.':['Mean','Tests'],\n",
    "'‑ Per‑metric means & confidence via single runs\\n\\n‑ Spearman correlation (response/evidence vs position)\\n\\n‑ Cohen’s\\xa0κ for human IAA (0.74‑0.77)':['Mean','Other'],\n",
    "'Mean, sample‑level Pearson\\xa0r, system‑level Pearson\\xa0r, pairwise win‑rate % (for agreement studies).':['Mean','Other'],\n",
    "'Mean, F1, balanced accuracy; 95% CIs via bootstrap.':['Mean','Std'],\n",
    "'Mean, Precision/Recall/F1, Balanced Accuracy; inter‑annotator agreement (Cohen’s\\xa0κ / raw %).':['Mean','Other'],\n",
    "'Simple mean, Spearman correlation (with p‑value <\\xa00.05)':['Mean','Other'],\n",
    "'The paper uses simple means for the primary evaluation metric. For each task, they report the percentage of correct predictions. For the overall score, they take a simple average across the five ethical categories. They also test whether models can distinguish ambiguous scenarios from clear-cut scenarios by using predictive uncertainty estimates (Area Under the Receiver Operating Characteristic curve).':['Mean'],\n",
    "'mean, std':['Mean','Std'],\n",
    "'Simple mean ± 95% confidence interval':['Mean','Std'],\n",
    "'simple mean/sum, mean and variance for accuracy and BLEU':['Mean','Std'],\n",
    "'Mean and variance, standard deviations':['Mean','Std'],\n",
    "'Simple mean/sum': ['Mean'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec6c5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_map = {}\n",
    "stats_raw_list = []\n",
    "stats_list = list(set(stats_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23365023",
   "metadata": {},
   "source": [
    "### Applying Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4125bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_maps = {'task_source': {'rename_map':source_map,'raw_names': sources_raw_list,'final_names': sources_list},\n",
    "                'dataset_sampling_method':{'rename_map': sampling_map, 'raw_names': sampling_raw_list, 'final_names': sampling_list},\n",
    "                'response_format':{'rename_map':response_map, 'raw_names':response_raw_list, 'final_names':response_list},\n",
    "                'metric_definition': {'rename_map':metrics_map, 'raw_names':metric_raw_list, 'final_names':metric_list},\n",
    "                'phenomenon_contested':{'rename_map':contested_map,'raw_names':contested_raw_list,'final_names':contested_list},\n",
    "                'task_face_validity':{'rename_map':face_validity_map,'raw_names':face_validity_raw_list,'final_names':face_validity_list},\n",
    "                'metric_face_validity':{'rename_map':face_validity_map,'raw_names':face_validity_raw_list,'final_names':face_validity_list},\n",
    "                'results_realism':{'rename_map':realism_map,'raw_names':realism_raw_list,'final_names':realism_list},\n",
    "                'results_author_validity':{'rename_map':author_map,'raw_names':author_raw_list,'final_names':author_list},\n",
    "                'task_ecology':{'rename_map':ecology_map,'raw_names':ecology_raw_list,'final_names':ecology_list},\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcef3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_str(list_str,split_list):\n",
    "    if isinstance(list_str,str):\n",
    "        for split in split_list:\n",
    "            if split in list_str:\n",
    "                list_str = list_str.replace(split+',',split+'///,')\n",
    "        list_str = list_str.split('///, ')\n",
    "        return list_str\n",
    "    else:\n",
    "        if pd.isna(list_str):\n",
    "            return ['']\n",
    "        return [list_str]\n",
    "    \n",
    "def match_occurences(items:list, count:list, header:str):\n",
    "    return {header+': '+key:key in items for key in count}\n",
    "\n",
    "\n",
    "def expand_columns(df, col_name,rename_map,raw_names,final_names):\n",
    "    try:\n",
    "        df[col_name + '_clean'] = df[col_name].apply(lambda x: [rename_map[y] for y in split_list_str(x,raw_names)])\n",
    "        return pd.concat([df,df[col_name].apply(lambda x: [rename_map[y] for y in split_list_str(x,raw_names)]).apply(lambda x: match_occurences(x,final_names,col_name)).apply(pd.Series)],axis=1)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} in column {col_name}\")\n",
    "        print(df[col_name].apply(lambda x: [y for y in split_list_str(x,raw_names)]).explode().unique())\n",
    "        raise e\n",
    "        return included_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8497ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in split_maps.keys():\n",
    "    included_df = expand_columns(included_df, col, **split_maps[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64c8c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df['metric_statistics_clean'] = included_df['metric_statistics'].apply(lambda x: metrics_agg_map[x] if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "154da4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#included_df.to_csv('../data/clean_codebook.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ba204d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_ecology: Representative\n",
      "task_ecology: Constructed\n",
      "task_ecology: Complete\n",
      "task_ecology: Partial\n",
      "task_ecology: \n",
      "results_author_validity: \n",
      "results_author_validity: No\n",
      "results_author_validity: Yes\n",
      "results_realism: Comparison made\n",
      "results_realism: No\n",
      "results_realism: Not possible\n",
      "results_realism: Realistic\n",
      "results_realism: No comparison made\n",
      "metric_face_validity: Partially\n",
      "metric_face_validity: No\n",
      "metric_face_validity: \n",
      "metric_face_validity: Yes\n",
      "task_face_validity: Partially\n",
      "task_face_validity: No\n",
      "task_face_validity: \n",
      "task_face_validity: Yes\n",
      "phenomenon_contested: No definition\n",
      "phenomenon_contested: Widely-agreed\n",
      "phenomenon_contested: Contested\n",
      "metric_definition: Unknown\n",
      "metric_definition: Soft match\n",
      "metric_definition: Reward\n",
      "metric_definition: Correlation\n",
      "metric_definition: Distribution\n",
      "metric_definition: LLM post-processing\n",
      "metric_definition: LLM-as-a-Judge\n",
      "metric_definition: Human ratings\n",
      "metric_definition: Exact match\n",
      "response_format: Unknown\n",
      "response_format: Logits\n",
      "response_format: Free response\n",
      "response_format: Short free response\n",
      "response_format: Multiple choice\n",
      "response_format: Interaction\n",
      "response_format: Structured\n",
      "dataset_sampling_method: Unknown\n",
      "dataset_sampling_method: Random\n",
      "dataset_sampling_method: Convenience\n",
      "dataset_sampling_method: Criterion\n",
      "dataset_sampling_method: Targeted\n",
      "task_dataset_metadata\n",
      "task_source: Real task\n",
      "task_source: Human exams\n",
      "task_source: LLM-generated\n",
      "task_source: Another benchmark\n",
      "task_source: Expert-crafted\n",
      "task_source: Procedurally-generated\n",
      "task_source: Unknown\n",
      "task_source: Crowd-sourced\n",
      "task_source: Author-crafted\n",
      "definition_scope\n",
      "definition_integrity\n",
      "phenomenon_contested\n",
      "phenomenon_defined\n",
      "phenomenon_short\n"
     ]
    }
   ],
   "source": [
    "categorized_columns = ['phenomenon_short','phenomenon_defined','phenomenon_contested','definition_integrity','definition_scope',\n",
    "'task_source: Author-crafted','task_source: Crowd-sourced','task_source: Unknown','task_source: Procedurally-generated','task_source: Expert-crafted','task_source: Another benchmark','task_source: LLM-generated','task_source: Human exams','task_source: Real task',\n",
    "'task_dataset_metadata','dataset_sampling_method: Targeted', 'dataset_sampling_method: Criterion',\n",
    "       'dataset_sampling_method: Convenience', 'dataset_sampling_method: Random','dataset_sampling_method: Unknown','response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown', 'metric_definition: Exact match',\n",
    "       'metric_definition: Human ratings', 'metric_definition: LLM-as-a-Judge',\n",
    "       'metric_definition: LLM post-processing',\n",
    "       'metric_definition: Distribution', 'metric_definition: Correlation',\n",
    "       'metric_definition: Reward', 'metric_definition: Soft match',\n",
    "       'metric_definition: Unknown',\n",
    "       'phenomenon_contested: Contested',\n",
    "'phenomenon_contested: Widely-agreed',\n",
    "'phenomenon_contested: No definition',\n",
    "'task_face_validity: Yes',\n",
    "'task_face_validity: ', \n",
    "'task_face_validity: No',\n",
    "'task_face_validity: Partially',\n",
    "'metric_face_validity: Yes',\n",
    "'metric_face_validity: ',\n",
    "'metric_face_validity: No',\n",
    "'metric_face_validity: Partially',\n",
    "'results_realism: No comparison made',\n",
    "'results_realism: Realistic',\n",
    "'results_realism: Not possible',\n",
    "'results_realism: No',\n",
    "'results_realism: Comparison made',\n",
    "'results_author_validity: Yes',\n",
    "'results_author_validity: No',\n",
    "'results_author_validity: ',\n",
    "'task_ecology: ',\n",
    "'task_ecology: Partial',\n",
    "'task_ecology: Complete',\n",
    "'task_ecology: Constructed',\n",
    "'task_ecology: Representative',\n",
    "]\n",
    "\n",
    "for col in categorized_columns.__reversed__():\n",
    "    temp = included_df[col]\n",
    "    if col in col_maps:\n",
    "        temp = temp.apply(lambda x: col_maps[col][x] if col_maps[col].get(x) else x)\n",
    "    temp = temp.value_counts()\n",
    "    temp = temp[temp > 0]\n",
    "    print(col)\n",
    "    #pie_helper(temp, temp.index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "765206c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root  phenomenon_taxonomy_leaf\n",
       "NLP                       Understanding               5\n",
       "Agents                    Tool Use                    2\n",
       "Alignment                 Alignment                   2\n",
       "                          Safety                      1\n",
       "Language Modelling        In-context Learning         1\n",
       "NLP                       Detection                   1\n",
       "                          Extraction                  1\n",
       "                          Summarization               1\n",
       "Reasoning                 Planning                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[(included_df['metric_definition: Correlation'])][['phenomenon_taxonomy_root','phenomenon_taxonomy_leaf']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f66d8229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>...</th>\n",
       "      <th>results_author_validity:</th>\n",
       "      <th>results_author_validity: No</th>\n",
       "      <th>results_author_validity: Yes</th>\n",
       "      <th>task_ecology_clean</th>\n",
       "      <th>task_ecology:</th>\n",
       "      <th>task_ecology: Partial</th>\n",
       "      <th>task_ecology: Representative</th>\n",
       "      <th>task_ecology: Constructed</th>\n",
       "      <th>task_ecology: Complete</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4/12/2025 18:03:29</td>\n",
       "      <td>Negar Foroutan</td>\n",
       "      <td>zhangMELAMultilingualEvaluation2024</td>\n",
       "      <td>MELA: Multilingual Evaluation of Linguistic Ac...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper intorduces a multilingual acceptabil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4/13/2025 20:33:00</td>\n",
       "      <td>Negar Foroutan</td>\n",
       "      <td>sunInformalLanguageProcessing2024</td>\n",
       "      <td>Toward Informal Language Processing: Knowledge...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Using movie subtitles, the authors construct a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4/13/2025 20:38:38</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>wangPretrainingLanguageModel2023</td>\n",
       "      <td>ON PRE-TRAINED LANGUAGE MODELS FOR ANTIBODY</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces the AnTibody Understandi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4/14/2025 10:35:13</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>xuPEERComprehensiveMultitask2022</td>\n",
       "      <td>PEER: A Comprehensive and Multi-Task Benchmark...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A benchmark called PEER (a\\ncomprehensive and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4/14/2025 21:02:00</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>hardalovBgGLUEBulgarianGeneral2023</td>\n",
       "      <td>bgGLUE: A Bulgarian General Language Understan...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bgGLUE (Bulgarian General Language\\nUnderstan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4/15/2025 14:35:04</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>berdicevskisSuperlimSwedishLanguage2023</td>\n",
       "      <td>Superlim: A Swedish Language Understanding Eva...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present Superlim, a multi-task NLP bench- m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4/16/2025 9:21:25</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>renBEACONBenchmarkComprehensive2024</td>\n",
       "      <td>BEACON: Benchmark for Comprehensive RNA Tasks ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Topic Exclusion (Is the paper about measuring ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper presents BEACON, the first comprehe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>4/17/2025 8:56:38</td>\n",
       "      <td>Anna Gausen</td>\n",
       "      <td>shenTaskBenchBenchmarkingLarge2024</td>\n",
       "      <td>TaskBench: Benchmarking Large Language Models ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TaskBench is a framework for evaluating how we...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>4/17/2025 17:21:15</td>\n",
       "      <td>Lujain Ibrahim</td>\n",
       "      <td>zhangMultiTrustComprehensiveBenchmark2024</td>\n",
       "      <td>MULTITRUST: A Comprehensive Benchmark Towards ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces a benchmark on the trustw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>4/17/2025 20:25:35</td>\n",
       "      <td>Angelika Romanou</td>\n",
       "      <td>chenCurriculumBroadcoverageBenchmark2022</td>\n",
       "      <td>Curriculum: A Broad-Coverage Benchmark for Lin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Current models do not provide insight into how...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>4/17/2025 21:24:02</td>\n",
       "      <td>Jan Batzner</td>\n",
       "      <td>ushioGenerativeLanguageModels2022</td>\n",
       "      <td>Generative Language Models for Paragraph-Level...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QG-Bench, a comprehensive benchmark for paragr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>4/17/2025 22:44:18</td>\n",
       "      <td>Yilun Zhao</td>\n",
       "      <td>chenMLLMasajudgeAssessingMultimodal2024</td>\n",
       "      <td>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces the MLLM-as-a-Judge benc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>4/18/2025 20:34:24</td>\n",
       "      <td>Jan Batzner</td>\n",
       "      <td>mackoMULTITuDELargescaleMultilingual2023</td>\n",
       "      <td>MULTITuDE: Large-Scale Multilingual Machine-Ge...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MULTITuDE, a benchmark dataset for multilingua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Tests]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>4/18/2025 23:53:19</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>fenogenovaMERAComprehensiveLLM2024</td>\n",
       "      <td>MERA: A Comprehensive LLM Evaluation in Russian</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper proposes a new instruction benchmark...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>4/19/2025 0:52:01</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>sunMeasuringEffectInfluential2023</td>\n",
       "      <td>Measuring the Effect of Influential Messages o...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The authors examine the task of predicting how...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>4/19/2025 20:14:57</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>liuWe`reAfraidLanguage2023</td>\n",
       "      <td>We're Afraid Language Models Aren't Modeling A...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the ability of LMs to hand...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>4/20/2025 17:13:53</td>\n",
       "      <td>Karolina Korgul</td>\n",
       "      <td>wangGTABenchmarkGeneral2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GTA is a benchmark designed to evaluate LLM-ba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>4/20/2025 21:33:41</td>\n",
       "      <td>Lujain Ibrahim</td>\n",
       "      <td>mireshghallahCanLLMsKeep2024</td>\n",
       "      <td>Can LLMs Keep a Secret? Testing Privacy Implic...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces a benchmark grounded in t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>4/22/2025 13:40:03</td>\n",
       "      <td>Karolina Korgul</td>\n",
       "      <td>maLargeLanguageModels2024</td>\n",
       "      <td>Large Language Models Play StarCraft II:Benchm...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TextStarCraft II turns the full StarCraft II v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>4/21/2025 6:57:30</td>\n",
       "      <td>Maria Grandury</td>\n",
       "      <td>leiterPrExMeLargeScale2024</td>\n",
       "      <td>PrExMe! Large Scale Prompt Exploration of Open...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces PrExMe, a benchmark of p...</td>\n",
       "      <td>They include emotion-CoT in prompt templates, ...</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>4/22/2025 17:42:02</td>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>zengMRbenMetareasoningBenchmark2024</td>\n",
       "      <td>MR-Ben: A Meta-Reasoning Benchmark for Evaluat...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset of question,answer pairs in which answ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>5/8/2025 14:34:36</td>\n",
       "      <td>Karolina Korgul</td>\n",
       "      <td>ramamurthyReinforcementLearningNot2023</td>\n",
       "      <td>Is Reinforcement Learning (Not) for Natural La...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper investigates the viability of reinfo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Timestamp        main_coder  \\\n",
       "22   4/12/2025 18:03:29    Negar Foroutan   \n",
       "34   4/13/2025 20:33:00    Negar Foroutan   \n",
       "35   4/13/2025 20:38:38    Anna Sotnikova   \n",
       "60   4/14/2025 10:35:13    Anna Sotnikova   \n",
       "71   4/14/2025 21:02:00    Anna Sotnikova   \n",
       "85   4/15/2025 14:35:04    Anna Sotnikova   \n",
       "127   4/16/2025 9:21:25    Anna Sotnikova   \n",
       "186   4/17/2025 8:56:38       Anna Gausen   \n",
       "207  4/17/2025 17:21:15    Lujain Ibrahim   \n",
       "229  4/17/2025 20:25:35  Angelika Romanou   \n",
       "233  4/17/2025 21:24:02       Jan Batzner   \n",
       "247  4/17/2025 22:44:18        Yilun Zhao   \n",
       "301  4/18/2025 20:34:24       Jan Batzner   \n",
       "314  4/18/2025 23:53:19  Valentin Hoffman   \n",
       "323   4/19/2025 0:52:01  Valentin Hoffman   \n",
       "333  4/19/2025 20:14:57  Valentin Hoffman   \n",
       "342  4/20/2025 17:13:53   Karolina Korgul   \n",
       "345  4/20/2025 21:33:41    Lujain Ibrahim   \n",
       "363  4/22/2025 13:40:03   Karolina Korgul   \n",
       "385   4/21/2025 6:57:30    Maria Grandury   \n",
       "424  4/22/2025 17:42:02       Thom Foster   \n",
       "445   5/8/2025 14:34:36   Karolina Korgul   \n",
       "\n",
       "                                        bibkey  \\\n",
       "22         zhangMELAMultilingualEvaluation2024   \n",
       "34           sunInformalLanguageProcessing2024   \n",
       "35            wangPretrainingLanguageModel2023   \n",
       "60            xuPEERComprehensiveMultitask2022   \n",
       "71          hardalovBgGLUEBulgarianGeneral2023   \n",
       "85     berdicevskisSuperlimSwedishLanguage2023   \n",
       "127        renBEACONBenchmarkComprehensive2024   \n",
       "186         shenTaskBenchBenchmarkingLarge2024   \n",
       "207  zhangMultiTrustComprehensiveBenchmark2024   \n",
       "229   chenCurriculumBroadcoverageBenchmark2022   \n",
       "233          ushioGenerativeLanguageModels2022   \n",
       "247    chenMLLMasajudgeAssessingMultimodal2024   \n",
       "301   mackoMULTITuDELargescaleMultilingual2023   \n",
       "314         fenogenovaMERAComprehensiveLLM2024   \n",
       "323          sunMeasuringEffectInfluential2023   \n",
       "333                 liuWe`reAfraidLanguage2023   \n",
       "342                wangGTABenchmarkGeneral2024   \n",
       "345               mireshghallahCanLLMsKeep2024   \n",
       "363                  maLargeLanguageModels2024   \n",
       "385                 leiterPrExMeLargeScale2024   \n",
       "424        zengMRbenMetareasoningBenchmark2024   \n",
       "445     ramamurthyReinforcementLearningNot2023   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "22   MELA: Multilingual Evaluation of Linguistic Ac...   Include   \n",
       "34   Toward Informal Language Processing: Knowledge...   Include   \n",
       "35         ON PRE-TRAINED LANGUAGE MODELS FOR ANTIBODY   Include   \n",
       "60   PEER: A Comprehensive and Multi-Task Benchmark...   Include   \n",
       "71   bgGLUE: A Bulgarian General Language Understan...   Include   \n",
       "85   Superlim: A Swedish Language Understanding Eva...   Include   \n",
       "127  BEACON: Benchmark for Comprehensive RNA Tasks ...   Include   \n",
       "186  TaskBench: Benchmarking Large Language Models ...   Include   \n",
       "207  MULTITRUST: A Comprehensive Benchmark Towards ...   Include   \n",
       "229  Curriculum: A Broad-Coverage Benchmark for Lin...   Include   \n",
       "233  Generative Language Models for Paragraph-Level...   Include   \n",
       "247  MLLM-as-a-Judge: Assessing Multimodal LLM-as-a...   Include   \n",
       "301  MULTITuDE: Large-Scale Multilingual Machine-Ge...   Include   \n",
       "314    MERA: A Comprehensive LLM Evaluation in Russian   Include   \n",
       "323  Measuring the Effect of Influential Messages o...   Include   \n",
       "333  We're Afraid Language Models Aren't Modeling A...   Include   \n",
       "342                                                NaN   Include   \n",
       "345  Can LLMs Keep a Secret? Testing Privacy Implic...   Include   \n",
       "363  Large Language Models Play StarCraft II:Benchm...   Include   \n",
       "385  PrExMe! Large Scale Prompt Exploration of Open...   Include   \n",
       "424  MR-Ben: A Meta-Reasoning Benchmark for Evaluat...   Include   \n",
       "445  Is Reinforcement Learning (Not) for Natural La...   Include   \n",
       "\n",
       "                                    exclusion_criteria  \\\n",
       "22                                                 NaN   \n",
       "34                                                 NaN   \n",
       "35                                                 NaN   \n",
       "60                                                 NaN   \n",
       "71                                                 NaN   \n",
       "85                                                 NaN   \n",
       "127  Topic Exclusion (Is the paper about measuring ...   \n",
       "186                                                NaN   \n",
       "207                                                NaN   \n",
       "229                                                NaN   \n",
       "233                                                NaN   \n",
       "247                                                NaN   \n",
       "301                                                NaN   \n",
       "314                                                NaN   \n",
       "323                                                NaN   \n",
       "333                                                NaN   \n",
       "342                                                NaN   \n",
       "345                                                NaN   \n",
       "363                                                NaN   \n",
       "385                                                NaN   \n",
       "424                                                NaN   \n",
       "445                                                NaN   \n",
       "\n",
       "    exclusion_criteria_detail  \\\n",
       "22                        NaN   \n",
       "34                        NaN   \n",
       "35                        NaN   \n",
       "60                        NaN   \n",
       "71                        NaN   \n",
       "85                        NaN   \n",
       "127                       NaN   \n",
       "186                       NaN   \n",
       "207                       NaN   \n",
       "229                       NaN   \n",
       "233                       NaN   \n",
       "247                       NaN   \n",
       "301                       NaN   \n",
       "314                       NaN   \n",
       "323                       NaN   \n",
       "333                       NaN   \n",
       "342                       NaN   \n",
       "345                       NaN   \n",
       "363                       NaN   \n",
       "385                       NaN   \n",
       "424                       NaN   \n",
       "445                       NaN   \n",
       "\n",
       "                                         short_summary  \\\n",
       "22   The paper intorduces a multilingual acceptabil...   \n",
       "34   Using movie subtitles, the authors construct a...   \n",
       "35   This paper introduces the AnTibody Understandi...   \n",
       "60   A benchmark called PEER (a\\ncomprehensive and ...   \n",
       "71    bgGLUE (Bulgarian General Language\\nUnderstan...   \n",
       "85   We present Superlim, a multi-task NLP bench- m...   \n",
       "127  This paper presents BEACON, the first comprehe...   \n",
       "186  TaskBench is a framework for evaluating how we...   \n",
       "207  The paper introduces a benchmark on the trustw...   \n",
       "229  Current models do not provide insight into how...   \n",
       "233  QG-Bench, a comprehensive benchmark for paragr...   \n",
       "247  This paper introduces the MLLM-as-a-Judge benc...   \n",
       "301  MULTITuDE, a benchmark dataset for multilingua...   \n",
       "314  The paper proposes a new instruction benchmark...   \n",
       "323  The authors examine the task of predicting how...   \n",
       "333  This paper examines the ability of LMs to hand...   \n",
       "342  GTA is a benchmark designed to evaluate LLM-ba...   \n",
       "345  The paper introduces a benchmark grounded in t...   \n",
       "363  TextStarCraft II turns the full StarCraft II v...   \n",
       "385  This paper introduces PrExMe, a benchmark of p...   \n",
       "424  Dataset of question,answer pairs in which answ...   \n",
       "445  The paper investigates the viability of reinfo...   \n",
       "\n",
       "                                          contribution  \\\n",
       "22                                                 NaN   \n",
       "34                                                 NaN   \n",
       "35                                                 NaN   \n",
       "60                                                 NaN   \n",
       "71                                                 NaN   \n",
       "85                                                 NaN   \n",
       "127                                                NaN   \n",
       "186                                                NaN   \n",
       "207                                                NaN   \n",
       "229                                                NaN   \n",
       "233                                                NaN   \n",
       "247                                                NaN   \n",
       "301                                                NaN   \n",
       "314                                                NaN   \n",
       "323                                                NaN   \n",
       "333                                                NaN   \n",
       "342                                                NaN   \n",
       "345                                                NaN   \n",
       "363                                                NaN   \n",
       "385  They include emotion-CoT in prompt templates, ...   \n",
       "424                                                NaN   \n",
       "445                                                NaN   \n",
       "\n",
       "                                      phenomenon_short  ...  \\\n",
       "22   General Capability (A broadly useful ability, ...  ...   \n",
       "34   Specific Application (A single use case, where...  ...   \n",
       "35   Specific Application (A single use case, where...  ...   \n",
       "60   Specific Application (A single use case, where...  ...   \n",
       "71   Specific Application (A single use case, where...  ...   \n",
       "85   General Capability (A broadly useful ability, ...  ...   \n",
       "127  Specific Application (A single use case, where...  ...   \n",
       "186  General Capability (A broadly useful ability, ...  ...   \n",
       "207  General Capability (A broadly useful ability, ...  ...   \n",
       "229  General Capability (A broadly useful ability, ...  ...   \n",
       "233  Specific Application (A single use case, where...  ...   \n",
       "247  General Capability (A broadly useful ability, ...  ...   \n",
       "301  Specific Application (A single use case, where...  ...   \n",
       "314  General Capability (A broadly useful ability, ...  ...   \n",
       "323  Specific Application (A single use case, where...  ...   \n",
       "333  General Capability (A broadly useful ability, ...  ...   \n",
       "342  General Capability (A broadly useful ability, ...  ...   \n",
       "345  General Capability (A broadly useful ability, ...  ...   \n",
       "363  General Capability (A broadly useful ability, ...  ...   \n",
       "385  Specific Application (A single use case, where...  ...   \n",
       "424  General Capability (A broadly useful ability, ...  ...   \n",
       "445  General Capability (A broadly useful ability, ...  ...   \n",
       "\n",
       "    results_author_validity:  results_author_validity: No  \\\n",
       "22                      False                        True   \n",
       "34                      False                        True   \n",
       "35                      False                        True   \n",
       "60                      False                        True   \n",
       "71                      False                        True   \n",
       "85                      False                        True   \n",
       "127                     False                        True   \n",
       "186                     False                       False   \n",
       "207                     False                        True   \n",
       "229                     False                        True   \n",
       "233                     False                       False   \n",
       "247                     False                       False   \n",
       "301                     False                        True   \n",
       "314                     False                        True   \n",
       "323                     False                        True   \n",
       "333                     False                        True   \n",
       "342                     False                       False   \n",
       "345                     False                       False   \n",
       "363                     False                       False   \n",
       "385                     False                       False   \n",
       "424                     False                        True   \n",
       "445                     False                       False   \n",
       "\n",
       "    results_author_validity: Yes                      task_ecology_clean  \\\n",
       "22                         False                           [Constructed]   \n",
       "34                         False                           [Constructed]   \n",
       "35                         False                               [Partial]   \n",
       "60                         False                               [Partial]   \n",
       "71                         False  [Partial, Representative, Constructed]   \n",
       "85                         False  [Partial, Representative, Constructed]   \n",
       "127                        False                  [Partial, Constructed]   \n",
       "186                         True               [Partial, Representative]   \n",
       "207                        False               [Partial, Representative]   \n",
       "229                        False                           [Constructed]   \n",
       "233                         True                        [Representative]   \n",
       "247                         True                  [Partial, Constructed]   \n",
       "301                        False                        [Representative]   \n",
       "314                        False           [Representative, Constructed]   \n",
       "323                        False                           [Constructed]   \n",
       "333                        False                           [Constructed]   \n",
       "342                         True           [Representative, Constructed]   \n",
       "345                         True                  [Partial, Constructed]   \n",
       "363                         True                        [Representative]   \n",
       "385                         True                  [Partial, Constructed]   \n",
       "424                        False                              [Complete]   \n",
       "445                         True                  [Partial, Constructed]   \n",
       "\n",
       "    task_ecology:  task_ecology: Partial task_ecology: Representative  \\\n",
       "22           False                 False                        False   \n",
       "34           False                 False                        False   \n",
       "35           False                  True                        False   \n",
       "60           False                  True                        False   \n",
       "71           False                  True                         True   \n",
       "85           False                  True                         True   \n",
       "127          False                  True                        False   \n",
       "186          False                  True                         True   \n",
       "207          False                  True                         True   \n",
       "229          False                 False                        False   \n",
       "233          False                 False                         True   \n",
       "247          False                  True                        False   \n",
       "301          False                 False                         True   \n",
       "314          False                 False                         True   \n",
       "323          False                 False                        False   \n",
       "333          False                 False                        False   \n",
       "342          False                 False                         True   \n",
       "345          False                  True                        False   \n",
       "363          False                 False                         True   \n",
       "385          False                  True                        False   \n",
       "424          False                 False                        False   \n",
       "445          False                  True                        False   \n",
       "\n",
       "    task_ecology: Constructed task_ecology: Complete metric_statistics_clean  \n",
       "22                       True                  False             [Mean, Std]  \n",
       "34                       True                  False                  [Mean]  \n",
       "35                      False                  False                     NaN  \n",
       "60                      False                  False             [Mean, Std]  \n",
       "71                       True                  False                  [Mean]  \n",
       "85                       True                  False             [Mean, Std]  \n",
       "127                      True                  False             [Mean, Std]  \n",
       "186                     False                  False                  [Mean]  \n",
       "207                     False                  False           [Mean, Other]  \n",
       "229                      True                  False                  [Mean]  \n",
       "233                     False                  False           [Mean, Other]  \n",
       "247                      True                  False                     NaN  \n",
       "301                     False                  False           [Mean, Tests]  \n",
       "314                      True                  False                  [Mean]  \n",
       "323                      True                  False                     NaN  \n",
       "333                      True                  False                  [Mean]  \n",
       "342                      True                  False           [Mean, Other]  \n",
       "345                      True                  False                  [Mean]  \n",
       "363                     False                  False             [Mean, Std]  \n",
       "385                      True                  False                  [Mean]  \n",
       "424                     False                   True                     NaN  \n",
       "445                      True                  False             [Mean, Std]  \n",
       "\n",
       "[22 rows x 128 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[(included_df['metric_definition: Correlation']) & (included_df['phenomenon_taxonomy_root']!='Language Modelling')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aad7ed41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root\n",
       "NLP                      91\n",
       "Reasoning                84\n",
       "Agents                   40\n",
       "Alignment                37\n",
       "Language Modelling       36\n",
       "Code Generation          26\n",
       "VQA                      15\n",
       "Medicine                 15\n",
       "Knowledge                13\n",
       "Retrieval                13\n",
       "Grounding                11\n",
       "User Interaction         10\n",
       "Law                       9\n",
       "Multilinguality           8\n",
       "Instruction Following     8\n",
       "Biology                   7\n",
       "Theory of Mind            5\n",
       "General Science           5\n",
       "Psychology                3\n",
       "Finance                   3\n",
       "Factuality                3\n",
       "LLM as a Judge            3\n",
       "Data Analysis             2\n",
       "General Purpose           2\n",
       "Mental Health             1\n",
       "Business                  1\n",
       "History                   1\n",
       "Chemistry                 1\n",
       "Education                 1\n",
       "Sports                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['phenomenon_taxonomy_root'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20ac77",
   "metadata": {},
   "source": [
    "### Saving Clean Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9035707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 128)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05eab529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['bibkey'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28a96721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp',\n",
       " 'main_coder',\n",
       " 'bibkey',\n",
       " 'title',\n",
       " 'inclusion',\n",
       " 'exclusion_criteria',\n",
       " 'exclusion_criteria_detail',\n",
       " 'short_summary',\n",
       " 'contribution',\n",
       " 'phenomenon_short',\n",
       " 'target_phenomenon',\n",
       " 'phenomenon_defined',\n",
       " 'phenomenon_definition',\n",
       " 'definition_scope',\n",
       " 'purpose_extra',\n",
       " 'task_definition',\n",
       " 'task_item_definition',\n",
       " 'task_definition_detail',\n",
       " 'task_source',\n",
       " 'task_dataset_size',\n",
       " 'task_dataset_metadata',\n",
       " 'dataset_metadata_detail',\n",
       " 'dataset_sampling_method',\n",
       " 'response_format',\n",
       " 'metric_definition',\n",
       " 'metric_definition_detail',\n",
       " 'task_source_detail',\n",
       " 'authorship',\n",
       " 'benchmark_availability',\n",
       " 'procedural_extra',\n",
       " 'notes_extra',\n",
       " 'task_train_val',\n",
       " 'task_dataset_size_extra',\n",
       " 'response_format_detail',\n",
       " 'metric_aggregation',\n",
       " 'metric_subscores',\n",
       " 'metric_subscores_detail',\n",
       " 'metric_metascoring',\n",
       " 'benchmark_location',\n",
       " 'benchmark',\n",
       " 'phenomenon_contested',\n",
       " 'task_face_validity',\n",
       " 'metric_face_validity',\n",
       " 'result_interpretation',\n",
       " 'results_comparison',\n",
       " 'results_comparison_explanation',\n",
       " 'results_realism',\n",
       " 'results_human_baseline',\n",
       " 'results_author_validity',\n",
       " 'results_author_validity_detail',\n",
       " 'metric_statistics',\n",
       " 'metric_access',\n",
       " 'task_ecology',\n",
       " 'task_ecology_detail',\n",
       " 'definition_integrity',\n",
       " 'definition_integrity_detail',\n",
       " 'task_dataset_size_detail',\n",
       " 'metric_fewshot',\n",
       " 'new_bibkey',\n",
       " 'phenomenon_taxonomy_root',\n",
       " 'phenomenon_taxonomy_leaf',\n",
       " 'phenomenon_taxonomy_alternate',\n",
       " 'validate_taxonomy',\n",
       " 'task_source_clean',\n",
       " 'task_source: Author-crafted',\n",
       " 'task_source: Crowd-sourced',\n",
       " 'task_source: Unknown',\n",
       " 'task_source: Procedurally-generated',\n",
       " 'task_source: Expert-crafted',\n",
       " 'task_source: Another benchmark',\n",
       " 'task_source: LLM-generated',\n",
       " 'task_source: Human exams',\n",
       " 'task_source: Real task',\n",
       " 'dataset_sampling_method_clean',\n",
       " 'dataset_sampling_method: Targeted',\n",
       " 'dataset_sampling_method: Criterion',\n",
       " 'dataset_sampling_method: Convenience',\n",
       " 'dataset_sampling_method: Random',\n",
       " 'dataset_sampling_method: Unknown',\n",
       " 'response_format_clean',\n",
       " 'response_format: Structured',\n",
       " 'response_format: Interaction',\n",
       " 'response_format: Multiple choice',\n",
       " 'response_format: Short free response',\n",
       " 'response_format: Free response',\n",
       " 'response_format: Logits',\n",
       " 'response_format: Unknown',\n",
       " 'metric_definition_clean',\n",
       " 'metric_definition: Exact match',\n",
       " 'metric_definition: Human ratings',\n",
       " 'metric_definition: LLM-as-a-Judge',\n",
       " 'metric_definition: LLM post-processing',\n",
       " 'metric_definition: Distribution',\n",
       " 'metric_definition: Correlation',\n",
       " 'metric_definition: Reward',\n",
       " 'metric_definition: Soft match',\n",
       " 'metric_definition: Unknown',\n",
       " 'phenomenon_contested_clean',\n",
       " 'phenomenon_contested: No definition',\n",
       " 'phenomenon_contested: Widely-agreed',\n",
       " 'phenomenon_contested: Contested',\n",
       " 'task_face_validity_clean',\n",
       " 'task_face_validity: ',\n",
       " 'task_face_validity: Partially',\n",
       " 'task_face_validity: No',\n",
       " 'task_face_validity: Yes',\n",
       " 'metric_face_validity_clean',\n",
       " 'metric_face_validity: ',\n",
       " 'metric_face_validity: Partially',\n",
       " 'metric_face_validity: No',\n",
       " 'metric_face_validity: Yes',\n",
       " 'results_realism_clean',\n",
       " 'results_realism: Realistic',\n",
       " 'results_realism: Comparison made',\n",
       " 'results_realism: No comparison made',\n",
       " 'results_realism: No',\n",
       " 'results_realism: Not possible',\n",
       " 'results_author_validity_clean',\n",
       " 'results_author_validity: ',\n",
       " 'results_author_validity: No',\n",
       " 'results_author_validity: Yes',\n",
       " 'task_ecology_clean',\n",
       " 'task_ecology: ',\n",
       " 'task_ecology: Partial',\n",
       " 'task_ecology: Representative',\n",
       " 'task_ecology: Constructed',\n",
       " 'task_ecology: Complete',\n",
       " 'metric_statistics_clean']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(included_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b038fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#included_df[['main_coder','bibkey','title','inclusion','short_summary','phenomenon_taxonomy_root', 'phenomenon_taxonomy_leaf', 'phenomenon_taxonomy_alternate','task_source','task_source_clean','dataset_sampling_method','dataset_sampling_method_clean','response_format','response_format_clean','metric_definition','metric_definition_clean','metric_statistics','metric_statistics_clean']].to_csv('../data/results_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8008025d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>phenomenon_taxonomy_root</th>\n",
       "      <th>phenomenon_taxonomy_leaf</th>\n",
       "      <th>phenomenon_taxonomy_alternate</th>\n",
       "      <th>task_source</th>\n",
       "      <th>task_source_clean</th>\n",
       "      <th>dataset_sampling_method</th>\n",
       "      <th>dataset_sampling_method_clean</th>\n",
       "      <th>response_format</th>\n",
       "      <th>response_format_clean</th>\n",
       "      <th>metric_definition</th>\n",
       "      <th>metric_definition_clean</th>\n",
       "      <th>metric_statistics</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Mayne</td>\n",
       "      <td>mundlerSWTBenchTestingValidating2024</td>\n",
       "      <td>SWT-Bench: Testing and Validating Real-World B...</td>\n",
       "      <td>Include</td>\n",
       "      <td>A benchmark for generating code tests (unit te...</td>\n",
       "      <td>Agents</td>\n",
       "      <td>Coding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real task examples (e.g. GitHub issues), Modif...</td>\n",
       "      <td>[Real task, Another benchmark]</td>\n",
       "      <td>Specific criteria (items were taken from a lar...</td>\n",
       "      <td>[Criterion]</td>\n",
       "      <td>Structured response (e.g. valid JSON, API call...</td>\n",
       "      <td>[Structured]</td>\n",
       "      <td>Whether the faulty code fails on the test and ...</td>\n",
       "      <td>[Reward]</td>\n",
       "      <td>simple mean</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jonathan Rystrøm</td>\n",
       "      <td>davidsonEvaluatingLanguageModel2024</td>\n",
       "      <td>EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...</td>\n",
       "      <td>Include</td>\n",
       "      <td>The paper introduces a dynamic framework for e...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Author-crafted]</td>\n",
       "      <td>Targeted items (creators defined a task space ...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>Extended interaction (e.g. conversation, calli...</td>\n",
       "      <td>[Interaction]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall),...</td>\n",
       "      <td>[Exact match, Reward]</td>\n",
       "      <td>mean with variance</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lennart Luettgau</td>\n",
       "      <td>helweMAFALDABenchmarkComprehensive2024</td>\n",
       "      <td>MAFALDA: A Benchmark and Comprehensive Study o...</td>\n",
       "      <td>Include</td>\n",
       "      <td>The paper introduces MAFALD, a benchmark that ...</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Logical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Author-crafted, Crowd-sourced, Another benchm...</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience, Targeted]</td>\n",
       "      <td>Multiple choice</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>simple mean/sum</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaili Liu</td>\n",
       "      <td>niuRAGTruthHallucinationCorpus2024</td>\n",
       "      <td>RAGTruth: A Hallucination Corpus for Developin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>This paper targets word-level hallucinations i...</td>\n",
       "      <td>Retrieval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Factuality</td>\n",
       "      <td>Real task examples (e.g. GitHub issues), Crowd...</td>\n",
       "      <td>[Real task, Crowd-sourced, Another benchmark, ...</td>\n",
       "      <td>Random sample (creators defined a task space a...</td>\n",
       "      <td>[Random, Targeted]</td>\n",
       "      <td>Short free response (e.g. single word or numbe...</td>\n",
       "      <td>[Short free response, Free response, Structured]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anna Gausen</td>\n",
       "      <td>wangIELMOpenInformation2022</td>\n",
       "      <td>IELM: An Open Information Extraction Benchmark...</td>\n",
       "      <td>Include</td>\n",
       "      <td>They introduce a new open information extracti...</td>\n",
       "      <td>NLP</td>\n",
       "      <td>Extraction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crowd-sourced task examples (e.g. Prolific-cre...</td>\n",
       "      <td>[Crowd-sourced, Procedurally-generated]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>Structured response (e.g. valid JSON, API call...</td>\n",
       "      <td>[Structured]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>The authors carry out some error analysis: \"We...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Andrew Bean</td>\n",
       "      <td>wangUsercentricMultiintentBenchmark2024</td>\n",
       "      <td>A User-Centric Multi-Intent Benchmark for Eval...</td>\n",
       "      <td>Include</td>\n",
       "      <td>The paper creates a dataset of user scenarios ...</td>\n",
       "      <td>General Purpose</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expert-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Expert-crafted, Crowd-sourced]</td>\n",
       "      <td>Random sample (creators defined a task space a...</td>\n",
       "      <td>[Random]</td>\n",
       "      <td>Free response (e.g. summary paragraph, executa...</td>\n",
       "      <td>[Free response]</td>\n",
       "      <td>Human ratings (text quality, preference, NOT m...</td>\n",
       "      <td>[Human ratings, LLM-as-a-Judge]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>XuOpenToMComprehensiveBenchmark2024</td>\n",
       "      <td>OpenToM: A Comprehensive Benchmark for Evaluat...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Benchmark to assess Theory of Mind in LLMs. Ea...</td>\n",
       "      <td>Theory of Mind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Author-crafted, Crowd-sourced, Procedurally-g...</td>\n",
       "      <td>Random sample (creators defined a task space a...</td>\n",
       "      <td>[Random, Convenience]</td>\n",
       "      <td>Multiple choice</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>chenPremiseOrderMatters2024</td>\n",
       "      <td>Premise Order Matters in Reasoning with Large ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Benchmark that shows a failure mode of LLM rea...</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Logical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Modified from another benchmark (e.g. translat...</td>\n",
       "      <td>[Another benchmark]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>Short free response (e.g. single word or number)</td>\n",
       "      <td>[Short free response]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>hanReadingBooksGreat2023</td>\n",
       "      <td>Reading Books is Great, But Not if You Are Dri...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Commonsense norms are defeasible by context:  ...</td>\n",
       "      <td>Grounding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crowd-sourced task examples (e.g. Prolific-cre...</td>\n",
       "      <td>[Crowd-sourced]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>Multiple choice, Free response (e.g. summary p...</td>\n",
       "      <td>[Multiple choice, Free response]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall),...</td>\n",
       "      <td>[Exact match, Soft match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>wangMMLUproMoreRobust2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Include</td>\n",
       "      <td>Extends MMLU (hard, diverse multiple choice ll...</td>\n",
       "      <td>Knowledge</td>\n",
       "      <td>General</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Modified from another benchmark (e.g. translat...</td>\n",
       "      <td>[Another benchmark]</td>\n",
       "      <td>Targeted items (creators defined a task space ...</td>\n",
       "      <td>[Targeted, Criterion]</td>\n",
       "      <td>Multiple choice</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           main_coder                                   bibkey  \\\n",
       "0         Harry Mayne     mundlerSWTBenchTestingValidating2024   \n",
       "1    Jonathan Rystrøm      davidsonEvaluatingLanguageModel2024   \n",
       "2    Lennart Luettgau   helweMAFALDABenchmarkComprehensive2024   \n",
       "3           Kaili Liu       niuRAGTruthHallucinationCorpus2024   \n",
       "4         Anna Gausen              wangIELMOpenInformation2022   \n",
       "..                ...                                      ...   \n",
       "450       Andrew Bean  wangUsercentricMultiintentBenchmark2024   \n",
       "451       Thom Foster      XuOpenToMComprehensiveBenchmark2024   \n",
       "452       Thom Foster              chenPremiseOrderMatters2024   \n",
       "453       Thom Foster                 hanReadingBooksGreat2023   \n",
       "454       Thom Foster                wangMMLUproMoreRobust2024   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "0    SWT-Bench: Testing and Validating Real-World B...   Include   \n",
       "1    EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...   Include   \n",
       "2    MAFALDA: A Benchmark and Comprehensive Study o...   Include   \n",
       "3    RAGTruth: A Hallucination Corpus for Developin...   Include   \n",
       "4    IELM: An Open Information Extraction Benchmark...   Include   \n",
       "..                                                 ...       ...   \n",
       "450  A User-Centric Multi-Intent Benchmark for Eval...   Include   \n",
       "451  OpenToM: A Comprehensive Benchmark for Evaluat...   Include   \n",
       "452  Premise Order Matters in Reasoning with Large ...   Include   \n",
       "453  Reading Books is Great, But Not if You Are Dri...   Include   \n",
       "454                                                NaN   Include   \n",
       "\n",
       "                                         short_summary  \\\n",
       "0    A benchmark for generating code tests (unit te...   \n",
       "1    The paper introduces a dynamic framework for e...   \n",
       "2    The paper introduces MAFALD, a benchmark that ...   \n",
       "3    This paper targets word-level hallucinations i...   \n",
       "4    They introduce a new open information extracti...   \n",
       "..                                                 ...   \n",
       "450  The paper creates a dataset of user scenarios ...   \n",
       "451  Benchmark to assess Theory of Mind in LLMs. Ea...   \n",
       "452  Benchmark that shows a failure mode of LLM rea...   \n",
       "453  Commonsense norms are defeasible by context:  ...   \n",
       "454  Extends MMLU (hard, diverse multiple choice ll...   \n",
       "\n",
       "    phenomenon_taxonomy_root phenomenon_taxonomy_leaf  \\\n",
       "0                     Agents                   Coding   \n",
       "1                  Alignment                Alignment   \n",
       "2                  Reasoning                  Logical   \n",
       "3                  Retrieval                      NaN   \n",
       "4                        NLP               Extraction   \n",
       "..                       ...                      ...   \n",
       "450          General Purpose                      NaN   \n",
       "451           Theory of Mind                      NaN   \n",
       "452                Reasoning                  Logical   \n",
       "453                Grounding                      NaN   \n",
       "454                Knowledge                  General   \n",
       "\n",
       "    phenomenon_taxonomy_alternate  \\\n",
       "0                             NaN   \n",
       "1                             NaN   \n",
       "2                             NaN   \n",
       "3                      Factuality   \n",
       "4                             NaN   \n",
       "..                            ...   \n",
       "450                           NaN   \n",
       "451                           NaN   \n",
       "452                           NaN   \n",
       "453                           NaN   \n",
       "454                           NaN   \n",
       "\n",
       "                                           task_source  \\\n",
       "0    Real task examples (e.g. GitHub issues), Modif...   \n",
       "1    Author-crafted task examples (e.g. hand-writte...   \n",
       "2    Author-crafted task examples (e.g. hand-writte...   \n",
       "3    Real task examples (e.g. GitHub issues), Crowd...   \n",
       "4    Crowd-sourced task examples (e.g. Prolific-cre...   \n",
       "..                                                 ...   \n",
       "450  Expert-crafted task examples (e.g. hand-writte...   \n",
       "451  Author-crafted task examples (e.g. hand-writte...   \n",
       "452  Modified from another benchmark (e.g. translat...   \n",
       "453  Crowd-sourced task examples (e.g. Prolific-cre...   \n",
       "454  Modified from another benchmark (e.g. translat...   \n",
       "\n",
       "                                     task_source_clean  \\\n",
       "0                       [Real task, Another benchmark]   \n",
       "1                                     [Author-crafted]   \n",
       "2    [Author-crafted, Crowd-sourced, Another benchm...   \n",
       "3    [Real task, Crowd-sourced, Another benchmark, ...   \n",
       "4              [Crowd-sourced, Procedurally-generated]   \n",
       "..                                                 ...   \n",
       "450                    [Expert-crafted, Crowd-sourced]   \n",
       "451  [Author-crafted, Crowd-sourced, Procedurally-g...   \n",
       "452                                [Another benchmark]   \n",
       "453                                    [Crowd-sourced]   \n",
       "454                                [Another benchmark]   \n",
       "\n",
       "                               dataset_sampling_method  \\\n",
       "0    Specific criteria (items were taken from a lar...   \n",
       "1    Targeted items (creators defined a task space ...   \n",
       "2    Convenience sample (creators found a set of ta...   \n",
       "3    Random sample (creators defined a task space a...   \n",
       "4    Convenience sample (creators found a set of ta...   \n",
       "..                                                 ...   \n",
       "450  Random sample (creators defined a task space a...   \n",
       "451  Random sample (creators defined a task space a...   \n",
       "452  Convenience sample (creators found a set of ta...   \n",
       "453  Convenience sample (creators found a set of ta...   \n",
       "454  Targeted items (creators defined a task space ...   \n",
       "\n",
       "    dataset_sampling_method_clean  \\\n",
       "0                     [Criterion]   \n",
       "1                      [Targeted]   \n",
       "2         [Convenience, Targeted]   \n",
       "3              [Random, Targeted]   \n",
       "4                   [Convenience]   \n",
       "..                            ...   \n",
       "450                      [Random]   \n",
       "451         [Random, Convenience]   \n",
       "452                 [Convenience]   \n",
       "453                 [Convenience]   \n",
       "454         [Targeted, Criterion]   \n",
       "\n",
       "                                       response_format  \\\n",
       "0    Structured response (e.g. valid JSON, API call...   \n",
       "1    Extended interaction (e.g. conversation, calli...   \n",
       "2                                      Multiple choice   \n",
       "3    Short free response (e.g. single word or numbe...   \n",
       "4    Structured response (e.g. valid JSON, API call...   \n",
       "..                                                 ...   \n",
       "450  Free response (e.g. summary paragraph, executa...   \n",
       "451                                    Multiple choice   \n",
       "452   Short free response (e.g. single word or number)   \n",
       "453  Multiple choice, Free response (e.g. summary p...   \n",
       "454                                    Multiple choice   \n",
       "\n",
       "                                response_format_clean  \\\n",
       "0                                        [Structured]   \n",
       "1                                       [Interaction]   \n",
       "2                                   [Multiple choice]   \n",
       "3    [Short free response, Free response, Structured]   \n",
       "4                                        [Structured]   \n",
       "..                                                ...   \n",
       "450                                   [Free response]   \n",
       "451                                 [Multiple choice]   \n",
       "452                             [Short free response]   \n",
       "453                  [Multiple choice, Free response]   \n",
       "454                                 [Multiple choice]   \n",
       "\n",
       "                                     metric_definition  \\\n",
       "0    Whether the faulty code fails on the test and ...   \n",
       "1    Exact Match (accuracy, F1, precision, recall),...   \n",
       "2        Exact Match (accuracy, F1, precision, recall)   \n",
       "3        Exact Match (accuracy, F1, precision, recall)   \n",
       "4        Exact Match (accuracy, F1, precision, recall)   \n",
       "..                                                 ...   \n",
       "450  Human ratings (text quality, preference, NOT m...   \n",
       "451      Exact Match (accuracy, F1, precision, recall)   \n",
       "452      Exact Match (accuracy, F1, precision, recall)   \n",
       "453  Exact Match (accuracy, F1, precision, recall),...   \n",
       "454      Exact Match (accuracy, F1, precision, recall)   \n",
       "\n",
       "             metric_definition_clean  \\\n",
       "0                           [Reward]   \n",
       "1              [Exact match, Reward]   \n",
       "2                      [Exact match]   \n",
       "3                      [Exact match]   \n",
       "4                      [Exact match]   \n",
       "..                               ...   \n",
       "450  [Human ratings, LLM-as-a-Judge]   \n",
       "451                    [Exact match]   \n",
       "452                    [Exact match]   \n",
       "453        [Exact match, Soft match]   \n",
       "454                    [Exact match]   \n",
       "\n",
       "                                     metric_statistics metric_statistics_clean  \n",
       "0                                          simple mean                  [Mean]  \n",
       "1                                   mean with variance             [Mean, Std]  \n",
       "2                                      simple mean/sum                  [Mean]  \n",
       "3                                                  NaN                     NaN  \n",
       "4    The authors carry out some error analysis: \"We...                 [Other]  \n",
       "..                                                 ...                     ...  \n",
       "450                                                NaN                     NaN  \n",
       "451                                                NaN                     NaN  \n",
       "452                                                NaN                     NaN  \n",
       "453                                                NaN                     NaN  \n",
       "454                                                NaN                     NaN  \n",
       "\n",
       "[455 rows x 18 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[['main_coder','bibkey','title','inclusion','short_summary','phenomenon_taxonomy_root',\n",
    " 'phenomenon_taxonomy_leaf', 'phenomenon_taxonomy_alternate','task_source','task_source_clean','dataset_sampling_method','dataset_sampling_method_clean','response_format','response_format_clean','metric_definition','metric_definition_clean','metric_statistics','metric_statistics_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc31a0",
   "metadata": {},
   "source": [
    "## Recommendation Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb1fc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_percents(df,col):\n",
    "    if col in col_maps.keys():\n",
    "        df[col] = df[col].apply(lambda x: col_maps[col][x] if col_maps[col].get(x) else x)\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    print(df[col].value_counts(dropna=False)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "caee982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phenomenon_defined\n",
      "Yes    348\n",
      "No      99\n",
      "NaN      8\n",
      "Name: count, dtype: int64\n",
      "phenomenon_defined\n",
      "Yes    0.764835\n",
      "No     0.217582\n",
      "NaN    0.017582\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'phenomenon_defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fbad6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phenomenon_contested\n",
      "Contested        225\n",
      "Widely-agreed    203\n",
      "Not defined       27\n",
      "Name: count, dtype: int64\n",
      "phenomenon_contested\n",
      "Contested        0.494505\n",
      "Widely-agreed    0.446154\n",
      "Not defined      0.059341\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'phenomenon_contested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "841cdad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition_integrity\n",
      "Composite phenomenon               278\n",
      "Single cohesive phenomenon         166\n",
      "Authors' description is unclear     10\n",
      "NaN                                  1\n",
      "Name: count, dtype: int64\n",
      "definition_integrity\n",
      "Composite phenomenon               0.610989\n",
      "Single cohesive phenomenon         0.364835\n",
      "Authors' description is unclear    0.021978\n",
      "NaN                                0.002198\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'definition_integrity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e89756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition_scope\n",
      "Subset           253\n",
      "Comprehensive    196\n",
      "NaN                6\n",
      "Name: count, dtype: int64\n",
      "definition_scope\n",
      "Subset           0.556044\n",
      "Comprehensive    0.430769\n",
      "NaN              0.013187\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'definition_scope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84bfd36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metric_statistics\n",
       "simple mean                                                                                                                    80\n",
       "simple mean/sum                                                                                                                29\n",
       "Simple mean                                                                                                                    23\n",
       "Mean                                                                                                                            6\n",
       "Mean,                                                                                                                           4\n",
       "                                                                                                                               ..\n",
       "simple mean. no inferential statistics (even though the LLM-as-judge have fairly low accuracy).                                 1\n",
       "mean with \"error bars from 3 runs at temperature 0.2\" (unsure if this is a standard error or just the range in scores) (17)     1\n",
       "Exact Match (EM), F1 Score (%)                                                                                                  1\n",
       "Mean and standard deviation\\n                                                                                                   1\n",
       "Experiments are repeated 5 times but resulting information onf uncertainty are not reported.                                    1\n",
       "Name: count, Length: 209, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['metric_statistics'].apply(lambda x: metrics_agg_map[x] if x in metrics_map.keys() else x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af6d31d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metric_statistics\n",
       "simple mean                                                                                                                    80\n",
       "simple mean/sum                                                                                                                29\n",
       "Simple mean                                                                                                                    23\n",
       "Mean                                                                                                                            6\n",
       "Mean,                                                                                                                           4\n",
       "                                                                                                                               ..\n",
       "simple mean. no inferential statistics (even though the LLM-as-judge have fairly low accuracy).                                 1\n",
       "mean with \"error bars from 3 runs at temperature 0.2\" (unsure if this is a standard error or just the range in scores) (17)     1\n",
       "Exact Match (EM), F1 Score (%)                                                                                                  1\n",
       "Mean and standard deviation\\n                                                                                                   1\n",
       "Experiments are repeated 5 times but resulting information onf uncertainty are not reported.                                    1\n",
       "Name: count, Length: 209, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['metric_statistics'].apply(lambda x: 'Std' in metrics_agg_map[x] if x in metrics_map.keys() else x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13da667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_sampling_method: Criterion\n",
      "0.46153846153846156\n",
      "dataset_sampling_method: Convenience\n",
      "0.3934065934065934\n",
      "dataset_sampling_method: Random\n",
      "0.17142857142857143\n",
      "dataset_sampling_method: Unknown\n",
      "0.04395604395604396\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in ['dataset_sampling_method: Criterion',\n",
    "       'dataset_sampling_method: Convenience', 'dataset_sampling_method: Random','dataset_sampling_method: Unknown'\n",
    "]:\n",
    "    print(col)\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e16c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_definition: Exact match\n",
      "370\n",
      "0.8131868131868132\n",
      "metric_definition: Human ratings\n",
      "59\n",
      "0.12967032967032968\n",
      "metric_definition: LLM-as-a-Judge\n",
      "78\n",
      "0.17142857142857143\n",
      "metric_definition: LLM post-processing\n",
      "43\n",
      "0.0945054945054945\n",
      "metric_definition: Distribution\n",
      "55\n",
      "0.12087912087912088\n",
      "metric_definition: Correlation\n",
      "23\n",
      "0.05054945054945055\n",
      "metric_definition: Reward\n",
      "40\n",
      "0.08791208791208792\n",
      "metric_definition: Soft match\n",
      "95\n",
      "0.2087912087912088\n",
      "metric_definition: Unknown\n",
      "1\n",
      "0.002197802197802198\n"
     ]
    }
   ],
   "source": [
    "for col in ['metric_definition: Exact match',\n",
    "       'metric_definition: Human ratings', 'metric_definition: LLM-as-a-Judge',\n",
    "       'metric_definition: LLM post-processing',\n",
    "       'metric_definition: Distribution', 'metric_definition: Correlation',\n",
    "       'metric_definition: Reward', 'metric_definition: Soft match',\n",
    "       'metric_definition: Unknown'\n",
    "]:\n",
    "    print(col)\n",
    "    print(included_df[col].sum())\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "570a4523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_source: Author-crafted\n",
      "0.432967032967033\n",
      "task_source: Crowd-sourced\n",
      "0.1912087912087912\n",
      "task_source: Unknown\n",
      "0.008791208791208791\n",
      "task_source: Procedurally-generated\n",
      "0.2681318681318681\n",
      "task_source: Expert-crafted\n",
      "0.12967032967032968\n",
      "task_source: Another benchmark\n",
      "0.42637362637362636\n",
      "task_source: LLM-generated\n",
      "0.3120879120879121\n",
      "task_source: Human exams\n",
      "0.0945054945054945\n",
      "task_source: Real task\n",
      "0.2967032967032967\n"
     ]
    }
   ],
   "source": [
    "for col in ['task_source: Author-crafted','task_source: Crowd-sourced','task_source: Unknown','task_source: Procedurally-generated','task_source: Expert-crafted','task_source: Another benchmark','task_source: LLM-generated','task_source: Human exams','task_source: Real task']:\n",
    "    print(col)\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8471a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_format: Structured\n",
      "96\n",
      "0.210989010989011\n",
      "response_format: Interaction\n",
      "30\n",
      "0.06593406593406594\n",
      "response_format: Multiple choice\n",
      "182\n",
      "0.4\n",
      "response_format: Short free response\n",
      "175\n",
      "0.38461538461538464\n",
      "response_format: Free response\n",
      "211\n",
      "0.46373626373626375\n",
      "response_format: Logits\n",
      "6\n",
      "0.013186813186813187\n",
      "response_format: Unknown\n",
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for col in ['response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown',]:\n",
    "    print(col)\n",
    "    print(included_df[col].sum())\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fd084fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3       True\n",
       "4      False\n",
       "       ...  \n",
       "450     True\n",
       "451    False\n",
       "452     True\n",
       "453     True\n",
       "454    False\n",
       "Length: 455, dtype: bool"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(included_df['response_format: Free response'] | included_df['response_format: Short free response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37e8f7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5340659340659341"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['results_author_validity: Yes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4eb6d510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9318681318681319"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['task_face_validity: Yes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e769e956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9296703296703297"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['metric_face_validity: Yes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e34033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root\n",
       "NLP                      91\n",
       "Reasoning                84\n",
       "Agents                   40\n",
       "Alignment                37\n",
       "Language Modelling       36\n",
       "Code Generation          26\n",
       "VQA                      15\n",
       "Medicine                 15\n",
       "Knowledge                13\n",
       "Retrieval                13\n",
       "Grounding                11\n",
       "User Interaction         10\n",
       "Law                       9\n",
       "Multilinguality           8\n",
       "Instruction Following     8\n",
       "Biology                   7\n",
       "Theory of Mind            5\n",
       "General Science           5\n",
       "Psychology                3\n",
       "Finance                   3\n",
       "Factuality                3\n",
       "LLM as a Judge            3\n",
       "Data Analysis             2\n",
       "General Purpose           2\n",
       "Mental Health             1\n",
       "Business                  1\n",
       "History                   1\n",
       "Chemistry                 1\n",
       "Education                 1\n",
       "Sports                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['phenomenon_taxonomy_root'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9409d7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>...</th>\n",
       "      <th>results_author_validity:</th>\n",
       "      <th>results_author_validity: No</th>\n",
       "      <th>results_author_validity: Yes</th>\n",
       "      <th>task_ecology_clean</th>\n",
       "      <th>task_ecology:</th>\n",
       "      <th>task_ecology: Partial</th>\n",
       "      <th>task_ecology: Representative</th>\n",
       "      <th>task_ecology: Constructed</th>\n",
       "      <th>task_ecology: Complete</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>4/20/2025 1:41:56</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>hallVisoGenderDatasetBenchmarking2023</td>\n",
       "      <td>VisoGender: A dataset for benchmarking gender ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines occupation-related gender ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>4/20/2025 2:45:15</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>hanInstinctiveBiasSpurious2024</td>\n",
       "      <td>The Instinctive Bias: Spurious Images lead to ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the extent to which multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>4/20/2025 8:43:27</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>esiobuROBBIERobustBias2023</td>\n",
       "      <td>ROBBIE: Robust Bias Evaluation of Large Genera...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper attempts to make bias evaluation mo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>4/21/2025 0:11:29</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>felknerWinoQueerCommunityintheloopBenchmark2023</td>\n",
       "      <td>WinoQueer: A Community-in-the-Loop Benchmark f...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines biases in LMs that harm th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>4/21/2025 7:28:55</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>sahooIndiBiasBenchmarkDataset2024</td>\n",
       "      <td>IndiBias: A Benchmark Dataset to Measure Socia...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines LMs' biases and stereotype...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>4/21/2025 9:12:20</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>marchiorimanerbaSocialBiasProbing2024</td>\n",
       "      <td>Social Bias Probing: Fairness Benchmarking for...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines social biases in LMs, focu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>4/21/2025 19:14:13</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>nangiaCrowSpairsChallengeDataset2020</td>\n",
       "      <td>CrowS-Pairs: A Challenge Dataset for Measuring...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines social biases in LMs again...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>4/21/2025 22:52:21</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>morabitoSTOPBenchmarkingLarge2024</td>\n",
       "      <td>STOP! Benchmarking Large Language Models with ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines social biases in LMs as th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>4/22/2025 0:16:12</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>halevyFlexTapeCan`t2024</td>\n",
       "      <td>\"Flex Tape Can't Fix That\": Bias and Misinform...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the extent to which model ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>4/22/2025 6:37:31</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>chenCrosscareAssessingHealthcare2024</td>\n",
       "      <td>Cross-Care: Assessing the Healthcare Implicati...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines how LMs associate disease ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>4/22/2025 8:42:08</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>wanFactualityTaxDiversityintervened2024</td>\n",
       "      <td>The Factuality Tax of Diversity-Intervened Tex...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the question of whether pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>4/22/2025 23:06:39</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>jhaSeeGULLStereotypeBenchmark2023</td>\n",
       "      <td>SeeGULL: A Stereotype Benchmark with Broad Geo...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces SeeGULL, a broad-coverag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Timestamp        main_coder  \\\n",
       "337   4/20/2025 1:41:56  Valentin Hoffman   \n",
       "338   4/20/2025 2:45:15  Valentin Hoffman   \n",
       "340   4/20/2025 8:43:27  Valentin Hoffman   \n",
       "359   4/21/2025 0:11:29  Valentin Hoffman   \n",
       "386   4/21/2025 7:28:55  Valentin Hoffman   \n",
       "387   4/21/2025 9:12:20  Valentin Hoffman   \n",
       "402  4/21/2025 19:14:13  Valentin Hoffman   \n",
       "406  4/21/2025 22:52:21  Valentin Hoffman   \n",
       "413   4/22/2025 0:16:12  Valentin Hoffman   \n",
       "422   4/22/2025 6:37:31  Valentin Hoffman   \n",
       "423   4/22/2025 8:42:08  Valentin Hoffman   \n",
       "426  4/22/2025 23:06:39  Valentin Hoffman   \n",
       "\n",
       "                                              bibkey  \\\n",
       "337            hallVisoGenderDatasetBenchmarking2023   \n",
       "338                   hanInstinctiveBiasSpurious2024   \n",
       "340                       esiobuROBBIERobustBias2023   \n",
       "359  felknerWinoQueerCommunityintheloopBenchmark2023   \n",
       "386                sahooIndiBiasBenchmarkDataset2024   \n",
       "387            marchiorimanerbaSocialBiasProbing2024   \n",
       "402             nangiaCrowSpairsChallengeDataset2020   \n",
       "406                morabitoSTOPBenchmarkingLarge2024   \n",
       "413                          halevyFlexTapeCan`t2024   \n",
       "422             chenCrosscareAssessingHealthcare2024   \n",
       "423          wanFactualityTaxDiversityintervened2024   \n",
       "426                jhaSeeGULLStereotypeBenchmark2023   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "337  VisoGender: A dataset for benchmarking gender ...   Include   \n",
       "338  The Instinctive Bias: Spurious Images lead to ...   Include   \n",
       "340  ROBBIE: Robust Bias Evaluation of Large Genera...   Include   \n",
       "359  WinoQueer: A Community-in-the-Loop Benchmark f...   Include   \n",
       "386  IndiBias: A Benchmark Dataset to Measure Socia...   Include   \n",
       "387  Social Bias Probing: Fairness Benchmarking for...   Include   \n",
       "402  CrowS-Pairs: A Challenge Dataset for Measuring...   Include   \n",
       "406  STOP! Benchmarking Large Language Models with ...   Include   \n",
       "413  \"Flex Tape Can't Fix That\": Bias and Misinform...   Include   \n",
       "422  Cross-Care: Assessing the Healthcare Implicati...   Include   \n",
       "423  The Factuality Tax of Diversity-Intervened Tex...   Include   \n",
       "426  SeeGULL: A Stereotype Benchmark with Broad Geo...   Include   \n",
       "\n",
       "    exclusion_criteria exclusion_criteria_detail  \\\n",
       "337                NaN                       NaN   \n",
       "338                NaN                       NaN   \n",
       "340                NaN                       NaN   \n",
       "359                NaN                       NaN   \n",
       "386                NaN                       NaN   \n",
       "387                NaN                       NaN   \n",
       "402                NaN                       NaN   \n",
       "406                NaN                       NaN   \n",
       "413                NaN                       NaN   \n",
       "422                NaN                       NaN   \n",
       "423                NaN                       NaN   \n",
       "426                NaN                       NaN   \n",
       "\n",
       "                                         short_summary contribution  \\\n",
       "337  This paper examines occupation-related gender ...          NaN   \n",
       "338  This paper examines the extent to which multim...          NaN   \n",
       "340  This paper attempts to make bias evaluation mo...          NaN   \n",
       "359  This paper examines biases in LMs that harm th...          NaN   \n",
       "386  This paper examines LMs' biases and stereotype...          NaN   \n",
       "387  This paper examines social biases in LMs, focu...          NaN   \n",
       "402  This paper examines social biases in LMs again...          NaN   \n",
       "406  This paper examines social biases in LMs as th...          NaN   \n",
       "413  This paper examines the extent to which model ...          NaN   \n",
       "422  This paper examines how LMs associate disease ...          NaN   \n",
       "423  This paper examines the question of whether pr...          NaN   \n",
       "426  This paper introduces SeeGULL, a broad-coverag...          NaN   \n",
       "\n",
       "          phenomenon_short  ... results_author_validity:   \\\n",
       "337  Specific form of bias  ...                     False   \n",
       "338  Specific form of bias  ...                     False   \n",
       "340   General form of bias  ...                     False   \n",
       "359   General form of bias  ...                     False   \n",
       "386   General form of bias  ...                     False   \n",
       "387   General form of bias  ...                     False   \n",
       "402   General form of bias  ...                     False   \n",
       "406   General form of bias  ...                     False   \n",
       "413  Specific form of bias  ...                     False   \n",
       "422  Specific form of bias  ...                     False   \n",
       "423  Specific form of bias  ...                     False   \n",
       "426   General form of bias  ...                     False   \n",
       "\n",
       "    results_author_validity: No results_author_validity: Yes  \\\n",
       "337                        True                        False   \n",
       "338                        True                        False   \n",
       "340                       False                         True   \n",
       "359                       False                         True   \n",
       "386                        True                        False   \n",
       "387                        True                        False   \n",
       "402                       False                         True   \n",
       "406                       False                         True   \n",
       "413                        True                        False   \n",
       "422                        True                        False   \n",
       "423                       False                         True   \n",
       "426                       False                         True   \n",
       "\n",
       "                task_ecology_clean task_ecology:  task_ecology: Partial  \\\n",
       "337  [Representative, Constructed]          False                 False   \n",
       "338                  [Constructed]          False                 False   \n",
       "340               [Representative]          False                 False   \n",
       "359                  [Constructed]          False                 False   \n",
       "386                  [Constructed]          False                 False   \n",
       "387                  [Constructed]          False                 False   \n",
       "402                  [Constructed]          False                 False   \n",
       "406                  [Constructed]          False                 False   \n",
       "413                  [Constructed]          False                 False   \n",
       "422                  [Constructed]          False                 False   \n",
       "423               [Representative]          False                 False   \n",
       "426                  [Constructed]          False                 False   \n",
       "\n",
       "    task_ecology: Representative task_ecology: Constructed  \\\n",
       "337                         True                      True   \n",
       "338                        False                      True   \n",
       "340                         True                     False   \n",
       "359                        False                      True   \n",
       "386                        False                      True   \n",
       "387                        False                      True   \n",
       "402                        False                      True   \n",
       "406                        False                      True   \n",
       "413                        False                      True   \n",
       "422                        False                      True   \n",
       "423                         True                     False   \n",
       "426                        False                      True   \n",
       "\n",
       "    task_ecology: Complete metric_statistics_clean  \n",
       "337                  False                  [Mean]  \n",
       "338                  False                  [Mean]  \n",
       "340                  False                     NaN  \n",
       "359                  False                  [Mean]  \n",
       "386                  False                  [Mean]  \n",
       "387                  False                  [Mean]  \n",
       "402                  False                  [Mean]  \n",
       "406                  False             [Mean, Std]  \n",
       "413                  False                  [Mean]  \n",
       "422                  False                  [Mean]  \n",
       "423                  False                  [Mean]  \n",
       "426                  False                  [Mean]  \n",
       "\n",
       "[12 rows x 128 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[included_df['phenomenon_taxonomy_leaf'] == 'Bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ba811",
   "metadata": {},
   "source": [
    "### Headline Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2dc40f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_map = {'Agents':'Agents',\n",
    "                'Alignment':'Alignment',\n",
    "                  'Reasoning':'Reasoning',\n",
    "                    'Retrieval':'Other',\n",
    "                      'NLP':'NLP',\n",
    "       'Factuality':'Other',\n",
    "         'Knowledge':'Other',\n",
    "           'Language Modelling':'Language Modelling',\n",
    "             'Law':'Domain Applications',\n",
    "       'Code Generation':'Code Generation',\n",
    "         'Multilinguality':'Other',\n",
    "           'Instruction Following':'Other',\n",
    "       'Finance':'Domain Applications',\n",
    "         'Biology':'Domain Applications',\n",
    "           'General Science':'Domain Applications',\n",
    "             'History':'Domain Applications',\n",
    "       'User Interaction':'Other',\n",
    "         'Mental Health':'Domain Applications',\n",
    "           'Domain Applications':'Domain Applications',\n",
    "       'Psychology':'Domain Applications',\n",
    "         'Business':'Domain Applications',\n",
    "           'Data Analysis':'Other',\n",
    "             'Medicine':'Domain Applications',\n",
    "               'Chemistry':'Domain Applications',\n",
    "       'Grounding':'Other',\n",
    "         'VQA':'Other',\n",
    "           'Education':'Domain Applications',\n",
    "             'Theory of Mind':'Other',\n",
    "       'LLM as a Judge':'Other',\n",
    "         'Sports':'Domain Applications'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c324efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root\n",
       "Other                  91\n",
       "NLP                    91\n",
       "Reasoning              84\n",
       "Domain Applications    48\n",
       "Agents                 40\n",
       "Alignment              37\n",
       "Language Modelling     36\n",
       "Code Generation        26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['phenomenon_taxonomy_root'].map(summary_map).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85b6d10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Choice\n",
      "182\n",
      "Short Free Response\n",
      "175\n",
      "Free Response\n",
      "211\n",
      "Structured\n",
      "96\n",
      "Interaction\n",
      "30\n",
      "Logits\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "['response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown',]\n",
    "\n",
    "print('Multiple Choice')\n",
    "print(included_df['response_format: Multiple choice'].sum())\n",
    "print('Short Free Response')\n",
    "print((included_df['response_format: Short free response']).sum())\n",
    "print('Free Response')\n",
    "print((included_df['response_format: Free response']).sum())\n",
    "print('Structured')\n",
    "print(included_df['response_format: Structured'].sum())\n",
    "print('Interaction')\n",
    "print(included_df['response_format: Interaction'].sum())\n",
    "print('Logits')\n",
    "print(included_df['response_format: Logits'].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc9458",
   "metadata": {},
   "source": [
    "## Complete Taxonomy Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24c69789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phenomenon_taxonomy(df, categories, label, caption):\n",
    "    header = \"\"\"\\\\begin{longtable}{p{.02\\\\textwidth}p{.2\\\\textwidth}p{.78\\\\textwidth}}\n",
    "        \\\\textbf{Category} && \\\\textbf{Included Papers} \\\\\\\\\n",
    "        \\\\toprule\n",
    "        \\\\endhead\n",
    "        \\\\bottomrule \\\\\\\\\n",
    "        \\caption{\\\\textbf{\"\"\"+caption+\"\"\"}}\n",
    "        \\label{\"\"\"+label+\"\"\"}\n",
    "        \\\\endlastfoot\n",
    "        \\\\bottomrule\n",
    "        \\\\endfoot\n",
    "    \"\"\"\n",
    "    \n",
    "    footer = \"\"\"\n",
    "        \\end{longtable}\n",
    "    \"\"\"\n",
    "\n",
    "    latex = header\n",
    "\n",
    "    for category in categories:\n",
    "        subcategories = df[df['phenomenon_taxonomy_root'] == category]['phenomenon_taxonomy_leaf'].value_counts(dropna=False).index.tolist()\n",
    "        subcategories = [x for x in subcategories if x != '']\n",
    "        if not all([isinstance(x,str) for x in subcategories]):\n",
    "            temp = [x for x in subcategories if isinstance(x,str)]\n",
    "            subcategories = temp + [x for x in subcategories if not isinstance(x,str)]\n",
    "        if len(subcategories) <= 1:\n",
    "            latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"}} & \"\"\"\n",
    "            papers = df[df['phenomenon_taxonomy_root'] == category]['new_bibkey'].unique()\n",
    "            papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "            latex += ', '.join(papers)+' \\\\\\\\ \\n'\n",
    "        else:\n",
    "            latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"}} & \\\\\\\\ \\n\"\"\"\n",
    "            for subcategory in subcategories:\n",
    "                if isinstance(subcategory,str):\n",
    "                    papers = df[(df['phenomenon_taxonomy_root'] == category) & (df['phenomenon_taxonomy_leaf'] == subcategory)]['new_bibkey'].unique()\n",
    "                    papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "                    latex += '& '+str(subcategory)+' & '+', '.join(papers)+' \\\\\\\\ \\n'\n",
    "                else:   \n",
    "                    other_categories = df['phenomenon_taxonomy_leaf'].apply(lambda x: isinstance(x,str)) & (df['phenomenon_taxonomy_root'] == category)\n",
    "                    papers = df[(df['phenomenon_taxonomy_root'] == category) & ~other_categories]['new_bibkey'].unique()\n",
    "                    papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "                    latex += '& Other & '+', '.join(papers)+' \\\\\\\\ \\n'\n",
    "        latex += \"\"\"\\\\midrule \\n\"\"\"\n",
    "\n",
    "    latex += footer\n",
    "\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac824672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{p{.02\\textwidth}p{.2\\textwidth}p{.78\\textwidth}}\n",
      "        \\textbf{Category} && \\textbf{Included Papers} \\\\\n",
      "        \\toprule\n",
      "        \\endhead\n",
      "        \\bottomrule \\\\\n",
      "        \\caption{\\textbf{Descriptive Taxonomy of LLM Benchmark Target General Phenomena.}}\n",
      "        \\label{tab:phenomena_general}\n",
      "        \\endlastfoot\n",
      "        \\bottomrule\n",
      "        \\endfoot\n",
      "    \\multicolumn{2}{l}{\\textbf{NLP}} & \\\\ \n",
      "& Understanding & \\textcite{senelCoDA21EvaluatingLanguage2022}, \\textcite{pengCOPENProbingConceptual2022a}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{shavrinaRussianSuperGLUERussianLanguage2020}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{zhangHumorAIMassive2024}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{herediaXNLIeuDatasetCrosslingual2024}, \\textcite{maruNibblingHardCore2022}, \\textcite{chenCurriculumBroadcoverageBenchmark2022}, \\textcite{bandarkarBelebeleBenchmarkParallel2024}, \\textcite{zhangMuCGECMultireferenceMultisource2022}, \\textcite{garcia-ferreroThisNotDataset2023a}, \\textcite{kumarVisionlanguageModelsUnderstand2024}, \\textcite{flachsGrammaticalErrorCorrection2020}, \\textcite{royBenchCLAMPBenchmarkEvaluating2023}, \\textcite{shivagundeLargerProbesTell2023}, \\textcite{devriesDUMBBenchmarkSmart2023}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{sheScoNeBenchmarkingNegation2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{aggarwalIndicXNLIEvaluatingMultilingual2022}, \\textcite{kotoLargeLanguageModels2023}, \\textcite{doddapaneniLeavingNoIndic2023}, \\textcite{zhaoCould`veAskedThat2024a}, \\textcite{chiyah-garciaRepairsBlockWorld2024} \\\\ \n",
      "& Long Context & \\textcite{maMMLONGBENCHDOCBenchmarkingLongcontext2024}, \\textcite{kuratovBABILongTestingLimits2024}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangMarathonRaceRealm2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{karpinskaOneThousandOne2024a}, \\textcite{castillo-boladoPromptsDynamicConversational2024}, \\textcite{liuExposingAttentionGlitches2023}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{maharanaEvaluatingVeryLongterm2024} \\\\ \n",
      "& Summarization & \\textcite{mahbubUnveilingEssencePoetry2023}, \\textcite{yeGlobeSummChallengingBenchmark2024}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{asthanaEvaluatingLLMsTargeted2024a}, \\textcite{huangEmbraceDivergenceRicher2024}, \\textcite{amarOpenAspBenchmarkMultidocument2023}, \\textcite{liuRevisitingGoldStandard2023}, \\textcite{cheangCanLMsGeneralize2023}, \\textcite{josephFactPICOFactualityEvaluation2024a}, \\textcite{ryanRevisitingNonEnglishText2023}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{ramprasadAnalyzingLLMBehavior2024a}, \\textcite{zhaoORCHIDChineseDebate2023} \\\\ \n",
      "& Extraction & \\textcite{wangIELMOpenInformation2022a}, \\textcite{qiPreservingKnowledgeInvariance2023}, \\textcite{wangMAVENARGCompletingPuzzle2024}, \\textcite{liTEGDBComprehensiveDataset2024}, \\textcite{duPAGEDBenchmarkProcedural2024}, \\textcite{wangCanLanguageModels2023}, \\textcite{yanComprehensiveStudyTextattributed2023}, \\textcite{liCanLargeLanguage2024}, \\textcite{zhangDTGBComprehensiveBenchmark2024}, \\textcite{merdjanovskaNoiseBenchBenchmarkingImpact2024}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{agrawalLargeLanguageModels2022} \\\\ \n",
      "& Detection & \\textcite{wuDetectRLBenchmarkingLLMgenerated2024}, \\textcite{chakrabortyCounterTuringTest2023}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{mackoMULTITuDELargescaleMultilingual2023} \\\\ \n",
      "& Other & \\textcite{naousReadMeBenchmarkingMultilingual2024}, \\textcite{taktashevaRuBLiMPRussianBenchmark2024}, \\textcite{casolaMultiPICoMultilingualPerspectivist2024a}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{edmanCUTEMeasuringLLMs2024}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{parkOpenKoLLMLeaderboard2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{labanSummEditsMeasuringLLM2023a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Reasoning}} & \\\\ \n",
      "& Logical & \\textcite{helweMAFALDABenchmarkComprehensive2024a}, \\textcite{sanyalRobustLRDiagnosticBenchmark2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{jacoviChainofthoughtStrongIts2024}, \\textcite{huangMetaLogicLogicalReasoning2022}, \\textcite{leeQASAAdvancedQuestion2023}, \\textcite{liWhenLlmsMeet2024}, \\textcite{tianDiagnosingFirstorderLogical2021}, \\textcite{zhouRICAEvaluatingRobust2021}, \\textcite{patelMultiLogiEvalEvaluatingMultistep2024}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{chenTheoremQATheoremdrivenQuestion2023a}, \\textcite{yeAnaloBenchBenchmarkingIdentification2024}, \\textcite{jinCanLargeLanguage2024}, \\textcite{hanFOLIONaturalLanguage2024}, \\textcite{chenPremiseOrderMatters2024} \\\\ \n",
      "& Mathematical & \\textcite{huSportsMetricsBlendingText2024}, \\textcite{liGSMplusComprehensiveBenchmark2024}, \\textcite{shiLargeLanguageModels2023}, \\textcite{kurticMathadorLMDynamicBenchmark2024}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{liFRoGEvaluatingFuzzy2024a}, \\textcite{heOlympiadBenchChallengingBenchmark2024a}, \\textcite{shiLanguageModelsAre2023}, \\textcite{xiongTRIGOBenchmarkingFormal2023a}, \\textcite{friederMathematicalCapabilitiesChatGPT2023a}, \\textcite{mishraNumGLUESuiteFundamental2022a}, \\textcite{paruchuriWhatAreOdds2024}, \\textcite{zhaoDocMathevalEvaluatingMath2024} \\\\ \n",
      "& Planning & \\textcite{nasirGameTraversalBenchmarkEvaluatingPlanning2024a}, \\textcite{valmeekamPlanBenchExtensibleBenchmark2023}, \\textcite{choiLoTabenchBenchmarkingLanguageoriented2024}, \\textcite{heExploringCapacityPretrained2023a}, \\textcite{maAgentBoardAnalyticalEvaluation2024}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{suActPlan1KBenchmarkingProcedural2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{xieTravelPlannerBenchmarkRealworld2024}, \\textcite{chenLLMArenaAssessingCapabilities2024} \\\\ \n",
      "& Compositional & \\textcite{yuksekgonulWhenWhyVisionlanguage2023}, \\textcite{rayColaBenchmarkCompositional2023}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{hsiehSugarCrepeFixingHackable2023}, \\textcite{huangConMeRethinkingEvaluation2024}, \\textcite{hareshClevrSkillsCompositionalLanguage2024} \\\\ \n",
      "& Commonsense & \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{bhargavaDiscoSenseCommonsenseReasoning2022}, \\textcite{spragueMuSRTestingLimits2024}, \\textcite{bittonWinoGAViLGamifiedAssociation2022a}, \\textcite{sunBenchmarkingChineseCommonsense2024} \\\\ \n",
      "& Temporal & \\textcite{suLivingMomentCan2024}, \\textcite{tanBenchmarkingImprovingTemporal2023}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{fierroMuLanStudyFact2024a} \\\\ \n",
      "& Spatial & \\textcite{mirzaeeSPARTQATextualQuestion2021}, \\textcite{wangPictureWorthThousand2024}, \\textcite{comsaBenchmarkReasoningSpatial2023a}, \\textcite{shiriEmpiricalAnalysisSpatial2024} \\\\ \n",
      "& Mathematics & \\textcite{zhaoFinanceMATHKnowledgeintensiveMath2024}, \\textcite{luMathVistaEvaluatingMathematical2024}, \\textcite{zhangCarefulExaminationLarge2024} \\\\ \n",
      "& Multimodal & \\textcite{yingMMTbenchComprehensiveMultimodal2024}, \\textcite{chenM3CoTNovelBenchmark2024} \\\\ \n",
      "& Other & \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{romanouCRABAssessingStrength2023}, \\textcite{ribeiroSTREETMULTITASKSTRUCTURED2023}, \\textcite{liMEQABenchmarkMultihop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{fanNPHardEvalDynamicBenchmark2024}, \\textcite{samdarshiConnectingDotsEvaluating2024}, \\textcite{jiangBRAINTEASERLateralThinking2023}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{huangOlympicArenaBenchmarkingMultidiscipline2024a}, \\textcite{kazemiBoardgameQADatasetNatural2023a}, \\textcite{bhuiyaSeeminglyPlausibleDistractors2024}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{wangSciBenchEvaluatingCollegelevel2024}, \\textcite{chenWeakevalstrongEvaluatingEliciting2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Agents}} & \\\\ \n",
      "& Web & \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{dengMobilebenchEvaluationBenchmark2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{drouinWorkArenaHowCapable2024}, \\textcite{boisvertWorkArenaCompositionalPlanning2024} \\\\ \n",
      "& Tool Use & \\textcite{yeRoTBenchMultilevelBenchmark2024}, \\textcite{huangMetaToolBenchmarkLarge2024}, \\textcite{zhuangToolQADatasetLLM2023}, \\textcite{zhangToolBeHonestMultilevelHallucination2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{basuAPIBLENDComprehensiveCorpora2024}, \\textcite{wangAppBenchPlanningMultiple2024} \\\\ \n",
      "& Coding & \\textcite{mundlerSWTbenchTestingValidating2024a}, \\textcite{wangMINTEvaluatingLLMs2024}, \\textcite{huangMLAgentBenchEvaluatingLanguage2024}, \\textcite{yangInterCodeStandardizingBenchmarking2023}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{mathaiKGymPlatformDataset2024}, \\textcite{jainR2ETurningAny2024} \\\\ \n",
      "& Other & \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{pressCiteMECanLanguage2024a}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{wuSmartPlayBenchmarkLLMs2024}, \\textcite{fanR2HBuildingMultimodal2023}, \\textcite{zhouHAZARDChallengeEmbodied2024}, \\textcite{mialonGAIABenchmarkGeneral2024}, \\textcite{boginSUPEREvaluatingAgents2024}, \\textcite{abdelnabiCooperationCompetitionMaliciousness2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Alignment}} & \\\\ \n",
      "& Alignment & \\textcite{davidsonEvaluatingLanguageModel2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{chaoJailbreakBenchOpenRobustness2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{renValueBenchComprehensivelyEvaluating2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{marraffiniGreatestGoodBenchmark2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{huangFlamesBenchmarkingValue2024}, \\textcite{liuAlignBenchBenchmarkingChinese2024a}, \\textcite{hendrycksAligningAIShared2020a}, \\textcite{panRewardsJustifyMeans2023}, \\textcite{ramamurthyReinforcementLearningNot2023} \\\\ \n",
      "& Bias & \\textcite{hallVisoGenderDatasetBenchmarking2023}, \\textcite{hanInstinctiveBiasSpurious2024}, \\textcite{esiobuROBBIERobustBias2023}, \\textcite{felknerWinoQueerCommunityintheloopBenchmark2023}, \\textcite{sahooIndiBiasBenchmarkDataset2024}, \\textcite{marchiorimanerbaSocialBiasProbing2024}, \\textcite{nangiaCrowSpairsChallengeDataset2020}, \\textcite{morabitoSTOPBenchmarkingLarge2024}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{chenCrosscareAssessingHealthcare2024}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{jhaSeeGULLStereotypeBenchmark2023} \\\\ \n",
      "& Safety & \\textcite{yinSafeWorldGeodiverseSafety2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{alamCTIBenchBenchmarkEvaluating2024}, \\textcite{jinJailbreakingLargeLanguage2024}, \\textcite{dengCOLDBenchmarkChinese2022}, \\textcite{liWMDPBenchmarkMeasuring2024}, \\textcite{toyerTensorTrustInterpretable2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{zhangSafetyBenchEvaluatingSafety2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Language Modelling}} & \\\\ \n",
      "& Robustness & \\textcite{dumpalaSUGARCREPEDatasetVisionlanguage2024}, \\textcite{caoWorstPromptPerformance2024}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{liNaturalBenchEvaluatingVisionlanguage2024}, \\textcite{zhangUnveilingTapestryConsistency2024}, \\textcite{siREADINChineseMultitask2023}, \\textcite{yangCanLargeLanguage2024}, \\textcite{tamkinTaskAmbiguityHumans2023} \\\\ \n",
      "& Updating & \\textcite{dengNewTermBenchmarkingRealtime2024a}, \\textcite{jangTemporalWikiLifelongBenchmark2022a}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{kasaiRealTimeQAWhats2023}, \\textcite{srinivasanCLiMBContinualLearning2022}, \\textcite{yinALCUNALargeLanguage2023a} \\\\ \n",
      "& Hallucination & \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{sunHeadtotailHowKnowledgeable2024a}, \\textcite{pratoLargeLanguageModels2024}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{chenFELMBenchmarkingFactuality2023a} \\\\ \n",
      "& In-context Learning & \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{tanzerBenchmarkLearningTranslate2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{asaiBUFFETBenchmarkingLarge2024} \\\\ \n",
      "& Adaptability & \\textcite{albalakFETABenchmarkFewsample2022a}, \\textcite{itoGeneralizationCapacityNeural2024}, \\textcite{zhouVLUEMultitaskMultidimension2022} \\\\ \n",
      "& Unlearning & \\textcite{jinRWKUBenchmarkingRealworld2024a} \\\\ \n",
      "& Copyright & \\textcite{chenCopyBenchMeasuringLiteral2024} \\\\ \n",
      "& Calibration & \\textcite{yeBenchmarkingLlmsUncertainty2024} \\\\ \n",
      "& Other & \\textcite{tanDevBenchMultimodalDevelopmental2024}, \\textcite{magnussonPalomaBenchmarkEvaluating2024a}, \\textcite{zhangM3ExamMultilingualMultimodal2023a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Code Generation}} & \\\\ \n",
      "& Natural Language & \\textcite{saparinaAMBROSIABenchmarkParsing2024a}, \\textcite{bhaskarBenchmarkingImprovingTexttoSQL2023a}, \\textcite{changDrspiderDiagnosticEvaluation2023}, \\textcite{shahStackEvalBenchmarkingLlms2024a} \\\\ \n",
      "& Other & \\textcite{tangStrucbenchAreLarge2024}, \\textcite{athiwaratkunMultilingualEvaluationCode2023}, \\textcite{liInfiBenchEvaluatingQuestionanswering2024}, \\textcite{duMercuryCodeEfficiency2024}, \\textcite{zhaoQTSummQueryfocusedSummarization2023}, \\textcite{zhangSelenePioneeringAutomated2024}, \\textcite{maSpreadsheetBenchChallengingReal2024}, \\textcite{konIaCevalCodeGeneration2024}, \\textcite{waghjaleECCOCanWe2024}, \\textcite{huangEffiBenchBenchmarkingEfficiency2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{huangDAcodeAgentData2024}, \\textcite{liEvoCodeBenchEvolvingCode2024}, \\textcite{gongEvaluationLLMsSyntaxaware2024}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{liuRepoBenchBenchmarkingRepositorylevel2024}, \\textcite{zhangBenchmarkingDataScience2024}, \\textcite{huInfiAgentDABenchEvaluatingAgents2024}, \\textcite{yinNaturalLanguageCode2023}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{liCanLLMAlready2023a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{VQA}} & \\\\ \n",
      "& Understanding & \\textcite{liMultimodalArXivDataset2024}, \\textcite{zhangMIntRec20LargescaleBenchmark2024} \\\\ \n",
      "& Long Context & \\textcite{wangNeedleMultimodalHaystack2024} \\\\ \n",
      "& Other & \\textcite{chenAreWeRight2024}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{yuMMvetEvaluatingLarge2024}, \\textcite{parcalabescuVALSETaskindependentBenchmark2022}, \\textcite{wuEvaluatingAnalyzingRelationship2024a}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{dasEXAMSVMultidisciplineMultilingual2024}, \\textcite{gingOpenendedVQABenchmarking2024}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{luoCODISBenchmarkingContextdependent2024}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{duEmbSpatialbenchBenchmarkingSpatial2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Knowledge}} & \\\\ \n",
      "& Cultural & \\textcite{huangCevalMultilevelMultidiscipline2023a}, \\textcite{myungBLEnDBenchmarkLlms2024a}, \\textcite{romeroCVQACulturallydiverseMultilingual2024}, \\textcite{bhatiaLocalConceptsUniversals2024}, \\textcite{kannenAestheticsCulturalCompetence2024}, \\textcite{yinGeoMLAMAGeodiverseCommonsense2022a}, \\textcite{caoWenMindComprehensiveBenchmark2024a} \\\\ \n",
      "& General & \\textcite{jiangXFACTRMultilingualFactual2020a}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{wangMMLUproMoreRobust2024} \\\\ \n",
      "& Conflicts & \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{wuClashEvalQuantifyingTugofwar2024a}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Retrieval}} & \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{kalyanWikiDONewBenchmark2024a}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{krojerImageRetrievalContextual2022}, \\textcite{wuSTaRKBenchmarkingLLM2024a}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{monteiroRepLiQAQuestionansweringDataset2024a}, \\textcite{gaoEnablingLargeLanguage2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{pratoEpiKevalEvaluationLanguage2023a}, \\textcite{zhuFanOutQAMultihopMultidocument2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Grounding}} & \\textcite{piUOUOUncontextualizedUncommon2024a}, \\textcite{lyuMMScanMultimodal3D2024}, \\textcite{wangCanLanguageModels2024}, \\textcite{krojerAreDiffusionModels2023a}, \\textcite{kesenViLMAZeroshotBenchmark2024}, \\textcite{chungCanVisualLanguage2024}, \\textcite{luoMMMRSMultimodalMultiGSD2024}, \\textcite{liCanLanguageModels2023}, \\textcite{moneaGlitchMatrixLocating2024a}, \\textcite{guLanguageModelsHave2023}, \\textcite{hanReadingBooksGreat2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{User Interaction}} & \\textcite{kwanMTevalMultiturnCapabilities2024}, \\textcite{panchalWhatSayWhen2024}, \\textcite{liuConvBenchMultiturnConversation2024}, \\textcite{liuMMDUMultiturnMultiimage2024}, \\textcite{chevalierLanguageModelsScience2024}, \\textcite{baiMTbench101FinegrainedBenchmark2024}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ouDialogBenchEvaluatingLLMs2024}, \\textcite{liDiplomatDialogueDataset2023}, \\textcite{tuCharacterEvalChineseBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Multilinguality}} & \\textcite{augustyniakThisWayDesigning2022}, \\textcite{zhangMELAMultilingualEvaluation2024}, \\textcite{etxanizLatxaOpenLanguage2024}, \\textcite{riemenschneiderExploringLargeLanguage2023}, \\textcite{marchisioUnderstandingMitigatingLanguage2024}, \\textcite{zhangXSemPLRCrosslingualSemantic2023}, \\textcite{sunInformalLanguageProcessing2024}, \\textcite{songSLINGSinoLinguistic2022a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Instruction Following}} & \\textcite{xiaFOFOBenchmarkEvaluate2024}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{jiangFollowBenchMultilevelFinegrained2024}, \\textcite{zengEvaluatingLargeLanguage2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{liEvaluatingInstructionfollowingRobustness2024}, \\textcite{wenBenchmarkingComplexInstructionfollowing2024}, \\textcite{abdinKITABEvaluatingLLMs2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Theory of Mind}} & \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{jinMMToMQAMultimodalTheory2024}, \\textcite{gandhiUnderstandingSocialReasoning2023}, \\textcite{chenToMBenchBenchmarkingTheory2024}, \\textcite{xuOpenToMComprehensiveBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Factuality}} & \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{suTextttConflictBankBenchmarkEvaluating2024}, \\textcite{ohERBenchEntityrelationshipBased2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{LLM as a Judge}} & \\textcite{chenMLLMasajudgeAssessingMultimodal2024}, \\textcite{wattsPARIKSHALargescaleInvestigation2024}, \\textcite{lanCriticEvalEvaluatingLargescale2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Data Analysis}} & \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{zhuAreLargeLanguage2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{General Purpose}} & \\textcite{zhengLMSYSchat1MLargescaleRealworld2024}, \\textcite{wangUsercentricMultiintentBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\n",
      "        \\end{longtable}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "categories = included_df['phenomenon_taxonomy_root'].value_counts(dropna=True).index.tolist()\n",
    "specific_categories = [\"Sports\",\"Medicine\",\"Law\",\"Finance\",\"Biology\",\"Chemistry\",\"Education\",\"Mental Health\",\"Psychology\",\"General Science\",\"History\",\"Business\"]\n",
    "categories = [x for x in categories if x not in specific_categories]\n",
    "\n",
    "\n",
    "print(create_phenomenon_taxonomy(included_df, categories,\"tab:phenomena_general\",\"Descriptive Taxonomy of LLM Benchmark Target General Phenomena.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c090e9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{p{.02\\textwidth}p{.2\\textwidth}p{.78\\textwidth}}\n",
      "        \\textbf{Category} && \\textbf{Included Papers} \\\\\n",
      "        \\toprule\n",
      "        \\endhead\n",
      "        \\bottomrule \\\\\n",
      "        \\caption{\\textbf{Descriptive Taxonomy of LLM Benchmark Target Specific Application Phenomena.}}\n",
      "        \\label{tab:phenomena_specific}\n",
      "        \\endlastfoot\n",
      "        \\bottomrule\n",
      "        \\endfoot\n",
      "    \\multicolumn{2}{l}{\\textbf{Sports}} & \\textcite{xiaSportQABenchmarkSports2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Medicine}} & \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{liuRevisitingDeidentificationElectronic2023a}, \\textcite{hanMedSafetyBenchEvaluatingImproving2024}, \\textcite{khandekarMedCalcbenchEvaluatingLarge2024a}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{sivasubramaniamSM3texttoquerySyntheticMultimodel2024a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{xiangCAREMIChineseBenchmark2023a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Law}} & \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{zuoPatentEvalUnderstandingErrors2024}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{chiPLUELanguageUnderstanding2023}, \\textcite{sanchetiAgentspecificDeonticModality2022}, \\textcite{braunAGBDECorpusAutomated2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Finance}} & \\textcite{shahWhenFLUEMeets2022}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{zhaoFinDVerExplainableClaim2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Biology}} & \\textcite{wangPretrainingLanguageModel2023}, \\textcite{xuPEERComprehensiveMultitask2022}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{tsurutaSARSCoV2InteractionDataset2024}, \\textcite{guoCanLlmsSolve2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Chemistry}} & \\textcite{guoWhatCanLarge2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Education}} & \\textcite{chenDrAcademyBenchmarkEvaluating2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Mental Health}} & \\textcite{hengleStillNotQuite2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Psychology}} & \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{sabourEmoBenchEvaluatingEmotional2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{General Science}} & \\textcite{bajpaiCanLLMsReplace2024}, \\textcite{sadatMSciNLIDiverseBenchmark2024a}, \\textcite{liangSceMQAScientificCollege2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{dinhSciExBenchmarkingLarge2024a} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{History}} & \\textcite{hauserLargeLanguageModelsExpertlevel2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Business}} & \\textcite{mitaStrikingGoldAdvertising2024} \\\\ \n",
      "\\midrule \n",
      "\n",
      "        \\end{longtable}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_phenomenon_taxonomy(included_df, specific_categories,\"tab:phenomena_specific\",\"Descriptive Taxonomy of LLM Benchmark Target Specific Application Phenomena.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "09984588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metric_taxonomy(df, categories, label, caption):\n",
    "    header = \"\"\"\\\\begin{longtable}{p{.02\\\\textwidth}p{.2\\\\textwidth}p{.78\\\\textwidth}}\n",
    "        \\\\textbf{Category} && \\\\textbf{Included Papers} \\\\\\\\\n",
    "        \\\\toprule\n",
    "        \\\\endhead\n",
    "        \\\\bottomrule \\\\\\\\\n",
    "        \\caption{\\\\textbf{\"\"\"+caption+\"\"\"}}\n",
    "        \\label{\"\"\"+label+\"\"\"}\n",
    "        \\\\endlastfoot\n",
    "        \\\\bottomrule\n",
    "        \\\\endfoot\n",
    "    \"\"\"\n",
    "    \n",
    "    footer = \"\"\"\n",
    "        \\end{longtable}\n",
    "    \"\"\"\n",
    "\n",
    "    latex = header\n",
    "\n",
    "    for category in categories:\n",
    "        latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"}} & \"\"\"\n",
    "        papers = df[df['metric_definition_clean'].apply(lambda x: category in x)]['new_bibkey'].unique()\n",
    "        papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "        if len(papers) > 200:\n",
    "            papers1 = papers[:200]\n",
    "            papers2 = papers[200:]\n",
    "            latex += ', '.join(papers1)+' \\\\\\\\ \\n'\n",
    "            latex += \"\"\"\\\\midrule \\n\"\"\"\n",
    "            latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"} (Cont.)} & \"\"\"\n",
    "            latex += ', '.join(papers2)+' \\\\\\\\ \\n'\n",
    "        else:\n",
    "            latex += ', '.join(papers)+' \\\\\\\\ \\n'\n",
    "        latex += \"\"\"\\\\midrule \\n\"\"\"\n",
    "    latex += footer\n",
    "\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6de6ee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{p{.02\\textwidth}p{.2\\textwidth}p{.78\\textwidth}}\n",
      "        \\textbf{Category} && \\textbf{Included Papers} \\\\\n",
      "        \\toprule\n",
      "        \\endhead\n",
      "        \\bottomrule \\\\\n",
      "        \\caption{\\textbf{Descriptive Taxonomy of LLM Benchmark Metric Definitions.}}\n",
      "        \\label{tab:metric_definitions}\n",
      "        \\endlastfoot\n",
      "        \\bottomrule\n",
      "        \\endfoot\n",
      "    \\multicolumn{2}{l}{\\textbf{Exact match}} & \\textcite{davidsonEvaluatingLanguageModel2024}, \\textcite{helweMAFALDABenchmarkComprehensive2024a}, \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{wangIELMOpenInformation2022a}, \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{huangCevalMultilevelMultidiscipline2023a}, \\textcite{myungBLEnDBenchmarkLlms2024a}, \\textcite{sanyalRobustLRDiagnosticBenchmark2022a}, \\textcite{albalakFETABenchmarkFewsample2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{nasirGameTraversalBenchmarkEvaluatingPlanning2024a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{yuksekgonulWhenWhyVisionlanguage2023}, \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{saparinaAMBROSIABenchmarkParsing2024a}, \\textcite{augustyniakThisWayDesigning2022}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{wangMINTEvaluatingLLMs2024}, \\textcite{valmeekamPlanBenchExtensibleBenchmark2023}, \\textcite{zhangMELAMultilingualEvaluation2024}, \\textcite{etxanizLatxaOpenLanguage2024}, \\textcite{riemenschneiderExploringLargeLanguage2023}, \\textcite{qiPreservingKnowledgeInvariance2023}, \\textcite{shahWhenFLUEMeets2022}, \\textcite{kalyanWikiDONewBenchmark2024a}, \\textcite{marchisioUnderstandingMitigatingLanguage2024}, \\textcite{itoGeneralizationCapacityNeural2024}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{zhangXSemPLRCrosslingualSemantic2023}, \\textcite{sunInformalLanguageProcessing2024}, \\textcite{wangPretrainingLanguageModel2023}, \\textcite{bajpaiCanLLMsReplace2024}, \\textcite{hauserLargeLanguageModelsExpertlevel2024}, \\textcite{sadatMSciNLIDiverseBenchmark2024a}, \\textcite{dengNewTermBenchmarkingRealtime2024a}, \\textcite{yeRoTBenchMultilevelBenchmark2024}, \\textcite{maMMLONGBENCHDOCBenchmarkingLongcontext2024}, \\textcite{kuratovBABILongTestingLimits2024}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{senelCoDA21EvaluatingLanguage2022}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangMarathonRaceRealm2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{mahbubUnveilingEssencePoetry2023}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{suLivingMomentCan2024}, \\textcite{krojerImageRetrievalContextual2022}, \\textcite{rayColaBenchmarkCompositional2023}, \\textcite{bhaskarBenchmarkingImprovingTexttoSQL2023a}, \\textcite{xuPEERComprehensiveMultitask2022}, \\textcite{jangTemporalWikiLifelongBenchmark2022a}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{huangMetaToolBenchmarkLarge2024}, \\textcite{huSportsMetricsBlendingText2024}, \\textcite{choiLoTabenchBenchmarkingLanguageoriented2024}, \\textcite{songSLINGSinoLinguistic2022a}, \\textcite{pengCOPENProbingConceptual2022a}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{kwanMTevalMultiturnCapabilities2024}, \\textcite{hengleStillNotQuite2024}, \\textcite{shavrinaRussianSuperGLUERussianLanguage2020}, \\textcite{taktashevaRuBLiMPRussianBenchmark2024}, \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{wuSTaRKBenchmarkingLLM2024a}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{wangMAVENARGCompletingPuzzle2024}, \\textcite{jiangFollowBenchMultilevelFinegrained2024}, \\textcite{romanouCRABAssessingStrength2023}, \\textcite{zhaoFinanceMATHKnowledgeintensiveMath2024}, \\textcite{zhaoFinDVerExplainableClaim2024a}, \\textcite{casolaMultiPICoMultilingualPerspectivist2024a}, \\textcite{jinRWKUBenchmarkingRealworld2024a}, \\textcite{jiangXFACTRMultilingualFactual2020a}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{karpinskaOneThousandOne2024a}, \\textcite{suTextttConflictBankBenchmarkEvaluating2024}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{liTEGDBComprehensiveDataset2024}, \\textcite{jacoviChainofthoughtStrongIts2024}, \\textcite{ribeiroSTREETMULTITASKSTRUCTURED2023}, \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{liangSceMQAScientificCollege2024}, \\textcite{zhangHumorAIMassive2024}, \\textcite{liMEQABenchmarkMultihop2024}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{huangMetaLogicLogicalReasoning2022}, \\textcite{mirzaeeSPARTQATextualQuestion2021}, \\textcite{bhargavaDiscoSenseCommonsenseReasoning2022}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{maSpreadsheetBenchChallengingReal2024}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{zhuAreLargeLanguage2024}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{wuClashEvalQuantifyingTugofwar2024a}, \\textcite{pressCiteMECanLanguage2024a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{zhuangToolQADatasetLLM2023}, \\textcite{chenCopyBenchMeasuringLiteral2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{duPAGEDBenchmarkProcedural2024}, \\textcite{zhangToolBeHonestMultilevelHallucination2024}, \\textcite{yeBenchmarkingLlmsUncertainty2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{wangCanLanguageModels2023}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{piUOUOUncontextualizedUncommon2024a}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{changDrspiderDiagnosticEvaluation2023}, \\textcite{zengEvaluatingLargeLanguage2024}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{yanComprehensiveStudyTextattributed2023}, \\textcite{liCanLargeLanguage2024}, \\textcite{zhangDTGBComprehensiveBenchmark2024}, \\textcite{luMathVistaEvaluatingMathematical2024}, \\textcite{liuRevisitingGoldStandard2023}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{dengMobilebenchEvaluationBenchmark2024}, \\textcite{romeroCVQACulturallydiverseMultilingual2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{chenAreWeRight2024}, \\textcite{dumpalaSUGARCREPEDatasetVisionlanguage2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{wangPictureWorthThousand2024}, \\textcite{kasaiRealTimeQAWhats2023}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{tanBenchmarkingImprovingTemporal2023}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{lyuMMScanMultimodal3D2024}, \\textcite{heExploringCapacityPretrained2023a}, \\textcite{edmanCUTEMeasuringLLMs2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{liWhenLlmsMeet2024}, \\textcite{wangNeedleMultimodalHaystack2024}, \\textcite{zhangMIntRec20LargescaleBenchmark2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{huangDAcodeAgentData2024}, \\textcite{parcalabescuVALSETaskindependentBenchmark2022}, \\textcite{huangConMeRethinkingEvaluation2024}, \\textcite{bhatiaLocalConceptsUniversals2024}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{zhangCarefulExaminationLarge2024}, \\textcite{wuEvaluatingAnalyzingRelationship2024a}, \\textcite{liNaturalBenchEvaluatingVisionlanguage2024}, \\textcite{liuRepoBenchBenchmarkingRepositorylevel2024}, \\textcite{zhangBenchmarkingDataScience2024}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{huInfiAgentDABenchEvaluatingAgents2024}, \\textcite{fanNPHardEvalDynamicBenchmark2024}, \\textcite{wangCanLanguageModels2024}, \\textcite{liGSMplusComprehensiveBenchmark2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{castillo-boladoPromptsDynamicConversational2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{srinivasanCLiMBContinualLearning2022}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{samdarshiConnectingDotsEvaluating2024}, \\textcite{herediaXNLIeuDatasetCrosslingual2024}, \\textcite{alamCTIBenchBenchmarkEvaluating2024}, \\textcite{jinJailbreakingLargeLanguage2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{liuExposingAttentionGlitches2023}, \\textcite{merdjanovskaNoiseBenchBenchmarkingImpact2024}, \\textcite{jinMMToMQAMultimodalTheory2024}, \\textcite{dengCOLDBenchmarkChinese2022}, \\textcite{tsurutaSARSCoV2InteractionDataset2024}, \\textcite{maAgentBoardAnalyticalEvaluation2024}, \\textcite{maruNibblingHardCore2022}, \\textcite{chenCurriculumBroadcoverageBenchmark2022}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{dasEXAMSVMultidisciplineMultilingual2024}, \\textcite{jiangBRAINTEASERLateralThinking2023}, \\textcite{gingOpenendedVQABenchmarking2024}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{shiLargeLanguageModels2023}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{comsaBenchmarkReasoningSpatial2023a}, \\textcite{luoCODISBenchmarkingContextdependent2024}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{yinGeoMLAMAGeodiverseCommonsense2022a}, \\textcite{kesenViLMAZeroshotBenchmark2024}, \\textcite{kurticMathadorLMDynamicBenchmark2024}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{gandhiUnderstandingSocialReasoning2023}, \\textcite{bandarkarBelebeleBenchmarkParallel2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Exact match} (Cont.)} & \\textcite{tianDiagnosingFirstorderLogical2021}, \\textcite{zhouRICAEvaluatingRobust2021}, \\textcite{agrawalLargeLanguageModels2022}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{chenToMBenchBenchmarkingTheory2024}, \\textcite{wuDetectRLBenchmarkingLLMgenerated2024}, \\textcite{patelMultiLogiEvalEvaluatingMultistep2024}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{zhangMuCGECMultireferenceMultisource2022}, \\textcite{spragueMuSRTestingLimits2024}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{sabourEmoBenchEvaluatingEmotional2024a}, \\textcite{garcia-ferreroThisNotDataset2023a}, \\textcite{liuRevisitingDeidentificationElectronic2023a}, \\textcite{zhangUnveilingTapestryConsistency2024}, \\textcite{fierroMuLanStudyFact2024a}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{duEmbSpatialbenchBenchmarkingSpatial2024}, \\textcite{chungCanVisualLanguage2024}, \\textcite{kumarVisionlanguageModelsUnderstand2024}, \\textcite{flachsGrammaticalErrorCorrection2020}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{royBenchCLAMPBenchmarkEvaluating2023}, \\textcite{siREADINChineseMultitask2023}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{yinNaturalLanguageCode2023}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{hareshClevrSkillsCompositionalLanguage2024}, \\textcite{shivagundeLargerProbesTell2023}, \\textcite{wuSmartPlayBenchmarkLLMs2024}, \\textcite{liWMDPBenchmarkMeasuring2024}, \\textcite{renValueBenchComprehensivelyEvaluating2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{chakrabortyCounterTuringTest2023}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{devriesDUMBBenchmarkSmart2023}, \\textcite{hsiehSugarCrepeFixingHackable2023}, \\textcite{toyerTensorTrustInterpretable2024}, \\textcite{mackoMULTITuDELargescaleMultilingual2023}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{shiriEmpiricalAnalysisSpatial2024}, \\textcite{yingMMTbenchComprehensiveMultimodal2024}, \\textcite{chenM3CoTNovelBenchmark2024}, \\textcite{ohERBenchEntityrelationshipBased2024a}, \\textcite{zhangM3ExamMultilingualMultimodal2023a}, \\textcite{khandekarMedCalcbenchEvaluatingLarge2024a}, \\textcite{liCanLanguageModels2023}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{sivasubramaniamSM3texttoquerySyntheticMultimodel2024a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{liEvaluatingInstructionfollowingRobustness2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{sheScoNeBenchmarkingNegation2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{yinALCUNALargeLanguage2023a}, \\textcite{drouinWorkArenaHowCapable2024}, \\textcite{hanInstinctiveBiasSpurious2024}, \\textcite{boisvertWorkArenaCompositionalPlanning2024}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{parkOpenKoLLMLeaderboard2024}, \\textcite{fanR2HBuildingMultimodal2023}, \\textcite{chiPLUELanguageUnderstanding2023}, \\textcite{aggarwalIndicXNLIEvaluatingMultilingual2022}, \\textcite{sanchetiAgentspecificDeonticModality2022}, \\textcite{kotoLargeLanguageModels2023}, \\textcite{braunAGBDECorpusAutomated2024}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{doddapaneniLeavingNoIndic2023}, \\textcite{zhouHAZARDChallengeEmbodied2024}, \\textcite{yangCanLargeLanguage2024}, \\textcite{guoCanLlmsSolve2024}, \\textcite{mathaiKGymPlatformDataset2024}, \\textcite{liFRoGEvaluatingFuzzy2024a}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{wenBenchmarkingComplexInstructionfollowing2024}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{tamkinTaskAmbiguityHumans2023}, \\textcite{shiLanguageModelsAre2023}, \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{mialonGAIABenchmarkGeneral2024}, \\textcite{sunHeadtotailHowKnowledgeable2024a}, \\textcite{monteiroRepLiQAQuestionansweringDataset2024a}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{abdinKITABEvaluatingLLMs2024}, \\textcite{xiaSportQABenchmarkSports2024a}, \\textcite{xiongTRIGOBenchmarkingFormal2023a}, \\textcite{chenTheoremQATheoremdrivenQuestion2023a}, \\textcite{huangOlympicArenaBenchmarkingMultidiscipline2024a}, \\textcite{mishraNumGLUESuiteFundamental2022a}, \\textcite{kazemiBoardgameQADatasetNatural2023a}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{xieTravelPlannerBenchmarkRealworld2024}, \\textcite{zhangSafetyBenchEvaluatingSafety2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{basuAPIBLENDComprehensiveCorpora2024}, \\textcite{bhuiyaSeeminglyPlausibleDistractors2024}, \\textcite{pratoLargeLanguageModels2024}, \\textcite{boginSUPEREvaluatingAgents2024}, \\textcite{wangAppBenchPlanningMultiple2024}, \\textcite{gaoEnablingLargeLanguage2023a}, \\textcite{tanDevBenchMultimodalDevelopmental2024}, \\textcite{morabitoSTOPBenchmarkingLarge2024}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{zhaoCould`veAskedThat2024a}, \\textcite{moneaGlitchMatrixLocating2024a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{labanSummEditsMeasuringLLM2023a}, \\textcite{xiangCAREMIChineseBenchmark2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{pratoEpiKevalEvaluationLanguage2023a}, \\textcite{ramprasadAnalyzingLLMBehavior2024a}, \\textcite{chenFELMBenchmarkingFactuality2023a}, \\textcite{lanCriticEvalEvaluatingLargescale2024a}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{hendrycksAligningAIShared2020a}, \\textcite{panRewardsJustifyMeans2023}, \\textcite{wangSciBenchEvaluatingCollegelevel2024}, \\textcite{chenWeakevalstrongEvaluatingEliciting2024}, \\textcite{yeAnaloBenchBenchmarkingIdentification2024}, \\textcite{zhaoORCHIDChineseDebate2023}, \\textcite{paruchuriWhatAreOdds2024}, \\textcite{zhuFanOutQAMultihopMultidocument2024}, \\textcite{zhaoDocMathevalEvaluatingMath2024}, \\textcite{jinCanLargeLanguage2024}, \\textcite{hanFOLIONaturalLanguage2024}, \\textcite{sunBenchmarkingChineseCommonsense2024}, \\textcite{guLanguageModelsHave2023}, \\textcite{jainR2ETurningAny2024}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{ouDialogBenchEvaluatingLLMs2024}, \\textcite{liDiplomatDialogueDataset2023}, \\textcite{xuOpenToMComprehensiveBenchmark2024}, \\textcite{chenPremiseOrderMatters2024}, \\textcite{hanReadingBooksGreat2023}, \\textcite{wangMMLUproMoreRobust2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Human ratings}} & \\textcite{saparinaAMBROSIABenchmarkParsing2024a}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{songSLINGSinoLinguistic2022a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{asthanaEvaluatingLLMsTargeted2024a}, \\textcite{zhaoQTSummQueryfocusedSummarization2023}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{mitaStrikingGoldAdvertising2024}, \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{zhangHumorAIMassive2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{leeQASAAdvancedQuestion2023}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{chenCopyBenchMeasuringLiteral2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{huangEmbraceDivergenceRicher2024}, \\textcite{cheangCanLMsGeneralize2023}, \\textcite{kannenAestheticsCulturalCompetence2024}, \\textcite{zhangCarefulExaminationLarge2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{suActPlan1KBenchmarkingProcedural2024}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{josephFactPICOFactualityEvaluation2024a}, \\textcite{dinhSciExBenchmarkingLarge2024a}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{ryanRevisitingNonEnglishText2023}, \\textcite{zuoPatentEvalUnderstandingErrors2024}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{sivasubramaniamSM3texttoquerySyntheticMultimodel2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{wattsPARIKSHALargescaleInvestigation2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{friederMathematicalCapabilitiesChatGPT2023a}, \\textcite{marraffiniGreatestGoodBenchmark2024}, \\textcite{chevalierLanguageModelsScience2024}, \\textcite{bittonWinoGAViLGamifiedAssociation2022a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{tuCharacterEvalChineseBenchmark2024}, \\textcite{wangUsercentricMultiintentBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{LLM-as-a-Judge}} & \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{xiaFOFOBenchmarkEvaluate2024}, \\textcite{tangStrucbenchAreLarge2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{kwanMTevalMultiturnCapabilities2024}, \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{jiangFollowBenchMultilevelFinegrained2024}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{asthanaEvaluatingLLMsTargeted2024a}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{zhangHumorAIMassive2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{hsiehSugarCrepeFixingHackable2023}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{chenCopyBenchMeasuringLiteral2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{liCanLargeLanguage2024}, \\textcite{cheangCanLMsGeneralize2023}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{caoWorstPromptPerformance2024}, \\textcite{chaoJailbreakBenchOpenRobustness2024}, \\textcite{chenDrAcademyBenchmarkEvaluating2024}, \\textcite{liWhenLlmsMeet2024}, \\textcite{yuMMvetEvaluatingLarge2024}, \\textcite{kannenAestheticsCulturalCompetence2024}, \\textcite{yinSafeWorldGeodiverseSafety2024}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{panchalWhatSayWhen2024}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{liuConvBenchMultiturnConversation2024}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{hanMedSafetyBenchEvaluatingImproving2024}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{liuMMDUMultiturnMultiimage2024}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{wattsPARIKSHALargescaleInvestigation2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{sunHeadtotailHowKnowledgeable2024a}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{chevalierLanguageModelsScience2024}, \\textcite{baiMTbench101FinegrainedBenchmark2024}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{liuAlignBenchBenchmarkingChinese2024a}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{zhengLMSYSchat1MLargescaleRealworld2024}, \\textcite{zhuFanOutQAMultihopMultidocument2024}, \\textcite{shahStackEvalBenchmarkingLlms2024a}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{wangUsercentricMultiintentBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Distribution}} & \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{senelCoDA21EvaluatingLanguage2022}, \\textcite{jangTemporalWikiLifelongBenchmark2022a}, \\textcite{naousReadMeBenchmarkingMultilingual2024}, \\textcite{tanDevBenchMultimodalDevelopmental2024}, \\textcite{taktashevaRuBLiMPRussianBenchmark2024}, \\textcite{wuSTaRKBenchmarkingLLM2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{magnussonPalomaBenchmarkEvaluating2024a}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{mitaStrikingGoldAdvertising2024}, \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{yeBenchmarkingLlmsUncertainty2024}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{bhatiaLocalConceptsUniversals2024}, \\textcite{liGSMplusComprehensiveBenchmark2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{samdarshiConnectingDotsEvaluating2024}, \\textcite{krojerAreDiffusionModels2023a}, \\textcite{huangConMeRethinkingEvaluation2024}, \\textcite{yinGeoMLAMAGeodiverseCommonsense2022a}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{wuDetectRLBenchmarkingLLMgenerated2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{flachsGrammaticalErrorCorrection2020}, \\textcite{shivagundeLargerProbesTell2023}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{chakrabortyCounterTuringTest2023}, \\textcite{luoMMMRSMultimodalMultiGSD2024}, \\textcite{mackoMULTITuDELargescaleMultilingual2023}, \\textcite{liCanLanguageModels2023}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{hallVisoGenderDatasetBenchmarking2023}, \\textcite{esiobuROBBIERobustBias2023}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{kotoLargeLanguageModels2023}, \\textcite{yangCanLargeLanguage2024}, \\textcite{felknerWinoQueerCommunityintheloopBenchmark2023}, \\textcite{sahooIndiBiasBenchmarkDataset2024}, \\textcite{marchiorimanerbaSocialBiasProbing2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{nangiaCrowSpairsChallengeDataset2020}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{lanCriticEvalEvaluatingLargescale2024a}, \\textcite{chenCrosscareAssessingHealthcare2024}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{jhaSeeGULLStereotypeBenchmark2023}, \\textcite{ramamurthyReinforcementLearningNot2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Correlation}} & \\textcite{zhangMELAMultilingualEvaluation2024}, \\textcite{sunInformalLanguageProcessing2024}, \\textcite{wangPretrainingLanguageModel2023}, \\textcite{xuPEERComprehensiveMultitask2022}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{chenCurriculumBroadcoverageBenchmark2022}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{chenMLLMasajudgeAssessingMultimodal2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{mackoMULTITuDELargescaleMultilingual2023}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{ramamurthyReinforcementLearningNot2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Reward}} & \\textcite{mundlerSWTbenchTestingValidating2024a}, \\textcite{davidsonEvaluatingLanguageModel2024}, \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{nasirGameTraversalBenchmarkEvaluatingPlanning2024a}, \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{huangMLAgentBenchEvaluatingLanguage2024}, \\textcite{athiwaratkunMultilingualEvaluationCode2023}, \\textcite{liInfiBenchEvaluatingQuestionanswering2024}, \\textcite{duMercuryCodeEfficiency2024}, \\textcite{zhangSelenePioneeringAutomated2024}, \\textcite{konIaCevalCodeGeneration2024}, \\textcite{waghjaleECCOCanWe2024}, \\textcite{changDrspiderDiagnosticEvaluation2023}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{huangEffiBenchBenchmarkingEfficiency2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{huangDAcodeAgentData2024}, \\textcite{liEvoCodeBenchEvolvingCode2024}, \\textcite{gongEvaluationLLMsSyntaxaware2024}, \\textcite{yangInterCodeStandardizingBenchmarking2023}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{jinJailbreakingLargeLanguage2024}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{yinNaturalLanguageCode2023}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{fanR2HBuildingMultimodal2023}, \\textcite{zhouHAZARDChallengeEmbodied2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{chenLLMArenaAssessingCapabilities2024}, \\textcite{liCanLLMAlready2023a}, \\textcite{panRewardsJustifyMeans2023}, \\textcite{chenWeakevalstrongEvaluatingEliciting2024}, \\textcite{abdelnabiCooperationCompetitionMaliciousness2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Soft match}} & \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{liMultimodalArXivDataset2024}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{zhangXSemPLRCrosslingualSemantic2023}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{mahbubUnveilingEssencePoetry2023}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{yeGlobeSummChallengingBenchmark2024}, \\textcite{liInfiBenchEvaluatingQuestionanswering2024}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{zhaoQTSummQueryfocusedSummarization2023}, \\textcite{mitaStrikingGoldAdvertising2024}, \\textcite{tanzerBenchmarkLearningTranslate2024}, \\textcite{ribeiroSTREETMULTITASKSTRUCTURED2023}, \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{zhangHumorAIMassive2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{leeQASAAdvancedQuestion2023}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{duPAGEDBenchmarkProcedural2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{wangCanLanguageModels2023}, \\textcite{piUOUOUncontextualizedUncommon2024a}, \\textcite{amarOpenAspBenchmarkMultidocument2023}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{lyuMMScanMultimodal3D2024}, \\textcite{wangNeedleMultimodalHaystack2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{liuRepoBenchBenchmarkingRepositorylevel2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{castillo-boladoPromptsDynamicConversational2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{gingOpenendedVQABenchmarking2024}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{suActPlan1KBenchmarkingProcedural2024}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{panchalWhatSayWhen2024}, \\textcite{zhangMuCGECMultireferenceMultisource2022}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{ryanRevisitingNonEnglishText2023}, \\textcite{siREADINChineseMultitask2023}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{guoCanLlmsSolve2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{heOlympiadBenchChallengingBenchmark2024a}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{basuAPIBLENDComprehensiveCorpora2024}, \\textcite{gaoEnablingLargeLanguage2023a}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{chiyah-garciaRepairsBlockWorld2024}, \\textcite{zhaoORCHIDChineseDebate2023}, \\textcite{zhuFanOutQAMultihopMultidocument2024}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{hanReadingBooksGreat2023} \\\\ \n",
      "\\midrule \n",
      "\n",
      "        \\end{longtable}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_metric_taxonomy(included_df, \n",
    "['Exact match','Human ratings','LLM-as-a-Judge','Distribution','Correlation','Reward','Soft match'],'tab:metric_definitions','Descriptive Taxonomy of LLM Benchmark Metric Definitions.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44b14375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task_taxonomy(df, categories, label, caption):\n",
    "    header = \"\"\"\\\\begin{longtable}{p{.02\\\\textwidth}p{.2\\\\textwidth}p{.78\\\\textwidth}}\n",
    "        \\\\textbf{Category} && \\\\textbf{Included Papers} \\\\\\\\\n",
    "        \\\\toprule\n",
    "        \\\\endhead\n",
    "        \\\\bottomrule \\\\\\\\\n",
    "        \\caption{\\\\textbf{\"\"\"+caption+\"\"\"}}\n",
    "        \\label{\"\"\"+label+\"\"\"}\n",
    "        \\\\endlastfoot\n",
    "        \\\\bottomrule\n",
    "        \\\\endfoot\n",
    "    \"\"\"\n",
    "    \n",
    "    footer = \"\"\"\n",
    "        \\end{longtable}\n",
    "    \"\"\"\n",
    "\n",
    "    latex = header\n",
    "\n",
    "    for category in categories:\n",
    "        latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"}} & \"\"\"\n",
    "        papers = df[df['response_format_clean'].apply(lambda x: category in x)]['new_bibkey'].unique()\n",
    "        papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "        latex += ', '.join(papers)+' \\\\\\\\ \\n'\n",
    "        latex += \"\"\"\\\\midrule \\n\"\"\"\n",
    "    latex += footer\n",
    "\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c504547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{p{.02\\textwidth}p{.2\\textwidth}p{.78\\textwidth}}\n",
      "        \\textbf{Category} && \\textbf{Included Papers} \\\\\n",
      "        \\toprule\n",
      "        \\endhead\n",
      "        \\bottomrule \\\\\n",
      "        \\caption{\\textbf{Descriptive Taxonomy of LLM Benchmark Task Definitions.}}\n",
      "        \\label{tab:task_definitions}\n",
      "        \\endlastfoot\n",
      "        \\bottomrule\n",
      "        \\endfoot\n",
      "    \\multicolumn{2}{l}{\\textbf{Structured}} & \\textcite{mundlerSWTbenchTestingValidating2024a}, \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{wangIELMOpenInformation2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{nasirGameTraversalBenchmarkEvaluatingPlanning2024a}, \\textcite{saparinaAMBROSIABenchmarkParsing2024a}, \\textcite{augustyniakThisWayDesigning2022}, \\textcite{xiaFOFOBenchmarkEvaluate2024}, \\textcite{valmeekamPlanBenchExtensibleBenchmark2023}, \\textcite{tangStrucbenchAreLarge2024}, \\textcite{qiPreservingKnowledgeInvariance2023}, \\textcite{shahWhenFLUEMeets2022}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{wangPretrainingLanguageModel2023}, \\textcite{yeRoTBenchMultilevelBenchmark2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{senelCoDA21EvaluatingLanguage2022}, \\textcite{zhangMarathonRaceRealm2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{xuPEERComprehensiveMultitask2022}, \\textcite{huSportsMetricsBlendingText2024}, \\textcite{choiLoTabenchBenchmarkingLanguageoriented2024}, \\textcite{athiwaratkunMultilingualEvaluationCode2023}, \\textcite{duMercuryCodeEfficiency2024}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{wangMAVENARGCompletingPuzzle2024}, \\textcite{zhaoFinanceMATHKnowledgeintensiveMath2024}, \\textcite{zhangSelenePioneeringAutomated2024}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{huangMetaLogicLogicalReasoning2022}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{maSpreadsheetBenchChallengingReal2024}, \\textcite{zhuAreLargeLanguage2024}, \\textcite{konIaCevalCodeGeneration2024}, \\textcite{waghjaleECCOCanWe2024}, \\textcite{zhuangToolQADatasetLLM2023}, \\textcite{zhangToolBeHonestMultilevelHallucination2024}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{dengMobilebenchEvaluationBenchmark2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{huangEffiBenchBenchmarkingEfficiency2024}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{huangDAcodeAgentData2024}, \\textcite{liEvoCodeBenchEvolvingCode2024}, \\textcite{gongEvaluationLLMsSyntaxaware2024}, \\textcite{yangInterCodeStandardizingBenchmarking2023}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{liuRepoBenchBenchmarkingRepositorylevel2024}, \\textcite{zhangBenchmarkingDataScience2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{samdarshiConnectingDotsEvaluating2024}, \\textcite{liuExposingAttentionGlitches2023}, \\textcite{merdjanovskaNoiseBenchBenchmarkingImpact2024}, \\textcite{maAgentBoardAnalyticalEvaluation2024}, \\textcite{suActPlan1KBenchmarkingProcedural2024}, \\textcite{agrawalLargeLanguageModels2022}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{liuRevisitingDeidentificationElectronic2023a}, \\textcite{dinhSciExBenchmarkingLarge2024a}, \\textcite{royBenchCLAMPBenchmarkEvaluating2023}, \\textcite{siREADINChineseMultitask2023}, \\textcite{yinNaturalLanguageCode2023}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{sivasubramaniamSM3texttoquerySyntheticMultimodel2024a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{guoCanLlmsSolve2024}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{xiongTRIGOBenchmarkingFormal2023a}, \\textcite{xieTravelPlannerBenchmarkRealworld2024}, \\textcite{basuAPIBLENDComprehensiveCorpora2024}, \\textcite{wangAppBenchPlanningMultiple2024}, \\textcite{liCanLLMAlready2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{ramprasadAnalyzingLLMBehavior2024a}, \\textcite{chenFELMBenchmarkingFactuality2023a}, \\textcite{lanCriticEvalEvaluatingLargescale2024a}, \\textcite{kotturSIMMC20Taskoriented2021} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Interaction}} & \\textcite{davidsonEvaluatingLanguageModel2024}, \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{wangMINTEvaluatingLLMs2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{dengMobilebenchEvaluationBenchmark2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{yangInterCodeStandardizingBenchmarking2023}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{panchalWhatSayWhen2024}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{hareshClevrSkillsCompositionalLanguage2024}, \\textcite{wuSmartPlayBenchmarkLLMs2024}, \\textcite{liMediQQuestionaskingLlms2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{drouinWorkArenaHowCapable2024}, \\textcite{boisvertWorkArenaCompositionalPlanning2024}, \\textcite{chenLLMArenaAssessingCapabilities2024}, \\textcite{chenWeakevalstrongEvaluatingEliciting2024}, \\textcite{jainR2ETurningAny2024}, \\textcite{tuCharacterEvalChineseBenchmark2024}, \\textcite{abdelnabiCooperationCompetitionMaliciousness2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Multiple choice}} & \\textcite{helweMAFALDABenchmarkComprehensive2024a}, \\textcite{huangCevalMultilevelMultidiscipline2023a}, \\textcite{myungBLEnDBenchmarkLlms2024a}, \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{sanyalRobustLRDiagnosticBenchmark2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{yuksekgonulWhenWhyVisionlanguage2023}, \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{zhangMELAMultilingualEvaluation2024}, \\textcite{etxanizLatxaOpenLanguage2024}, \\textcite{riemenschneiderExploringLargeLanguage2023}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{sunInformalLanguageProcessing2024}, \\textcite{bajpaiCanLLMsReplace2024}, \\textcite{hauserLargeLanguageModelsExpertlevel2024}, \\textcite{sadatMSciNLIDiverseBenchmark2024a}, \\textcite{dengNewTermBenchmarkingRealtime2024a}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{rayColaBenchmarkCompositional2023}, \\textcite{songSLINGSinoLinguistic2022a}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{naousReadMeBenchmarkingMultilingual2024}, \\textcite{hengleStillNotQuite2024}, \\textcite{shavrinaRussianSuperGLUERussianLanguage2020}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{romanouCRABAssessingStrength2023}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{karpinskaOneThousandOne2024a}, \\textcite{suTextttConflictBankBenchmarkEvaluating2024}, \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{jacoviChainofthoughtStrongIts2024}, \\textcite{liangSceMQAScientificCollege2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{mirzaeeSPARTQATextualQuestion2021}, \\textcite{bhargavaDiscoSenseCommonsenseReasoning2022}, \\textcite{hsiehSugarCrepeFixingHackable2023}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{yeBenchmarkingLlmsUncertainty2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{zengEvaluatingLargeLanguage2024}, \\textcite{yanComprehensiveStudyTextattributed2023}, \\textcite{zhangDTGBComprehensiveBenchmark2024}, \\textcite{luMathVistaEvaluatingMathematical2024}, \\textcite{liuRevisitingGoldStandard2023}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{romeroCVQACulturallydiverseMultilingual2024}, \\textcite{chenAreWeRight2024}, \\textcite{dumpalaSUGARCREPEDatasetVisionlanguage2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{wangPictureWorthThousand2024}, \\textcite{tanBenchmarkingImprovingTemporal2023}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{liWhenLlmsMeet2024}, \\textcite{wangNeedleMultimodalHaystack2024}, \\textcite{zhangMIntRec20LargescaleBenchmark2024}, \\textcite{parcalabescuVALSETaskindependentBenchmark2022}, \\textcite{huangConMeRethinkingEvaluation2024}, \\textcite{bhatiaLocalConceptsUniversals2024}, \\textcite{liNaturalBenchEvaluatingVisionlanguage2024}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{alamCTIBenchBenchmarkEvaluating2024}, \\textcite{jinJailbreakingLargeLanguage2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{jinMMToMQAMultimodalTheory2024}, \\textcite{dengCOLDBenchmarkChinese2022}, \\textcite{tsurutaSARSCoV2InteractionDataset2024}, \\textcite{maruNibblingHardCore2022}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{dasEXAMSVMultidisciplineMultilingual2024}, \\textcite{jiangBRAINTEASERLateralThinking2023}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{kesenViLMAZeroshotBenchmark2024}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{gandhiUnderstandingSocialReasoning2023}, \\textcite{bandarkarBelebeleBenchmarkParallel2024}, \\textcite{tianDiagnosingFirstorderLogical2021}, \\textcite{zhouRICAEvaluatingRobust2021}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{chenToMBenchBenchmarkingTheory2024}, \\textcite{wuDetectRLBenchmarkingLLMgenerated2024}, \\textcite{patelMultiLogiEvalEvaluatingMultistep2024}, \\textcite{spragueMuSRTestingLimits2024}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{sabourEmoBenchEvaluatingEmotional2024a}, \\textcite{zhangUnveilingTapestryConsistency2024}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{duEmbSpatialbenchBenchmarkingSpatial2024}, \\textcite{kumarVisionlanguageModelsUnderstand2024}, \\textcite{siREADINChineseMultitask2023}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{liWMDPBenchmarkMeasuring2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{chakrabortyCounterTuringTest2023}, \\textcite{mackoMULTITuDELargescaleMultilingual2023}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{shiriEmpiricalAnalysisSpatial2024}, \\textcite{yingMMTbenchComprehensiveMultimodal2024}, \\textcite{chenM3CoTNovelBenchmark2024}, \\textcite{zhangM3ExamMultilingualMultimodal2023a}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{wangCMBComprehensiveMedical2024a}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{sheScoNeBenchmarkingNegation2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{yinALCUNALargeLanguage2023a}, \\textcite{hallVisoGenderDatasetBenchmarking2023}, \\textcite{hanInstinctiveBiasSpurious2024}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{parkOpenKoLLMLeaderboard2024}, \\textcite{chiPLUELanguageUnderstanding2023}, \\textcite{aggarwalIndicXNLIEvaluatingMultilingual2022}, \\textcite{sanchetiAgentspecificDeonticModality2022}, \\textcite{kotoLargeLanguageModels2023}, \\textcite{braunAGBDECorpusAutomated2024}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{doddapaneniLeavingNoIndic2023}, \\textcite{zhouHAZARDChallengeEmbodied2024}, \\textcite{liFRoGEvaluatingFuzzy2024a}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{mialonGAIABenchmarkGeneral2024}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{xiaSportQABenchmarkSports2024a}, \\textcite{huangOlympicArenaBenchmarkingMultidiscipline2024a}, \\textcite{marraffiniGreatestGoodBenchmark2024}, \\textcite{zhangSafetyBenchEvaluatingSafety2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{morabitoSTOPBenchmarkingLarge2024}, \\textcite{moneaGlitchMatrixLocating2024a}, \\textcite{labanSummEditsMeasuringLLM2023a}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{jhaSeeGULLStereotypeBenchmark2023}, \\textcite{hendrycksAligningAIShared2020a}, \\textcite{panRewardsJustifyMeans2023}, \\textcite{yeAnaloBenchBenchmarkingIdentification2024}, \\textcite{zhaoORCHIDChineseDebate2023}, \\textcite{jinCanLargeLanguage2024}, \\textcite{hanFOLIONaturalLanguage2024}, \\textcite{sunBenchmarkingChineseCommonsense2024}, \\textcite{guLanguageModelsHave2023}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ouDialogBenchEvaluatingLLMs2024}, \\textcite{liDiplomatDialogueDataset2023}, \\textcite{xuOpenToMComprehensiveBenchmark2024}, \\textcite{hanReadingBooksGreat2023}, \\textcite{wangMMLUproMoreRobust2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Short free response}} & \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{myungBLEnDBenchmarkLlms2024a}, \\textcite{albalakFETABenchmarkFewsample2022a}, \\textcite{beanLINGOLYBenchmarkOlympiadlevel2024a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{yuksekgonulWhenWhyVisionlanguage2023}, \\textcite{augustyniakThisWayDesigning2022}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{wangMINTEvaluatingLLMs2024}, \\textcite{shahWhenFLUEMeets2022}, \\textcite{kalyanWikiDONewBenchmark2024a}, \\textcite{itoGeneralizationCapacityNeural2024}, \\textcite{liMultimodalArXivDataset2024}, \\textcite{maMMLONGBENCHDOCBenchmarkingLongcontext2024}, \\textcite{kuratovBABILongTestingLimits2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{suLivingMomentCan2024}, \\textcite{krojerImageRetrievalContextual2022}, \\textcite{rayColaBenchmarkCompositional2023}, \\textcite{jangTemporalWikiLifelongBenchmark2022a}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{pengCOPENProbingConceptual2022a}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{tanDevBenchMultimodalDevelopmental2024}, \\textcite{shavrinaRussianSuperGLUERussianLanguage2020}, \\textcite{taktashevaRuBLiMPRussianBenchmark2024}, \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{wuSTaRKBenchmarkingLLM2024a}, \\textcite{krumdickBizBenchQuantitativeReasoning2024a}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{casolaMultiPICoMultilingualPerspectivist2024a}, \\textcite{jinRWKUBenchmarkingRealworld2024a}, \\textcite{jiangXFACTRMultilingualFactual2020a}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024a}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{karpinskaOneThousandOne2024a}, \\textcite{suTextttConflictBankBenchmarkEvaluating2024}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{liangSceMQAScientificCollege2024}, \\textcite{liMEQABenchmarkMultihop2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{zhuAreLargeLanguage2024}, \\textcite{houWikiContradictBenchmarkEvaluating2024a}, \\textcite{wuClashEvalQuantifyingTugofwar2024a}, \\textcite{pressCiteMECanLanguage2024a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{zhangToolBeHonestMultilevelHallucination2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{wangCanLanguageModels2023}, \\textcite{piUOUOUncontextualizedUncommon2024a}, \\textcite{luMathVistaEvaluatingMathematical2024}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{kasaiRealTimeQAWhats2023}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{heExploringCapacityPretrained2023a}, \\textcite{edmanCUTEMeasuringLLMs2024}, \\textcite{wangNeedleMultimodalHaystack2024}, \\textcite{yuMMvetEvaluatingLarge2024}, \\textcite{kannenAestheticsCulturalCompetence2024}, \\textcite{zhangCarefulExaminationLarge2024}, \\textcite{wuEvaluatingAnalyzingRelationship2024a}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{huInfiAgentDABenchEvaluatingAgents2024}, \\textcite{fanNPHardEvalDynamicBenchmark2024}, \\textcite{liGSMplusComprehensiveBenchmark2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{castillo-boladoPromptsDynamicConversational2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{srinivasanCLiMBContinualLearning2022}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{herediaXNLIeuDatasetCrosslingual2024}, \\textcite{alamCTIBenchBenchmarkEvaluating2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{liuExposingAttentionGlitches2023}, \\textcite{maAgentBoardAnalyticalEvaluation2024}, \\textcite{chenCurriculumBroadcoverageBenchmark2022}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{gingOpenendedVQABenchmarking2024}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{shiLargeLanguageModels2023}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{comsaBenchmarkReasoningSpatial2023a}, \\textcite{luoCODISBenchmarkingContextdependent2024}, \\textcite{yinGeoMLAMAGeodiverseCommonsense2022a}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{chenMLLMasajudgeAssessingMultimodal2024}, \\textcite{zhouRICAEvaluatingRobust2021}, \\textcite{agrawalLargeLanguageModels2022}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{garcia-ferreroThisNotDataset2023a}, \\textcite{zhangUnveilingTapestryConsistency2024}, \\textcite{fierroMuLanStudyFact2024a}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{siREADINChineseMultitask2023}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{shivagundeLargerProbesTell2023}, \\textcite{renValueBenchComprehensivelyEvaluating2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{devriesDUMBBenchmarkSmart2023}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{khandekarMedCalcbenchEvaluatingLarge2024a}, \\textcite{liCanLanguageModels2023}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{yinALCUNALargeLanguage2023a}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{parkOpenKoLLMLeaderboard2024}, \\textcite{chiPLUELanguageUnderstanding2023}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{yangCanLargeLanguage2024}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{tamkinTaskAmbiguityHumans2023}, \\textcite{shiLanguageModelsAre2023}, \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{sunHeadtotailHowKnowledgeable2024a}, \\textcite{monteiroRepLiQAQuestionansweringDataset2024a}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenTheoremQATheoremdrivenQuestion2023a}, \\textcite{huangOlympicArenaBenchmarkingMultidiscipline2024a}, \\textcite{mishraNumGLUESuiteFundamental2022a}, \\textcite{kazemiBoardgameQADatasetNatural2023a}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{bhuiyaSeeminglyPlausibleDistractors2024}, \\textcite{dumpalaSUGARCREPEDatasetVisionlanguage2024}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{moneaGlitchMatrixLocating2024a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{hendrycksAligningAIShared2020a}, \\textcite{wangSciBenchEvaluatingCollegelevel2024}, \\textcite{chiyah-garciaRepairsBlockWorld2024}, \\textcite{paruchuriWhatAreOdds2024}, \\textcite{zhuFanOutQAMultihopMultidocument2024}, \\textcite{zhaoDocMathevalEvaluatingMath2024}, \\textcite{liDiplomatDialogueDataset2023}, \\textcite{chenPremiseOrderMatters2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Free response}} & \\textcite{niuRAGTruthHallucinationCorpus2024a}, \\textcite{heTGEAErrorannotatedDataset2021a}, \\textcite{yaoWebShopScalableRealworld2022a}, \\textcite{feiLawBenchBenchmarkingLegal2024a}, \\textcite{huiUDABenchmarkSuite2024a}, \\textcite{valmeekamPlanBenchExtensibleBenchmark2023}, \\textcite{marchisioUnderstandingMitigatingLanguage2024}, \\textcite{liMultimodalArXivDataset2024}, \\textcite{zhangXSemPLRCrosslingualSemantic2023}, \\textcite{yeRoTBenchMultilevelBenchmark2024}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{mahbubUnveilingEssencePoetry2023}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{bhaskarBenchmarkingImprovingTexttoSQL2023a}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{huangMetaToolBenchmarkLarge2024}, \\textcite{huangMLAgentBenchEvaluatingLanguage2024}, \\textcite{yeGlobeSummChallengingBenchmark2024}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{kwanMTevalMultiturnCapabilities2024}, \\textcite{liInfiBenchEvaluatingQuestionanswering2024}, \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{jiangFollowBenchMultilevelFinegrained2024}, \\textcite{zhaoFinanceMATHKnowledgeintensiveMath2024}, \\textcite{zhaoFinDVerExplainableClaim2024a}, \\textcite{magnussonPalomaBenchmarkEvaluating2024a}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{asthanaEvaluatingLLMsTargeted2024a}, \\textcite{zhaoQTSummQueryfocusedSummarization2023}, \\textcite{yangCRAGComprehensiveRAG2024a}, \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{liTEGDBComprehensiveDataset2024}, \\textcite{mitaStrikingGoldAdvertising2024}, \\textcite{tanzerBenchmarkLearningTranslate2024}, \\textcite{ribeiroSTREETMULTITASKSTRUCTURED2023}, \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{zhangHumorAIMassive2024}, \\textcite{liMEQABenchmarkMultihop2024}, \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{huangMetaLogicLogicalReasoning2022}, \\textcite{leeQASAAdvancedQuestion2023}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{pressCiteMECanLanguage2024a}, \\textcite{jinShoppingMMLUMassive2024}, \\textcite{zhuangToolQADatasetLLM2023}, \\textcite{chenCopyBenchMeasuringLiteral2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{heMedEvalMultilevelMultitask2023a}, \\textcite{duPAGEDBenchmarkProcedural2024}, \\textcite{guoWhatCanLarge2023}, \\textcite{wangCanLanguageModels2023}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{changDrspiderDiagnosticEvaluation2023}, \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{liCanLargeLanguage2024}, \\textcite{zhangDTGBComprehensiveBenchmark2024}, \\textcite{huangEmbraceDivergenceRicher2024}, \\textcite{amarOpenAspBenchmarkMultidocument2023}, \\textcite{cheangCanLMsGeneralize2023}, \\textcite{romeroCVQACulturallydiverseMultilingual2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{caoWorstPromptPerformance2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{huangEffiBenchBenchmarkingEfficiency2024}, \\textcite{chaoJailbreakBenchOpenRobustness2024}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{chenDrAcademyBenchmarkEvaluating2024}, \\textcite{lyuMMScanMultimodal3D2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{liWhenLlmsMeet2024}, \\textcite{yuMMvetEvaluatingLarge2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{yinSafeWorldGeodiverseSafety2024}, \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{wangCanLanguageModels2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{zhouVLUEMultitaskMultidimension2022}, \\textcite{krojerAreDiffusionModels2023a}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{kurticMathadorLMDynamicBenchmark2024}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{panchalWhatSayWhen2024}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{zhangMuCGECMultireferenceMultisource2022}, \\textcite{caoWenMindComprehensiveBenchmark2024a}, \\textcite{ouyangCliMedBenchLargescaleChinese2024a}, \\textcite{josephFactPICOFactualityEvaluation2024a}, \\textcite{liuConvBenchMultiturnConversation2024}, \\textcite{dinhSciExBenchmarkingLarge2024a}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{chungCanVisualLanguage2024}, \\textcite{flachsGrammaticalErrorCorrection2020}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{royBenchCLAMPBenchmarkEvaluating2023}, \\textcite{ryanRevisitingNonEnglishText2023}, \\textcite{siREADINChineseMultitask2023}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{zuoPatentEvalUnderstandingErrors2024}, \\textcite{asaiBUFFETBenchmarkingLarge2024}, \\textcite{hanMedSafetyBenchEvaluatingImproving2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{luoMMMRSMultimodalMultiGSD2024}, \\textcite{devriesDUMBBenchmarkSmart2023}, \\textcite{toyerTensorTrustInterpretable2024}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{hareshClevrSkillsCompositionalLanguage2024}, \\textcite{ohERBenchEntityrelationshipBased2024a}, \\textcite{liuMMDUMultiturnMultiimage2024}, \\textcite{wuMedJourneyBenchmarkEvaluation2024a}, \\textcite{fenogenovaMERAComprehensiveLLM2024a}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023a}, \\textcite{liuBenchmarkingLargeLanguage2023a}, \\textcite{xiaCARESComprehensiveBenchmark2024a}, \\textcite{kweonEHRNoteQALLMBenchmark2024a}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{liuLargeLanguageModels2024a}, \\textcite{sunMeasuringEffectInfluential2023a}, \\textcite{liLexEvalComprehensiveChinese2024a}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{liEvaluatingInstructionfollowingRobustness2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{sheScoNeBenchmarkingNegation2023a}, \\textcite{liuWe`reAfraidLanguage2023a}, \\textcite{drouinWorkArenaHowCapable2024}, \\textcite{hallVisoGenderDatasetBenchmarking2023}, \\textcite{wattsPARIKSHALargescaleInvestigation2024}, \\textcite{esiobuROBBIERobustBias2023}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{fanR2HBuildingMultimodal2023}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{yangCanLargeLanguage2024}, \\textcite{mathaiKGymPlatformDataset2024}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{wenBenchmarkingComplexInstructionfollowing2024}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{heOlympiadBenchChallengingBenchmark2024a}, \\textcite{liHaluEvalLargescaleHallucination2023a}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024a}, \\textcite{chenExploringPotentialLarge2024a}, \\textcite{abdinKITABEvaluatingLLMs2024}, \\textcite{friederMathematicalCapabilitiesChatGPT2023a}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{xieTravelPlannerBenchmarkRealworld2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{chevalierLanguageModelsScience2024}, \\textcite{pratoLargeLanguageModels2024}, \\textcite{huangFlamesBenchmarkingValue2024}, \\textcite{baiMTbench101FinegrainedBenchmark2024}, \\textcite{boginSUPEREvaluatingAgents2024}, \\textcite{gaoEnablingLargeLanguage2023a}, \\textcite{liangUHGEvalBenchmarkingHallucination2024a}, \\textcite{diaoDoolittleBenchmarksCorpora2023a}, \\textcite{bittonWinoGAViLGamifiedAssociation2022a}, \\textcite{zhaoCould`veAskedThat2024a}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{xiangCAREMIChineseBenchmark2023a}, \\textcite{buchmannAttributeAbstainLarge2024a}, \\textcite{pratoEpiKevalEvaluationLanguage2023a}, \\textcite{liuAlignBenchBenchmarkingChinese2024a}, \\textcite{ramprasadAnalyzingLLMBehavior2024a}, \\textcite{lanCriticEvalEvaluatingLargescale2024a}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{maharanaEvaluatingVeryLongterm2024}, \\textcite{zhengLMSYSchat1MLargescaleRealworld2024}, \\textcite{zhaoORCHIDChineseDebate2023}, \\textcite{shahStackEvalBenchmarkingLlms2024a}, \\textcite{jainR2ETurningAny2024}, \\textcite{kotturSIMMC20Taskoriented2021}, \\textcite{ramamurthyReinforcementLearningNot2023}, \\textcite{wangUsercentricMultiintentBenchmark2024}, \\textcite{hanReadingBooksGreat2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Logits}} & \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{felknerWinoQueerCommunityintheloopBenchmark2023}, \\textcite{sahooIndiBiasBenchmarkDataset2024}, \\textcite{marchiorimanerbaSocialBiasProbing2024}, \\textcite{nangiaCrowSpairsChallengeDataset2020}, \\textcite{chenCrosscareAssessingHealthcare2024} \\\\ \n",
      "\\midrule \n",
      "\n",
      "        \\end{longtable}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_task_taxonomy(included_df,\n",
    "['Structured','Interaction','Multiple choice','Short free response','Free response','Logits'],'tab:task_definitions','Descriptive Taxonomy of LLM Benchmark Task Definitions.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
