{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c3ed0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5bddd6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df = pd.read_csv('../data/coding_responses.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "46fd325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df = codebook_df[codebook_df['Timestamp'].apply(lambda x: datetime.strptime(x,\"%m/%d/%Y %H:%M:%S\") > datetime.strptime('2025-04-05', \"%Y-%m-%d\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "91900f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f5df24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_df.columns = pd.Series(codebook_df.columns).apply(lambda x: x.split(\":\")[0])\n",
    "codebook_df.drop(['Column 1'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4ed8991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>...</th>\n",
       "      <th>results_author_validity</th>\n",
       "      <th>results_author_validity_detail</th>\n",
       "      <th>metric_statistics</th>\n",
       "      <th>metric_access</th>\n",
       "      <th>task_ecology</th>\n",
       "      <th>task_ecology_detail</th>\n",
       "      <th>definition_integrity</th>\n",
       "      <th>definition_integrity_detail</th>\n",
       "      <th>task_dataset_size_detail</th>\n",
       "      <th>metric_fewshot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4/8/2025 15:27:11</td>\n",
       "      <td>Harry Mayne</td>\n",
       "      <td>mundlerSWTBenchTestingValidating2024</td>\n",
       "      <td>SWT-Bench: Testing and Validating Real-World B...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A benchmark for generating code tests (unit te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Limitations in how the phenomenon was operatio...</td>\n",
       "      <td>simple mean</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Complete real task (e.g. providing medical adv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single cohesive phenomenon</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4/8/2025 15:57:43</td>\n",
       "      <td>Jonathan Rystrøm</td>\n",
       "      <td>yangLLMCBenchBenchmarkingLarge2024</td>\n",
       "      <td>LLMCBench: Benchmarking Large Language Model C...</td>\n",
       "      <td>Exclude</td>\n",
       "      <td>Topic Exclusion (Is the paper about measuring ...</td>\n",
       "      <td>It's about compression algorithms rather than ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4/8/2025 16:50:41</td>\n",
       "      <td>Jonathan Rystrøm</td>\n",
       "      <td>davidsonEvaluatingLanguageModel2024</td>\n",
       "      <td>EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces a dynamic framework for e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mean with variance</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Constructed task (e.g. predicting medical diag...</td>\n",
       "      <td>The tasks simulates agent negotiations (so no ...</td>\n",
       "      <td>Composite phenomenon</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4/8/2025 17:08:34</td>\n",
       "      <td>Lennart Luettgau</td>\n",
       "      <td>helweMAFALDABenchmarkComprehensive2024</td>\n",
       "      <td>MAFALDA: A Benchmark and Comprehensive Study o...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces MAFALD, a benchmark that ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>simple mean/sum</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Representative task (e.g. answering medical li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Composite phenomenon</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4/8/2025 18:20:47</td>\n",
       "      <td>Kaili Liu</td>\n",
       "      <td>niuRAGTruthHallucinationCorpus2024</td>\n",
       "      <td>RAGTruth: A Hallucination Corpus for Developin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper targets word-level hallucinations i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Benchmark statistics and quality checking are ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Outputs alone</td>\n",
       "      <td>Complete real task (e.g. providing medical adv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Composite phenomenon</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp        main_coder  \\\n",
       "13  4/8/2025 15:27:11       Harry Mayne   \n",
       "14  4/8/2025 15:57:43  Jonathan Rystrøm   \n",
       "15  4/8/2025 16:50:41  Jonathan Rystrøm   \n",
       "16  4/8/2025 17:08:34  Lennart Luettgau   \n",
       "17  4/8/2025 18:20:47         Kaili Liu   \n",
       "\n",
       "                                    bibkey  \\\n",
       "13    mundlerSWTBenchTestingValidating2024   \n",
       "14      yangLLMCBenchBenchmarkingLarge2024   \n",
       "15     davidsonEvaluatingLanguageModel2024   \n",
       "16  helweMAFALDABenchmarkComprehensive2024   \n",
       "17      niuRAGTruthHallucinationCorpus2024   \n",
       "\n",
       "                                                title inclusion  \\\n",
       "13  SWT-Bench: Testing and Validating Real-World B...   Include   \n",
       "14  LLMCBench: Benchmarking Large Language Model C...   Exclude   \n",
       "15  EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...   Include   \n",
       "16  MAFALDA: A Benchmark and Comprehensive Study o...   Include   \n",
       "17  RAGTruth: A Hallucination Corpus for Developin...   Include   \n",
       "\n",
       "                                   exclusion_criteria  \\\n",
       "13                                                NaN   \n",
       "14  Topic Exclusion (Is the paper about measuring ...   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "                            exclusion_criteria_detail  \\\n",
       "13                                                NaN   \n",
       "14  It's about compression algorithms rather than ...   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "                                        short_summary contribution  \\\n",
       "13  A benchmark for generating code tests (unit te...          NaN   \n",
       "14                                                NaN          NaN   \n",
       "15  The paper introduces a dynamic framework for e...          NaN   \n",
       "16  The paper introduces MAFALD, a benchmark that ...          NaN   \n",
       "17  This paper targets word-level hallucinations i...          NaN   \n",
       "\n",
       "                                     phenomenon_short  ...  \\\n",
       "13  Specific Application (A single use case, where...  ...   \n",
       "14                                                NaN  ...   \n",
       "15  General Capability (A broadly useful ability, ...  ...   \n",
       "16  General Capability (A broadly useful ability, ...  ...   \n",
       "17  General Capability (A broadly useful ability, ...  ...   \n",
       "\n",
       "   results_author_validity                     results_author_validity_detail  \\\n",
       "13                     Yes  Limitations in how the phenomenon was operatio...   \n",
       "14                     NaN                                                NaN   \n",
       "15                      No                                                NaN   \n",
       "16                      No                                                NaN   \n",
       "17                     Yes  Benchmark statistics and quality checking are ...   \n",
       "\n",
       "     metric_statistics  metric_access  \\\n",
       "13         simple mean  Outputs alone   \n",
       "14                 NaN            NaN   \n",
       "15  mean with variance  Outputs alone   \n",
       "16     simple mean/sum  Outputs alone   \n",
       "17                 NaN  Outputs alone   \n",
       "\n",
       "                                         task_ecology  \\\n",
       "13  Complete real task (e.g. providing medical adv...   \n",
       "14                                                NaN   \n",
       "15  Constructed task (e.g. predicting medical diag...   \n",
       "16  Representative task (e.g. answering medical li...   \n",
       "17  Complete real task (e.g. providing medical adv...   \n",
       "\n",
       "                                  task_ecology_detail  \\\n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15  The tasks simulates agent negotiations (so no ...   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "          definition_integrity definition_integrity_detail  \\\n",
       "13  Single cohesive phenomenon              Not applicable   \n",
       "14                         NaN                         NaN   \n",
       "15        Composite phenomenon                         Yes   \n",
       "16        Composite phenomenon                         Yes   \n",
       "17        Composite phenomenon                         Yes   \n",
       "\n",
       "   task_dataset_size_detail metric_fewshot  \n",
       "13                      NaN            NaN  \n",
       "14                      NaN            NaN  \n",
       "15                      NaN            NaN  \n",
       "16                      NaN            NaN  \n",
       "17                      NaN            NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebook_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2f1e56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df = codebook_df[codebook_df['inclusion'] == 'Include']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3daa08",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "25eb8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_embedding(text):\n",
    "\n",
    "    client = openai.OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0c592297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_helper(series,labels):\n",
    "    series.index = labels\n",
    "    plt.pie(series, labels=series.index, startangle=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293aba08",
   "metadata": {},
   "source": [
    "## Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6f167225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = codebook_df['exclusion_criteria'].value_counts()\n",
    "temp['Include'] = codebook_df['inclusion'].value_counts()['Include']\n",
    "pie_helper(temp, ['Novelty','Topic','Empirical','Modality','Include'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "32f49760",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenomena_df = pd.read_csv('../data/phenomena_taxonomy.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e6b7f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df = included_df.merge(phenomena_df,left_on='bibkey',right_on='bibkey',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "284e3f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = codebook_df['phenomenon_defined'].value_counts()\n",
    "pie_helper(temp, temp.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3c208bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenomenon_map = {'General Capability (A broadly useful ability, which could be relevant to multiple applications)': 'General Capability',\n",
    "                     'Specific Application (A single use case, where the benchmark is likely to be examples of that use case)': 'Specific Application',\n",
    "                     'General form of bias':'General Capability',\n",
    "                     'Specific form of bias':'General Capability',\n",
    "                     'General Capability (A broadly useful ability, which could be relevant to multiple applications), MERA as a whole tries to measure general \"capabilities\", but the individual tasks evaluate more specific applications (e.g., question answering).': 'General Capability',\n",
    "                     'General Capability (A broadly useful ability, which could be relevant to multiple applications), Specific Application (A single use case, where the benchmark is likely to be examples of that use case)': 'Both'}\n",
    "\n",
    "\n",
    "contested_map = {'Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)':'Contested',\n",
    "                 'Contested':'Contested',\n",
    "                 'Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)':'Widely-agreed',\n",
    "                 'Widely-agreed': 'Widely-agreed',\n",
    "                 'No definition provided': 'Not defined',\n",
    "                 'Very specialized phenomenon without any standard definition':'Widely-agreed',\n",
    "                 'A new problem is presented so agreement is unclear; but the definition is generally interpretable in a widely-agreeable way.':'Widely-agreed',\n",
    "                 'Very special phenomenon that is only introduced by the authors. It is neither widely-agreed upon nor contested.':'Widely-agreed',\n",
    "                 'Very specific phenomenon discovered in this paper (i.e., no prior definition exists)':'Widely-agreed',\n",
    "                 'Very specialized phenomenon with no standard definition ': 'Widely-agreed',}\n",
    "\n",
    "scope_map = {'The benchmark measures the effects of multi-turn and code interpretor on the target phenomena.':'Subset',\n",
    "             'The title claims the benchmark is \"comprehensive\", but in the Limitations section they say that the \"evaluation might not comprehensively assess LLM’s abilities.\"':'Comprehensive'}\n",
    "\n",
    "col_maps = {'phenomenon_contested': contested_map,\n",
    "            'phenomenon_short':phenomenon_map,\n",
    "            'definition_scope':scope_map,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cd77e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_map = {'Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)': 'Author-crafted',\n",
    "              'Modified from another benchmark (e.g. translation into another language)':'Another benchmark',\n",
    "              'LLM-generated task examples (e.g. Filtered from responses to a prompt)':'LLM-generated',\n",
    "              'Real task examples (e.g. GitHub issues)':'Real task',\n",
    "              'Procedurally-generated task examples (e.g. Creating instances from a template)':'Procedurally-generated',\n",
    "              'Crowd-sourced task examples (e.g. Prolific-created tasks)':'Crowd-sourced',\n",
    "              'Expert-crafted task examples (e.g. hand-written examples)':'Expert-crafted',\n",
    "              'Human exam questions (e.g. GRE questions)':'Human exams',\n",
    "              'LLM- and VLM- generated task examples':'LLM-generated',\n",
    "              'Text snippets from books':'Author-crafted',\n",
    "              'Human-crafted task examples from an existing human game (Choose-Your-Own-Adventure)':'Procedurally-generated',\n",
    "              'Scraped from social media (Reddit)':'Procedurally-generated',\n",
    "              'The dataset is derived from the open BYTESIZED32 corpus.':'Another benchmark',\n",
    "              '':'Unknown',\n",
    "              'Produced media (TV sitcom scenes)':'Author-crafted',\n",
    "              'Unclear':'Unknown',\n",
    "              'Expert-annotated task examples (PhD students)':'Expert-crafted',\n",
    "              'The examples are created by a linguist ':'Expert-crafted',\n",
    "              'Domain expert annotators':'Expert-crafted',\n",
    "              'Wikidata':'Crowd-sourced',\n",
    "              'Human expert created the examples':'Expert-crafted',\n",
    "              'Not explained ':'Unknown',\n",
    "              'For some part of the data they include human generated prompts ':'Author-crafted',\n",
    "              'hand-written by linguists in respective languages, taken from textbooks, handbooks and journal articles in theoretical syntax + some examples taken from previous benchmarks ':'Expert-crafted',\n",
    "              'Human TV show; Human chitchat dialogues':'Author-crafted',\n",
    "              'Based on knowledge graphs (KG) e.g. Wikidata':'Procedurally-generated',\n",
    "              'Original benchmark modified through an agent automatically and through crowdsourcing it was filtered for quality.':'Crowd-sourced' \n",
    "              }\n",
    "sources_raw_list = ['Human exam questions (e.g. GRE questions)','Real task examples (e.g. GitHub issues)','Author-crafted task examples (e.g. hand-written examples, manual transformation of existing data into questions)','Expert-crafted task examples (e.g. hand-written examples)','Crowd-sourced task examples (e.g. Prolific-created tasks)','Modified from another benchmark (e.g. translation into another language)','Procedurally-generated task examples (e.g. Creating instances from a template)','LLM-generated task examples (e.g. Filtered from responses to a prompt)']\n",
    "sources_list = ['Author-crafted','Crowd-sourced','Unknown','Procedurally-generated','Expert-crafted','Another benchmark','LLM-generated','Human exams','Real task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "932fdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_map = {'Targeted items (creators defined a task space and chose tasks within it strategically)':'Targeted',\n",
    "                'Specific criteria (items were taken from a larger set based on specified rules)':'Criterion',\n",
    "                'Convenience sample (creators found a set of tasks that was readily accessible)':'Convenience',\n",
    "                'Random sample (creators defined a task space and sampled from it)':'Random',\n",
    "                'Unknown':'Unknown',\n",
    "                '':'Unknown'\n",
    "                }\n",
    "sampling_raw_list = ['Specific criteria (items were taken from a larger set based on specified rules)','Targeted items (creators defined a task space and chose tasks within it strategically)','Convenience sample (creators found a set of tasks that was readily accessible)','Random sample (creators defined a task space and sampled from it)']\n",
    "sampling_list = ['Targeted','Criterion','Convenience','Random','Unknown']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f83a7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_map = {'Structured response (e.g. valid JSON, API call alone)': 'Structured',\n",
    "       'Extended interaction (e.g. conversation, calling an API and processing the response)':'Interaction',\n",
    "       'Multiple choice':'Multiple choice',\n",
    "       'Short free response (e.g. single word or number)': 'Short free response',\n",
    "       'Free response (e.g. summary paragarph)' : 'Free response',\n",
    "       'Depends on the subtask category (Utterance Classification, Dialogue Classification, Multiple Choice, Span Extraction)':'Short free response',\n",
    "       'Retrieval ': 'Short free response',\n",
    "       'functioning code (i.e., a .py script or model artifacts)':'Free response',\n",
    "       'Choice of one input sentence':'Multiple choice',\n",
    "       'predicted label':'Multiple choice',\n",
    "       'Log-likelihood of a given free response':'Logits',\n",
    "       'Free response (e.g. summary paragraph, executable code)':'Free response',\n",
    "       '':'',\n",
    "       'Generated image':'Free response',\n",
    "       'Sequencing':'Free response', \n",
    "       'image':'Free response', \n",
    "       'Image':'Free response',\n",
    "       'Movement trajectory to complete task':'Free response', \n",
    "       'Ranking of images':'Free response',\n",
    "       'This task is not based on model responses; it exclusively relies on the probability assigned to input tokens.':'Logits',\n",
    "       'This task is not based on LM responses; it solely relies on measuring the probabilities assigned to tokens in the sentence pairs.':'Logits',\n",
    "       'This task is not based on model responses; it relies solely on perplexity measurements.':'Logits',\n",
    "       'The task is not based on model responses; it solely relies on the probabilities assigned to the tokens in the two sentences.':'Logits',\n",
    "       'The task is not based on responses; it relies solely on the probability assigned to the tokens in the sentence.':'Logits',\n",
    "       'Numeric response (for utilitarian task)':'Short free response'}\n",
    "\n",
    "response_raw_list = ['Multiple choice','Short free response (e.g. single word or number)','Free response (e.g. summary paragraph, executable code)','Free response (e.g. summary paragarph)','Extended interaction (e.g. conversation, calling an API and processing the response)','Structured response (e.g. valid JSON, API call alone)']\n",
    "response_list = ['Structured', 'Interaction', 'Multiple choice', 'Short free response', 'Free response', 'Logits', 'Unknown']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d1592de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map = {'Whether the faulty code fails on the test and the gold-standard code passes it.':'Reward',\n",
    "       'Exact Match (accuracy, F1, precision, recall)':\"Exact match\",\n",
    "       'Number of rounds completted': 'Reward',\n",
    "       'n-gram (BLEU, ROUGE, chrF)': 'Soft match',\n",
    "       'Distribution (perplexity, calibration, correlation)':'Distribution',\n",
    "       'LLM post-processing (extracting answers, reformatting for automated scoring)': 'LLM post-processing',\n",
    "       'reward is computed based on the final product chosen by the agent, compared against known attributes, options, and price of the target product.':'Reward',\n",
    "       'The paper defines a reward score':'Reward',\n",
    "       'LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)':'LLM-as-a-Judge',\n",
    "       'Win rate':'Reward',\n",
    "       'Human ratings (text quality, preference, NOT manual scoring of other metrics)':'Human ratings',\n",
    "       'Matthews Correlation Coefficient (MCC, Matthews), which is a measure of similarity between binary distributions taking values from -1 to 1 and always yielding 0 for any two uncorrelated distributions, regardless of class imbalance.':'Correlation',\n",
    "       'P-Score (Prompting Score) and H-Score (Heuristical Score)':'LLM-as-a-Judge',\n",
    "       'The paper introduces 2 new metrics for language confusion. Line-level pass rate (LPR) and Word-level pass rate (WPR).':'Exact match',\n",
    "       'They also report two metrics to compare an LLM’s predictive confidence in slang usages relative to their literal counterparts.':'Correlation',\n",
    "       'Matthews Correlation Coefficient (MCC), and AUC (Area Under the ROC Curve)':'Correlation',\n",
    "       'instruction following rate':'Exact match',\n",
    "       'Human accuracy evaluation':'Human ratings',\n",
    "       'cosine similarity, log generation probability':'Distribution',\n",
    "       'pass rate':'Exact match',\n",
    "       'Normalized score relative to GPT-3.5-Turbo-16K performance':'',\n",
    "       'Spearman’s ρ, L/5 precision, RMSE':'Correlation',\n",
    "       'Score improvement of script':'Reward',\n",
    "       'Accuracy when the generated function is executed.':'Reward',\n",
    "       'Pear./Spear. Corr , Avg. Precision':'Correlation',\n",
    "       'exact match, MCC (Matthews Correlation Coefficient)':'Exact match',\n",
    "       'Also consider unit tests for some questions.':'Reward',\n",
    "       'runtime percentile of the LLM-generated code on the runtime distribution supported by corresponding solutions (the Leetcode solutions)':'Reward',\n",
    "       'Krippendorff’s α':'Correlation',\n",
    "       'Memorization score':'Exact match',\n",
    "       'Generated proof verified by an independent prover system.':'Reward',\n",
    "       'Reasoning Graph Accuracy and Reasoning Graph Similarity based on graph edit distance and textual similarity e.g. BLEURT.':'Soft match',\n",
    "       'Meteor':'Soft match',\n",
    "       'Explanation Completeness P/R/F1, Explanation Logical Consistency %':'Exact match',\n",
    "       'Unordered/Ordered BERT-F1 using DeBERTa-based BERTScore':'LLM-as-a-Judge',\n",
    "       'Binary F1 for Evidence IDs':'Exact match',\n",
    "       'Landmark Coverage Rate (LCR(%)) for route-planning':'Exact match',\n",
    "       'recall@1':'Exact match',\n",
    "       'Macro-F1 for the multi-class certainty prediction':'Exact match',\n",
    "       'Top L Precision, Top-k ACC, R^2, AUC, MCRMSE, Spearmann core':'Correlation',\n",
    "       'Execute the code and evaluate exact match of table vs ground truth table.':'Exact match',\n",
    "       'Functional correctness checks. Evaluated by (1) producing a dependency graph from the code (2) using an IaC policy engine to check whether the instruction specification are in the program.':'Reward',\n",
    "       'Code \"Speedup\" and \"Memory Reduction\" versus reference solutions.':'Reward',\n",
    "       'They use an ambiguity classifier as well from previous work':'LLM-as-a-Judge',\n",
    "       'partial credit':'Soft match',\n",
    "       'Mean IoU (Intersection over Union)':'Soft match',\n",
    "       'The code is executed and results are verified against ground truth results':'Reward',\n",
    "       'checkpoint coverage':'Exact match',\n",
    "       'Custom metrics: multi-modal gain, multi-modal leakage':'Exact match',\n",
    "       'For dialogue assessment, they introduce four metrics: average turn (AT), average dialogue refinement (ADR), average turn refinement (ATR), and refinement ratio (RR).':'Reward',\n",
    "       'Execution time and memory usage efficiency; unit test correctness':'Reward',\n",
    "       '3D IoU-based Average Precision':'Soft match', \n",
    "       'Unit test cases':'Reward',\n",
    "       'Soft Accuracy for counting task':'Soft match',\n",
    "       \"Correlation (Matthew's correlation, Pearson's r)\":'Correlation',\n",
    "       \"Execution-based evaluation. e.g. run the agent's code and see if it matches the ground-truth results. Plus different rubrics for each task.\":'Reward',\n",
    "       'Execution-based / functional correctness. Pass unit tests.':'Reward',\n",
    "       'Execution-Based Evaluation (unit tests)':'Reward',\n",
    "       'Diversity@k: that measures the cultural diversity among the retrieved images, helping to identify models’ bias towards specific countries or regions.':'Distribution',\n",
    "       'Execution-based evaluation (unit tests)':'Reward',\n",
    "       'Execution-based metrics.':'Reward', \n",
    "       'Elo ratings, Win rate':'Reward', \n",
    "       'VQAScore':'Exact match',\n",
    "       'Intersection over Union (IoU)':'Soft match',\n",
    "       'A non-defined \"jailbreak success rate\". likely LLM-as-a-Judge but unclear.':'Reward',\n",
    "       'Execution-based evaluation scripts':'Reward',\n",
    "       '- Contains: A less restrictive option is to consider a response correct if the prediction contains the true class name after preprocessing - ClipMatch: matching the prediction and label using cosine similarity in a vector embedding space':'Soft match',\n",
    "       'Mean Absolute Error':'Soft match',\n",
    "       'Mel-Cepstral Distortion is a measure of audio quality for TTS. A custom index is defined to balance all the evaluation metrics. ':'',\n",
    "       'MCD, MSD, PSNR, SSIM':'Soft match', \n",
    "       'BERTScore, GLEU':'Soft match',\n",
    "       'Execution-based (unit tests)':'Reward',\n",
    "       'Generation Metric and Generation Quality Drop are never explicitly defined in the paper. ':'LLM-as-a-Judge',\n",
    "       '(school) grade':'Soft match',\n",
    "       'Custom reward functions (e.g. must_include, eval_vqa, eval_fuzzy_image_match)':'Exact match',\n",
    "       'Execution-based scoring.':'Reward',\n",
    "       'Execution-based evaluation. Fairly comprehensive.':'Reward',\n",
    "       'Execution-based evaluation':'Reward',\n",
    "       'execution-based verification, file-based comparison, information-based validation':'Reward',\n",
    "       'edit-f1':'Soft match',\n",
    "       \"must_include', 'fuzzy_match', and programmatic checks which don't fit standard categories\":'Soft match',\n",
    "       'Resolution task: accuracy gap. Retrieval bias: Bias@K, Skew@K, NDKL.':'Distribution',\n",
    "       'They also report accuracy drop between cases where the image supports the correct answer choice and the cases where it supports one of the incorrect answer choices.':'',\n",
    "       'BiasScore: percentage of demographic groups in a dataset for which the LM continuations are more negative (e.g., toxic) than the average percentage of negative generations across demographic groups':'Distribution',\n",
    "       'Toxicity Score, Entailment Score':'LLM-as-a-Judge', \n",
    "       'Ko-H5 score':'Unknown',\n",
    "       \"The primary metrics (GP, SR, SPL, PWSR) measure the task performer's navigation success when guided by the helper. Helper effectiveness is inferred from these outcomes, with SPL and PWSR combining success and path efficiency.\":'Reward',\n",
    "       'For multiple correct multiple choice questions, if there were 4 correct answers and the taker selects 3, they score 0.75. If they selected 4 correct and an additional 5th incorrect, they score 0. This is to mimic actual IEE exam scoring. ':'Soft match',\n",
    "       'rescued value rate, averaged rescue step, averaged damaged rate':'Reward',\n",
    "       'Consistency Score, Relative Consistency Score':'Distribution',\n",
    "       'Many chemistry specific metrics, such as molecule validity, Fingerprint Tanimoto Similarity etc.':'Soft match',\n",
    "       'Bias Score: percentage of examples for which the likelihood of the more stereotypical sentence is higher than the likelihood of the less stereotypical sentence.':'Distribution',\n",
    "       'Win Rate, Population Block Ratio (PBR), Resource Utilisation Ratio (RUR), Average Population Utilization (APU), Technology Rate (TR)':'Reward',\n",
    "       'Post-processing with heuristics':'Soft match',\n",
    "       'Bias Percentage: percentage of sentence pairs for which the more stereotypical sentence has a higher probability than the less stereotypical sentence.':'Distribution',\n",
    "       'SoFa Score: variance in normalized log perplexity across grouped sentences (i.e., sentences with the same stereotype and different identities)':'Distribution',\n",
    "       'TrueScore, win rates, reward (game specific)':'Reward',\n",
    "       'Percentage of items (i.e., sentence pairs) for which an LM assigns a higher (psuedo-)likelihood to the stereotyping sentence over the less stereotyping sentence':'Distribution',\n",
    "       'BertScore, kwPrec':'Soft match',\n",
    "       'Semantic Similarity, BARTScore, Char-level edit distance':'Soft match',\n",
    "       'Execution Accuracy (EX) and Valid Efficiency Score (VES)':'Reward',\n",
    "       'Jaccard Index between model predictions and human-labeled associations':'Human ratings',\n",
    "       'Output probability change of attribute':'',\n",
    "       'Mean of the output logits':'Distribution',\n",
    "       'Factual Diversity Divergence (quantifies the divergence in the level of demographic diversity in model generations compared with the factual ground truth)':'Distribution',\n",
    "       'Define MR-Score, a weighted combination of matthews correlation, and two variants of accuracy':'Correlation',\n",
    "       'FactScore (Min et al., 2023), a method that evaluates the factuality of generated text by decomposing both the reference and hypothesis into atomic facts; MMRelevance':'Exact match',\n",
    "       'Mean entailment':'Distribution',\n",
    "       'A key metric is: Score = # harms committed by agent / # harms committed by random baseline (aka a normalised ratio relative to random baseline of 1000 random trajectories)':'Reward',\n",
    "       'Define 2 new metrics, RND and OCC which handle intricacies of the mutli-turn evaluation':'Reward',\n",
    "       'IOU':'Soft match'}\n",
    "\n",
    "metric_raw_list = ['Exact Match (accuracy, F1, precision, recall)','n-gram (BLEU, ROUGE, chrF)','Human ratings (text quality, preference, NOT manual scoring of other metrics)','LLM-as-a-Judge (text quality, preferences, NOT extracting answers for other metrics)','LLM post-processing (extracting answers, reformatting for automated scoring)','Distribution (perplexity, calibration, correlation)',\"Correlation (Matthew's correlation, Pearson's r)\"]\n",
    "metric_list = ['Exact match', 'Human ratings', 'LLM-as-a-Judge', 'LLM post-processing', 'Distribution', 'Correlation', 'Reward', 'Soft match', 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ddf6b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "contested_map = {'Widely-agreed':'Widely-agreed',\n",
    "                 'Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)':'Widely-agreed',\n",
    "                 'Contested':'Contested',\n",
    "                 'Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)':'Contested',\n",
    "                 'No definition provided':'No definition',\n",
    "                 'A new problem is presented so agreement is unclear; but the definition is generally interpretable in a widely-agreeable way.':'Widely-agreed',\n",
    "                 'Very special phenomenon that is only introduced by the authors. It is neither widely-agreed upon nor contested.':'Widely-agreed',\n",
    "                 'Very specific phenomenon discovered in this paper (i.e., no prior definition exists)':'Widely-agreed',\n",
    "                 'Very specialized phenomenon with no standard definition ':'Widely-agreed',\n",
    "                 'Very specialized phenomenon without any standard definition':'Widely-agreed'\n",
    "                 }\n",
    "contested_raw_list = ['Widely-agreed','Widely-agreed (e.g. if I say \"pronoun resolution\" everyone agrees on what I mean)','Contested (e.g. if I say \"reasoning\" there is a lot of variation in what I could mean)','No definition provided'] \n",
    "contested_list = list(set(contested_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "77434782",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_validity_map = {'Yes':'Yes',\n",
    "                     'Partially':'Partially',\n",
    "                     \"They follow the lead of popular knowledge and reasoning benchmarks, so it's hard to say here. \":'Partially',\n",
    "                     '':'',\n",
    "                    'It is evaluating temporal misaglignment through the specific lens of factual information on Wikipedia.':'Partially',\n",
    "                    'Too vaguely defined phenomenon':'No',\n",
    "                    'No':'No',\n",
    "                    'but fairly poor task definition.':'Partially',\n",
    "                    'Somewhat. Certain tasks in the benchmark align well with how real-world analysts evaluate cyber threat intelligence, suggesting some face validity. However, other tasks focus more on knowledge retrieval, which may not reflect the full nature of cyber threat intelligence, where knowledge retrieval, understanding, reasoning, and application are all important. These aspects are tested separately, so the benchmark doesn’t provide a full picture of end-to-end evaluation.':'Partially',\n",
    "                    \"There is no specified phenomenon besides the models' ability to answer open-ended questions.\":'No',\n",
    "                    'Highly simplified version of the phenomena':'Partially',\n",
    "                    'Tricky to say since the paper does not provide a principled definition of the target phenomenon. It just talks of general \"capabilities,\" as well as the ten skills mentioned above. As for the ten skills, face validity varies -- for some (e.g., mathematics) it seems higher than for others (e.g., ethics).':'Partially',\n",
    "                    'yes, but only a small subset':'Partially',\n",
    "                    'Too broad to tell.':'No',\n",
    "                    'It really depends on the phenomena and task':'Partially',\n",
    "                    'Only partly':'Partially',\n",
    "                    'Not sure about this. Compared to other similar benchmarks, yes. In general, probably not. ':'Partially',\n",
    "                    \"rima facie reason to believe that perplexity on factual completions is a valid metric for benchmarking a language model's ability to adapt to changing knowledge over time (the target phenomenon of temporal misalignment). But the task format is very synthetic.\":'Yes',\n",
    "                    'Mixed. Keywords/n-grams are a limited way of assessing performance.':'Partially',\n",
    "                    'The Vera and Grammar models may be well-established and commonly used in compositionality or linguistic tasks, but it is not apparent in the paper. No justification is provided for the use of Vera and Grammar.':'Partially',\n",
    "                    'Mixed. For the execution-based tasks, yes, but for the code summarisation tasks they use BLEU/CodeBLEU':'Partially',\n",
    "                    'It works since most LMs do not output toxic content all the time, but this does not make it a metric that is suitable for bias measurement in principle.':'Partially',\n",
    "                    'High validity for detecting the presence of stereotypes, but low validity for measuring the absence of stereotypes. The authors acknowledge this distinction.':'Partially',\n",
    "                    'High values of the metric indicate presence of bias, but low values do not mean that a model is unbiased. The authors do not acknowledge that.':'Partially',\n",
    "                    \"Maybe - it relies on the GPT-4 evaluator being able to assess the correctness of the answer relative to human key points. If the GPT-4-as-a-judge lacked nuanced scientific understanding, it may fail to evaluate another LLM's response against the key points (e.g., requires capabilities for classic entailment,contradiction task). Howver, they do show high correlation empirically between GPT-4-as-a-judge and human evaluators. \":'Partially',\n",
    "                    'Maybe - they only keep very non-ambiguous examples but this only covers a subset of human values which can have disagreements and be ambigous in some settings':'Partially',\n",
    "                    'Depends on the game':'Partially',\n",
    "                    'Computing the mean of the logits does not seem mathematically sound, but the general approach of examining the output probabilities is valid.':'No',\n",
    "                    'The metric is new and not very well motivated':'No'}\n",
    "face_validity_raw_list = ['Yes','No']\n",
    "face_validity_list = list(set(face_validity_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "69a7d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "realism_map = {'The benchmark is itself realistic':'Realistic',\n",
    " 'It is an entirely constructed scenario (no available realistic setting)':'Not possible',\n",
    " 'No':'No comparison made', 'Yes':'Comparison made',\n",
    " 'They do a partial study with actual human feedback on the benchmark tasks.':'Comparison made',\n",
    " 'Given that the benchmark is trying to measure general capabilities, it is unclear how a more realistic setting would look like.':'Not possible',\n",
    " '':'No',\n",
    " 'No - but you could say the commonsense morality task is scraped from social media so has some realism':'Realistic'}\n",
    "realism_raw_list = ['Yes','No','The benchmark is itself realistic']\n",
    "realism_list = list(set(realism_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b9d04b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_map = {'Yes':'Yes', 'No':'No',\n",
    " 'for short-answer questions, there is a human evaluation, which to some extent can represent the validity of the questions':'Yes',\n",
    " '':'', 'Somewhat':'Yes', 'Somehwat':'Yes',\n",
    " 'The papers justify the improvement of the task design displayed in their benchmark, but not the choice of the task itself.':'Yes',\n",
    " 'Partially; addressed their own limitations.':'Yes', 'indirectly address it':'Yes',\n",
    " 'They indirectly address it':'Yes', 'Indirectly address it':'Yes', 'implicitly':'Yes',\n",
    " 'They compare scores on benchmark to human judgment':'Yes',\n",
    " 'Partial pre-analysis.':'Yes', 'A bit (but not a strong Yes)':'Yes',\n",
    " 'They acknowledge the lateral thinking is hard to measure: \"In this paper, we seek to explore and elicit the lateral thinking ability of LLMs. However, accurately evaluating this capability poses significant challenges due to the complexity of measuring creative thinking [29 , 19 ] and the difficulty of obtaining relevant data. The generation of novel ideas is inherently non-trivial, even for humans [13 , 14 ]. Considering these challenges, we propose the exploration of lateral thinking in LLMs by situation puzzles as a primary research tool\"':'Yes'}\n",
    "author_raw_list = ['Yes','No']\n",
    "author_list = list(set(author_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "01c5092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecology_map = {'Complete real task (e.g. providing medical advice to real people interactively)':'Complete',\n",
    " \"Constructed task (e.g. predicting medical diagnoses from clinicians' notes)\":'Constructed',\n",
    " 'Representative task (e.g. answering medical licensing exam questions)':'Representative',\n",
    " 'Partial real task (e.g. answering medical questions collected from real people)':'Partial',\n",
    " '':'', 'Low ecology':'Constructed',\n",
    " 'Low ecology, humans wouldn’t usually ask LLMs to do these tasks.':'Constructed',\n",
    " 'Artificial task':'Constructed',\n",
    " 'Proxy task - tries to get at real-world scenarios of agents via fictional adventures':'Representative'}\n",
    "\n",
    "ecology_raw_list = ['Complete real task (e.g. providing medical advice to real people interactively)','Partial real task (e.g. answering medical questions collected from real people)','Representative task (e.g. answering medical licensing exam questions)',\"Constructed task (e.g. predicting medical diagnoses from clinicians' notes)\"]\n",
    "ecology_list = list(set(ecology_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f964e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_agg_map = {\n",
    " 'simple mean': ['Mean'],\n",
    " 'simple mean/sum': ['Mean'],\n",
    " 'Simple mean': ['Mean'],\n",
    " 'Mean': ['Mean'],\n",
    " 'Mean, ': ['Mean'],\n",
    " 'Simple mean and standard deviation': ['Mean','Std'],\n",
    " 'mean and variance': ['Mean','Std'],\n",
    " 'simple mean and std': ['Mean','Std'],\n",
    " 'simple mean, std': ['Mean','Std'],\n",
    " 'mean and variance, t-tests': ['Mean','Std','Tests'],\n",
    " 'mean': ['Mean'],\n",
    " 'Simple Mean': ['Mean'],\n",
    " 'Averages and win/tie percentages in human eval; no advanced statistics reported\\n': ['Mean'],\n",
    " 'Means, standard deviations, and Spearman/Pearson correlations with expert rankings.\\n': ['Mean','Std','Other'],\n",
    " 'simple mean, statistical tests': ['Mean','Tests'],\n",
    " 'Flesch Kincaid, Rouge-L, Kendalls, Spearmans\\n' :['Other'],\n",
    " 'simple mean, standard error of the mean': ['Mean'],\n",
    " 'Pearson correlation, RMSE, differences to student baselines\\n': ['Other'],\n",
    " 'Simple mean, average':['Mean'],\n",
    " 'Average':['Mean'],\n",
    " 'mean with variance':['Mean','Std'],\n",
    "'The authors carry out some error analysis: \"We argue that we are measuring a lower bound for what LMs know. To further understand the shortcomings of the current method, we conduct an error analysis of the errors in precision on all datasets. We choose BERTLARGE for the study. We sample 100 documents from the Wikidata-OIE dataset, and manually check the reasons for the errors\"':['Other'],\n",
    "\"Simple means for performance metrics; agreement percentages and Cohen's Kappa for annotation reliability.\":['Mean','Other'],\n",
    "'simple mean, Anova for p-values, Tukey-HSD':['Mean','Tests'],\n",
    "'The authors report average task score and success rate across trials. They also include standard deviation/error bars in some result plots (e.g. Figure 4), mainly to show the variation across multiple runs.':['Mean','Std'],\n",
    "'mean of weighted-F1 scores':['Mean'],\n",
    "\"Mean, and they they show a delta (for change in aggregate sources across all tasks). It is unclear if this is a range or a standard deviation. I think it's a range.\":['Mean'],\n",
    "'The authors use a weighted mean in calculating an approximate human performance threshold but not for model performance. They take a weighted average of the annual medal thresholds for ‘Advanced’ problems. ':['Mean'],\n",
    "'simple mean and STD':['Mean','Std'],\n",
    "'Simple means and macro-averaging (mean across tasks, which is identical here because each task has same # of instances)':['Mean'],\n",
    "'macro-accuracy':['Mean'], \n",
    "'Simple mean (no variance or standard reported)':['Mean'],\n",
    "'mean and standard deviation':['Mean','Std'],\n",
    "'Simple mean/sum; % improvement between contexts':['Mean','Other'],\n",
    "'simple mean and standard deviation ':['Mean','Std'],\n",
    "'accuracy, F1, standard deviation':['Mean','Std','Other'],\n",
    "'retrieval rate R@K metric':['Other'],\n",
    "'For each tuple, the F1 is computed, then across a clique the minimum is computed and aggregated across the dataset as mean.':['Mean'],\n",
    "'Simple mean: F1 scores and accuracy. MSE. nDCG and MRR. Perplexity':['Mean','Other'],\n",
    "'Mean and standard deviation':['Mean','Std'], \n",
    "'simple mean (as percentage)':['Mean'],\n",
    "'Simple average of perplexity for different snapshots of the wikipedia data.':['Mean'],\n",
    "'Aggregated scores (no additional stats)':['Mean'], \n",
    "'mean over 8 runs. ':['Mean'],\n",
    "'Simple summary stats. ':['Mean'], \n",
    "'Success rate':['Mean'],\n",
    "'simple mean, for tasks with more than one metric (like Pearson and Spearman correlation for sentiment regression), scores are averaged to get a single task score':['Mean'],\n",
    "'Min, max, average':['Mean','Other'],\n",
    "'Weighted Precision, Recall, F1 scores, and macro-F1 scores for binary and multi-class classification. Hamming loss is also reported for multi-class classification. ':['Mean','Other'],\n",
    "'Visual semantic tasks were measured with representational similarity analysis (RSA), while the other tasks were measured with a novel metric: softmax-optimized Kullback-Leibler divergence':['Other'],\n",
    "'simple mean,  inter-annotator agreement with WAWA and the Dawid-Skene method for vote aggregation.  delta-scores to measure performance differences between models under different dataset filtering conditions':['Mean','Other'],\n",
    "'Mean, standard deviation.':['Mean','Std'],\n",
    "'Simple mean/average scores (MSQA Correctness Score C, MSNN Accuracy) are used to aggregate results. Different models or settings are compared directly based on these mean scores presented in tables.':['Mean'],\n",
    "'Simple mean/average of Hit@k, Recall@k, and MRR over the test sets.':['Mean'],\n",
    "'Accuracy, MRR, Precision, Recall, F1, BLEU, ROUGE-L, Keyword Recall, Mean Likert scores.':['Mean','Other'],\n",
    "'BLEU-4, ROUGE-L, MRR, Precision@3. Mean scores are reported, sometimes with standard deviation (e.g., for text lengths in Table 2 ).':['Mean','Std'],\n",
    "'Precision, Recall, F1 score, Exact Match (EM)':['Mean','Other'],\n",
    "'Macro F1 score, Exact Match (EM)':['Mean'], \n",
    "' simple mean/sum':['Mean'],\n",
    "'simple mean, mean and std, averaging across multiple metrics':['Mean','Std'],\n",
    "'simple mean/sum, t-tests':['Mean','Tests'], \n",
    "'simple mean/sum, GLMs':['Mean','Tests'],\n",
    "'Accuracy is reported for classification in both open and closed-world settings. Fine-tuned accuracy and linear probing accuracy are reported in a closed-world setting, while 1NN-genus probing accuracy is reported in an open-world setting. AMI is reported for zero-shot transfer learning, and in multimodal retrieval learning, micro and macro top-1 accuracy is reported. ':['Mean','Other'],\n",
    "'The metrics are averaged and normalized against human performance':['Mean'],\n",
    "'BLEU-4, Rouge-1, BERTScore, Keyword Insertion Rates (KWD), Sentence Length Regulation Compliance Rates (REG), Pearson and Spearman Correlation for Human Evaluation':['Other'],\n",
    "' Macro F1 score, per-class F1 score':['Mean'], \n",
    "'Unknown':['Unknown'],\n",
    "'Answer Accuracy (Exact Match %), Reasoning Graph Accuracy (%), Reasoning Graph Similarity (%).':['Mean'],\n",
    "'Factuality is calculated with Named Entity Recognition (NER) empowered accuracy, described in the paper. Style is measured with BLEU. Insightfulness is measured by human assessments based on impact (breadth of claim), and significance (magnitude of changes) on a 5 point Likert scale, and the average of the human review is reported. ':['Mean'],\n",
    "'F1, EM, R1, R2, MET':['Mean'],\n",
    "'Accuracy (%). Kappa score used for error analysis inter-rater reliability.':['Mean','Other'],\n",
    "'Simple mean and variance on accuracy are used to assess the overall and best pick comparisons for cartoons, and expectation adjusted distinct N-grams (EAD) and Sentence-BERT embedding cosine similarity (SBERT) are used to assess caption diversity. ':['Mean','Std','Other'],\n",
    "'Precision, Recall, F1 score, Completeness (P/R/F1), Logical Consistency (%).':['Mean'],\n",
    "'BERT-F1, ROUGE-L F1, Human Judgement Proportions (%), Pearson Correlation (r) for metric validation.':['Mean'],\n",
    "'Accuracy, F1 score, BERTScore F1, Average score (1-5 scale).':['Mean'],\n",
    "'simple mean/sum, percentage point improvements':['Mean'],\n",
    "'F1 score, AllCorrect (Exact Match), Accuracy, Macro F1':['Mean'],\n",
    "'Precision, Recall, F1, ROUGE-1, ROUGE-2, ROUGE-L, Human evaluation win/tie/lose rates (%).':['Mean'],\n",
    "'Accuracy (%), F1 Score (%)':['Mean'],\n",
    "'Accuracy (%), Standard Deviation, Error Rate (%)':['Mean','Std'],\n",
    "'Reports average scores for commonsense Vera score gap and Grammar score gap. The paper also reports the pairwise better ratio between SugarCrepe and ARO+CREPE. ':['Mean'],\n",
    "'Mean and std':['Mean','Std'], 'Mean, variance':['Mean','Std'], 'simple average ':['Mean'],\n",
    "'mean and standard dev':['Mean','Std'],\n",
    "'No statistical methods used. just simple mean and differences in means.':['Mean'],\n",
    "'simple mean and for rating-based evaluations they measure \"hedging rate\"':['Mean'],\n",
    "'simple mean to aggregate performance over scenarios and roles':['Mean'],\n",
    "'simple mean/sum, plus comparisons to scores from the base LLMs comprising the multi-modal models (called \"multi-modal gain\" and \"multi-modal leakage\" statistics)':['Mean'],\n",
    "'Mean, worst and best out of 11':['Mean'],\n",
    "'simple mean. no inferential statistics (even though the LLM-as-judge have fairly low accuracy).':['Mean'],\n",
    "'mean with \"error bars from 3 runs at temperature 0.2\" (unsure if this is a standard error or just the range in scores) (17)':['Mean','Std'],\n",
    "'Exact Match (EM), F1 Score (%)':['Mean'], 'Mean and standard deviation\\n':['Mean'],\n",
    "'Accuracy (%)':['Mean'], 'Accuracy (%), BLEU-4, ROUGE-L':['Mean'],\n",
    "'Simple mean to aggregate automatic scores, Pearson and Spearman correlation between human and automatic ratings\\u200b, and Krippendorff’s Alpha inter-rater agreement for human ratings.':['Mean','Other'],\n",
    "'mean/sum, where problem correct means all subproblems must be correct':['Mean'],\n",
    "'simple means to report F1 scores and ROUGE metrics':['Mean'], 'Mean,':['Mean'],\n",
    "'simple mean + std':['Mean','Std'], 'Mean, standard errors':['Mean','Std'],\n",
    "'Mean, standard deviation':['Mean','Std'],\n",
    "'Mean, Spearman/Pearson Correlations\\n(For completeness, they also report the standard Pearson but also mention that Pearson is not the ideal metric due to the curve-of-best-fit not appearing linear.)':['Mean','Other'],\n",
    "'mean,':['Mean'], 'Means, comparisons with percentage point gaps.':['Mean'],\n",
    "'Wilcoxon tests, variance analysis':['Other'],\n",
    "'Means and percentage differences':['Mean'],\n",
    "'simple mean/sum, correlation between overall rankings and general capabilities based on MMBench':['Mean','Other'],\n",
    "'Mean, percentage':['Mean'],\n",
    "'simple mean, std, relative performance changes':['Mean','Std'],\n",
    "'Simple Means, the percentage of questions answered correctly.':['Mean'],\n",
    "'simple mean, weighted and unweighted clustering scores, frequency counts, Fleiss Kappa':['Mean','Other'],\n",
    "'Experiments are repeated 5 times but resulting information onf uncertainty are not reported.':['Unknown'],\n",
    "'Mean error, scatter plots, attention heatmaps':['Mean'],\n",
    "'Micro-averaged entity-level F1 score reported as means across 3 runs with standard deviations. Simple means used for comparing approaches across different noise types.':['Mean','Std'],\n",
    "'Reporting accuracy, precision, recall, and F1 scores, both as macro averages across all categories and separately for offensive and non-offensive classes.':['Mean'],\n",
    "'Image retrieval error, effect score bias':['Mean'],\n",
    "'F1 scores (micro-averaged and macro-averaged) as the primary statistical method.':['Mean'],\n",
    "'Simple mean/accuracy':['Mean'],\n",
    "'Just simple mean, with occasional reporting of variance or distribution plots.':['Mean','Std'],\n",
    "'Simple proportion, human/model comparisons':['Mean'],\n",
    "\"Simple mean scores for each metric. For correlation analysis between automatic metrics and human judgments: Spearman's rank correlation coefficient.\":['Mean','Other'],\n",
    "'simple mean and variance':['Mean','Std'], \n",
    "'Percentage, comparison':['Mean'],\n",
    "'Simple means for the main metrics':['Mean'],\n",
    "'Means, standard deviations, weighted Fleiss‑k':['Mean','Std'],\n",
    "'simple mean, 95% confidence interval, percentage point of performance gains over baselines':['Mean','Tests'],\n",
    "'Simple mean/sum, custom normalized aggregate metric':['Mean','Other'],\n",
    "'Mean with 95% Confidence Interval ':['Mean','Tests'], 'Simple mean accuracy':['Mean'],\n",
    "'Simple mean + standard deviations':['Mean','Std'],\n",
    "'Primary metrics used are F1 scores, accuracy, recall, and precision. Partially, micro and macro averages are reported.':['Mean'],\n",
    "'Mean and Standard deviation':['Mean','Std'], 'simple mean with percentage point':['Mean'],\n",
    "'Simple mean to aggregate across different settings. For each detector, they report AUROC and F1 Score values for each specific condition and the average across those conditions. ':['Mean'],\n",
    "'simple mean ':['Mean'],\n",
    "'Simple mean, char-based F0.5 scores for overall performance, along with precision and recall.':['Mean'],\n",
    "'mean + standard deviatin, significance test. proportion':['Mean','Std'],\n",
    "'Stratified human agreement evaluation on LLM-graded items; comparisons to BLEU/F1 for scoring validity.\\n':['Mean'],\n",
    "'Accuracy, Fleiss’ k for human agreement\\n':['Mean','Other'],\n",
    "'Mean scores across different data splits and standard deviation for low-resource settings':['Mean','Std'],\n",
    "\"Automatic Evaluation Metrics (SARI; BLEU); Measure inter-annotator agreement using Krippendorff's alpha\":['Mean'],\n",
    "'Micro-Average, Worst-Average':['Mean'],\n",
    "\"Win and draw rates from pairwise comparisons. For automated metrics and human judgment evaluation: Kendall's Tau \":['Mean','Other'],\n",
    "'Mean, error bars on figures in appendix.':['Mean','Std'],\n",
    "'Dataset score is calculated as a macro-average of the per-language score.':['Mean'],\n",
    "'Simple means, McNemar test, Minimum detectable effect (MDE)':['Mean','Other'],\n",
    "'Scores are normalised relative to human performance.':['Other'],\n",
    "'simple mean, weighted mean':['Mean'],\n",
    "\"mean, standard deviation, entropy calculations, z-scores, p-values, bootstrapping, Le Cam's lemma, multiplicative damping factors\":['Mean','Std','Tests','Other'],\n",
    "'True Positive Rate, True Negative Rate, Generation Metric and Generation Quality Drop':['Mean'],\n",
    "'Binomial mixed effects regression models':['Tests'],\n",
    "'Mean, ANOVA, post-hoc pairwise tests':['Mean','Tests'],\n",
    "'Means, standard deviations, comparisons':['Mean','Std'],\n",
    "'Descriptive accuracy only\\n':['Mean'],\n",
    "'Binomial standard error reported\\n':['Tests'],\n",
    "'Inter-rater agreement (Krippendorff’s alpha), statistical comparisons\\n':['Other'],\n",
    "'Basic comparisons across models; no significance tests\\n':['Mean'],\n",
    "'Simple mean success rates across tasks and subsets. There is no formal hypothesis testing or statistical significance tests.':['Mean'],\n",
    "'Mean, error bars on some plots':['Mean','Std'],\n",
    "'Simple mean and standard error of the mean (for plots like accuracy vs. steps)':['Mean','Std'],\n",
    "'simple mean across samples, then the relative decline in accuracy due to injection is computed.':['Mean'],\n",
    "'Simple mean/sum (Success Rate %)':['Mean'],\n",
    "'Mean and standard error, with bootstrap confidence intervals':['Mean','Std'],\n",
    "'Simple mean (for direct assessment scores), Elo rating via Maximum Likelihood Estimation (MLE) (for pairwise comparisons), Fleiss’ Kappa and Percentage Agreement (for inter-annotator agreement and human-LLM agreement), and Kendall’s Tau (for human-LLM leaderboard rank correlation).':['Mean','Other'],\n",
    "'simple mean\\xa0and\\xa0standard error':['Mean','Std'],\n",
    "\"Simple mean/sum, correlation (Pearson's r)\":['Mean','Other'],\n",
    "'simple mean, correlation':['Mean','Other'], 'Simple means with error bars':['Mean','Std'],\n",
    "'simple mean, correlation, calibration':['Mean','Other'], 'Unkown ':['Unknown'],\n",
    "'simple mean, optionally the negative scoring as above':['Mean'],\n",
    "'Simple mean, standard deviation':['Mean','Std'],\n",
    "'Simple sum/mean - raw percentages':['Mean'],\n",
    "'Mean, standard deviation, Spearman correlation for difficulty alignment (IRT vs human/GPT4)':['Mean','Std'],\n",
    "'Mean and pairwise agreement figures;':['Mean','Other'],\n",
    "'Simple mean scores, simple sum scores, inter‑annotator agreement percentages.':['Mean'],\n",
    "'Mean accuracy is used to aggregate model performance across tasks and conditions':['Mean'],\n",
    "'They report simple percentage means, and compute Spearman’s\\xa0ρ and Pearson’s\\xa0r between rule‑ and LLM‑based scores to show consistency; and give a 98\\xa0% human‑vs‑ChatGPT agreement figure (percent agreement, not kappa).':['Mean','Other'],\n",
    "'Mean, recall, F1 with standard error comparisons':['Mean','Std'],\n",
    "'Mean and standard error; calibration (ECE) and AUROC for confidence analyses.':['Mean','Std'],\n",
    "'Mean; McNemar x^2 test for mining':['Mean','Tests'],\n",
    "'simple mean, significance clusters':['Mean'],\n",
    "'Mean and breakdown by category':['Mean'],\n",
    "'simple mean + standard errors, standard deviations':['Mean','Std'],\n",
    "'The models are primarily ordered by rank.':['Mean'],\n",
    "'In the main table, they only reported the weighted mean. In the appendix, they say \"Where plotted, the error bars show what the standard error would be if estimated using bootstrapping. To estimate the standard error of our reported model scores, we analytically estimate the standard error.\"':['Mean','Std'],\n",
    "'Weighted mean based on test samples.':['Mean'],\n",
    "'Simple mean across questions for each category. They also compute correlation metrics between human and GPT-4 evaluations: Pearson, Spearman, and Kendall-τ coefficients to validate GPT-4 as a reliable evaluator.':['Mean','Other'],\n",
    "'Simple mean over examples (accuracy)':['Mean'],\n",
    "'The paper reports simple means for calculating the overall accuracy as a macro average across all five dimensions':['Mean'],\n",
    "'mostly mean - variance only reported for best model. ':['Mean','Std'],\n",
    "'simple mean. ':['Mean'],\n",
    "'Mean, and human–automatic correlation (Cohen’s Kappa coefficient) for validation.':['Mean','Other'],\n",
    "'simple mean, standard deviation':['Mean','Std'],\n",
    "'Simple mean, and for annotation agreement Cohen’s Kappa coefficient was used.':['Mean','Other'],\n",
    "'Simple mean is used for aggregation.':['Mean'],\n",
    "'Mean; authors additionally report t‑tests for MGCT effect differences and classification accuracy for the detector.':['Mean','Tests'],\n",
    "'‑ Per‑metric means & confidence via single runs\\n\\n‑ Spearman correlation (response/evidence vs position)\\n\\n‑ Cohen’s\\xa0κ for human IAA (0.74‑0.77)':['Mean','Other'],\n",
    "'Mean, sample‑level Pearson\\xa0r, system‑level Pearson\\xa0r, pairwise win‑rate % (for agreement studies).':['Mean','Other'],\n",
    "'Mean, F1, balanced accuracy; 95% CIs via bootstrap.':['Mean','Std'],\n",
    "'Mean, Precision/Recall/F1, Balanced Accuracy; inter‑annotator agreement (Cohen’s\\xa0κ / raw %).':['Mean','Other'],\n",
    "'Simple mean, Spearman correlation (with p‑value <\\xa00.05)':['Mean','Other'],\n",
    "'The paper uses simple means for the primary evaluation metric. For each task, they report the percentage of correct predictions. For the overall score, they take a simple average across the five ethical categories. They also test whether models can distinguish ambiguous scenarios from clear-cut scenarios by using predictive uncertainty estimates (Area Under the Receiver Operating Characteristic curve).':['Mean'],\n",
    "'mean, std':['Mean','Std']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ec6c5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_map = {}\n",
    "stats_raw_list = []\n",
    "stats_list = list(set(stats_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4125bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_maps = {'task_source': {'rename_map':source_map,'raw_names': sources_raw_list,'final_names': sources_list},\n",
    "                'dataset_sampling_method':{'rename_map': sampling_map, 'raw_names': sampling_raw_list, 'final_names': sampling_list},\n",
    "                'response_format':{'rename_map':response_map, 'raw_names':response_raw_list, 'final_names':response_list},\n",
    "                'metric_definition': {'rename_map':metrics_map, 'raw_names':metric_raw_list, 'final_names':metric_list},\n",
    "                'phenomenon_contested':{'rename_map':contested_map,'raw_names':contested_raw_list,'final_names':contested_list},\n",
    "                'task_face_validity':{'rename_map':face_validity_map,'raw_names':face_validity_raw_list,'final_names':face_validity_list},\n",
    "                'metric_face_validity':{'rename_map':face_validity_map,'raw_names':face_validity_raw_list,'final_names':face_validity_list},\n",
    "                'results_realism':{'rename_map':realism_map,'raw_names':realism_raw_list,'final_names':realism_list},\n",
    "                'results_author_validity':{'rename_map':author_map,'raw_names':author_raw_list,'final_names':author_list},\n",
    "                'task_ecology':{'rename_map':ecology_map,'raw_names':ecology_raw_list,'final_names':ecology_list},\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fcef3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_str(list_str,split_list):\n",
    "    if isinstance(list_str,str):\n",
    "        for split in split_list:\n",
    "            if split in list_str:\n",
    "                list_str = list_str.replace(split+',',split+'///,')\n",
    "        list_str = list_str.split('///, ')\n",
    "        return list_str\n",
    "    else:\n",
    "        if pd.isna(list_str):\n",
    "            return ['']\n",
    "        return [list_str]\n",
    "    \n",
    "def match_occurences(items:list, count:list, header:str):\n",
    "    return {header+': '+key:key in items for key in count}\n",
    "\n",
    "\n",
    "def expand_columns(df, col_name,rename_map,raw_names,final_names):\n",
    "    try:\n",
    "        df[col_name + '_clean'] = df[col_name].apply(lambda x: [rename_map[y] for y in split_list_str(x,raw_names)])\n",
    "        return pd.concat([df,df[col_name].apply(lambda x: [rename_map[y] for y in split_list_str(x,raw_names)]).apply(lambda x: match_occurences(x,final_names,col_name)).apply(pd.Series)],axis=1)\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        print(df[col_name].apply(lambda x: [y for y in split_list_str(x,raw_names)]).explode().unique())\n",
    "        return included_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8497ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in split_maps.keys():\n",
    "    included_df = expand_columns(included_df, col, **split_maps[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "64c8c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_df['metric_statistics_clean'] = included_df['metric_statistics'].apply(lambda x: metrics_agg_map[x] if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "154da4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#included_df.to_csv('../data/clean_codebook.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0ba204d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_ecology: Representative\n",
      "task_ecology: Constructed\n",
      "task_ecology: Complete\n",
      "task_ecology: Partial\n",
      "task_ecology: \n",
      "results_author_validity: \n",
      "results_author_validity: No\n",
      "results_author_validity: Yes\n",
      "results_realism: Comparison made\n",
      "results_realism: No\n",
      "results_realism: Not possible\n",
      "results_realism: Realistic\n",
      "results_realism: No comparison made\n",
      "metric_face_validity: Partially\n",
      "metric_face_validity: No\n",
      "metric_face_validity: \n",
      "metric_face_validity: Yes\n",
      "task_face_validity: Partially\n",
      "task_face_validity: No\n",
      "task_face_validity: \n",
      "task_face_validity: Yes\n",
      "phenomenon_contested: No definition\n",
      "phenomenon_contested: Widely-agreed\n",
      "phenomenon_contested: Contested\n",
      "metric_definition: Unknown\n",
      "metric_definition: Soft match\n",
      "metric_definition: Reward\n",
      "metric_definition: Correlation\n",
      "metric_definition: Distribution\n",
      "metric_definition: LLM post-processing\n",
      "metric_definition: LLM-as-a-Judge\n",
      "metric_definition: Human ratings\n",
      "metric_definition: Exact match\n",
      "response_format: Unknown\n",
      "response_format: Logits\n",
      "response_format: Free response\n",
      "response_format: Short free response\n",
      "response_format: Multiple choice\n",
      "response_format: Interaction\n",
      "response_format: Structured\n",
      "dataset_sampling_method: Unknown\n",
      "dataset_sampling_method: Random\n",
      "dataset_sampling_method: Convenience\n",
      "dataset_sampling_method: Criterion\n",
      "dataset_sampling_method: Targeted\n",
      "task_dataset_metadata\n",
      "task_source: Real task\n",
      "task_source: Human exams\n",
      "task_source: LLM-generated\n",
      "task_source: Another benchmark\n",
      "task_source: Expert-crafted\n",
      "task_source: Procedurally-generated\n",
      "task_source: Unknown\n",
      "task_source: Crowd-sourced\n",
      "task_source: Author-crafted\n",
      "definition_scope\n",
      "definition_integrity\n",
      "phenomenon_contested\n",
      "phenomenon_defined\n",
      "phenomenon_short\n"
     ]
    }
   ],
   "source": [
    "categorized_columns = ['phenomenon_short','phenomenon_defined','phenomenon_contested','definition_integrity','definition_scope',\n",
    "'task_source: Author-crafted','task_source: Crowd-sourced','task_source: Unknown','task_source: Procedurally-generated','task_source: Expert-crafted','task_source: Another benchmark','task_source: LLM-generated','task_source: Human exams','task_source: Real task',\n",
    "'task_dataset_metadata','dataset_sampling_method: Targeted', 'dataset_sampling_method: Criterion',\n",
    "       'dataset_sampling_method: Convenience', 'dataset_sampling_method: Random','dataset_sampling_method: Unknown','response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown', 'metric_definition: Exact match',\n",
    "       'metric_definition: Human ratings', 'metric_definition: LLM-as-a-Judge',\n",
    "       'metric_definition: LLM post-processing',\n",
    "       'metric_definition: Distribution', 'metric_definition: Correlation',\n",
    "       'metric_definition: Reward', 'metric_definition: Soft match',\n",
    "       'metric_definition: Unknown',\n",
    "       'phenomenon_contested: Contested',\n",
    "'phenomenon_contested: Widely-agreed',\n",
    "'phenomenon_contested: No definition',\n",
    "'task_face_validity: Yes',\n",
    "'task_face_validity: ', \n",
    "'task_face_validity: No',\n",
    "'task_face_validity: Partially',\n",
    "'metric_face_validity: Yes',\n",
    "'metric_face_validity: ',\n",
    "'metric_face_validity: No',\n",
    "'metric_face_validity: Partially',\n",
    "'results_realism: No comparison made',\n",
    "'results_realism: Realistic',\n",
    "'results_realism: Not possible',\n",
    "'results_realism: No',\n",
    "'results_realism: Comparison made',\n",
    "'results_author_validity: Yes',\n",
    "'results_author_validity: No',\n",
    "'results_author_validity: ',\n",
    "'task_ecology: ',\n",
    "'task_ecology: Partial',\n",
    "'task_ecology: Complete',\n",
    "'task_ecology: Constructed',\n",
    "'task_ecology: Representative',\n",
    "]\n",
    "\n",
    "for col in categorized_columns.__reversed__():\n",
    "    temp = included_df[col]\n",
    "    if col in col_maps:\n",
    "        temp = temp.apply(lambda x: col_maps[col][x] if col_maps[col].get(x) else x)\n",
    "    temp = temp.value_counts()\n",
    "    temp = temp[temp > 0]\n",
    "    print(col)\n",
    "    #pie_helper(temp, temp.index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "765206c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root  phenomenon_taxonomy_leaf\n",
       "NLP                       Understanding               5\n",
       "Agents                    Tool Use                    2\n",
       "Alignment                 Alignment                   1\n",
       "                          Safety                      1\n",
       "Language Modelling        In-context Learning         1\n",
       "NLP                       Detection                   1\n",
       "                          Extraction                  1\n",
       "                          Summarization               1\n",
       "Reasoning                 Planning                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[(included_df['metric_definition: Correlation'])][['phenomenon_taxonomy_root','phenomenon_taxonomy_leaf']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f66d8229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>...</th>\n",
       "      <th>results_author_validity:</th>\n",
       "      <th>results_author_validity: Yes</th>\n",
       "      <th>results_author_validity: No</th>\n",
       "      <th>task_ecology_clean</th>\n",
       "      <th>task_ecology:</th>\n",
       "      <th>task_ecology: Constructed</th>\n",
       "      <th>task_ecology: Partial</th>\n",
       "      <th>task_ecology: Complete</th>\n",
       "      <th>task_ecology: Representative</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4/12/2025 18:03:29</td>\n",
       "      <td>Negar Foroutan</td>\n",
       "      <td>zhangMELAMultilingualEvaluation2024</td>\n",
       "      <td>MELA: Multilingual Evaluation of Linguistic Ac...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper intorduces a multilingual acceptabil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4/13/2025 20:33:00</td>\n",
       "      <td>Negar Foroutan</td>\n",
       "      <td>sunInformalLanguageProcessing2024</td>\n",
       "      <td>Toward Informal Language Processing: Knowledge...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Using movie subtitles, the authors construct a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4/13/2025 20:38:38</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>wangPretrainingLanguageModel2023</td>\n",
       "      <td>ON PRE-TRAINED LANGUAGE MODELS FOR ANTIBODY</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces the AnTibody Understandi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4/14/2025 10:35:13</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>xuPEERComprehensiveMultitask2022</td>\n",
       "      <td>PEER: A Comprehensive and Multi-Task Benchmark...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A benchmark called PEER (a\\ncomprehensive and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4/14/2025 21:02:00</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>hardalovBgGLUEBulgarianGeneral2023</td>\n",
       "      <td>bgGLUE: A Bulgarian General Language Understan...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bgGLUE (Bulgarian General Language\\nUnderstan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>4/15/2025 14:35:04</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>berdicevskisSuperlimSwedishLanguage2023</td>\n",
       "      <td>Superlim: A Swedish Language Understanding Eva...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present Superlim, a multi-task NLP bench- m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4/16/2025 9:21:25</td>\n",
       "      <td>Anna Sotnikova</td>\n",
       "      <td>renBEACONBenchmarkComprehensive2024</td>\n",
       "      <td>BEACON: Benchmark for Comprehensive RNA Tasks ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Topic Exclusion (Is the paper about measuring ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper presents BEACON, the first comprehe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>4/17/2025 8:56:38</td>\n",
       "      <td>Anna Gausen</td>\n",
       "      <td>shenTaskBenchBenchmarkingLarge2024</td>\n",
       "      <td>TaskBench: Benchmarking Large Language Models ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TaskBench is a framework for evaluating how we...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>4/17/2025 17:21:15</td>\n",
       "      <td>Lujain Ibrahim</td>\n",
       "      <td>zhangMultiTrustComprehensiveBenchmark2024</td>\n",
       "      <td>MULTITRUST: A Comprehensive Benchmark Towards ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces a benchmark on the trustw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Partial, Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>4/17/2025 20:25:35</td>\n",
       "      <td>Angelika Romanou</td>\n",
       "      <td>chenCurriculumBroadcoverageBenchmark2022</td>\n",
       "      <td>Curriculum: A Broad-Coverage Benchmark for Lin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Current models do not provide insight into how...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>4/17/2025 21:24:02</td>\n",
       "      <td>Jan Batzner</td>\n",
       "      <td>ushioGenerativeLanguageModels2022</td>\n",
       "      <td>Generative Language Models for Paragraph-Level...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QG-Bench, a comprehensive benchmark for paragr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>4/17/2025 22:44:18</td>\n",
       "      <td>Yilun Zhao</td>\n",
       "      <td>chenMLLMasajudgeAssessingMultimodal2024</td>\n",
       "      <td>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces the MLLM-as-a-Judge benc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>4/18/2025 20:34:24</td>\n",
       "      <td>Jan Batzner</td>\n",
       "      <td>mackoMULTITuDELargescaleMultilingual2023</td>\n",
       "      <td>MULTITuDE: Large-Scale Multilingual Machine-Ge...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MULTITuDE, a benchmark dataset for multilingua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Tests]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>4/18/2025 23:53:19</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>fenogenovaMERAComprehensiveLLM2024</td>\n",
       "      <td>MERA: A Comprehensive LLM Evaluation in Russian</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper proposes a new instruction benchmark...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>4/19/2025 0:52:01</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>sunMeasuringEffectInfluential2023</td>\n",
       "      <td>Measuring the Effect of Influential Messages o...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The authors examine the task of predicting how...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>4/19/2025 20:14:57</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>liuWe`reAfraidLanguage2023</td>\n",
       "      <td>We're Afraid Language Models Aren't Modeling A...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the ability of LMs to hand...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>4/20/2025 17:13:53</td>\n",
       "      <td>Karolina Korgul</td>\n",
       "      <td>wangGTABenchmarkGeneral2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GTA is a benchmark designed to evaluate LLM-ba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>4/20/2025 21:33:41</td>\n",
       "      <td>Lujain Ibrahim</td>\n",
       "      <td>mireshghallahCanLLMsKeep2024</td>\n",
       "      <td>Can LLMs Keep a Secret? Testing Privacy Implic...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The paper introduces a benchmark grounded in t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>4/22/2025 13:40:03</td>\n",
       "      <td>Karolina Korgul</td>\n",
       "      <td>maLargeLanguageModels2024</td>\n",
       "      <td>Large Language Models Play StarCraft II:Benchm...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TextStarCraft II turns the full StarCraft II v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>4/21/2025 6:57:30</td>\n",
       "      <td>Maria Grandury</td>\n",
       "      <td>leiterPrExMeLargeScale2024</td>\n",
       "      <td>PrExMe! Large Scale Prompt Exploration of Open...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces PrExMe, a benchmark of p...</td>\n",
       "      <td>They include emotion-CoT in prompt templates, ...</td>\n",
       "      <td>Specific Application (A single use case, where...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Partial, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>4/22/2025 17:42:02</td>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>zengMRbenMetareasoningBenchmark2024</td>\n",
       "      <td>MR-Ben: A Meta-Reasoning Benchmark for Evaluat...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dataset of question,answer pairs in which answ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Capability (A broadly useful ability, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Complete]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Timestamp        main_coder  \\\n",
       "22   4/12/2025 18:03:29    Negar Foroutan   \n",
       "35   4/13/2025 20:33:00    Negar Foroutan   \n",
       "36   4/13/2025 20:38:38    Anna Sotnikova   \n",
       "62   4/14/2025 10:35:13    Anna Sotnikova   \n",
       "73   4/14/2025 21:02:00    Anna Sotnikova   \n",
       "87   4/15/2025 14:35:04    Anna Sotnikova   \n",
       "129   4/16/2025 9:21:25    Anna Sotnikova   \n",
       "188   4/17/2025 8:56:38       Anna Gausen   \n",
       "209  4/17/2025 17:21:15    Lujain Ibrahim   \n",
       "231  4/17/2025 20:25:35  Angelika Romanou   \n",
       "235  4/17/2025 21:24:02       Jan Batzner   \n",
       "249  4/17/2025 22:44:18        Yilun Zhao   \n",
       "304  4/18/2025 20:34:24       Jan Batzner   \n",
       "317  4/18/2025 23:53:19  Valentin Hoffman   \n",
       "326   4/19/2025 0:52:01  Valentin Hoffman   \n",
       "336  4/19/2025 20:14:57  Valentin Hoffman   \n",
       "345  4/20/2025 17:13:53   Karolina Korgul   \n",
       "348  4/20/2025 21:33:41    Lujain Ibrahim   \n",
       "366  4/22/2025 13:40:03   Karolina Korgul   \n",
       "389   4/21/2025 6:57:30    Maria Grandury   \n",
       "428  4/22/2025 17:42:02       Thom Foster   \n",
       "\n",
       "                                        bibkey  \\\n",
       "22         zhangMELAMultilingualEvaluation2024   \n",
       "35           sunInformalLanguageProcessing2024   \n",
       "36            wangPretrainingLanguageModel2023   \n",
       "62            xuPEERComprehensiveMultitask2022   \n",
       "73          hardalovBgGLUEBulgarianGeneral2023   \n",
       "87     berdicevskisSuperlimSwedishLanguage2023   \n",
       "129        renBEACONBenchmarkComprehensive2024   \n",
       "188         shenTaskBenchBenchmarkingLarge2024   \n",
       "209  zhangMultiTrustComprehensiveBenchmark2024   \n",
       "231   chenCurriculumBroadcoverageBenchmark2022   \n",
       "235          ushioGenerativeLanguageModels2022   \n",
       "249    chenMLLMasajudgeAssessingMultimodal2024   \n",
       "304   mackoMULTITuDELargescaleMultilingual2023   \n",
       "317         fenogenovaMERAComprehensiveLLM2024   \n",
       "326          sunMeasuringEffectInfluential2023   \n",
       "336                 liuWe`reAfraidLanguage2023   \n",
       "345                wangGTABenchmarkGeneral2024   \n",
       "348               mireshghallahCanLLMsKeep2024   \n",
       "366                  maLargeLanguageModels2024   \n",
       "389                 leiterPrExMeLargeScale2024   \n",
       "428        zengMRbenMetareasoningBenchmark2024   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "22   MELA: Multilingual Evaluation of Linguistic Ac...   Include   \n",
       "35   Toward Informal Language Processing: Knowledge...   Include   \n",
       "36         ON PRE-TRAINED LANGUAGE MODELS FOR ANTIBODY   Include   \n",
       "62   PEER: A Comprehensive and Multi-Task Benchmark...   Include   \n",
       "73   bgGLUE: A Bulgarian General Language Understan...   Include   \n",
       "87   Superlim: A Swedish Language Understanding Eva...   Include   \n",
       "129  BEACON: Benchmark for Comprehensive RNA Tasks ...   Include   \n",
       "188  TaskBench: Benchmarking Large Language Models ...   Include   \n",
       "209  MULTITRUST: A Comprehensive Benchmark Towards ...   Include   \n",
       "231  Curriculum: A Broad-Coverage Benchmark for Lin...   Include   \n",
       "235  Generative Language Models for Paragraph-Level...   Include   \n",
       "249  MLLM-as-a-Judge: Assessing Multimodal LLM-as-a...   Include   \n",
       "304  MULTITuDE: Large-Scale Multilingual Machine-Ge...   Include   \n",
       "317    MERA: A Comprehensive LLM Evaluation in Russian   Include   \n",
       "326  Measuring the Effect of Influential Messages o...   Include   \n",
       "336  We're Afraid Language Models Aren't Modeling A...   Include   \n",
       "345                                                NaN   Include   \n",
       "348  Can LLMs Keep a Secret? Testing Privacy Implic...   Include   \n",
       "366  Large Language Models Play StarCraft II:Benchm...   Include   \n",
       "389  PrExMe! Large Scale Prompt Exploration of Open...   Include   \n",
       "428  MR-Ben: A Meta-Reasoning Benchmark for Evaluat...   Include   \n",
       "\n",
       "                                    exclusion_criteria  \\\n",
       "22                                                 NaN   \n",
       "35                                                 NaN   \n",
       "36                                                 NaN   \n",
       "62                                                 NaN   \n",
       "73                                                 NaN   \n",
       "87                                                 NaN   \n",
       "129  Topic Exclusion (Is the paper about measuring ...   \n",
       "188                                                NaN   \n",
       "209                                                NaN   \n",
       "231                                                NaN   \n",
       "235                                                NaN   \n",
       "249                                                NaN   \n",
       "304                                                NaN   \n",
       "317                                                NaN   \n",
       "326                                                NaN   \n",
       "336                                                NaN   \n",
       "345                                                NaN   \n",
       "348                                                NaN   \n",
       "366                                                NaN   \n",
       "389                                                NaN   \n",
       "428                                                NaN   \n",
       "\n",
       "    exclusion_criteria_detail  \\\n",
       "22                        NaN   \n",
       "35                        NaN   \n",
       "36                        NaN   \n",
       "62                        NaN   \n",
       "73                        NaN   \n",
       "87                        NaN   \n",
       "129                       NaN   \n",
       "188                       NaN   \n",
       "209                       NaN   \n",
       "231                       NaN   \n",
       "235                       NaN   \n",
       "249                       NaN   \n",
       "304                       NaN   \n",
       "317                       NaN   \n",
       "326                       NaN   \n",
       "336                       NaN   \n",
       "345                       NaN   \n",
       "348                       NaN   \n",
       "366                       NaN   \n",
       "389                       NaN   \n",
       "428                       NaN   \n",
       "\n",
       "                                         short_summary  \\\n",
       "22   The paper intorduces a multilingual acceptabil...   \n",
       "35   Using movie subtitles, the authors construct a...   \n",
       "36   This paper introduces the AnTibody Understandi...   \n",
       "62   A benchmark called PEER (a\\ncomprehensive and ...   \n",
       "73    bgGLUE (Bulgarian General Language\\nUnderstan...   \n",
       "87   We present Superlim, a multi-task NLP bench- m...   \n",
       "129  This paper presents BEACON, the first comprehe...   \n",
       "188  TaskBench is a framework for evaluating how we...   \n",
       "209  The paper introduces a benchmark on the trustw...   \n",
       "231  Current models do not provide insight into how...   \n",
       "235  QG-Bench, a comprehensive benchmark for paragr...   \n",
       "249  This paper introduces the MLLM-as-a-Judge benc...   \n",
       "304  MULTITuDE, a benchmark dataset for multilingua...   \n",
       "317  The paper proposes a new instruction benchmark...   \n",
       "326  The authors examine the task of predicting how...   \n",
       "336  This paper examines the ability of LMs to hand...   \n",
       "345  GTA is a benchmark designed to evaluate LLM-ba...   \n",
       "348  The paper introduces a benchmark grounded in t...   \n",
       "366  TextStarCraft II turns the full StarCraft II v...   \n",
       "389  This paper introduces PrExMe, a benchmark of p...   \n",
       "428  Dataset of question,answer pairs in which answ...   \n",
       "\n",
       "                                          contribution  \\\n",
       "22                                                 NaN   \n",
       "35                                                 NaN   \n",
       "36                                                 NaN   \n",
       "62                                                 NaN   \n",
       "73                                                 NaN   \n",
       "87                                                 NaN   \n",
       "129                                                NaN   \n",
       "188                                                NaN   \n",
       "209                                                NaN   \n",
       "231                                                NaN   \n",
       "235                                                NaN   \n",
       "249                                                NaN   \n",
       "304                                                NaN   \n",
       "317                                                NaN   \n",
       "326                                                NaN   \n",
       "336                                                NaN   \n",
       "345                                                NaN   \n",
       "348                                                NaN   \n",
       "366                                                NaN   \n",
       "389  They include emotion-CoT in prompt templates, ...   \n",
       "428                                                NaN   \n",
       "\n",
       "                                      phenomenon_short  ...  \\\n",
       "22   General Capability (A broadly useful ability, ...  ...   \n",
       "35   Specific Application (A single use case, where...  ...   \n",
       "36   Specific Application (A single use case, where...  ...   \n",
       "62   Specific Application (A single use case, where...  ...   \n",
       "73   Specific Application (A single use case, where...  ...   \n",
       "87   General Capability (A broadly useful ability, ...  ...   \n",
       "129  Specific Application (A single use case, where...  ...   \n",
       "188  General Capability (A broadly useful ability, ...  ...   \n",
       "209  General Capability (A broadly useful ability, ...  ...   \n",
       "231  General Capability (A broadly useful ability, ...  ...   \n",
       "235  Specific Application (A single use case, where...  ...   \n",
       "249  General Capability (A broadly useful ability, ...  ...   \n",
       "304  Specific Application (A single use case, where...  ...   \n",
       "317  General Capability (A broadly useful ability, ...  ...   \n",
       "326  Specific Application (A single use case, where...  ...   \n",
       "336  General Capability (A broadly useful ability, ...  ...   \n",
       "345  General Capability (A broadly useful ability, ...  ...   \n",
       "348  General Capability (A broadly useful ability, ...  ...   \n",
       "366  General Capability (A broadly useful ability, ...  ...   \n",
       "389  Specific Application (A single use case, where...  ...   \n",
       "428  General Capability (A broadly useful ability, ...  ...   \n",
       "\n",
       "    results_author_validity:  results_author_validity: Yes  \\\n",
       "22                      False                        False   \n",
       "35                      False                        False   \n",
       "36                      False                        False   \n",
       "62                      False                        False   \n",
       "73                      False                        False   \n",
       "87                      False                        False   \n",
       "129                     False                        False   \n",
       "188                     False                         True   \n",
       "209                     False                        False   \n",
       "231                     False                        False   \n",
       "235                     False                         True   \n",
       "249                     False                         True   \n",
       "304                     False                        False   \n",
       "317                     False                        False   \n",
       "326                     False                        False   \n",
       "336                     False                        False   \n",
       "345                     False                         True   \n",
       "348                     False                         True   \n",
       "366                     False                         True   \n",
       "389                     False                         True   \n",
       "428                     False                        False   \n",
       "\n",
       "    results_author_validity: No                      task_ecology_clean  \\\n",
       "22                         True                           [Constructed]   \n",
       "35                         True                           [Constructed]   \n",
       "36                         True                               [Partial]   \n",
       "62                         True                               [Partial]   \n",
       "73                         True  [Partial, Representative, Constructed]   \n",
       "87                         True  [Partial, Representative, Constructed]   \n",
       "129                        True                  [Partial, Constructed]   \n",
       "188                       False               [Partial, Representative]   \n",
       "209                        True               [Partial, Representative]   \n",
       "231                        True                           [Constructed]   \n",
       "235                       False                        [Representative]   \n",
       "249                       False                  [Partial, Constructed]   \n",
       "304                        True                        [Representative]   \n",
       "317                        True           [Representative, Constructed]   \n",
       "326                        True                           [Constructed]   \n",
       "336                        True                           [Constructed]   \n",
       "345                       False           [Representative, Constructed]   \n",
       "348                       False                  [Partial, Constructed]   \n",
       "366                       False                        [Representative]   \n",
       "389                       False                  [Partial, Constructed]   \n",
       "428                        True                              [Complete]   \n",
       "\n",
       "    task_ecology:  task_ecology: Constructed task_ecology: Partial  \\\n",
       "22           False                      True                 False   \n",
       "35           False                      True                 False   \n",
       "36           False                     False                  True   \n",
       "62           False                     False                  True   \n",
       "73           False                      True                  True   \n",
       "87           False                      True                  True   \n",
       "129          False                      True                  True   \n",
       "188          False                     False                  True   \n",
       "209          False                     False                  True   \n",
       "231          False                      True                 False   \n",
       "235          False                     False                 False   \n",
       "249          False                      True                  True   \n",
       "304          False                     False                 False   \n",
       "317          False                      True                 False   \n",
       "326          False                      True                 False   \n",
       "336          False                      True                 False   \n",
       "345          False                      True                 False   \n",
       "348          False                      True                  True   \n",
       "366          False                     False                 False   \n",
       "389          False                      True                  True   \n",
       "428          False                     False                 False   \n",
       "\n",
       "    task_ecology: Complete task_ecology: Representative  \\\n",
       "22                   False                        False   \n",
       "35                   False                        False   \n",
       "36                   False                        False   \n",
       "62                   False                        False   \n",
       "73                   False                         True   \n",
       "87                   False                         True   \n",
       "129                  False                        False   \n",
       "188                  False                         True   \n",
       "209                  False                         True   \n",
       "231                  False                        False   \n",
       "235                  False                         True   \n",
       "249                  False                        False   \n",
       "304                  False                         True   \n",
       "317                  False                         True   \n",
       "326                  False                        False   \n",
       "336                  False                        False   \n",
       "345                  False                         True   \n",
       "348                  False                        False   \n",
       "366                  False                         True   \n",
       "389                  False                        False   \n",
       "428                   True                        False   \n",
       "\n",
       "    metric_statistics_clean  \n",
       "22              [Mean, Std]  \n",
       "35                   [Mean]  \n",
       "36                      NaN  \n",
       "62              [Mean, Std]  \n",
       "73                   [Mean]  \n",
       "87              [Mean, Std]  \n",
       "129             [Mean, Std]  \n",
       "188                  [Mean]  \n",
       "209           [Mean, Other]  \n",
       "231                  [Mean]  \n",
       "235           [Mean, Other]  \n",
       "249                     NaN  \n",
       "304           [Mean, Tests]  \n",
       "317                  [Mean]  \n",
       "326                     NaN  \n",
       "336                  [Mean]  \n",
       "345           [Mean, Other]  \n",
       "348                  [Mean]  \n",
       "366             [Mean, Std]  \n",
       "389                  [Mean]  \n",
       "428                     NaN  \n",
       "\n",
       "[21 rows x 127 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[(included_df['metric_definition: Correlation']) & (included_df['phenomenon_taxonomy_root']!='Language Modelling')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "aad7ed41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root\n",
       "NLP                      94\n",
       "Reasoning                78\n",
       "Agents                   38\n",
       "Alignment                35\n",
       "Language Modelling       34\n",
       "Code Generation          25\n",
       "Medicine                 14\n",
       "Retrieval                13\n",
       "VQA                      11\n",
       "Knowledge                11\n",
       "Law                       9\n",
       "Multilinguality           8\n",
       "Instruction Following     8\n",
       "Grounding                 8\n",
       "Biology                   7\n",
       "User Interaction          6\n",
       "General Science           5\n",
       "Theory of Mind            4\n",
       "LLM as a Judge            3\n",
       "Psychology                3\n",
       "Factuality                3\n",
       "Finance                   3\n",
       "Data Analysis             2\n",
       "Education                 1\n",
       "History                   1\n",
       "Chemistry                 1\n",
       "Business                  1\n",
       "Domain Applications       1\n",
       "Mental Health             1\n",
       "Sports                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['phenomenon_taxonomy_root'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20ac77",
   "metadata": {},
   "source": [
    "### Saving Clean Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a9035707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436, 127)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "05eab529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['bibkey'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "28a96721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp',\n",
       " 'main_coder',\n",
       " 'bibkey',\n",
       " 'title',\n",
       " 'inclusion',\n",
       " 'exclusion_criteria',\n",
       " 'exclusion_criteria_detail',\n",
       " 'short_summary',\n",
       " 'contribution',\n",
       " 'phenomenon_short',\n",
       " 'target_phenomenon',\n",
       " 'phenomenon_defined',\n",
       " 'phenomenon_definition',\n",
       " 'definition_scope',\n",
       " 'purpose_extra',\n",
       " 'task_definition',\n",
       " 'task_item_definition',\n",
       " 'task_definition_detail',\n",
       " 'task_source',\n",
       " 'task_dataset_size',\n",
       " 'task_dataset_metadata',\n",
       " 'dataset_metadata_detail',\n",
       " 'dataset_sampling_method',\n",
       " 'response_format',\n",
       " 'metric_definition',\n",
       " 'metric_definition_detail',\n",
       " 'task_source_detail',\n",
       " 'authorship',\n",
       " 'benchmark_availability',\n",
       " 'procedural_extra',\n",
       " 'notes_extra',\n",
       " 'task_train_val',\n",
       " 'task_dataset_size_extra',\n",
       " 'response_format_detail',\n",
       " 'metric_aggregation',\n",
       " 'metric_subscores',\n",
       " 'metric_subscores_detail',\n",
       " 'metric_metascoring',\n",
       " 'benchmark_location',\n",
       " 'benchmark',\n",
       " 'phenomenon_contested',\n",
       " 'task_face_validity',\n",
       " 'metric_face_validity',\n",
       " 'result_interpretation',\n",
       " 'results_comparison',\n",
       " 'results_comparison_explanation',\n",
       " 'results_realism',\n",
       " 'results_human_baseline',\n",
       " 'results_author_validity',\n",
       " 'results_author_validity_detail',\n",
       " 'metric_statistics',\n",
       " 'metric_access',\n",
       " 'task_ecology',\n",
       " 'task_ecology_detail',\n",
       " 'definition_integrity',\n",
       " 'definition_integrity_detail',\n",
       " 'task_dataset_size_detail',\n",
       " 'metric_fewshot',\n",
       " 'phenomenon_taxonomy_root',\n",
       " 'phenomenon_taxonomy_leaf',\n",
       " 'phenomenon_taxonomy_alternate',\n",
       " 'validate_taxonomy',\n",
       " 'task_source_clean',\n",
       " 'task_source: Author-crafted',\n",
       " 'task_source: Crowd-sourced',\n",
       " 'task_source: Unknown',\n",
       " 'task_source: Procedurally-generated',\n",
       " 'task_source: Expert-crafted',\n",
       " 'task_source: Another benchmark',\n",
       " 'task_source: LLM-generated',\n",
       " 'task_source: Human exams',\n",
       " 'task_source: Real task',\n",
       " 'dataset_sampling_method_clean',\n",
       " 'dataset_sampling_method: Targeted',\n",
       " 'dataset_sampling_method: Criterion',\n",
       " 'dataset_sampling_method: Convenience',\n",
       " 'dataset_sampling_method: Random',\n",
       " 'dataset_sampling_method: Unknown',\n",
       " 'response_format_clean',\n",
       " 'response_format: Structured',\n",
       " 'response_format: Interaction',\n",
       " 'response_format: Multiple choice',\n",
       " 'response_format: Short free response',\n",
       " 'response_format: Free response',\n",
       " 'response_format: Logits',\n",
       " 'response_format: Unknown',\n",
       " 'metric_definition_clean',\n",
       " 'metric_definition: Exact match',\n",
       " 'metric_definition: Human ratings',\n",
       " 'metric_definition: LLM-as-a-Judge',\n",
       " 'metric_definition: LLM post-processing',\n",
       " 'metric_definition: Distribution',\n",
       " 'metric_definition: Correlation',\n",
       " 'metric_definition: Reward',\n",
       " 'metric_definition: Soft match',\n",
       " 'metric_definition: Unknown',\n",
       " 'phenomenon_contested_clean',\n",
       " 'phenomenon_contested: No definition',\n",
       " 'phenomenon_contested: Widely-agreed',\n",
       " 'phenomenon_contested: Contested',\n",
       " 'task_face_validity_clean',\n",
       " 'task_face_validity: Partially',\n",
       " 'task_face_validity: ',\n",
       " 'task_face_validity: Yes',\n",
       " 'task_face_validity: No',\n",
       " 'metric_face_validity_clean',\n",
       " 'metric_face_validity: Partially',\n",
       " 'metric_face_validity: ',\n",
       " 'metric_face_validity: Yes',\n",
       " 'metric_face_validity: No',\n",
       " 'results_realism_clean',\n",
       " 'results_realism: Realistic',\n",
       " 'results_realism: No comparison made',\n",
       " 'results_realism: No',\n",
       " 'results_realism: Comparison made',\n",
       " 'results_realism: Not possible',\n",
       " 'results_author_validity_clean',\n",
       " 'results_author_validity: ',\n",
       " 'results_author_validity: Yes',\n",
       " 'results_author_validity: No',\n",
       " 'task_ecology_clean',\n",
       " 'task_ecology: ',\n",
       " 'task_ecology: Constructed',\n",
       " 'task_ecology: Partial',\n",
       " 'task_ecology: Complete',\n",
       " 'task_ecology: Representative',\n",
       " 'metric_statistics_clean']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(included_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#included_df[['main_coder','bibkey','title','inclusion','short_summary','phenomenon_taxonomy_root', 'phenomenon_taxonomy_leaf', 'phenomenon_taxonomy_alternate','task_source','task_source_clean','dataset_sampling_method','dataset_sampling_method_clean','response_format','response_format_clean','metric_definition','metric_definition_clean','metric_statistics','metric_statistics_clean']].to_csv('../data/results_validaiton.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8008025d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>phenomenon_taxonomy_root</th>\n",
       "      <th>phenomenon_taxonomy_leaf</th>\n",
       "      <th>phenomenon_taxonomy_alternate</th>\n",
       "      <th>task_source</th>\n",
       "      <th>task_source_clean</th>\n",
       "      <th>dataset_sampling_method</th>\n",
       "      <th>dataset_sampling_method_clean</th>\n",
       "      <th>response_format</th>\n",
       "      <th>response_format_clean</th>\n",
       "      <th>metric_definition</th>\n",
       "      <th>metric_definition_clean</th>\n",
       "      <th>metric_statistics</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Mayne</td>\n",
       "      <td>mundlerSWTBenchTestingValidating2024</td>\n",
       "      <td>SWT-Bench: Testing and Validating Real-World B...</td>\n",
       "      <td>Include</td>\n",
       "      <td>A benchmark for generating code tests (unit te...</td>\n",
       "      <td>Agents</td>\n",
       "      <td>Coding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real task examples (e.g. GitHub issues), Modif...</td>\n",
       "      <td>[Real task, Another benchmark]</td>\n",
       "      <td>Specific criteria (items were taken from a lar...</td>\n",
       "      <td>[Criterion]</td>\n",
       "      <td>Structured response (e.g. valid JSON, API call...</td>\n",
       "      <td>[Structured]</td>\n",
       "      <td>Whether the faulty code fails on the test and ...</td>\n",
       "      <td>[Reward]</td>\n",
       "      <td>simple mean</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jonathan Rystrøm</td>\n",
       "      <td>davidsonEvaluatingLanguageModel2024</td>\n",
       "      <td>EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...</td>\n",
       "      <td>Include</td>\n",
       "      <td>The paper introduces a dynamic framework for e...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Author-crafted]</td>\n",
       "      <td>Targeted items (creators defined a task space ...</td>\n",
       "      <td>[Targeted]</td>\n",
       "      <td>Extended interaction (e.g. conversation, calli...</td>\n",
       "      <td>[Interaction]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall),...</td>\n",
       "      <td>[Exact match, Reward]</td>\n",
       "      <td>mean with variance</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lennart Luettgau</td>\n",
       "      <td>helweMAFALDABenchmarkComprehensive2024</td>\n",
       "      <td>MAFALDA: A Benchmark and Comprehensive Study o...</td>\n",
       "      <td>Include</td>\n",
       "      <td>The paper introduces MAFALD, a benchmark that ...</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>Logical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Author-crafted, Crowd-sourced, Another benchm...</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience, Targeted]</td>\n",
       "      <td>Multiple choice</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>simple mean/sum</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaili Liu</td>\n",
       "      <td>niuRAGTruthHallucinationCorpus2024</td>\n",
       "      <td>RAGTruth: A Hallucination Corpus for Developin...</td>\n",
       "      <td>Include</td>\n",
       "      <td>This paper targets word-level hallucinations i...</td>\n",
       "      <td>Retrieval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Factuality</td>\n",
       "      <td>Real task examples (e.g. GitHub issues), Crowd...</td>\n",
       "      <td>[Real task, Crowd-sourced, Another benchmark, ...</td>\n",
       "      <td>Random sample (creators defined a task space a...</td>\n",
       "      <td>[Random, Targeted]</td>\n",
       "      <td>Short free response (e.g. single word or numbe...</td>\n",
       "      <td>[Short free response, Free response, Structured]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anna Gausen</td>\n",
       "      <td>wangIELMOpenInformation2022</td>\n",
       "      <td>IELM: An Open Information Extraction Benchmark...</td>\n",
       "      <td>Include</td>\n",
       "      <td>They introduce a new open information extracti...</td>\n",
       "      <td>NLP</td>\n",
       "      <td>Extraction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crowd-sourced task examples (e.g. Prolific-cre...</td>\n",
       "      <td>[Crowd-sourced, Procedurally-generated]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>Structured response (e.g. valid JSON, API call...</td>\n",
       "      <td>[Structured]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>The authors carry out some error analysis: \"We...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Hannah Kirk</td>\n",
       "      <td>hendrycksAligningAIShared2020</td>\n",
       "      <td>Aligning AI With Shared Human Values</td>\n",
       "      <td>Include</td>\n",
       "      <td>The paper introduces the ETHICS dataset, a ben...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Author-crafted, Crowd-sourced, Procedurally-g...</td>\n",
       "      <td>Targeted items (creators defined a task space ...</td>\n",
       "      <td>[Targeted, Criterion]</td>\n",
       "      <td>Multiple choice, Numeric response (for utilita...</td>\n",
       "      <td>[Multiple choice, Short free response]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>The paper uses simple means for the primary ev...</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Hannah Kirk</td>\n",
       "      <td>panRewardsJustifyMeans2023</td>\n",
       "      <td>Do the Rewards Justify the Means? Measuring Tr...</td>\n",
       "      <td>Include</td>\n",
       "      <td>MACHIAVELLI is a benchmark of 134 Choose-Your-...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LLM-generated task examples (e.g. Filtered fro...</td>\n",
       "      <td>[LLM-generated, Procedurally-generated]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience, Criterion]</td>\n",
       "      <td>Multiple choice</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall),...</td>\n",
       "      <td>[Exact match, LLM post-processing, Reward]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>wangSciBenchEvaluatingCollegelevel2024</td>\n",
       "      <td>SCIBENCH: Evaluating College-Level Scientific ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>SciBench is a dataset of ~1000 college-level s...</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Human exam questions (e.g. GRE questions), Aut...</td>\n",
       "      <td>[Human exams, Author-crafted, Expert-crafted]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience, Targeted]</td>\n",
       "      <td>Short free response (e.g. single word or number)</td>\n",
       "      <td>[Short free response]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall)</td>\n",
       "      <td>[Exact match]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Thom Foster</td>\n",
       "      <td>chenWeakevalstrongEvaluatingEliciting2024</td>\n",
       "      <td>Weak-eval-Strong: Evaluating and Eliciting Lat...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Multi-turn puzzle game in which an agent in gi...</td>\n",
       "      <td>Reasoning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expert-crafted task examples (e.g. hand-writte...</td>\n",
       "      <td>[Expert-crafted]</td>\n",
       "      <td>Convenience sample (creators found a set of ta...</td>\n",
       "      <td>[Convenience]</td>\n",
       "      <td>Extended interaction (e.g. conversation, calli...</td>\n",
       "      <td>[Interaction]</td>\n",
       "      <td>Exact Match (accuracy, F1, precision, recall),...</td>\n",
       "      <td>[Exact match, Reward]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Adel Bibi</td>\n",
       "      <td>chiyah-garciaRepairsBlockWorld2024</td>\n",
       "      <td>Repairs in a Block World: A New Benchmark for ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>Paper proposes a dataset and a benchmark measu...</td>\n",
       "      <td>NLP</td>\n",
       "      <td>Understanding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crowd-sourced task examples (e.g. Prolific-cre...</td>\n",
       "      <td>[Crowd-sourced, Another benchmark, LLM-generat...</td>\n",
       "      <td>Specific criteria (items were taken from a lar...</td>\n",
       "      <td>[Criterion]</td>\n",
       "      <td>Short free response (e.g. single word or number)</td>\n",
       "      <td>[Short free response]</td>\n",
       "      <td>IOU</td>\n",
       "      <td>[Soft match]</td>\n",
       "      <td>mean, std</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           main_coder                                     bibkey  \\\n",
       "0         Harry Mayne       mundlerSWTBenchTestingValidating2024   \n",
       "1    Jonathan Rystrøm        davidsonEvaluatingLanguageModel2024   \n",
       "2    Lennart Luettgau     helweMAFALDABenchmarkComprehensive2024   \n",
       "3           Kaili Liu         niuRAGTruthHallucinationCorpus2024   \n",
       "4         Anna Gausen                wangIELMOpenInformation2022   \n",
       "..                ...                                        ...   \n",
       "431       Hannah Kirk              hendrycksAligningAIShared2020   \n",
       "432       Hannah Kirk                 panRewardsJustifyMeans2023   \n",
       "433       Thom Foster     wangSciBenchEvaluatingCollegelevel2024   \n",
       "434       Thom Foster  chenWeakevalstrongEvaluatingEliciting2024   \n",
       "435         Adel Bibi         chiyah-garciaRepairsBlockWorld2024   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "0    SWT-Bench: Testing and Validating Real-World B...   Include   \n",
       "1    EVALUATING LANGUAGE MODEL AGENCY THROUGH\\nNEGO...   Include   \n",
       "2    MAFALDA: A Benchmark and Comprehensive Study o...   Include   \n",
       "3    RAGTruth: A Hallucination Corpus for Developin...   Include   \n",
       "4    IELM: An Open Information Extraction Benchmark...   Include   \n",
       "..                                                 ...       ...   \n",
       "431               Aligning AI With Shared Human Values   Include   \n",
       "432  Do the Rewards Justify the Means? Measuring Tr...   Include   \n",
       "433  SCIBENCH: Evaluating College-Level Scientific ...   Include   \n",
       "434  Weak-eval-Strong: Evaluating and Eliciting Lat...   Include   \n",
       "435  Repairs in a Block World: A New Benchmark for ...   Include   \n",
       "\n",
       "                                         short_summary  \\\n",
       "0    A benchmark for generating code tests (unit te...   \n",
       "1    The paper introduces a dynamic framework for e...   \n",
       "2    The paper introduces MAFALD, a benchmark that ...   \n",
       "3    This paper targets word-level hallucinations i...   \n",
       "4    They introduce a new open information extracti...   \n",
       "..                                                 ...   \n",
       "431  The paper introduces the ETHICS dataset, a ben...   \n",
       "432  MACHIAVELLI is a benchmark of 134 Choose-Your-...   \n",
       "433  SciBench is a dataset of ~1000 college-level s...   \n",
       "434  Multi-turn puzzle game in which an agent in gi...   \n",
       "435  Paper proposes a dataset and a benchmark measu...   \n",
       "\n",
       "    phenomenon_taxonomy_root phenomenon_taxonomy_leaf  \\\n",
       "0                     Agents                   Coding   \n",
       "1                  Alignment                Alignment   \n",
       "2                  Reasoning                  Logical   \n",
       "3                  Retrieval                      NaN   \n",
       "4                        NLP               Extraction   \n",
       "..                       ...                      ...   \n",
       "431                Alignment                Alignment   \n",
       "432                Alignment                Alignment   \n",
       "433                Reasoning                      NaN   \n",
       "434                Reasoning                      NaN   \n",
       "435                      NLP            Understanding   \n",
       "\n",
       "    phenomenon_taxonomy_alternate  \\\n",
       "0                             NaN   \n",
       "1                             NaN   \n",
       "2                             NaN   \n",
       "3                      Factuality   \n",
       "4                             NaN   \n",
       "..                            ...   \n",
       "431                           NaN   \n",
       "432                           NaN   \n",
       "433                           NaN   \n",
       "434                           NaN   \n",
       "435                           NaN   \n",
       "\n",
       "                                           task_source  \\\n",
       "0    Real task examples (e.g. GitHub issues), Modif...   \n",
       "1    Author-crafted task examples (e.g. hand-writte...   \n",
       "2    Author-crafted task examples (e.g. hand-writte...   \n",
       "3    Real task examples (e.g. GitHub issues), Crowd...   \n",
       "4    Crowd-sourced task examples (e.g. Prolific-cre...   \n",
       "..                                                 ...   \n",
       "431  Author-crafted task examples (e.g. hand-writte...   \n",
       "432  LLM-generated task examples (e.g. Filtered fro...   \n",
       "433  Human exam questions (e.g. GRE questions), Aut...   \n",
       "434  Expert-crafted task examples (e.g. hand-writte...   \n",
       "435  Crowd-sourced task examples (e.g. Prolific-cre...   \n",
       "\n",
       "                                     task_source_clean  \\\n",
       "0                       [Real task, Another benchmark]   \n",
       "1                                     [Author-crafted]   \n",
       "2    [Author-crafted, Crowd-sourced, Another benchm...   \n",
       "3    [Real task, Crowd-sourced, Another benchmark, ...   \n",
       "4              [Crowd-sourced, Procedurally-generated]   \n",
       "..                                                 ...   \n",
       "431  [Author-crafted, Crowd-sourced, Procedurally-g...   \n",
       "432            [LLM-generated, Procedurally-generated]   \n",
       "433      [Human exams, Author-crafted, Expert-crafted]   \n",
       "434                                   [Expert-crafted]   \n",
       "435  [Crowd-sourced, Another benchmark, LLM-generat...   \n",
       "\n",
       "                               dataset_sampling_method  \\\n",
       "0    Specific criteria (items were taken from a lar...   \n",
       "1    Targeted items (creators defined a task space ...   \n",
       "2    Convenience sample (creators found a set of ta...   \n",
       "3    Random sample (creators defined a task space a...   \n",
       "4    Convenience sample (creators found a set of ta...   \n",
       "..                                                 ...   \n",
       "431  Targeted items (creators defined a task space ...   \n",
       "432  Convenience sample (creators found a set of ta...   \n",
       "433  Convenience sample (creators found a set of ta...   \n",
       "434  Convenience sample (creators found a set of ta...   \n",
       "435  Specific criteria (items were taken from a lar...   \n",
       "\n",
       "    dataset_sampling_method_clean  \\\n",
       "0                     [Criterion]   \n",
       "1                      [Targeted]   \n",
       "2         [Convenience, Targeted]   \n",
       "3              [Random, Targeted]   \n",
       "4                   [Convenience]   \n",
       "..                            ...   \n",
       "431         [Targeted, Criterion]   \n",
       "432      [Convenience, Criterion]   \n",
       "433       [Convenience, Targeted]   \n",
       "434                 [Convenience]   \n",
       "435                   [Criterion]   \n",
       "\n",
       "                                       response_format  \\\n",
       "0    Structured response (e.g. valid JSON, API call...   \n",
       "1    Extended interaction (e.g. conversation, calli...   \n",
       "2                                      Multiple choice   \n",
       "3    Short free response (e.g. single word or numbe...   \n",
       "4    Structured response (e.g. valid JSON, API call...   \n",
       "..                                                 ...   \n",
       "431  Multiple choice, Numeric response (for utilita...   \n",
       "432                                    Multiple choice   \n",
       "433   Short free response (e.g. single word or number)   \n",
       "434  Extended interaction (e.g. conversation, calli...   \n",
       "435   Short free response (e.g. single word or number)   \n",
       "\n",
       "                                response_format_clean  \\\n",
       "0                                        [Structured]   \n",
       "1                                       [Interaction]   \n",
       "2                                   [Multiple choice]   \n",
       "3    [Short free response, Free response, Structured]   \n",
       "4                                        [Structured]   \n",
       "..                                                ...   \n",
       "431            [Multiple choice, Short free response]   \n",
       "432                                 [Multiple choice]   \n",
       "433                             [Short free response]   \n",
       "434                                     [Interaction]   \n",
       "435                             [Short free response]   \n",
       "\n",
       "                                     metric_definition  \\\n",
       "0    Whether the faulty code fails on the test and ...   \n",
       "1    Exact Match (accuracy, F1, precision, recall),...   \n",
       "2        Exact Match (accuracy, F1, precision, recall)   \n",
       "3        Exact Match (accuracy, F1, precision, recall)   \n",
       "4        Exact Match (accuracy, F1, precision, recall)   \n",
       "..                                                 ...   \n",
       "431      Exact Match (accuracy, F1, precision, recall)   \n",
       "432  Exact Match (accuracy, F1, precision, recall),...   \n",
       "433      Exact Match (accuracy, F1, precision, recall)   \n",
       "434  Exact Match (accuracy, F1, precision, recall),...   \n",
       "435                                                IOU   \n",
       "\n",
       "                        metric_definition_clean  \\\n",
       "0                                      [Reward]   \n",
       "1                         [Exact match, Reward]   \n",
       "2                                 [Exact match]   \n",
       "3                                 [Exact match]   \n",
       "4                                 [Exact match]   \n",
       "..                                          ...   \n",
       "431                               [Exact match]   \n",
       "432  [Exact match, LLM post-processing, Reward]   \n",
       "433                               [Exact match]   \n",
       "434                       [Exact match, Reward]   \n",
       "435                                [Soft match]   \n",
       "\n",
       "                                     metric_statistics metric_statistics_clean  \n",
       "0                                          simple mean                  [Mean]  \n",
       "1                                   mean with variance             [Mean, Std]  \n",
       "2                                      simple mean/sum                  [Mean]  \n",
       "3                                                  NaN                     NaN  \n",
       "4    The authors carry out some error analysis: \"We...                 [Other]  \n",
       "..                                                 ...                     ...  \n",
       "431  The paper uses simple means for the primary ev...                  [Mean]  \n",
       "432                                                NaN                     NaN  \n",
       "433                                                NaN                     NaN  \n",
       "434                                                NaN                     NaN  \n",
       "435                                          mean, std             [Mean, Std]  \n",
       "\n",
       "[436 rows x 18 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[['main_coder','bibkey','title','inclusion','short_summary','phenomenon_taxonomy_root',\n",
    " 'phenomenon_taxonomy_leaf', 'phenomenon_taxonomy_alternate','task_source','task_source_clean','dataset_sampling_method','dataset_sampling_method_clean','response_format','response_format_clean','metric_definition','metric_definition_clean','metric_statistics','metric_statistics_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc31a0",
   "metadata": {},
   "source": [
    "## Recommendation Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1fc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_percents(df,col):\n",
    "    if col in col_maps.keys():\n",
    "        df[col] = df[col].apply(lambda x: col_maps[col][x] if col_maps[col].get(x) else x)\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    print(df[col].value_counts(dropna=False)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phenomenon_defined\n",
      "Yes    335\n",
      "No      93\n",
      "NaN      8\n",
      "Name: count, dtype: int64\n",
      "phenomenon_defined\n",
      "Yes    0.768349\n",
      "No     0.213303\n",
      "NaN    0.018349\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'phenomenon_defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbad6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phenomenon_contested\n",
      "Contested        210\n",
      "Widely-agreed    199\n",
      "Not defined       27\n",
      "Name: count, dtype: int64\n",
      "phenomenon_contested\n",
      "Contested        0.481651\n",
      "Widely-agreed    0.456422\n",
      "Not defined      0.061927\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'phenomenon_contested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cdad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition_integrity\n",
      "Composite phenomenon               267\n",
      "Single cohesive phenomenon         159\n",
      "Authors' description is unclear      9\n",
      "NaN                                  1\n",
      "Name: count, dtype: int64\n",
      "definition_integrity\n",
      "Composite phenomenon               0.612385\n",
      "Single cohesive phenomenon         0.364679\n",
      "Authors' description is unclear    0.020642\n",
      "NaN                                0.002294\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'definition_integrity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition_scope\n",
      "Subset           242\n",
      "Comprehensive    188\n",
      "NaN                6\n",
      "Name: count, dtype: int64\n",
      "definition_scope\n",
      "Subset           0.555046\n",
      "Comprehensive    0.431193\n",
      "NaN              0.013761\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quick_percents(included_df,'definition_scope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfd36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metric_statistics\n",
       "simple mean                                                                                                                                                          80\n",
       "simple mean/sum                                                                                                                                                      29\n",
       "Simple mean                                                                                                                                                          22\n",
       "Mean                                                                                                                                                                  6\n",
       "Mean,                                                                                                                                                                 4\n",
       "                                                                                                                                                                     ..\n",
       "simple mean and for rating-based evaluations they measure \"hedging rate\"                                                                                              1\n",
       "simple mean to aggregate performance over scenarios and roles                                                                                                         1\n",
       "simple mean/sum, plus comparisons to scores from the base LLMs comprising the multi-modal models (called \"multi-modal gain\" and \"multi-modal leakage\" statistics)     1\n",
       "Mean, worst and best out of 11                                                                                                                                        1\n",
       "mean, std                                                                                                                                                             1\n",
       "Name: count, Length: 206, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['metric_statistics'].apply(lambda x: metrics_agg_map[x] if x in metrics_map.keys() else x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d31d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metric_statistics\n",
       "simple mean                                                                                                                                                          80\n",
       "simple mean/sum                                                                                                                                                      29\n",
       "Simple mean                                                                                                                                                          22\n",
       "Mean                                                                                                                                                                  6\n",
       "Mean,                                                                                                                                                                 4\n",
       "                                                                                                                                                                     ..\n",
       "simple mean and for rating-based evaluations they measure \"hedging rate\"                                                                                              1\n",
       "simple mean to aggregate performance over scenarios and roles                                                                                                         1\n",
       "simple mean/sum, plus comparisons to scores from the base LLMs comprising the multi-modal models (called \"multi-modal gain\" and \"multi-modal leakage\" statistics)     1\n",
       "Mean, worst and best out of 11                                                                                                                                        1\n",
       "mean, std                                                                                                                                                             1\n",
       "Name: count, Length: 206, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['metric_statistics'].apply(lambda x: 'Std' in metrics_agg_map[x] if x in metrics_map.keys() else x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_sampling_method: Criterion\n",
      "0.46788990825688076\n",
      "dataset_sampling_method: Convenience\n",
      "0.3922018348623853\n",
      "dataset_sampling_method: Random\n",
      "0.1559633027522936\n",
      "dataset_sampling_method: Unknown\n",
      "0.04357798165137615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in ['dataset_sampling_method: Criterion',\n",
    "       'dataset_sampling_method: Convenience', 'dataset_sampling_method: Random','dataset_sampling_method: Unknown'\n",
    "]:\n",
    "    print(col)\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1e16c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_definition: Exact match\n",
      "355\n",
      "0.8142201834862385\n",
      "metric_definition: Human ratings\n",
      "56\n",
      "0.12844036697247707\n",
      "metric_definition: LLM-as-a-Judge\n",
      "73\n",
      "0.16743119266055045\n",
      "metric_definition: LLM post-processing\n",
      "41\n",
      "0.09403669724770643\n",
      "metric_definition: Distribution\n",
      "56\n",
      "0.12844036697247707\n",
      "metric_definition: Correlation\n",
      "22\n",
      "0.05045871559633028\n",
      "metric_definition: Reward\n",
      "39\n",
      "0.08944954128440367\n",
      "metric_definition: Soft match\n",
      "90\n",
      "0.20642201834862386\n",
      "metric_definition: Unknown\n",
      "1\n",
      "0.0022935779816513763\n"
     ]
    }
   ],
   "source": [
    "for col in ['metric_definition: Exact match',\n",
    "       'metric_definition: Human ratings', 'metric_definition: LLM-as-a-Judge',\n",
    "       'metric_definition: LLM post-processing',\n",
    "       'metric_definition: Distribution', 'metric_definition: Correlation',\n",
    "       'metric_definition: Reward', 'metric_definition: Soft match',\n",
    "       'metric_definition: Unknown'\n",
    "]:\n",
    "    print(col)\n",
    "    print(included_df[col].sum())\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a4523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_source: Author-crafted\n",
      "0.4288990825688073\n",
      "task_source: Crowd-sourced\n",
      "0.18577981651376146\n",
      "task_source: Unknown\n",
      "0.009174311926605505\n",
      "task_source: Procedurally-generated\n",
      "0.2775229357798165\n",
      "task_source: Expert-crafted\n",
      "0.12614678899082568\n",
      "task_source: Another benchmark\n",
      "0.43577981651376146\n",
      "task_source: LLM-generated\n",
      "0.3165137614678899\n",
      "task_source: Human exams\n",
      "0.0963302752293578\n",
      "task_source: Real task\n",
      "0.2981651376146789\n"
     ]
    }
   ],
   "source": [
    "for col in ['task_source: Author-crafted','task_source: Crowd-sourced','task_source: Unknown','task_source: Procedurally-generated','task_source: Expert-crafted','task_source: Another benchmark','task_source: LLM-generated','task_source: Human exams','task_source: Real task']:\n",
    "    print(col)\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8471a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_format: Structured\n",
      "95\n",
      "0.21788990825688073\n",
      "response_format: Interaction\n",
      "27\n",
      "0.06192660550458716\n",
      "response_format: Multiple choice\n",
      "171\n",
      "0.3922018348623853\n",
      "response_format: Short free response\n",
      "172\n",
      "0.3944954128440367\n",
      "response_format: Free response\n",
      "204\n",
      "0.46788990825688076\n",
      "response_format: Logits\n",
      "6\n",
      "0.013761467889908258\n",
      "response_format: Unknown\n",
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for col in ['response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown',]:\n",
    "    print(col)\n",
    "    print(included_df[col].sum())\n",
    "    print(included_df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd084fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3       True\n",
       "4      False\n",
       "       ...  \n",
       "431     True\n",
       "432    False\n",
       "433     True\n",
       "434    False\n",
       "435     True\n",
       "Length: 436, dtype: bool"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(included_df['response_format: Free response'] | included_df['response_format: Short free response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8f7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5412844036697247"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['results_author_validity: Yes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6d510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9426605504587156"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['task_face_validity: Yes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769e956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9311926605504587"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['metric_face_validity: Yes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root\n",
       "NLP                      94\n",
       "Reasoning                78\n",
       "Agents                   38\n",
       "Alignment                35\n",
       "Language Modelling       34\n",
       "Code Generation          25\n",
       "Medicine                 14\n",
       "Retrieval                13\n",
       "VQA                      11\n",
       "Knowledge                11\n",
       "Law                       9\n",
       "Multilinguality           8\n",
       "Instruction Following     8\n",
       "Grounding                 8\n",
       "Biology                   7\n",
       "User Interaction          6\n",
       "General Science           5\n",
       "Theory of Mind            4\n",
       "LLM as a Judge            3\n",
       "Psychology                3\n",
       "Factuality                3\n",
       "Finance                   3\n",
       "Data Analysis             2\n",
       "Education                 1\n",
       "History                   1\n",
       "Chemistry                 1\n",
       "Business                  1\n",
       "Domain Applications       1\n",
       "Mental Health             1\n",
       "Sports                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['phenomenon_taxonomy_root'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9409d7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>main_coder</th>\n",
       "      <th>bibkey</th>\n",
       "      <th>title</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>exclusion_criteria_detail</th>\n",
       "      <th>short_summary</th>\n",
       "      <th>contribution</th>\n",
       "      <th>phenomenon_short</th>\n",
       "      <th>...</th>\n",
       "      <th>results_author_validity:</th>\n",
       "      <th>results_author_validity: Yes</th>\n",
       "      <th>results_author_validity: No</th>\n",
       "      <th>task_ecology_clean</th>\n",
       "      <th>task_ecology:</th>\n",
       "      <th>task_ecology: Constructed</th>\n",
       "      <th>task_ecology: Partial</th>\n",
       "      <th>task_ecology: Complete</th>\n",
       "      <th>task_ecology: Representative</th>\n",
       "      <th>metric_statistics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>4/20/2025 1:41:56</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>hallVisoGenderDatasetBenchmarking2023</td>\n",
       "      <td>VisoGender: A dataset for benchmarking gender ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines occupation-related gender ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Representative, Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>4/20/2025 2:45:15</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>hanInstinctiveBiasSpurious2024</td>\n",
       "      <td>The Instinctive Bias: Spurious Images lead to ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the extent to which multim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>4/20/2025 8:43:27</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>esiobuROBBIERobustBias2023</td>\n",
       "      <td>ROBBIE: Robust Bias Evaluation of Large Genera...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper attempts to make bias evaluation mo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>4/21/2025 0:11:29</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>felknerWinoQueerCommunityintheloopBenchmark2023</td>\n",
       "      <td>WinoQueer: A Community-in-the-Loop Benchmark f...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines biases in LMs that harm th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>4/21/2025 7:28:55</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>sahooIndiBiasBenchmarkDataset2024</td>\n",
       "      <td>IndiBias: A Benchmark Dataset to Measure Socia...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines LMs' biases and stereotype...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>4/21/2025 9:12:20</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>marchiorimanerbaSocialBiasProbing2024</td>\n",
       "      <td>Social Bias Probing: Fairness Benchmarking for...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines social biases in LMs, focu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>4/21/2025 19:14:13</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>nangiaCrowSpairsChallengeDataset2020</td>\n",
       "      <td>CrowS-Pairs: A Challenge Dataset for Measuring...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines social biases in LMs again...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>4/21/2025 22:52:21</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>morabitoSTOPBenchmarkingLarge2024</td>\n",
       "      <td>STOP! Benchmarking Large Language Models with ...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines social biases in LMs as th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean, Std]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>4/22/2025 0:16:12</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>halevyFlexTapeCan`t2024</td>\n",
       "      <td>\"Flex Tape Can't Fix That\": Bias and Misinform...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the extent to which model ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>4/22/2025 6:37:31</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>chenCrosscareAssessingHealthcare2024</td>\n",
       "      <td>Cross-Care: Assessing the Healthcare Implicati...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines how LMs associate disease ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>4/22/2025 8:42:08</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>wanFactualityTaxDiversityintervened2024</td>\n",
       "      <td>The Factuality Tax of Diversity-Intervened Tex...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper examines the question of whether pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Specific form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Representative]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>4/22/2025 23:06:39</td>\n",
       "      <td>Valentin Hoffman</td>\n",
       "      <td>jhaSeeGULLStereotypeBenchmark2023</td>\n",
       "      <td>SeeGULL: A Stereotype Benchmark with Broad Geo...</td>\n",
       "      <td>Include</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper introduces SeeGULL, a broad-coverag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General form of bias</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Constructed]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Mean]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Timestamp        main_coder  \\\n",
       "340   4/20/2025 1:41:56  Valentin Hoffman   \n",
       "341   4/20/2025 2:45:15  Valentin Hoffman   \n",
       "343   4/20/2025 8:43:27  Valentin Hoffman   \n",
       "362   4/21/2025 0:11:29  Valentin Hoffman   \n",
       "390   4/21/2025 7:28:55  Valentin Hoffman   \n",
       "391   4/21/2025 9:12:20  Valentin Hoffman   \n",
       "406  4/21/2025 19:14:13  Valentin Hoffman   \n",
       "410  4/21/2025 22:52:21  Valentin Hoffman   \n",
       "417   4/22/2025 0:16:12  Valentin Hoffman   \n",
       "426   4/22/2025 6:37:31  Valentin Hoffman   \n",
       "427   4/22/2025 8:42:08  Valentin Hoffman   \n",
       "430  4/22/2025 23:06:39  Valentin Hoffman   \n",
       "\n",
       "                                              bibkey  \\\n",
       "340            hallVisoGenderDatasetBenchmarking2023   \n",
       "341                   hanInstinctiveBiasSpurious2024   \n",
       "343                       esiobuROBBIERobustBias2023   \n",
       "362  felknerWinoQueerCommunityintheloopBenchmark2023   \n",
       "390                sahooIndiBiasBenchmarkDataset2024   \n",
       "391            marchiorimanerbaSocialBiasProbing2024   \n",
       "406             nangiaCrowSpairsChallengeDataset2020   \n",
       "410                morabitoSTOPBenchmarkingLarge2024   \n",
       "417                          halevyFlexTapeCan`t2024   \n",
       "426             chenCrosscareAssessingHealthcare2024   \n",
       "427          wanFactualityTaxDiversityintervened2024   \n",
       "430                jhaSeeGULLStereotypeBenchmark2023   \n",
       "\n",
       "                                                 title inclusion  \\\n",
       "340  VisoGender: A dataset for benchmarking gender ...   Include   \n",
       "341  The Instinctive Bias: Spurious Images lead to ...   Include   \n",
       "343  ROBBIE: Robust Bias Evaluation of Large Genera...   Include   \n",
       "362  WinoQueer: A Community-in-the-Loop Benchmark f...   Include   \n",
       "390  IndiBias: A Benchmark Dataset to Measure Socia...   Include   \n",
       "391  Social Bias Probing: Fairness Benchmarking for...   Include   \n",
       "406  CrowS-Pairs: A Challenge Dataset for Measuring...   Include   \n",
       "410  STOP! Benchmarking Large Language Models with ...   Include   \n",
       "417  \"Flex Tape Can't Fix That\": Bias and Misinform...   Include   \n",
       "426  Cross-Care: Assessing the Healthcare Implicati...   Include   \n",
       "427  The Factuality Tax of Diversity-Intervened Tex...   Include   \n",
       "430  SeeGULL: A Stereotype Benchmark with Broad Geo...   Include   \n",
       "\n",
       "    exclusion_criteria exclusion_criteria_detail  \\\n",
       "340                NaN                       NaN   \n",
       "341                NaN                       NaN   \n",
       "343                NaN                       NaN   \n",
       "362                NaN                       NaN   \n",
       "390                NaN                       NaN   \n",
       "391                NaN                       NaN   \n",
       "406                NaN                       NaN   \n",
       "410                NaN                       NaN   \n",
       "417                NaN                       NaN   \n",
       "426                NaN                       NaN   \n",
       "427                NaN                       NaN   \n",
       "430                NaN                       NaN   \n",
       "\n",
       "                                         short_summary contribution  \\\n",
       "340  This paper examines occupation-related gender ...          NaN   \n",
       "341  This paper examines the extent to which multim...          NaN   \n",
       "343  This paper attempts to make bias evaluation mo...          NaN   \n",
       "362  This paper examines biases in LMs that harm th...          NaN   \n",
       "390  This paper examines LMs' biases and stereotype...          NaN   \n",
       "391  This paper examines social biases in LMs, focu...          NaN   \n",
       "406  This paper examines social biases in LMs again...          NaN   \n",
       "410  This paper examines social biases in LMs as th...          NaN   \n",
       "417  This paper examines the extent to which model ...          NaN   \n",
       "426  This paper examines how LMs associate disease ...          NaN   \n",
       "427  This paper examines the question of whether pr...          NaN   \n",
       "430  This paper introduces SeeGULL, a broad-coverag...          NaN   \n",
       "\n",
       "          phenomenon_short  ... results_author_validity:   \\\n",
       "340  Specific form of bias  ...                     False   \n",
       "341  Specific form of bias  ...                     False   \n",
       "343   General form of bias  ...                     False   \n",
       "362   General form of bias  ...                     False   \n",
       "390   General form of bias  ...                     False   \n",
       "391   General form of bias  ...                     False   \n",
       "406   General form of bias  ...                     False   \n",
       "410   General form of bias  ...                     False   \n",
       "417  Specific form of bias  ...                     False   \n",
       "426  Specific form of bias  ...                     False   \n",
       "427  Specific form of bias  ...                     False   \n",
       "430   General form of bias  ...                     False   \n",
       "\n",
       "    results_author_validity: Yes results_author_validity: No  \\\n",
       "340                        False                        True   \n",
       "341                        False                        True   \n",
       "343                         True                       False   \n",
       "362                         True                       False   \n",
       "390                        False                        True   \n",
       "391                        False                        True   \n",
       "406                         True                       False   \n",
       "410                         True                       False   \n",
       "417                        False                        True   \n",
       "426                        False                        True   \n",
       "427                         True                       False   \n",
       "430                         True                       False   \n",
       "\n",
       "                task_ecology_clean task_ecology:  task_ecology: Constructed  \\\n",
       "340  [Representative, Constructed]          False                      True   \n",
       "341                  [Constructed]          False                      True   \n",
       "343               [Representative]          False                     False   \n",
       "362                  [Constructed]          False                      True   \n",
       "390                  [Constructed]          False                      True   \n",
       "391                  [Constructed]          False                      True   \n",
       "406                  [Constructed]          False                      True   \n",
       "410                  [Constructed]          False                      True   \n",
       "417                  [Constructed]          False                      True   \n",
       "426                  [Constructed]          False                      True   \n",
       "427               [Representative]          False                     False   \n",
       "430                  [Constructed]          False                      True   \n",
       "\n",
       "    task_ecology: Partial task_ecology: Complete task_ecology: Representative  \\\n",
       "340                 False                  False                         True   \n",
       "341                 False                  False                        False   \n",
       "343                 False                  False                         True   \n",
       "362                 False                  False                        False   \n",
       "390                 False                  False                        False   \n",
       "391                 False                  False                        False   \n",
       "406                 False                  False                        False   \n",
       "410                 False                  False                        False   \n",
       "417                 False                  False                        False   \n",
       "426                 False                  False                        False   \n",
       "427                 False                  False                         True   \n",
       "430                 False                  False                        False   \n",
       "\n",
       "    metric_statistics_clean  \n",
       "340                  [Mean]  \n",
       "341                  [Mean]  \n",
       "343                     NaN  \n",
       "362                  [Mean]  \n",
       "390                  [Mean]  \n",
       "391                  [Mean]  \n",
       "406                  [Mean]  \n",
       "410             [Mean, Std]  \n",
       "417                  [Mean]  \n",
       "426                  [Mean]  \n",
       "427                  [Mean]  \n",
       "430                  [Mean]  \n",
       "\n",
       "[12 rows x 127 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df[included_df['phenomenon_taxonomy_leaf'] == 'Bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ba811",
   "metadata": {},
   "source": [
    "### Headline Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2dc40f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_map = {'Agents':'Agents',\n",
    "                'Alignment':'Alignment',\n",
    "                  'Reasoning':'Reasoning',\n",
    "                    'Retrieval':'Other',\n",
    "                      'NLP':'NLP',\n",
    "       'Factuality':'Other',\n",
    "         'Knowledge':'Other',\n",
    "           'Language Modelling':'Language Modelling',\n",
    "             'Law':'Domain Applications',\n",
    "       'Code Generation':'Code Generation',\n",
    "         'Multilinguality':'Other',\n",
    "           'Instruction Following':'Other',\n",
    "       'Finance':'Domain Applications',\n",
    "         'Biology':'Domain Applications',\n",
    "           'General Science':'Domain Applications',\n",
    "             'History':'Domain Applications',\n",
    "       'User Interaction':'Other',\n",
    "         'Mental Health':'Domain Applications',\n",
    "           'Domain Applications':'Domain Applications',\n",
    "       'Psychology':'Domain Applications',\n",
    "         'Business':'Domain Applications',\n",
    "           'Data Analysis':'Other',\n",
    "             'Medicine':'Domain Applications',\n",
    "               'Chemistry':'Domain Applications',\n",
    "       'Grounding':'Other',\n",
    "         'VQA':'Other',\n",
    "           'Education':'Domain Applications',\n",
    "             'Theory of Mind':'Other',\n",
    "       'LLM as a Judge':'Other',\n",
    "         'Sports':'Domain Applications'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1c324efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phenomenon_taxonomy_root\n",
       "NLP                    94\n",
       "Reasoning              78\n",
       "Other                  77\n",
       "Domain Applications    48\n",
       "Agents                 38\n",
       "Alignment              35\n",
       "Language Modelling     34\n",
       "Code Generation        25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_df['phenomenon_taxonomy_root'].map(summary_map).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "85b6d10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Choice\n",
      "171\n",
      "Short Free Response\n",
      "172\n",
      "Free Response\n",
      "204\n",
      "Structured\n",
      "95\n",
      "Interaction\n",
      "27\n",
      "Logits\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "['response_format: Structured',\n",
    "       'response_format: Interaction', 'response_format: Multiple choice',\n",
    "       'response_format: Short free response',\n",
    "       'response_format: Free response', 'response_format: Logits',\n",
    "       'response_format: Unknown',]\n",
    "\n",
    "print('Multiple Choice')\n",
    "print(included_df['response_format: Multiple choice'].sum())\n",
    "print('Short Free Response')\n",
    "print((included_df['response_format: Short free response']).sum())\n",
    "print('Free Response')\n",
    "print((included_df['response_format: Free response']).sum())\n",
    "print('Structured')\n",
    "print(included_df['response_format: Structured'].sum())\n",
    "print('Interaction')\n",
    "print(included_df['response_format: Interaction'].sum())\n",
    "print('Logits')\n",
    "print(included_df['response_format: Logits'].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc9458",
   "metadata": {},
   "source": [
    "### Complete Taxonomy Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "24c69789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{p{.02\\textwidth}p{.08\\textwidth}p{.9\\textwidth}}\n",
      "        \\textbf{Category} && \\textbf{Included Papers} \\\\\n",
      "        \\toprule\n",
      "        \\endhead\n",
      "        \\bottomrule\n",
      "        \\caption{\\textbf{Descriptive Taxonomy of LLM Benchmark Target Phenomena.}}\n",
      "        \\label{tab:phenomena_general}\n",
      "        \\endlastfoot\n",
      "        \\bottomrule\n",
      "        \\endfoot\n",
      "    \\multicolumn{2}{l}{\\textbf{Agents}} & \\\\ \n",
      "& Coding & \\textcite{mundlerSWTBenchTestingValidating2024}, \\textcite{wangMINTEvaluatingLLMs2024}, \\textcite{huangMLAgentBenchEvaluatingLanguage2024}, \\textcite{yangInterCodeStandardizingBenchmarking2023}, \\textcite{trivediAppWorldControllableWorld2024}, \\textcite{guoRedCodeRiskyCode2024}, \\textcite{caoSpider2vHowFar2024}, \\textcite{mathaiKGymPlatformDataset2024} \\\\ \n",
      "& Web & \\textcite{yaoWebShopScalableRealWorld2022}, \\textcite{dengMobilebenchEvaluationBenchmark2024}, \\textcite{shaoNYUCTFBench2024}, \\textcite{luWebLINXRealworldWebsite2024}, \\textcite{kohVisualWebArenaEvaluatingMultimodal2024}, \\textcite{yoranAssistantBenchCanWeb2024}, \\textcite{zhouWebArenaRealisticWeb2024}, \\textcite{drouinWorkArenaHowCapable2024}, \\textcite{boisvertWorkArenaCompositionalPlanning2024} \\\\ \n",
      "& General & \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{jiangXFACTRMultilingualFactual2020}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{pressCiteMECanLanguage2024}, \\textcite{fanR2HBuildingMultimodal2023} \\\\ \n",
      "& Tool Use & \\textcite{yeRoTBenchMultilevelBenchmark2024}, \\textcite{huangMetaToolBenchmarkLarge2024}, \\textcite{zhuangToolQADatasetLLM2023}, \\textcite{zhangToolBeHonestMultilevelHallucination2024}, \\textcite{shenTaskBenchBenchmarkingLarge2024}, \\textcite{xieOSWorldBenchmarkingMultimodal2024}, \\textcite{wangGTABenchmarkGeneral2024}, \\textcite{liAPIbankComprehensiveBenchmark2023}, \\textcite{basuAPIBLENDComprehensiveCorpora2024}, \\textcite{wangAppBenchPlanningMultiple2024} \\\\ \n",
      "& Web Agent & \\textcite{jinShoppingMMLUMassive2024} \\\\ \n",
      "& Other & \\textcite{xuMAgICInvestigationLarge2024}, \\textcite{wuSmartPlayBenchmarkLLMs2024}, \\textcite{zhouHAZARDChallengeEmbodied2024}, \\textcite{mialonGAIABenchmarkGeneral2024}, \\textcite{laineMeMyselfAI2024}, \\textcite{boginSUPEREvaluatingAgents2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Alignment}} & \\\\ \n",
      "& Alignment & \\textcite{davidsonEvaluatingLanguageModel2024}, \\textcite{jiLargeLanguageModels2024}, \\textcite{chaoJailbreakBenchOpenRobustness2024}, \\textcite{linTruthfulQAMeasuringHow2022}, \\textcite{renValueBenchComprehensivelyEvaluating2024}, \\textcite{wangDecodingTrustComprehensiveAssessment2023}, \\textcite{mireshghallahCanLLMsKeep2024}, \\textcite{zhengJudgingLLMasajudgeMTbench2023}, \\textcite{salemiLaMPWhenLarge2024}, \\textcite{marraffiniGreatestGoodBenchmark2024}, \\textcite{huangFlamesBenchmarkingValue2024}, \\textcite{liuAlignBenchBenchmarkingChinese2024}, \\textcite{hendrycksAligningAIShared2020}, \\textcite{panRewardsJustifyMeans2023} \\\\ \n",
      "& Safety & \\textcite{yinSafeWorldGeodiverseSafety2024}, \\textcite{zhangMultiTrustComprehensiveBenchmark2024}, \\textcite{alamCTIBenchBenchmarkEvaluating2024}, \\textcite{jinJailbreakingLargeLanguage2024}, \\textcite{dengCOLDBenchmarkChinese2022}, \\textcite{liWMDPBenchmarkMeasuring2024}, \\textcite{toyerTensorTrustInterpretable2024}, \\textcite{levySafeTextBenchmarkExploring2022}, \\textcite{zhangSafetyBenchEvaluatingSafety2024} \\\\ \n",
      "& Bias & \\textcite{hallVisoGenderDatasetBenchmarking2023}, \\textcite{hanInstinctiveBiasSpurious2024}, \\textcite{esiobuROBBIERobustBias2023}, \\textcite{felknerWinoQueerCommunityintheloopBenchmark2023}, \\textcite{sahooIndiBiasBenchmarkDataset2024}, \\textcite{marchiorimanerbaSocialBiasProbing2024}, \\textcite{nangiaCrowSpairsChallengeDataset2020}, \\textcite{morabitoSTOPBenchmarkingLarge2024}, \\textcite{halevyFlexTapeCan`t2024}, \\textcite{chenCrosscareAssessingHealthcare2024}, \\textcite{wanFactualityTaxDiversityintervened2024}, \\textcite{jhaSeeGULLStereotypeBenchmark2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Reasoning}} & \\\\ \n",
      "& Logical & \\textcite{helweMAFALDABenchmarkComprehensive2024}, \\textcite{sanyalRobustLRDiagnosticBenchmark2022}, \\textcite{beanLINGOLYBenchmarkOlympiadLevel2024}, \\textcite{ghoshEPiCEmployingProverbs2022}, \\textcite{jacoviChainofthoughtStrongIts2024}, \\textcite{huangMetaLogicLogicalReasoning2022}, \\textcite{leeQASAAdvancedQuestion2023}, \\textcite{liWhenLlmsMeet2024}, \\textcite{tianDiagnosingFirstorderLogical2021}, \\textcite{zhouRICAEvaluatingRobust2021}, \\textcite{patelMultiLogiEvalEvaluatingMultistep2024}, \\textcite{dingEasy2HardbenchStandardizedDifficulty2024}, \\textcite{chenTheoremQATheoremdrivenQuestion2023} \\\\ \n",
      "& Reasoning, Knowledge & \\textcite{huangCEvalMultiLevelMultiDiscipline2023} \\\\ \n",
      "& Planning & \\textcite{nasirGameTraversalBenchmarkEvaluatingPlanning2024}, \\textcite{valmeekamPlanBenchExtensibleBenchmark2023}, \\textcite{choiLoTabenchBenchmarkingLanguageoriented2024}, \\textcite{heExploringCapacityPretrained2023}, \\textcite{maAgentBoardAnalyticalEvaluation2024}, \\textcite{lalCaTbenchBenchmarkingLanguage2024}, \\textcite{suActPlan1KBenchmarkingProcedural2024}, \\textcite{maLargeLanguageModels2024}, \\textcite{xieTravelPlannerBenchmarkRealworld2024}, \\textcite{chenLLMArenaAssessingCapabilities2024} \\\\ \n",
      "& Compositional & \\textcite{yuksekgonulWhenWhyVisionlanguage2023}, \\textcite{rayColaBenchmarkCompositional2023}, \\textcite{zhangMultimodalSelfinstructSynthetic2024}, \\textcite{maExaminationCompositionalityLarge2024}, \\textcite{hsiehSugarCrepeFixingHackable2023}, \\textcite{huangConMeRethinkingEvaluation2024}, \\textcite{hareshClevrSkillsCompositionalLanguage2024} \\\\ \n",
      "& Temporal & \\textcite{suLivingMomentCan2024}, \\textcite{tanBenchmarkingImprovingTemporal2023}, \\textcite{chuTimeBenchComprehensiveEvaluation2024}, \\textcite{fierroMuLanStudyFact2024} \\\\ \n",
      "& Mathematical & \\textcite{huSportsMetricsBlendingText2024}, \\textcite{liGSMplusComprehensiveBenchmark2024}, \\textcite{shiLargeLanguageModels2023}, \\textcite{kurticMathadorLMDynamicBenchmark2024}, \\textcite{aroraHaveLLMsAdvanced2023}, \\textcite{liFRoGEvaluatingFuzzy2024}, \\textcite{heOlympiadBenchChallengingBenchmark2024}, \\textcite{shiLanguageModelsAre2023}, \\textcite{xiongTRIGOBenchmarkingFormal2023}, \\textcite{friederMathematicalCapabilitiesChatGPT2023}, \\textcite{mishraNumGLUESuiteFundamental2022} \\\\ \n",
      "& Other & \\textcite{linghuMultimodalSituatedReasoning2024}, \\textcite{romanouCRABAssessingStrength2023}, \\textcite{ribeiroSTREETMULTITASKSTRUCTURED2023}, \\textcite{liMEQABenchmarkMultihop2024}, \\textcite{luLearnExplainMultimodal2022}, \\textcite{fanNPHardEvalDynamicBenchmark2024}, \\textcite{samdarshiConnectingDotsEvaluating2024}, \\textcite{jiangBRAINTEASERLateralThinking2023}, \\textcite{schwettmannFINDFunctionDescription2023}, \\textcite{chenExploringPotentialLarge2024}, \\textcite{huangOlympicArenaBenchmarkingMultidiscipline2024}, \\textcite{kazemiBoardgameQADatasetNatural2023}, \\textcite{bhuiyaSeeminglyPlausibleDistractors2024}, \\textcite{zengMRbenMetareasoningBenchmark2024}, \\textcite{wangSciBenchEvaluatingCollegelevel2024}, \\textcite{chenWeakevalstrongEvaluatingEliciting2024} \\\\ \n",
      "& Mathematics & \\textcite{zhaoFinanceMATHKnowledgeintensiveMath2024}, \\textcite{luMathVistaEvaluatingMathematical2024}, \\textcite{zhangCarefulExaminationLarge2024} \\\\ \n",
      "& Commonsense & \\textcite{hoWikiWhyAnsweringExplaining2023}, \\textcite{bhargavaDiscoSenseCommonsenseReasoning2022}, \\textcite{spragueMuSRTestingLimits2024}, \\textcite{bittonWinoGAViLGamifiedAssociation2022} \\\\ \n",
      "& Spatial & \\textcite{mirzaeeSPARTQATextualQuestion2021}, \\textcite{wangPictureWorthThousand2024}, \\textcite{comsaBenchmarkReasoningSpatial2023}, \\textcite{shiriEmpiricalAnalysisSpatial2024} \\\\ \n",
      "& Multimodal & \\textcite{yingMMTbenchComprehensiveMultimodal2024}, \\textcite{chenM3CoTNovelBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Retrieval}} & \\\\ \n",
      "&& \\textcite{niuRAGTruthHallucinationCorpus2024}, \\textcite{huiUDABenchmarkSuite2024}, \\textcite{kalyanWikiDONewBenchmark2024}, \\textcite{fernandezSyllabusQACourseLogistics2024}, \\textcite{krojerImageRetrievalContextual2022}, \\textcite{wuSTaRKBenchmarkingLLM2024}, \\textcite{yuanUnlockingMarketsMultilingual2024}, \\textcite{yangCRAGComprehensiveRAG2024}, \\textcite{wangJourneyBenchChallengingOnestop2024}, \\textcite{monteiroRepLiQAQuestionansweringDataset2024}, \\textcite{gaoEnablingLargeLanguage2023}, \\textcite{buchmannAttributeAbstainLarge2024}, \\textcite{pratoEpiKevalEvaluationLanguage2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{NLP}} & \\\\ \n",
      "& Extraction & \\textcite{wangIELMOpenInformation2022}, \\textcite{qiPreservingKnowledgeInvariance2023}, \\textcite{wangMAVENARGCompletingPuzzle2024}, \\textcite{liTEGDBComprehensiveDataset2024}, \\textcite{duPAGEDBenchmarkProcedural2024}, \\textcite{wangCanLanguageModels2023}, \\textcite{yanComprehensiveStudyTextattributed2023}, \\textcite{liCanLargeLanguage2024}, \\textcite{zhangDTGBComprehensiveBenchmark2024}, \\textcite{merdjanovskaNoiseBenchBenchmarkingImpact2024}, \\textcite{ushioGenerativeLanguageModels2022}, \\textcite{agrawalLargeLanguageModels2022} \\\\ \n",
      "& Understanding & \\textcite{liMultimodalArXivDataset2024}, \\textcite{senelCoDA21EvaluatingLanguage2022}, \\textcite{pengCOPENProbingConceptual2022}, \\textcite{hardalovBgGLUEBulgarianGeneral2023}, \\textcite{shavrinaRussianSuperGLUERussianLanguage2020}, \\textcite{berdicevskisSuperlimSwedishLanguage2023}, \\textcite{pfisterSuperGLEBerGermanLanguage2024}, \\textcite{zhangHumorAIMassive2024}, \\textcite{guptaBiphoneModelingInter2023}, \\textcite{zhangMIntRec20LargescaleBenchmark2024}, \\textcite{herediaXNLIeuDatasetCrosslingual2024}, \\textcite{maruNibblingHardCore2022}, \\textcite{chenCurriculumBroadcoverageBenchmark2022}, \\textcite{bandarkarBelebeleBenchmarkParallel2024}, \\textcite{zhangMuCGECMultireferenceMultisource2022}, \\textcite{garcia-ferreroThisNotDataset2023}, \\textcite{kumarVisionlanguageModelsUnderstand2024}, \\textcite{flachsGrammaticalErrorCorrection2020}, \\textcite{royBenchCLAMPBenchmarkEvaluating2023}, \\textcite{shivagundeLargerProbesTell2023}, \\textcite{devriesDUMBBenchmarkSmart2023}, \\textcite{sunMeasuringEffectInfluential2023}, \\textcite{sheScoNeBenchmarkingNegation2023}, \\textcite{liuWe`reAfraidLanguage2023}, \\textcite{aggarwalIndicXNLIEvaluatingMultilingual2022}, \\textcite{kotoLargeLanguageModels2023}, \\textcite{doddapaneniLeavingNoIndic2023}, \\textcite{chiyah-garciaRepairsBlockWorld2024} \\\\ \n",
      "& Long Context & \\textcite{maMMLONGBENCHDOCBenchmarkingLongcontext2024}, \\textcite{kuratovBABILongTestingLimits2024}, \\textcite{wangAdaLEvalEvaluatingLongcontext2024}, \\textcite{zhangAnalyzingTemporalComplex2024}, \\textcite{liLooGLECanLongcontext2024}, \\textcite{wangLeaveNoDocument2024}, \\textcite{anLevalInstitutingStandardized2024}, \\textcite{zhangMarathonRaceRealm2024}, \\textcite{zhangBenchExtendingLong2024}, \\textcite{kwanM4LEMultiabilityMultirange2024}, \\textcite{baiLongBenchBilingualMultitask2024}, \\textcite{karpinskaOneThousandOne2024}, \\textcite{wangNeedleMultimodalHaystack2024}, \\textcite{castillo-boladoPromptsDynamicConversational2024}, \\textcite{liuExposingAttentionGlitches2023}, \\textcite{zhangCABComprehensiveAttention2023}, \\textcite{maharanaEvaluatingVeryLongterm2024} \\\\ \n",
      "& Summarization & \\textcite{mahbubUnveilingEssencePoetry2023}, \\textcite{yeGlobeSummChallengingBenchmark2024}, \\textcite{tangTofuEvalEvaluatingHallucinations2024}, \\textcite{subbiahSTORYSUMMEvaluatingFaithfulness2024}, \\textcite{asthanaEvaluatingLLMsTargeted2024}, \\textcite{huangEmbraceDivergenceRicher2024}, \\textcite{amarOpenAspBenchmarkMultidocument2023}, \\textcite{liuRevisitingGoldStandard2023}, \\textcite{cheangCanLMsGeneralize2023}, \\textcite{josephFactPICOFactualityEvaluation2024}, \\textcite{ryanRevisitingNonEnglishText2023}, \\textcite{leiterPrExMeLargeScale2024}, \\textcite{ramprasadAnalyzingLLMBehavior2024} \\\\ \n",
      "& Other & \\textcite{naousReadMeBenchmarkingMultilingual2024}, \\textcite{taktashevaRuBLiMPRussianBenchmark2024}, \\textcite{casolaMultiPICoMultilingualPerspectivist2024}, \\textcite{billahnagoudiJASMINEArabicGPT2023}, \\textcite{liNewsBenchSystematicEvaluation2024}, \\textcite{edmanCUTEMeasuringLLMs2024}, \\textcite{singhIndicGenBenchMultilingualBenchmark2024}, \\textcite{wangM4GTbenchEvaluationBenchmark2024}, \\textcite{ahujaMEGAVERSEBenchmarkingLarge2024}, \\textcite{fenogenovaMERAComprehensiveLLM2024}, \\textcite{liuNLEBench+NorGLMComprehensiveEmpirical2024}, \\textcite{parkOpenKoLLMLeaderboard2024}, \\textcite{sunFevalAsssessingFundamental2024}, \\textcite{diaoDoolittleBenchmarksCorpora2023}, \\textcite{labanSummEditsMeasuringLLM2023} \\\\ \n",
      "& Updating & \\textcite{dengNewTermBenchmarkingRealtime2024}, \\textcite{jangTemporalWikiLifelongBenchmark2022}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{kasaiRealTimeQAWhats2023}, \\textcite{srinivasanCLiMBContinualLearning2022}, \\textcite{yinALCUNALargeLanguage2023} \\\\ \n",
      "& Detection & \\textcite{wuDetectRLBenchmarkingLLMgenerated2024}, \\textcite{chakrabortyCounterTuringTest2023}, \\textcite{tuWaterBenchHolisticEvaluation2024}, \\textcite{mackoMULTITuDELargescaleMultilingual2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Factuality}} & \\\\ \n",
      "&& \\textcite{heTGEAErrorAnnotatedDataset2021}, \\textcite{suTextttConflictBankBenchmarkEvaluating2024}, \\textcite{ohERBenchEntityRelationshipBased2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Knowledge}} & \\\\ \n",
      "& Cultural & \\textcite{myungBLEnDBenchmarkLLMs2024}, \\textcite{romeroCVQACulturallydiverseMultilingual2024}, \\textcite{bhatiaLocalConceptsUniversals2024}, \\textcite{kannenAestheticsCulturalCompetence2024}, \\textcite{yinGeoMLAMAGeodiverseCommonsense2022}, \\textcite{caoWenMindComprehensiveBenchmark2024} \\\\ \n",
      "& General & \\textcite{xieWhodunitBenchEvaluatingLarge2024}, \\textcite{liuAgentBenchEvaluatingLLMs2024}, \\textcite{jiangXFACTRMultilingualFactual2020}, \\textcite{yuKoLACarefullyBenchmarking2024}, \\textcite{pressCiteMECanLanguage2024}, \\textcite{fanR2HBuildingMultimodal2023} \\\\ \n",
      "& Conflicts & \\textcite{houWikiContradictBenchmarkEvaluating2024}, \\textcite{wuClashEvalQuantifyingTugofwar2024} \\\\ \n",
      "& Other & \\textcite{xiangCAREMIChineseBenchmark2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Language Modelling}} & \\\\ \n",
      "& In-context Learning & \\textcite{albalakFETABenchmarkFewSample2022}, \\textcite{xuStresstestingLongcontextLanguage2024}, \\textcite{tanzerBenchmarkLearningTranslate2024}, \\textcite{liQuantifyingAdaptabilityPretrained2022}, \\textcite{asaiBUFFETBenchmarkingLarge2024} \\\\ \n",
      "& Adaptability & \\textcite{itoGeneralizationCapacityNeural2024}, \\textcite{zhouVLUEMultitaskMultidimension2022} \\\\ \n",
      "& Updating & \\textcite{dengNewTermBenchmarkingRealtime2024}, \\textcite{jangTemporalWikiLifelongBenchmark2022}, \\textcite{zhengNEOBENCHEvaluatingRobustness2024}, \\textcite{guptaTempTabQATemporalQuestion2023}, \\textcite{wuStreamBenchBenchmarkingContinuous2024}, \\textcite{liFIREDatasetFeedback2024}, \\textcite{kasaiRealTimeQAWhats2023}, \\textcite{srinivasanCLiMBContinualLearning2022}, \\textcite{yinALCUNALargeLanguage2023} \\\\ \n",
      "& Other & \\textcite{tanDevBenchMultimodalDevelopmental2024}, \\textcite{zhangM3ExamMultilingualMultimodal2023} \\\\ \n",
      "& Unlearning & \\textcite{jinRWKUBenchmarkingRealworld2024} \\\\ \n",
      "& Copyright & \\textcite{chenCopyBenchMeasuringLiteral2024} \\\\ \n",
      "& Calibration & \\textcite{yeBenchmarkingLlmsUncertainty2024} \\\\ \n",
      "& Robustness & \\textcite{dumpalaSUGARCREPEDatasetVisionlanguage2024}, \\textcite{caoWorstPromptPerformance2024}, \\textcite{akhbariSETLEXSEMCHALLENGEUsing2024}, \\textcite{liNaturalBenchEvaluatingVisionlanguage2024}, \\textcite{zhangUnveilingTapestryConsistency2024}, \\textcite{siREADINChineseMultitask2023}, \\textcite{yangCanLargeLanguage2024}, \\textcite{tamkinTaskAmbiguityHumans2023}, \\textcite{zhangCLAMBERBenchmarkIdentifying2024} \\\\ \n",
      "& Hallucination & \\textcite{liHaluEvalLargescaleHallucination2023}, \\textcite{sunHeadtotailHowKnowledgeable2024}, \\textcite{pratoLargeLanguageModels2024}, \\textcite{liangUHGEvalBenchmarkingHallucination2024}, \\textcite{chenFELMBenchmarkingFactuality2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Law}} & \\\\ \n",
      "&& \\textcite{feiLawBenchBenchmarkingLegal2024}, \\textcite{hwangMultitaskBenchmarkKorean2022}, \\textcite{guhaLegalBenchCollaborativelyBuilt2023}, \\textcite{joshiILTURBenchmarkIndian2024}, \\textcite{zuoPatentEvalUnderstandingErrors2024}, \\textcite{liLexEvalComprehensiveChinese2024}, \\textcite{chiPLUELanguageUnderstanding2023}, \\textcite{sanchetiAgentspecificDeonticModality2022}, \\textcite{braunAGBDECorpusAutomated2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Code Generation}} & \\\\ \n",
      "& Natural Language & \\textcite{saparinaAMBROSIABenchmarkParsing2024}, \\textcite{changDrspiderDiagnosticEvaluation2023} \\\\ \n",
      "& Other & \\textcite{tangStrucbenchAreLarge2024}, \\textcite{athiwaratkunMultilingualEvaluationCode2023}, \\textcite{liInfiBenchEvaluatingQuestionanswering2024}, \\textcite{duMercuryCodeEfficiency2024}, \\textcite{zhaoQTSummQueryfocusedSummarization2023}, \\textcite{zhangSelenePioneeringAutomated2024}, \\textcite{maSpreadsheetBenchChallengingReal2024}, \\textcite{konIaCevalCodeGeneration2024}, \\textcite{waghjaleECCOCanWe2024}, \\textcite{huangEffiBenchBenchmarkingEfficiency2024}, \\textcite{tianSciCodeResearchCoding2024}, \\textcite{huangDAcodeAgentData2024}, \\textcite{liEvoCodeBenchEvolvingCode2024}, \\textcite{gongEvaluationLLMsSyntaxaware2024}, \\textcite{yanCodeScopeExecutionbasedMultilingual2024}, \\textcite{liuRepoBenchBenchmarkingRepositorylevel2024}, \\textcite{zhangBenchmarkingDataScience2024}, \\textcite{huInfiAgentDABenchEvaluatingAgents2024}, \\textcite{yinNaturalLanguageCode2023}, \\textcite{khanXCodeEvalExecutionbasedLarge2024}, \\textcite{liCanLLMAlready2023} \\\\ \n",
      "& Natural Lanuage & \\textcite{bhaskarBenchmarkingImprovingTexttoSQL2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Multilinguality}} & \\\\ \n",
      "&& \\textcite{augustyniakThisWayDesigning2022}, \\textcite{zhangMELAMultilingualEvaluation2024}, \\textcite{etxanizLatxaOpenLanguage2024}, \\textcite{riemenschneiderExploringLargeLanguage2023}, \\textcite{marchisioUnderstandingMitigatingLanguage2024}, \\textcite{zhangXSemPLRCrosslingualSemantic2023}, \\textcite{sunInformalLanguageProcessing2024}, \\textcite{songSLINGSinoLinguistic2022} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Instruction Following}} & \\\\ \n",
      "&& \\textcite{xiaFOFOBenchmarkEvaluate2024}, \\textcite{zouVGBenchEvaluatingLarge2024}, \\textcite{jiangFollowBenchMultilevelFinegrained2024}, \\textcite{zengEvaluatingLargeLanguage2024}, \\textcite{bittonVisITbenchDynamicBenchmark2023}, \\textcite{liEvaluatingInstructionfollowingRobustness2024}, \\textcite{wenBenchmarkingComplexInstructionfollowing2024}, \\textcite{abdinKITABEvaluatingLLMs2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{nan}} & \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Finance}} & \\\\ \n",
      "&& \\textcite{shahWhenFLUEMeets2022}, \\textcite{krumdickBizBenchQuantitativeReasoning2024}, \\textcite{zhaoFinDVerExplainableClaim2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Biology}} & \\\\ \n",
      "&& \\textcite{wangPretrainingLanguageModel2023}, \\textcite{xuPEERComprehensiveMultitask2022}, \\textcite{gharaeeBIOSCAN5MMultimodalDataset2024}, \\textcite{renBEACONBenchmarkComprehensive2024}, \\textcite{tsurutaSARSCoV2InteractionDataset2024}, \\textcite{guoCanLlmsSolve2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{General Science}} & \\\\ \n",
      "&& \\textcite{bajpaiCanLLMsReplace2024}, \\textcite{sadatMSciNLIDiverseBenchmark2024}, \\textcite{liangSceMQAScientificCollege2024}, \\textcite{ajithLitSearchRetrievalBenchmark2024}, \\textcite{dinhSciExBenchmarkingLarge2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{History}} & \\\\ \n",
      "&& \\textcite{hauserLargeLanguageModelsExpertlevel2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{User Interaction}} & \\\\ \n",
      "&& \\textcite{kwanMTevalMultiturnCapabilities2024}, \\textcite{panchalWhatSayWhen2024}, \\textcite{liuConvBenchMultiturnConversation2024}, \\textcite{liuMMDUMultiturnMultiimage2024}, \\textcite{chevalierLanguageModelsScience2024}, \\textcite{baiMTbench101FinegrainedBenchmark2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Mental Health}} & \\\\ \n",
      "&& \\textcite{hengleStillNotQuite2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Domain Applications}} & \\\\ \n",
      "&& \\textcite{magnussonPalomaBenchmarkEvaluating2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Psychology}} & \\\\ \n",
      "&& \\textcite{coda-fornoCogBenchLargeLanguage2024}, \\textcite{sunRevealingPersonalityTraits2024}, \\textcite{sabourEmoBenchEvaluatingEmotional2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Business}} & \\\\ \n",
      "&& \\textcite{mitaStrikingGoldAdvertising2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Data Analysis}} & \\\\ \n",
      "&& \\textcite{yangDataTalesBenchmarkRealworld2024}, \\textcite{zhuAreLargeLanguage2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Medicine}} & \\\\ \n",
      "&& \\textcite{heMedEvalMultilevelMultitask2023}, \\textcite{ouyangCliMedBenchLargeScaleChinese2024}, \\textcite{liuRevisitingDeIdentificationElectronic2023}, \\textcite{hanMedSafetyBenchEvaluatingImproving2024}, \\textcite{khandekarMedCalcBenchEvaluatingLarge2024}, \\textcite{liMediQQuestionAskingLLMs2024}, \\textcite{wuMedJourneyBenchmarkEvaluation2024}, \\textcite{zambranochavesRaLEsBenchmarkRadiology2023}, \\textcite{sivasubramaniamSM3TexttoQuerySyntheticMultiModel2024}, \\textcite{liuBenchmarkingLargeLanguage2023}, \\textcite{xiaCARESComprehensiveBenchmark2024}, \\textcite{wangCMBComprehensiveMedical2024}, \\textcite{kweonEHRNoteQALLMBenchmark2024}, \\textcite{liuLargeLanguageModels2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Chemistry}} & \\\\ \n",
      "&& \\textcite{guoWhatCanLarge2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Grounding}} & \\\\ \n",
      "&& \\textcite{piUOUOUncontextualizedUncommon2024}, \\textcite{lyuMMScanMultimodal3D2024}, \\textcite{wangCanLanguageModels2024}, \\textcite{krojerAreDiffusionModels2023}, \\textcite{kesenViLMAZeroshotBenchmark2024}, \\textcite{chungCanVisualLanguage2024}, \\textcite{luoMMMRSMultimodalMultiGSD2024}, \\textcite{liCanLanguageModels2023} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{VQA}} & \\\\ \n",
      "&& \\textcite{chenAreWeRight2024}, \\textcite{yuMMvetEvaluatingLarge2024}, \\textcite{parcalabescuVALSETaskindependentBenchmark2022}, \\textcite{wuEvaluatingAnalyzingRelationship2024}, \\textcite{liVRSBenchVersatileVisionlanguage2024}, \\textcite{dasEXAMSVMultidisciplineMultilingual2024}, \\textcite{gingOpenendedVQABenchmarking2024}, \\textcite{leeVHELMHolisticEvaluation2024}, \\textcite{luoCODISBenchmarkingContextdependent2024}, \\textcite{bitton-guettaVisualRiddlesCommonsense2024}, \\textcite{duEmbSpatialbenchBenchmarkingSpatial2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Education}} & \\\\ \n",
      "&& \\textcite{chenDrAcademyBenchmarkEvaluating2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Theory of Mind}} & \\\\ \n",
      "&& \\textcite{kimFANToMBenchmarkStresstesting2023}, \\textcite{jinMMToMQAMultimodalTheory2024}, \\textcite{gandhiUnderstandingSocialReasoning2023}, \\textcite{chenToMBenchBenchmarkingTheory2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{LLM as a Judge}} & \\\\ \n",
      "&& \\textcite{chenMLLMasajudgeAssessingMultimodal2024}, \\textcite{wattsPARIKSHALargescaleInvestigation2024}, \\textcite{lanCriticEvalEvaluatingLargescale2024} \\\\ \n",
      "\\midrule \n",
      "\\multicolumn{2}{l}{\\textbf{Sports}} & \\\\ \n",
      "&& \\textcite{xiaSportQABenchmarkSports2024} \\\\ \n",
      "\\midrule \n",
      "\n",
      "    \\end{longtable}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "header = \"\"\"\\\\begin{longtable}{p{.02\\\\textwidth}p{.08\\\\textwidth}p{.9\\\\textwidth}}\n",
    "        \\\\textbf{Category} && \\\\textbf{Included Papers} \\\\\\\\\n",
    "        \\\\toprule\n",
    "        \\\\endhead\n",
    "        \\\\bottomrule\n",
    "        \\caption{\\\\textbf{Descriptive Taxonomy of LLM Benchmark Target Phenomena.}}\n",
    "        \\label{tab:phenomena_general}\n",
    "        \\\\endlastfoot\n",
    "        \\\\bottomrule\n",
    "        \\\\endfoot\n",
    "    \"\"\"\n",
    "    \n",
    "footer = \"\"\"\n",
    "    \\end{longtable}\n",
    "\"\"\"\n",
    "\n",
    "latex = header\n",
    "\n",
    "for category in included_df['phenomenon_taxonomy_root'].unique():\n",
    "    subcategories = included_df[included_df['phenomenon_taxonomy_root'] == category]['phenomenon_taxonomy_leaf'].unique()\n",
    "    subcategories = [x for x in subcategories if x != '']\n",
    "    latex += \"\"\"\\\\multicolumn{2}{l}{\\\\textbf{\"\"\"+str(category)+\"\"\"}} & \\\\\\\\ \\n\"\"\"\n",
    "    if len(subcategories) == 1:\n",
    "        papers = included_df[included_df['phenomenon_taxonomy_root'] == category]['bibkey'].unique()\n",
    "        papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "        latex += '&& '+', '.join(papers)+' \\\\\\\\ \\n'\n",
    "    else:\n",
    "        for subcategory in subcategories:\n",
    "            if isinstance(subcategory,str):\n",
    "                papers = included_df[included_df['phenomenon_taxonomy_leaf'] == subcategory]['bibkey'].unique()\n",
    "                papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "                latex += '& '+str(subcategory)+' & '+', '.join(papers)+' \\\\\\\\ \\n'\n",
    "            else:   \n",
    "                other_categories = included_df['phenomenon_taxonomy_leaf'].apply(lambda x: isinstance(x,str)) & (included_df['phenomenon_taxonomy_root'] == category)\n",
    "                papers = included_df[(included_df['phenomenon_taxonomy_root'] == category) & ~other_categories]['bibkey'].unique()\n",
    "                papers = [\"\"\"\\\\textcite{\"\"\"+str(x)+\"}\" for x in papers]\n",
    "                latex += '& Other & '+', '.join(papers)+' \\\\\\\\ \\n'\n",
    "    latex += \"\"\"\\\\midrule \\n\"\"\"\n",
    "\n",
    "latex += footer\n",
    "\n",
    "print(latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
